{"pages":[{"title":"Tags","date":"2021-09-23T09:00:48.414Z","path":"tags/index.html","text":""},{"title":"潜心一技 直到极致","date":"2021-09-02T06:08:22.677Z","path":"about/index.html","text":"道阻 且长不错的学习网站推荐掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io字节跳动后端面试题集知乎 Java快速进阶通道To Be Top Javaer - Java工程师成神之路Div-wangJava 全栈知识体系https://github.com/crossoverJie/JCSprouthttps://github.com/ZhongFuCheng3y/3y ‼️算法图文分析Laravel 学院https://gitee.com/veal98/CS-Wiki 工具网站ProcessOn示说 「 提供了很多优质的PPT 还有很多大厂的沙龙视屏以及材料」 技术团队推荐小米信息部技术团队有赞技术团队美团技术团队 面试题合集2020年大厂Java面试前复习的正确姿势（800+面试题附答案解析）Java集合面试题（总结最全面的面试题）学妹字节、蘑菇街、阿里、莉莉丝…面经 - 敖丙2021大厂面试真题合集 友情链接https://blog.csdn.net/weixin_43314519 数据结构与算法 👇 不错的总结 https://programmercarl.com/ 两年学说话 一生学闭嘴"},{"title":"Categories","date":"2016-01-21T10:46:15.000Z","path":"categories/index.html","text":""}],"posts":[{"title":"MySQL为什么有时会选错索引","date":"2021-12-07T11:48:16.000Z","path":"wiki/MySQL为什么有时会选错索引/","text":"MySQL为什么有时会选错索引前面我们介绍过索引，你已经知道了在MySQL中一张表其实是可以支持多个索引的。但是，你 写SQL语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由MySQL来确 定的。 不知道你有没有碰到过这种情况，一条本来可以执行得很快的语句，却由于MySQL选错了索 引，而导致执行速度变得很慢? 我们一起来看一个例子吧。 我们先建一个简单的表，表里有a、b两个字段，并分别建上索引: 12345678CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`)) ENGINE=InnoDB; 然后，我们往表t中插入10万行记录，取值按整数递增，即:(1,1,1)，(2,2,2)，(3,3,3) 直到 (100000,100000,100000)。 我是用存储过程来插入数据的，这里我贴出来方便你复现: 123456789101112delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=100000)do insert into t values(i, i, i); set i=i+1; end while;end;;delimiter ;call idata(); 接下来，我们分析一条SQL语句: 1mysql&gt; select * from t where a between 10000 and 20000; 你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的。 你说得没错，图1显示的就是使用explain命令看到的这条语句的执行情况。 从图1看上去，这条查询语句的执行也确实符合预期，key这个字段值是’a’，表示优化器选择了索 引a。 不过别急，这个案例不会这么简单。在我们已经准备好的包含了10万行数据的表上，我们再做 如下操作。 这里，session A的操作你已经很熟悉了，它就是开启了一个事务。随后，session B把数据都删 除后，又调用了 idata这个存储过程，插入了10万行数据。 这时候，session B的查询语句select * from t where a between 10000 and 20000就不会再选择索引a了。我们可以通过慢查询日志(slow log)来查看一下具体的执行情况。 为了说明优化器选择的结果是否正确，我增加了一个对照，即:使用force index(a)来让优化器强 制使用索引a(这部分内容，我还会在这篇文章的后半部分中提到)。 下面的三条SQL语句，就是这个实验过程。 123set long_query_time=0;select * from t where a between 10000 and 20000; /*Q1*/select * from t force index(a) where a between 10000 and 20000;/*Q2*/ 第一句，是将慢查询日志的阈值设置为0，表示这个线程接下来的语句都会被记录入慢查询日 志中; 第二句，Q1是session B原来的查询; 第三句，Q2是加了force index(a)来和session B原来的查询语句执行情况对比。 如图3所示是这三条SQL语句执行完成后的慢查询日志。 可以看到，Q1扫描了10万行，显然是走了全表扫描，执行时间是40毫秒。Q2扫描了10001行， 执行了21毫秒。也就是说，我们在没有使用force index的时候，MySQL用错了索引，导致了更长的执行时间。 这个例子对应的是我们平常不断地删除历史数据和新增数据的场景。这时，MySQL竟然会选错索引，是不是有点奇怪呢?今天，我们就从这个奇怪的结果说起吧。 优化器的逻辑在第一篇文章中，我们就提到过，选择索引是优化器的工作。 而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越 少，消耗的CPU资源越少。 当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。 我们这个简单的查询语句并没有涉及到临时表和排序，所以MySQL选错索引肯定是在判断扫描行数的时候出问题了。 那么，问题就是:扫描行数是怎么判断的? MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。 这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。 而一个索引上不同的值的个数，我们称之为“基数”(cardinality)。也就是说，这个基数越 大，索引的区分度越好。 我们可以使用show index方法，看到一个索引的基数。 如图4所示，就是表t的show index 的结果 。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不 同，而且其实都不准确。 那么，MySQL是怎样得到索引的基数的呢?这里，我给你简单介绍一下MySQL采样统计的方法。 为什么要采样统计呢? 因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太 高了，所以只能选择“采样统计”。 采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过1/M的 时候，会自动触发重新做一次索引统计。 在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选择: 设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。 设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16。 由于是采样统计，所以不管N是20还是8，这个基数都是很容易不准的。 但，这还不是全部。 你可以从图4中看到，这次的索引统计值(cardinality列)虽然不够精确，但大体上还是差不多 的，选错索引一定还有别的原因。 其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要 扫描多少行。 接下来，我们再一起看看优化器预估的，这两个语句的扫描行数是多少。 rows这个字段表示的是预计扫描行数。 其中，Q1的结果还是符合预期的，rows的值是104620;但是Q2的rows值是37116，偏差就大了。而图1中我们用explain命令看到的rows是只有10001行，是这个偏差误导了优化器的判断。 到这里，可能你的第一个疑问不是为什么不准，而是优化器为什么放着扫描37000行的执行计划不用，却选择了扫描行数是100000的执行计划呢? 这是因为，如果使用索引a，每次从索引a上拿到一个值，都要回到主键索引上查出整行数据， 这个代价优化器也要算进去的。 而如果选择扫描10万行，是直接在主键索引上扫描的，没有额外的代价。 优化器会估算这两个选择的代价，从结果看来，优化器认为直接扫描主键索引更快。当然，从执行时间看来，这个选择并不是最优的。 使用普通索引需要把回表的代价算进去 在图1执行explain的时候，也考虑了这个策略的代价 ，但图1的选择是对的。也就是说，这个策略并没有问题。 所以冤有头债有主，MySQL选错索引，这件事儿 还得归咎到没能准确地判断出扫描行数。 既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。 我们来看 一下执行效果。 所以在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个 方法来处理。 其实，如果只是索引统计不准确，通过analyze命令可以解决很多问题，但是前面我们说了，优 化器可不止是看扫描行数。 依然是基于这个表t，我们看看另外一个语句: 1mysql&gt; select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1 从条件上看，这个查询没有符合条件的记录，因此会返回空集合。 在开始执行这条语句之前，你可以先设想一下，如果你来选择索引，会选择哪一个呢? 为了便于分析，我们先来看一下a、b这两个索引的结构图。 如果使用索引a进行查询，那么就是扫描索引a的前1000个值，然后取到对应的id，再到主键索 引上去查出每一行，然后根据字段b来过滤。显然这样需要扫描1000行。 如果使用索引b进行查询，那么就是扫描索引b的最后50001个值，与上面的执行过程相同，也是 需要回到主键索引上取值再判断，所以需要扫描50001行。 所以你一定会想，如果使用索引a的话，执行速度明显会快很多。那么，下面我们就来看看到底 是不是这么一回事儿。 可以看到，返回结果中key字段显示，这次优化器选择了索引b，而rows字段显示需要扫描的行 数是50198。 从这个结果中，你可以得到两个结论: 扫描行数的估计值依然不准确; 这个例子里MySQL又选错了索引 索引选择异常和处理其实大多数时候优化器都能找到正确的索引，但偶尔你还是会碰到我们上面举例的这两种情况: 原本可以执行得很快的SQL语句，执行速度却比你预期的慢很多，你应该怎么办呢? 一种方法是，像我们第一个例子一样，采用force index强行选择一个索引。MySQL会根据 词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需 要扫描多少行。如果force index指定的索引在候选索引列表中，就直接选择这个索引，不再评估 其他索引的执行代价。 我们来看看第二个例子。刚开始分析时，我们认为选择索引a会更好。现在，我们就来看看执行 效果: 可以看到，原本语句需要执行2.23秒，而当你使用force index(a)的时候，只用了0.05秒，比优化 器的选择快了40多倍。 也就是说，优化器没有选择正确的索引，force index起到了“矫正”的作用。 不过很多程序员不喜欢使用force index，一来这么写不优美，二来如果索引改了名字，这个语句也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容。 但其实使用force index最主要的问题还是变更的及时性。因为选错索引的情况还是比较少出现 的，所以开发的时候通常不会先写上force index。而是等到线上出现问题的时候，你才会再去修 改SQL语句、加上force index。但是修改之后还要测试和发布，对于生产系统来说，这个过程不够敏捷。 所以，数据库的问题最好还是在数据库内部来解决。那么，在数据库里面该怎样解决呢? 既然优化器放弃了使用索引a，说明a还不够合适，所以第二种方法就是，我们可以考虑修改 语句，引导MySQL使用我们期望的索引。比如，在这个例子里，显然把“orderbyblimit1”改 成 “order by b,a limit 1” ，语义的逻辑是相同的。 我们来看看改之后的效果: 之前优化器选择使用索引b，是因为它认为使用索引b可以避免排序(b本身是索引，已经是有序 的了，如果选择索引b的话，不需要再做排序，只需要遍历)，所以即使扫描行数多，也判定为 代价更小。 现在order by b,a 这种写法，要求按照b,a排序，就意味着使用这两个索引都需要排序。因此，扫 描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描1000行的索引a。 当然，这种修改并不是通用的优化手段，只是刚好在这个语句里面有limit 1，因此如果有满足条 件的记录， order by b limit 1和order by b,a limit 1 都会返回b是最小的那一行，逻辑上一致，才 可以这么做。 如果你觉得修改语义这件事儿不太好，这里还有一种改法，图11是执行效果。 在这个例子里，我们用limit 100让优化器意识到，使用b索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。 第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选 择，或删掉误用的索引。 不过，在这个例子中，我没有找到通过新增索引来改变优化器行为的方法。这种情况其实比较 少，尤其是经过DBA索引优化过的库，再碰到这个bug，找到一个更合适的索引一般比较难。 如果我说还有一个方法是删掉索引b，你可能会觉得好笑。但实际上我碰到过两次这样的例子， 最终是DBA跟业务开发沟通后，发现这个优化器错误选择的索引其实根本没有必要存在，于是 就删掉了这个索引，优化器也就重新选择到了正确的索引。 问题前面我们在构造第一个例子的过程中，通过session A的配合， 让session B删除数据后又重新插入了一遍数据，然后就发现explain结果中，rows字段从10001 变成37000多。 而如果没有session A的配合，只是单独执行delete from t 、call idata()、explain这三句话，会看 到rows字段其实还是10000左右。你可以自己验证一下这个结果。 这是为什么呢？ 为什么经过这个操作序列，explain的结果就不对了?这 里，我来为你分析一下原因。 delete 语句删掉了所有的数据，然后再通过call idata()插入了10万行数据，看上去是覆盖了原来 的10万行。 但是，session A开启了事务并没有提交，所以之前插入的10万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是delete之前的数据，新版本是标记为deleted的数 据。 这样，索引a上的数据其实就有两份。 然后你会说，不对啊，主键上的数据也不能删，那没有使用force index的语句，使用explain命令 看到的扫描行数为什么还是100000左右?(潜台词，如果这个也翻倍，也许优化器还会认为选 字段a作为索引更合适) 是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show table status的值。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"字符串字段如何创建索引","date":"2021-12-07T11:46:33.000Z","path":"wiki/字符串字段如何创建索引/","text":"怎么给字符串字段添加索引现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是我们今天 要讨论的问题。 假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的: 12345mysql&gt; create table SUser(ID bigint unsigned primary key,email varchar(64),...)engine=innodb; 由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句: 1mysql&gt; select f1, f2 from SUser where email=&#x27;xxx&#x27;; 从第4和第5篇讲解索引的文章中，我们可以知道，如果email这个字段上没有索引，那么这个语 句就只能做全表扫描。 同时，MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。 比如，这两个在email字段上创建索引的语句: 123mysql&gt; alter table SUser add index index1(email);或mysql&gt; alter table SUser add index index2(email(6)); 第一个语句创建的index1索引里面，包含了每个记录的整个字符串;而第二个语句创建的index2 索引里面，对于每个记录都是只取前6个字节。 那么，这两种不同的定义在数据结构和存储上有什么区别呢?如图2和3所示，就是这两个索引 的示意图。 索引2 👇 从图中你可以看到，由于email(6)这个索引结构中每个邮箱字段都只取前6个字节(即:zhangs)，所以占用的空间会更小，这就是使用前缀索引的优势。 但，这同时带来的损失是，可能会增加额外的记录扫描次数。 接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。 1select id,name,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;; 如果使用的是index1(即email整个字符串的索引结构)，执行顺序是这样的: 从index1索引树找到满足索引值是’&#122;&#104;&#x61;&#x6e;&#103;&#115;&#115;&#120;&#121;&#x7a;&#64;&#120;&#x78;&#x78;&#x2e;&#x63;&#111;&#x6d;’的这条记录，取得ID2的值; 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集; 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足 email=‘&#122;&#104;&#97;&#x6e;&#x67;&#115;&#x73;&#120;&#121;&#x7a;&#x40;&#120;&#x78;&#120;&#x2e;&#99;&#x6f;&#109;’的条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。 如果使用的是index2(即email(6)索引结构)，执行顺序是这样的: 从index2索引树找到满足索引值是’zhangs’的记录，找到的第一个是ID1; 到主键上查到主键值是ID1的行，判断出email的值不是’&#x7a;&#x68;&#97;&#110;&#103;&#115;&#x73;&#120;&#x79;&#x7a;&#x40;&#120;&#120;&#x78;&#46;&#99;&#111;&#109;’，这行记录丢 弃; 取index2上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取 整行然后判断，这次值对了，将这行记录加入结果集; 重复上一步，直到在idxe2上取到的值不是’zhangs’时，循环结束。 在这个过程中，要回主键索引取4次数据，也就是扫描了4行。 通过这个对比，你很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变多。 但是，对于这个查询语句来说，如果你定义的index2不是email(6)而是email(7)，也就是说取 email字段的前7个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到 ID2，只扫描一行就结束了。 也就是说使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查 询成本。 如何定义前缀索引的长度呢于是，你就有个问题:当要给字符串创建前缀索引时，有什么方法能够确定我应该使用多长的前缀呢? 实际上，我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的 键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。 首先，你可以使用下面这个语句，算出这个列上有多少个不同的值: 1mysql&gt; select count(distinct email) as L from SUser; 然后，依次选取不同长度的前缀来看这个值，比如我们要看一下4~7个字节的前缀索引，可以用 这个语句: 123mysql&gt; selectcount(distinct left(email,4))as L4, count(distinct left(email,5))as L5, count(distinct left(email,6))as L6, count(distinct left(email,7))as L7,from SUser; 当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的L4~L7中，找出不小于 L * 95%的值，假设这里L6、L7都满足，你就可以 选择前缀长度为6。 前缀索引对覆盖索引的影响前面我们说了使用前缀索引可能会增加扫描行数，这会影响到性能。其实，前缀索引的影响不止 如此，我们再看一下另外一个场景。 你先来看看这个SQL语句: 1select id,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;; 与前面例子中的SQL语句 1select id,name,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;; 相比，这个语句只要求返回id和email字段。 所以，如果使用index1(即email整个字符串的索引结构)的话，可以利用覆盖索引，从index1查 到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2(即email(6)索引结 构)的话，就不得不回到ID索引再去判断email字段的值。 即使你将index2的定义修改为email(18)的前缀索引，这时候虽然index2已经包含了所有的信息， 但InnoDB还是要回到id索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。 也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀 索引时需要考虑的一个因素。 其他方式对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢? 比如，我们国家的身份证号，一共18位，其中前6位是地址码，所以同一个县的人的身份证号前 6位一般会是相同的。 假设你维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为6的前缀索引的话，这个索引的区分度就非常低了。 按照我们前面说的方法，可能你需要创建长度为12以上的前缀索引，才能够满足区分度要求。 但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的 效率也就会越低。 那么，如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，还有没有别的处理 方法呢?这种方法，既可以占用更小的空间，也能达到相同的查询效率。 答案是，有的。 第一种方式是使用倒序存储如果你存储身份证号的时候把它倒过来存，每次查询的时候，你 可以这么写: 1mysql&gt; select field_list from t where id_card = reverse(&#x27;input_id_card_string&#x27;); 由于身份证号的最后6位没有地址码这样的重复逻辑，所以最后这6位很可能就提供了足够的区 分度。当然了，实践中你不要忘记使用count(distinct)方法去做个验证。 第二种方式是使用hash字段你可以在表上再创建一个整数字段，来保存身份证的校验码， 同时在这个字段上创建索引。 1mysql&gt; alter table t add id_card_crc int unsigned, add index(id_card_crc); 然后每次插入新记录的时候，都同时用crc32()这个函数得到校验码填到这个新字段。由于校验码 可能存在冲突，也就是说两个不同的身份证号通过crc32()函数得到的结果可能是相同的，所以你 的查询语句where部分要判断id_card的值是否精确相同。 这样，索引的长度变成了4个字节，比原来小了很多。 接下来，我们再一起看看使用倒序存储和使用hash字段这两种方法的异同点。 首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的 方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样 地，hash字段的方式也只能支持等值查询。 它们的区别，主要体现在以下三个方面: 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字 段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如 果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash 字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的 话，reverse函数额外消耗的CPU资源会更小些。 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽 然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储 方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。 总结在今天这篇文章中，我跟你聊了聊字符串字段创建索引的场景。我们来回顾一下，你可以使用的 方式有: 直接创建完整索引，这样可能比较占用空间; 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引; 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题; 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。 在实际应用中，你要根据业务字段的特点选择使用哪种方式。 问题好了，又到了最后的问题时间。 如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号@gmail.com”, 而学号的规则是:十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第 十位是入学年份、最后五位是顺序编号。 系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢? 由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面6位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定的，邮箱后缀都是@gamil.com，因此可以只存入学年份加顺序编号，它们的长度是9位。 而其实在此基础上，可以用数字类型来存这9位数字。比如201100001，这样只需要占4个字节。其实这个就是一种hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好我们设定的这个背景，可以保证这个转换后结果的唯一性。","tags":[],"categories":[]},{"title":"innodb是如何处理脏页的","date":"2021-12-07T11:45:11.000Z","path":"wiki/innodb是如何处理脏页的/","text":"我的mysql为什么会抖一下 innodb引擎是如何处理 脏页 的 平时的工作中，不知道你有没有遇到过这样的场景，一条SQL语句，正常执行的时候特别快，但 是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。 你的SQL语句为什么变“慢”了在前面《一条SQL更新语句是如何执行的?》中，介绍了WAL机制。现在你知道了，InnoDB在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志 叫作redo log(重做日志)，也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写 完redo log后，就返回给客户端，本次更新成功。 做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件(redo log)，掌柜的记忆就是内存。 掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是 flush。在这个flush操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一 致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。 内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。 接下来，我们用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账10 文，这次又要赊9文。 回到文章开头的问题，你不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页(flush)。 那么，什么情况会引发数据库的flush过程呢?我们还是继续用咸亨酒店掌柜的这个例子，想一想:掌柜在什么情况下会把粉板上的赊账记录改 到账本上? 第一种场景粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活 儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确 的账目记录到账本中才行。 这个场景，对应的就是InnoDB的redo log写满了。这时候系统会停止所有更新操作，把 checkpoint往前推进，redo log留出空间可以继续写。我在第二讲画了一个redo log的示意 图，这里我改成环形，便于大家理解。 checkpoint可不是随便往前修改一下位置就可以的。比如图2中 👆，把checkpoint位置从CP推进到CP’，就需要将两个点之间的日志(浅绿色部分)，对应的所有脏页都flush到磁盘上。之后，图 中从write pos到CP’之间就是可以再写入的redo log的区域。 第二种场景这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出 账本把孔乙己这笔账先加进去。 这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。 你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿redo log出来应用不就行了? 这里其实是从性能考虑的。 如果刷脏页一定会写盘， 就保证了每个数据页有两种状态: 一种是内存里存在，内存里就肯定是正确的结果，直接返回; 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。 这样的效率最高。 第三种场景生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不 如更新账本。 这种场景，对应的就是MySQL认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起 来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也 要见缝插针地找时间，只要有机会就刷一点“脏页”。 第四种场景年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都 记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。 这种场景，对应的就是MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁 盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 接下来，你可以分析一下上面四种场景对性能的影响。其中，第三种情况是属于MySQL空闲时的操作，这时系统没什么压力，而第四种场景是数据库 本来就要关闭了。这两种情况下，你不会太关注“性能”问题。所以这里，我们主要来分析一下前 两种场景下的性能问题。 第一种是“redo log写满了，要flush脏页”，这种情况是InnoDB要尽量避免的。因为出现这种情况 的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更 新数会跌为0。 第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。 InnoDB用缓冲池 (buffer pool)管理内存，缓冲池中的内存页有三种状态: 第一种是，还没有使用的; 第二种是，使用了并且是干净页; 第三种是，使用了并且是脏页。 InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。 而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。 这时候只能把最久不使用的数据页从内存中淘汰掉: 如果要淘汰的是一个干净页，就直接释放出来复用; 但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。 所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的: 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长; 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。 所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。 InnoDB刷脏页的控制策略接下来，我就来和你说说InnoDB脏页的控制策略，以及和这些策略相关的参数。 首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时 候，可以刷多快。 这就要用到 innodb_io_capacity 这个参数了，它会告诉InnoDB你的磁盘能力。这个值我建议你设置成磁盘的IOPS。 磁盘的IOPS可以通过fio这个工具来测试，下面的语句是我用来测试磁盘随机 读写的命令: 1fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 其实，因为没能正确地设置innodb_io_capacity参数，而导致的性能问题也比比皆是。之前，就曾有其他公司的开发负责人找我看一个库的性能问题，说MySQL的写入速度很慢，TPS很低， 但是数据库主机的IO压力并不大。经过一番排查，发现罪魁祸首就是这个参数的设置出了问题。 他的主机磁盘用的是SSD，但是innodb_io_capacity的值设置的是300。于是，InnoDB认为这个 系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能。 虽然我们现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧? 毕竟磁盘能力不能只用来刷脏页，还需要服务用户请求。所以接下来，我们就一起看看InnoDB怎么控制引擎按 照“全力”的百分比来刷脏页。 根据我前面提到的知识点，试想一下，如果你来设计策略控制刷脏页的速度，会参考哪些因素呢?这个问题可以这么想，如果刷太慢，会出现什么情况? 首先是内存脏页太多，其次是redo log写满。 所以，InnoDB的刷盘速度就是要参考这两个因素:一个是脏页比例，一个是redo log写盘速度。 InnoDB会根据这两个因素先单独算出两个数字。 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是75%。InnoDB会根据当前的脏页比例(假设为M)，算出一个范围在0到100之间的数字，计算这个数字的伪代码类似这样: 12345F1(M) &#123; if M&gt;=innodb_max_dirty_pages_pct then return 100; return 100*M/innodb_max_dirty_pages_pct;&#125; InnoDB每次写入的日志都有一个序号，当前写入的序号跟checkpoint对应的序号之间的差值， 我们假设为N。InnoDB会根据这个N算出一个范围在0到100之间的数字，这个计算公式可以记为 F2(N)。F2(N)算法比较复杂，你只要知道N越大，算出来的值越大就好了。 然后，根据上述算得的F1(M)和F2(N)两个值，取其中较大的值记为R，之后引擎就可以按照 innodb_io_capacity 定义的能力**乘以R%**来控制刷脏页的速度。 上述的计算流程比较抽象，不容易理解，所以我画了一个简单的流程图。图中的F1、F2就是上 面我们通过脏页比例和redo log写入速度算出来的两个值。 现在你知道了，InnoDB会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。 所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用IO资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到MySQL“抖”了一下的原因。 要尽量避免这种情况，你就要合理地设置innodb_io_capacity的值，并且平时要多关注脏页比例，不要让它经常接近75%。 其中，脏页比例是通过Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total得到 的，具体的命令参考下面的代码: 123mysql&gt; select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = &#x27;Innodb_buffer_pool_pages_dirty&#x27;;select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = &#x27;Innodb_buffer_pool_pages_total&#x27;;select @a/@b; 接下来，我们再看一个有趣的策略。 一旦一个查询请求需要在执行过程中先flush掉一个脏页时，这个查询就可能要比平时慢了。而MySQL中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在InnoDB中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为1的时候会有上述的“连坐”机制，值为0时表示不找邻居，自己刷自己的。 找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机IO。机械硬盘的随机IOPS一般只有几百，相同的逻辑操作减少随机IO就意味着系统性能的大幅度提升。 而如果使用的是SSD这类IOPS比较高的设备的话，我就建议你把innodb_flush_neighbors的值设置成0。因为这时候IOPS往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少SQL语句响应时间。 在MySQL 8.0中，innodb_flush_neighbors参数的默认值已经是0了。 小结今天这篇文章，我延续第2篇中介绍的WAL的概念，和你解释了这个机制后续需要的刷脏页操作和执行时机。利用WAL技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。 但是，由此也带来了内存脏页的问题。脏页会被后台线程自动flush，也会由于数据页淘汰而触发flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。在文章里，我也给你介绍了控制刷脏页的方法和对应的监控方式。 问题一个内存配置为128GB、innodb_io_capacity设置为20000的大规格实例，正常会建议你将redo log设置成4个1GB的文件。 但如果你在配置的时候不慎将redo log设置成了1个100M的文件，会发生什么情况呢？又为什么会出现这样的情况呢？","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"一条更新语句是如何执行的","date":"2021-12-06T10:47:01.000Z","path":"wiki/一条更新语句是如何执行的/","text":"一条更新语句是如何执行的前面我们系统了解了一个查询语句的执行流程，并介绍了执行过程中涉及的处理模块。相信你还 记得，一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后 到达存储引擎。 那么，一条更新语句的执行流程又是怎样的呢? 之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态，惊叹的同时，你 是不是心中也会不免会好奇，这是怎样做到的呢? 我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个 整型字段c: 1mysql&gt; create table T(ID int primary key, c int); 如果要将ID=2这一行的值加1，SQL语句就会这么写: 1mysql&gt; update T set c=c+1 where ID=2; 前面我有跟你介绍过SQL语句基本的执行链路，这里我再把那张图拿过来，你也可以先简单看看 这个图回顾下。首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。 你执行语句前要先连接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会 把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。 然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主 角:redo log(重做日志)和 binlog(归档日志)。如果接触MySQL，那这两个词肯定是绕不过 的，我后面的内容里也会不断地和你强调。不过话说回来，redo log和binlog在设计上有很多有 意思的地方，这些设计思路也可以用到你自己的程序里。 重要的日志模块:redo log不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记 录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会 有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。 如果有人要赊账或者还账的话，掌柜一般有两种做法: 一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉; 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到 这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老 花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。 这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的 帮助，每次记账都得翻账本，效率是不是低得让人难以忍受? 同样，在MySQL里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到 对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。为了解决这个问 题，MySQL的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是Write- Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log(粉板)里 面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作 记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎 么办呢?这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把 这些记录从粉板上擦掉，为记新账腾出空间。 与此类似，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环 写，如下面这个图所示。 write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。 checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文 件。 write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos 追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint推进一下。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或 写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板 上的数据明确赊账账目。 重要的日志模块:binlog前面我们讲过，MySQL整体来看，其实就有两块:一块是Server层，它主要做的是MySQL功能 层面的事情;还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是 InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog(归档日志)。 我想你肯定会问，为什么会有两份日志呢? 因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有 crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL 的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统— — 也就是redo log来实现crash-safe能力。 这两种日志有以下三点不同。 redolog是InnoDB引擎特有的; binlog是MySQL的Server层实现的，所有引擎都可以使用。 redolog是物理日志，记录的是“在某个数据页上做了什么修改”; binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redolog是循环写的，空间固定会用完;binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语 句时的内部流程。 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一 行所在的数据页本来就在内存中，就直接返回给执行器;否则，需要先从磁盘读入内存，然 后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行 数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redolog里面，此时redolog处 于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redolog改成提交(commit)状态，更新完成。 这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。 你可能注意到了，最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤:prepare和commit，这就是”两阶段提交”。 两阶段提交为什么必须有“两阶段提交”呢?这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得 从文章开头的那个问题说起:怎样让数据库恢复到半个月内任意一秒的状态? 前面我们说过了，binlog会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的DBA承 诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期 做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数 据，那你可以这么做: 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备 份恢复到临时库; 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法 来进行解释。 由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语 句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢? 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异 常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回 来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份 日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这 个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以 后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日 志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀? 其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用binlog来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 小结今天，我介绍了MySQL里面最重要的两个日志，即物理日志redo log和逻辑日志binlog。 redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候， 表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证 MySQL异常重启之后数据不丢失。 sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建 议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 我还跟你介绍了与MySQL日志系统密切相关的“两阶段提交”。两阶段提交是跨系统维持数据逻辑 一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。 文章的最后，我给你留一个思考题吧。前面我说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢?或者说，它影响了这个数据库系统的哪个指标? 问题回答一天一备好处是“最长恢复时间”更短。 在一天一备的模式里，最坏情况下需要应用一天的binlog。比如，你每天0点做一次全量备份， 而要恢复出一个到昨天晚上23点的备份。 一周一备最坏情况就要应用一周的binlog了。系统的对应指标就是RTO(恢复目标时间)。 当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间，所以这个RTO是成本换来 的，就需要你根据业务重要性来评估了。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"事务隔离：为什么你改了我还看不见","date":"2021-12-06T10:45:45.000Z","path":"wiki/事务隔离/","text":"事务隔离：为什么你改了我还看不见提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转 账，你要给朋友小王转100块钱，而此时你的银行卡只有100块钱。 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必 须保证是一体的，不然等程序查完之后，还没做减法之前，你这100块钱，完全可以借着这个时 间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么?这时就要用到“事 务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事 务支持是在引擎层实现的。你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引 擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代 的重要原因之一。 今天的文章里，我将会以InnoDB为例，剖析MySQL在事务支持方面的特定实现，并基于原理给 出相应的实践建议，希望这些案例能加深你对MySQL事务原理的理解。 隔离性与隔离级别提到事务，你肯定会想到ACID(Atomicity、Consistency、Isolation、Durability，即原子性、一 致性、隔离性、持久性)，今天我们就来说说其中I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读(dirty read)、不可重复读(non-repeatable read)、幻读(phantom read)的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要 在二者之间寻找一个平衡点。SQL标准的事务隔离级别包括:读未提交(read uncommitted)、 读提交(read committed)、可重复读(repeatable read)和串行化(serializable )。下面我逐 一为你解释: 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一 致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突 的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表T中 只有一列，其中一行的值为1，下面是按照时间顺序执行两个事务的行为。 123mysql&gt; create table T(c int) engine=InnoDB;insert into T(c) values(1); 我们来看看在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3 的返回值分别是什么。 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被 A看到了。因此，V2、V3也都是2。 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求: 事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后， 事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离 级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级 别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离 级别下直接返回记录上的最新值，没有视图概念;而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其 实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致， 你一定要记得将MySQL的隔离级别设置为“读提交”。 配置的方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED。你可以用show variables来查看当前的值。 123456mysql&gt; show variables like &#x27;transaction_isolation&#x27;;+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。 我想你可能会问那什么时候需要“可重复读”的场景呢?我们来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时 候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。 你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务 更新的影响。 事务隔离的实现理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复 读”。 在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通 过回滚操作，都可以得到前一个状态的值。 假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看 到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多 个版本，就是数据库的多版本并发控制(MVCC)。对于read-view A，要得到1，就必须将当前 值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将4改成5，这个事务跟read-view A、B、C对应的 事务是不会冲突的。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢? 答案是，在不需要的时候才删除。 也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢?就是当系统里没有比这个回滚日志更早的read-view的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数 据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占 用存储空间。 在MySQL 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终 提交，回滚段被清理，文件也不会变小。我见过数据只有20GB，而回滚段有200GB的库。最终 只好为了清理回滚段，重建整个库。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候 展开。 事务的启动方式如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并 不是有意使用长事务，通常是由于误用所致。MySQL的事务启动方式有以下几种: 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个set autocommit=0的命令。这就导致接下来的 查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，我会建议你总是使用set autocommit=1, 通过显式语句的方式来启动事务。 但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式 每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾 虑，我建议你使用commit work and chain语法。 在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语 句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查 找持续时间超过60s的事务。 select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 问题我给你留一个问题吧。你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也 是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢? 这个问题，我们可以从应用开发端和数据库端来看。 首先，从应用开发端来看: 确认是否使用了set autocommit=0。这个确认工作可以在测试环境中开展，把MySQL的 general_log开起来，然后随便跑一个业务逻辑，通过general_log的日志来确认。一般框架 如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用begin/commit框起来。我 见过有些是业务并没有这个需要，但是也把好几个select语句放到了事务中。这种只读事务 可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过SETMAX_EXECUTION_TIME命令， 来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。(为什么会意外?在后 续的文章中会提到这类案例) 其次，从数据库端来看: 监控 information_schema.Innodb_trx表，设置长事务阈值，超过就报警/或者kill; Percona的pt-kill这个工具不错，推荐使用; 在业务功能测试阶段要求输出所有的general_log，分析日志行为提前发现问题; 如果使用的是MySQL5.6或者更新版本，把innodb_undo_tablespaces设置成2(或更大的 值)。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"全局锁和表锁：给表加个字段怎么这么多阻碍","date":"2021-12-06T10:43:52.000Z","path":"wiki/全局锁和表锁/","text":"全局锁和表锁：给表加个字段怎么这么多阻碍今天我要跟你聊聊MySQL的锁。数据库锁设计的初衷是处理并发问题。作为多用户共享的资 源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。 根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类。今天这篇文章，我会和你分享全局锁和表级锁。而关于行锁的内容，我会留着在下一篇文章中再和你详细介 绍。 这里需要说明的是，锁的设计比较复杂，这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。 全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞:数据更新语句(数据的增删改)、数据定义语句(包括 建表、修改表结构等)和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。 以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。 注意，在备份过程中整个库完全处于只读状态。 但是让整库都只读，听上去就很危险: 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆; 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 看来加全局锁不太好。但是细想一下，备份为什么要加锁呢? 我们来看一下不加锁会有什么问题。 假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。 现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉 他的余额，然后往已购课程里面加上一门课。 如果时间顺序上是先备份账户余额表(u_account)，然后用户购买，然后备份用户课程表 (u_course)，会怎么样呢?你可以看一下这个图: 可以看到，这个备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了 一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了。 作为用户可别觉得这样可真好啊，你可以试想一下:如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果? 也就是说，不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致 的。 说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视 图的，对吧? 是的，就是在可重复读隔离级别下开启一个事务。 官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导 数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是 可以正常更新的。 你一定在疑惑，有了这个功能，为什么还需要FTWRL呢?一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是 只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。 所以，single-transaction方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用 InnoDB替代MyISAM的原因之一。 你也许会问，既然要全库只读，为什么不使用 set global readonly=true的方式呢? 确实 readonly方式也可以让全库进入只读状态，但我还是会建议你用FTWRL方式，主要有两个原因: 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备 库。因此，修改global变量的方式影响面更大，我不建议你使用。 在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么 MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个 库长时间处于不可写状态，风险较高。 业务的更新不只是增删改数据(DML)，还有可能是加字段等修改表结构的操作(DDL)。不论 是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。 但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍 的表级锁。 表级锁MySQL里面表级别的锁有两种:一种是表锁，一种是元数据锁(meta data lock，MDL)。 表锁的语法是 lock tables ...read/write。与FTWRL类似，可以用unlock tables主动释放锁， 也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写 外，也限定了本线程接下来的操作对象。 举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读 写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操 作。连写t1都不允许，自然也不能访问其他表。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持 行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。 metadata lock另一类表级的锁是MDL(metadata lock)。MDL不需要显式使用，在访问一个表的时候会被自动加上。 MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个 表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果 跟表结构对不上，肯定是不行的。 因此，在MySQL 5.5版本中引入了MDL，**当对一个表做增删改查操作的时候，加MDL读锁;当 要对表做结构变更操作的时候，加MDL写锁。** 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线 程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 虽然MDL锁是系统默认会加的，但却是你不能忽略的一个机制。 比如下面这个例子，我经常看到有人掉到这个坑里:给一个小表加个字段，导致整个库挂了。 你肯定知道，给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操 作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也 会出问题。我们来看一下下面的操作序列，假设表t是一个小表。 ⚠️备注:这里的实验环境是MySQL 5.6。 我们可以看到session A先启动，这时候会对表t加一个MDL读锁。由于session B需要的也是 MDL读锁，因此可以正常执行。 之后session C会被blocked，是因为session A的MDL读锁还没有释放，而session C需要MDL写 锁，因此只能被阻塞。 如果只有session C自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也 会被session C阻塞。前面我们说了，所有对表的增删改查操作都需要先申请MDL读锁，就都被 锁住，等于这个表现在完全不可读写了。 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session 再请求的话，这个库的线程很快就会爆满。 你现在应该知道了，事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释 放，而会等到整个事务提交后再释放。 基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段? 首先我们要解决长事务，事务不提交，就会一直占着MDL锁。 在MySQL的information_schema 库的 innodb_trx表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务 在执行，要考虑先暂停DDL，或者kill掉这个长事务。 但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频 繁，而你不得不加个字段，你该怎么做呢? 这时候kill可能未必管用，因为新的请求马上就来了。比较理想的机制是，在alter table语句里面 设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后 面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。 MariaDB已经合并了AliSQL的这个功能，所以这两个开源分支目前都支持DDL NOWAIT/WAIT n 这个语法。 12ALTER TABLE tbl_name NOWAIT add column ...ALTER TABLE tbl_name WAIT N add column ... 问题备份一般都会在备库上执行，你在用–single-transaction方法做逻 辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从 备库上会看到什么现象呢? 参考答案https://blog.csdn.net/qq_26502245/article/details/111688120","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"一条sql是如何执行的","date":"2021-12-06T10:35:29.000Z","path":"wiki/一条sql是如何执行的/","text":"一条sql到底是怎么执行的？这个问题等同于： 请将一下mysql的基础架构 MySQL的逻辑架构我们经常说，看一个事儿千万不 要直接陷入细节里，你应该先鸟瞰其全貌，这样能够帮助你从高维度理解问题。同样，对于 MySQL的学习也是这样。平时我们使用数据库，看到的通常都是一个整体。比如，你有个最简 单的表，表里只有一个ID字段，在执行下面这个查询语句时: 1mysql&gt; select * from T where ID=10; 我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在MySQL内部的执行过程。 所以今天我想和你一起把MySQL拆解一下，看看里面都有哪些“零件”，希望借由这个拆解过程， 让你对MySQL有更深入的理解。这样当我们碰到MySQL的一些异常或者问题时，就能够直戳本 质，更为快速地定位并解决问题。 下面我给出的是MySQL的基本架构示意图，从中你可以清楚地看到SQL语句在MySQL的各个功 能模块中的执行过程。 大体来说，MySQL可以分为Server层和存储引擎层两部分。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务 功能，以及所有的内置函数(如日期、时间、数学和加密函数等)，所有跨存储引擎的功能都在 这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、 Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了 默认存储引擎。 也就是说，你执行create table建表的时候，如果不指定引擎类型，默认使用的就是InnoDB。不 过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在create table语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功 能也不同，在后面的文章中，我们会讨论到引擎的选择。 从图中不难看出，不同的存储引擎共用一个Server层 ，也就是从连接器到执行器的部分。你可 以先对每个组件的名字有个印象，接下来我会结合开头提到的那条SQL语句，带你走一遍整个执 行流程，依次看下每个组件的作用。 连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连 接、获取权限、维持和管理连接。连接命令一般是这么写的: 1mysql -h$ip -P$port -u$user -p 输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在-p后面写在命令行 中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。 连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器 就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。 如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序 结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面 的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在show processlist命 令中看到它。文本中这个图是show processlist的结果，其中的Command列显示为“Sleep”的这 一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制 的，默认值是8小时。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒: Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接 则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为 MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉(OOM)，从现 象看就是MySQL异常重启了。 怎么解决这个问题呢?你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开 连接，之后要查询再重连。 如果你用的是MySQL5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证， 但是会将连接恢复到刚刚创建完时的状态。 查询缓存连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步**:查询缓存。** MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过 的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是 查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客 户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存 中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结 果，这个效率会很高。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢? 因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此 很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库 来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。 比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成 DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语 句，可以用SQL_CACHE显式指定，像下面这个语句一样: 1mysql&gt; select SQL_CACHE * from T where ID=10; 需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有 这个功能了。 分析器如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识 别出里面的字符串分别是什么，代表什么。 MySQL从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别 成“表名T”，把字符串“ID”识别成“列ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则， 判断你输入的这个SQL语句是否满足MySQL语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个 语句select少打了开头的字母“s”。 一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 优化器经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引;或者在一个语句有多表关联(join)的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的join: 1mysql&gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是 否等于20。 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否 等于10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些 疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，在后面的文章中单独展开说明优化器的内容。 执行器MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶 段，开始执行语句。 开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有 权限的错误，如下所示。 123mysql&gt; select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user &#x27;b&#x27;@&#x27;localhost&#x27; for table &#x27;T&#x27; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的: 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中; 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。 第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。 这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined并不是完全相同的。我们后面会专门有一篇文章来讲存储引擎的内部机制， 里面会有详细的说明。 问题我给你留一个问题吧，如果表T中没有字段k，而你执行了这个语句 select * from T where k=1, 那 肯定是会报“不存在这个列”的错误: “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是 在我们上面提到的哪个阶段报出来的呢?","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"Elasticsearch使用规范","date":"2021-11-01T06:45:39.000Z","path":"wiki/Elasticsearch使用规范/","text":"Elasticsearch使用规范查询规范建议 定义好mappings和settings，不同的数据类型查询效率不一样，建议只需做精确查询以及范围查询的字段设置为keyword类型。对于要进行全文检索的字段设置合理的分词器。 对于只需要查询数据结果而不需要结果的相关度计算的情况，使用filter query能大幅提升你的查询效率。例如过滤某车牌和号码。 避免一次性取出大量的数量：Elasticsearch被设计为一个搜索引擎，这使得它非常擅长获取与查询匹配的最优文档，但是不适合用来检索与特定查询匹配的所有文档。 Elasticsearch为了避免深分页，不允许使用分页（from&amp;size）查询10000条以后的数据，如果需要这样做，请确保使用Scroll API。 （Scroll API 后来不被推荐使用 可以使用search after） 尽量细化查询条件，查询的条件越细，查询效率越高。 选择合适的查询类型，比如term查询效率相对会高一些。 优化路由 Elasticsearch写入文档时，文档会通过一个公式路由到一个索引中的一个分片上。默认公式如下： shard_num = hash(_routing) % num_primary_shards _routing字段的取值，默认是_id字段，可以根据业务场景设置经常查询的字段作为路由字段。例如可以考虑将用户id、地区作为路由字段，查询时可以过滤不必要的分片，加快查询速度。 避免使用wildcard模糊匹配查询 Elasticsearch默认支持通过*？正则表达式来做模糊匹配，数据量级别达到TB+甚至更高之后，模糊匹配查询通常会耗时比较长，甚至可能导致内存溢出，卡死乃至崩溃宕机的情况。所以数据量大的情况下，不要使用模糊匹配查询。 合理的配置使用index属性，analyzed和not_analyzed，根据业务需求来控制字段是否分词或不分词。只有groupby需求的字段，配置时就设置成not_analyzed,以提高查询或聚类的效率。 query_string或multi_match的查询字段越多，查询越慢。 可以在mapping阶段，利用copy_to属性将多字段的值索引到一个新字段，multi_match时，用新的字段查询。 日期字段的查询 尤其是用now的查询实际上是不存在缓存的，因此， 可以从业务的角度来考虑是否一定要用now,毕竟利用query cache是能够大大提高查询效率的。 查询结果集的大小不能随意设置成大得离谱的值 如query.setSize不能设置成Integer.MAX_VALUE，因为ES内部需要建立一个数据结构来放指定大小的结果集数据。 尽量避免使用script，万不得已需要使用的话，选择painless &amp; experssions引擎。 一旦使用script查询，一定要注意控制返回，千万不要有死循环，因为ES没有脚本运行的超时控制，只要当前的脚本没执行完，该查询会一直阻塞。 容量规划 分片(shard)容量 非日志型(搜索型、线上业务型)的shard容量在10~30GB（建议在10G） 日志型的shard容量在30~100GB（建议30G） 单个shard的文档个数不能超过21亿左右(Integer.MAX_VALUE - 128)注：一个shard就是一个lucene分片，ES底层基于lucene实现。主分片个数一旦确定，就不可以更改。副本分片个数可以根据需要随时修改。 索引(index)数量大索引需要拆分：增强性能，风险分散。反例：一个10T的索引，例如按date查询、name查询正例：index_name拆成多个index_name_${date}正例：index_name按hash拆分index_name_{1,2,3,…100..}提示：索引和shard数并不是越多越好，对于批量读写都会有性能下降，所以要综合考虑性能和容量规划，同时配合压力测试，不存在真正的最优解。 节点、分片、索引一个节点管理的shard数不要超过200个 配置使用规范 shard个数（number_of_shards） primery shard ：默认数量是1 replica shard数量为1： 是每个primary shard 有多少个副本分片的意思 primery shard = 1 ; replica shard = 2 ; 意味着一个索引，一共存在9个shard refresh频率（refresh_interval） ES的定位是准实时搜索引擎，该值默认是1s，表示写入后1秒后可被搜索到，所以这里的值取决于业务对实时性的要求，注意这里并不是越小越好，刷新频率高也意味着对ES的开销也大，通常业务类型在1-5s，日志型在30s-120s，如果集中导入数据可将其设置为-1，ES会自动完成数据刷新（注意完成后更改回来，否则后续会出现搜索不到数据） 使用别名（aliases）：不要过度依赖别名功能 慢日志（slowlog） 设置合理的routing key(默认是id) id不均衡：集群容量和访问不均衡，对于分布式存储是致命的 关闭_all ES6.0已经去掉，对容量（索引过大）和性能（性能下降）都有影响。 避免大宽表 ES默认最大1000，但建议不要超过100. text类型的字段不要使用聚合查询。 text类型fileddata会加大对内存的占用，如果有需求使用，建议使用keyword 聚合查询避免使用过多嵌套 聚合查询的中间结果和最终结果都会在内存中进行，嵌套过多，会导致内存耗尽 修改index_buffer_size的设置 可以设置成百分数，也可设置成具体的大小，大小可根据集群的规模做不同的设置测试。 indices.memory.index_buffer_size：10%（默认） indices.memory.min_index_buffer_size： 48mb（默认） indices.memory.max_index_buffer_size 修改translog相关的设置 控制数据从内存到硬盘的操作频率，以减少硬盘IO。可将sync_interval的时间设置大一些。 index.translog.sync_interval：5s(默认) 控制tranlog数据块的大小，达到threshold大小时，才会flush到lucene索引文件。 index.translog.flush_threshold_size：512mb(默认) _id字段的使用 应尽可能避免自定义_id,以避免针对ID的版本管理；建议使用ES的默认ID生成策略或使用数字类型ID做为主键 Cache的设置及使用 QueryCache: ES查询的时候，使用filter查询会使用query cache,如果业务场景中的过滤查询比较多，建议将querycache设置大一些，以提高查询速度。indices.queries.cache.size： 10%（默认），可设置成百分比，也可设置成具体值，如256mb。 当然也可以禁用查询缓存（默认是开启）, 通过index.queries.cache.enabled：false设置。 FieldDataCache:在聚类或排序时，field data cache会使用频繁，因此，设置字段数据缓存的大小，在聚类或排序场景较多的情形下很有必要 可通过indices.fielddata.cache.size：30%或具体值10GB来设置。但是如果场景或数据变更比较频繁，设置cache并不是好的做法，因为缓存加载的开销也是特别大的。 ShardRequestCache查询请求发起后，每个分片会将结果返回给协调节点(Coordinating Node),由协调节点将结果整合。如果有需求，可以设置开启;通过设置index.requests.cache.enable: true来开启。不过，shard request cache只缓存hits.total, aggregations, suggestions类型的数据，并不会缓存hits的内容。也可以通过设置indices.requests.cache.size: 1%（默认）来控制缓存空间大小。 字段设计规范 text和keyword的用途必须分清：分词和关键词（确定字段是否需要分词） 确定字段是否需要独立存储 字段类型不支持修改，必须谨慎。 对不需要进行聚合/排序的字段禁用doc_values 不要在text做模糊搜索： 违规操作 原则：不要忽略设计，快就是慢，坏的索引设计后患无穷. 拒绝大聚合 ：ES计算都在JVM内存中完成。 拒绝模糊查询：es一大杀手 123456&#123; &quot;query&quot;:&#123; &quot;wildcard&quot;:&#123; &quot;title.keyword&quot;:&quot;*张三*&quot; &#125;&#125; 拒绝深度分页 ES获取数据时，每次默认最多获取10000条，获取更多需要分页，但存在深度分页问题，一定不要使用from/Size方式，建议使用scroll或者searchAfter方式。 scroll会把上一次查询结果缓存一定时间（通过配置scroll=1m实现)，所以在使用scroll时一定要保证search结果集不要太大。 基数查询尽量不要用基数查询去查询去重后的数据量大小（kibana中界面上显示是Unique Count，Distinct Count等），即少用如下的查询： 12345&quot;aggregations&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;userId&quot; &#125; &#125; 禁止查询 indexName-* 避免使用script、update_by_query、delete_by_query，对线上性能影响较大。 建议操作 复用预索引数据方式来提高AGG性能 如通过terms aggregations替代range aggregations， 如要根据年龄来分组，分组目标是:少年（14岁以下） 青年（14-28） 中年（29-50） 老年（51以上）， 可以在索引的时候设置一个age_group字段，预先将数据进行分类。从而不用按age来做range aggregations,通过age_group字段就可以了。 避免将不相关的数据放在同一个索引中，以避免稀疏，将这些文件放在不同的索引中往往更好。 索引及字段命名规范索引索引受文件系统的限制。仅可能为小写字母，不能下划线开头。同时需遵守下列规则： 不能包括 , /, *, ?, “, &lt;, &gt;, |, 空格, 逗号, # 7.0版本之前可以使用冒号:,但不建议使用并在7.0版本之后不再支持 不能以这些字符 -, _, + 开头 不能包括 . 或 … 长度不能超过 255 个字符 以上这些命名限制是因为当Elasticsearch使用索引名称作为磁盘上的目录名称，这些名称必须符合不同操作系统的约定。未来可能会放开这些限制，因为我们使用uuid关联索引放在磁盘上，而不使用索引名称 类型7.0版本之后不再支持类型，默认为_doc 常见问题 一个索引的shard数一旦确定不能改变 ES不支持事务ACID特性。 reindex：reindex可以实现索引的shard变更，但代价非常大：速度慢、对性能有影响，所以好的设计和规划更重要 field一旦创建不能更改mapping，如果需要修改，则必须重新创建索引 参考文档 Elasticsearch 使用规范 Elasticsearch索引及字段命名规范","tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/tags/Elasticsearch/"}],"categories":[]},{"title":"数据结构及算法-树","date":"2021-09-30T08:36:56.000Z","path":"wiki/数据结构及算法-树/","text":"参考资料","tags":[],"categories":[]},{"title":"","date":"2021-09-27T14:11:31.478Z","path":"wiki/draft/远程通信协议/","text":"分布式架构基石-远程通信协议1、什么是通信常见的通信有两种： 1、客户端与服务端之间的通信，通过http协议进行通信，或者服务器与服务器之间也会进行通信。 2、微服务架构中，节点之间的通信，通过RPC，实现类似于单体架构中的service方法的调用。 2、Java中是如何进行通信呢通过socket套接字实现 换行符 坑 导致提示Connection Reset (readline)","tags":[],"categories":[{"name":"draft","slug":"draft","permalink":"http://example.com/categories/draft/"}]},{"title":"二分查找（迭代&递归）","date":"2021-09-27T09:00:45.000Z","path":"wiki/二分查找（迭代-递归）/","text":"https://leetcode-cn.com/problems/binary-search/ 1. 二分查找给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 示例 1:输入: nums = [-1,0,3,5,9,12], target = 9输出: 4解释: 9 出现在 nums 中并且下标为 4 示例 2:输入: nums = [-1,0,3,5,9,12], target = 2输出: -1解释: 2 不存在 nums 中因此返回 -1 提示： 你可以假设 nums 中的所有元素是不重复的。n 将在 [1, 10000]之间。nums 的每个元素都将在 [-9999, 9999]之间。 递归写法1234567891011121314151617181920212223242526class Solution &#123; public int search(int[] nums, int target) &#123; // 注意边界 return find(0, nums.length - 1, target, nums); &#125; public int find(int left, int right, int target, int[] nums) &#123; if (left &lt;= right) &#123; // 注意mid取值的写法，避免越界异常 int mid = (right - left) / 2 + left; if (nums[mid] == target) &#123; return mid; &#125; if (nums[mid] &gt; target) &#123; return find(left, mid - 1, target, nums); &#125; if (nums[mid] &lt; target) &#123; return find(mid + 1, right, target, nums); &#125; &#125; return -1; &#125;&#125; 迭代写法12345678910111213141516171819202122class Solution &#123; public int search(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while(left &lt;= right) &#123; int mid = (right - left) / 2 + left; if (nums[mid] == target) &#123; return mid; &#125; if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; if (nums[mid] &lt; target) &#123; left = mid + 1; &#125; &#125; return -1; &#125;&#125; 2. 查找第一个错误版本leetcode278 你是产品经理，目前正在带领一个团队开发新的产品。不幸的是，你的产品的最新版本没有通过质量检测。由于每个版本都是基于之前的版本开发的，所以错误的版本之后的所有版本都是错的。 假设你有 n 个版本 [1, 2, …, n]，你想找出导致之后所有版本出错的第一个错误的版本。 你可以通过调用 bool isBadVersion(version) 接口来判断版本号 version 是否在单元测试中出错。实现一个函数来查找第一个错误的版本。你应该尽量减少对调用 API 的次数。 示例 1：1234567输入：n = 5, bad = 4输出：4解释：调用 isBadVersion(3) -&gt; false 调用 isBadVersion(5) -&gt; true 调用 isBadVersion(4) -&gt; true所以，4 是第一个错误的版本。 迭代写法12345678910111213141516public class Solution extends VersionControl &#123; public int firstBadVersion(int n) &#123; int left = 1; int right = n; while (left &lt;= right) &#123; int mid = (right - left) / 2 + left; if (isBadVersion(mid)) &#123; right = mid - 1; &#125;else &#123; left = mid + 1; &#125; &#125; return left; &#125;&#125; 3. 搜索插入位置Leetcode 35 给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。 请必须使用时间复杂度为 O(log n) 的算法。 递归写法12345678910111213141516171819202122class Solution &#123; public int searchInsert(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while (left &lt;= right) &#123; int mid = (right - left) / 2 + left; if (nums[mid] == target) &#123; return mid; &#125; if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; if (nums[mid] &lt; target) &#123; left = mid + 1; &#125; &#125; return left; &#125;&#125;","tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"http://example.com/tags/Leetcode/"},{"name":"二分","slug":"二分","permalink":"http://example.com/tags/%E4%BA%8C%E5%88%86/"}],"categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"http://example.com/categories/Leetcode/"},{"name":"二分","slug":"Leetcode/二分","permalink":"http://example.com/categories/Leetcode/%E4%BA%8C%E5%88%86/"}]},{"title":"面试官真的问我「分布式事务」了","date":"2021-09-18T09:07:46.000Z","path":"wiki/面试官真的问我「分布式事务」了/","text":"CAP理论CAP定理，又被叫作布鲁尔定理。对于设计分布式系统来说(不仅仅是分布式事务)的架构师来说，CAP就是你的入门理论。 C (一致性):对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据上来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。A (可用性)：非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40。P (分区容错性):当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。 熟悉CAP的人都知道，三者不能共有，如果感兴趣可以搜索CAP的证明，在分布式系统中，网络无法100%可靠，分区其实是一个必然现象，如果我们选择了CA而放弃了P，那么当发生分区现象时，为了保证一致性，这个时候必须拒绝请求，但是A又不允许，所以分布式系统理论上不可能选择CA架构，只能选择CP或者AP架构。对于CP来说，放弃可用性，追求一致性和分区容错性，我们的zookeeper其实就是追求的强一致。对于AP来说，放弃一致性(这里说的一致性是强一致性)，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的BASE也是根据AP来扩展。顺便一提，CAP理论中是忽略网络延迟，也就是当事务提交时，从节点A复制到节点B，但是在现实中这个是明显不可能的，所以总会有一定的时间是不一致。同时CAP中选择两个，比如你选择了CP，并不是叫你放弃A。因为P出现的概率实在是太小了，大部分的时间你仍然需要保证CA。就算分区出现了你也要为后来的A做准备，比如通过一些日志的手段，是其他机器回复至可用。 BASE理论BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展 基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。 BASE解决了CAP中理论没有网络延迟，在BASE中用软状态和最终一致，保证了延迟后的一致性。BASE和 ACID 是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。 分布式事务解决方案2PCTCC参考资料 bilibili 分布式事务详解 微服务分布式事务4种解决方案实战 再有人问你分布式事务，把这篇扔给他 https://juejin.cn/post/6844903573667446797#heading-5 https://juejin.cn/post/7012425995634343966?utm_source=gold_browser_extension","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"分布式事务","slug":"分布式事务","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"Redis-高性能IO模型","date":"2021-09-16T03:54:08.000Z","path":"wiki/Redis-高性能IO模型/","text":"参考资料 Redis基础篇（二）高性能IO模型 聊聊IO多路复用之select、poll、epoll详解 聊聊Linux 五种IO模型","tags":[],"categories":[]},{"title":"基础算法分类和思想","date":"2021-09-15T11:44:40.000Z","path":"wiki/基础算法分类和思想/","text":"常见的算法解题思路以及模版回溯算法可以参照下面👇文章：回溯算法套路详解 练习题库都是回溯算法的题目：👇https://leetcode-cn.com/tag/backtracking/problemset/ 整理的比较不错的资源📖labuladong 的算法小抄 整体来说还是相当不错的！","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"categories":[]},{"title":"理解ClassNotFoundException与NoClassDefFoundError的区别","date":"2021-09-15T07:43:20.000Z","path":"wiki/理解ClassNotFoundException与NoClassDefFoundError的区别/","text":"ClassNotFoundException类加载时在指定路径下没有找到类文件 NoClassDefFoundError1、编译时存在某个类，但是运行时却找不到 编译完成之后，手动删除一个类的class文件 2、类根本就没有初始化成功，结果你还把它当做正常类使用，所以这事也不小，必须抛出ERROR告诉你不能再使用了 https://cloud.tencent.com/developer/article/1356060 https://blog.csdn.net/u012129558/article/details/81540804","tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"}],"categories":[]},{"title":"Redis实现分布式锁","date":"2021-09-15T03:21:41.000Z","path":"wiki/Redis实现分布式锁/","text":"Redis实现分布式锁的种种细节1、redis分布式锁直接使用 setNx 获取锁🔒，del key 释放锁 会造成 「 死锁 」的问题，获取锁的线程没有释放锁，进程死掉了，其他进程永远无法获取锁 2、给锁对应的key添加过期时间不就可以解决死锁的问题了吗？ 127.0.0.1:6379&gt; SETNX lock 1 // 加锁(integer) 127.0.0.1:6379&gt; EXPIRE lock 10 // 10s后自动过期(integer) 上面两个命令有什么问题吗？ 不是原子操作，可能 expire没有执行！使用如下复合命令 👇 127.0.0.1:6379&gt; SET lock 1 EX 10 NX 3、这样还会存在一个问题，进程2释放的是进程1的锁 进程1操作时间太久，还没有主动释放锁，锁就过期了，然后进程2获取锁，然后执行，进程2还没有执行完成，进程1执行完了，释放 锁，但是释放的是进程2的锁。「 释放他人锁」和 「 锁过期时间问题」 加锁：SET lock_key $unique_id EX $expire_time NX 操作共享资源 释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁 1234567// 判断锁是自己的，才释放if redis.call(&quot;GET&quot;,KEYS[1]) == ARGV[1]then return redis.call(&quot;DEL&quot;,KEYS[1])else return 0end 4、锁过期时间不好评估怎么办？ 假设一个方案： 加锁时，先设置一个过期时间，然后我们开启一个「守护线程」，定时去检测这个锁的失效时间，如果锁快要过期了，操作共享资源还未完成，那么就自动对锁进行「续期」，重新设置过期时间。 如果你是 Java 技术栈，幸运的是，已经有一个库把这些工作都封装好了：Redisson。 Redisson 是一个 Java 语言实现的 Redis SDK 客户端，在使用分布式锁时，它就采用了「自动续期」的方案来避免锁过期，这个守护线程我们一般也把它叫做「看门狗」线程。 以上都是基于单机redis的角度思考的redis分布式锁的问题，主要有三点 👇 1、死锁问题 （加过期时间解决） 2、释放他人锁 （添加线程标志） 3、锁过期时间问题 （守护线程自动续期） 如果是redis集群模式下会有哪些问题呢 👇 在redis主从模式下，如果master节点突然宕机了，锁还没有同步到从节点是，是不是分布式锁就丢了？？？ Redis 作者提出的 Redlock 方案，是如何解决主从切换后，锁失效问题的。如何解决这个问题呢 ？ 「 **RedLock**」 Redlock 的方案基于 2 个前提： 不再需要部署从库和哨兵实例，只部署主库 但主库要部署多个，官方推荐至少 5 个实例 也就是说，想用使用 Redlock，你至少要部署 5 个 Redis 实例，而且都是主库，它们之间没有任何关系，都是一个个孤立的实例。 注意：不是部署 Redis Cluster，就是部署 5 个简单的 Redis 实例。 参考资料 深度剖析：Redis 分布式锁到底安全吗？看完这篇文章彻底懂了！","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[]},{"title":"LinkedHashMap实现简易LRU","date":"2021-09-15T02:07:58.000Z","path":"wiki/LinkedHashMap实现简易LRU/","text":"题目 #运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。 进阶: 你是否可以在 O(1) 时间复杂度内完成这两种操作？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class LruCache extends LinkedHashMap&lt;Integer, Integer&gt; &#123; private final int capacity; public LruCache(int capacity) &#123; super(capacity , 0.75f, true); this.capacity = capacity; &#125; public int get(int key) &#123; Integer integer = super.get(key); return integer == null ? -1 : integer; &#125; public void put(int key, int value) &#123; super.put(key, value); &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; this.capacity; &#125; public static void main(String[] args) &#123; LruCache cache = new LruCache(2); cache.put(1,1); cache.put(2,2); System.err.println(&quot;get1 -&gt; &quot; + cache.get(1)); System.err.println(&quot;get2 -&gt; &quot; + cache.get(2)); System.err.println(&quot;----------------------------&quot;); cache.put(3,3); System.err.println(&quot;get1 -&gt; &quot; + cache.get(1)); System.err.println(&quot;get2 -&gt; &quot; + cache.get(2)); System.err.println(&quot;get3 -&gt; &quot; + cache.get(3)); System.err.println(&quot;----------------------------&quot;); cache.put(4,4); System.err.println(&quot;get1 -&gt; &quot; + cache.get(1)); System.err.println(&quot;get2 -&gt; &quot; + cache.get(2)); System.err.println(&quot;get3 -&gt; &quot; + cache.get(3)); System.err.println(&quot;get4 -&gt; &quot; + cache.get(4)); &#125;&#125;","tags":[{"name":"LRU","slug":"LRU","permalink":"http://example.com/tags/LRU/"}],"categories":[]},{"title":"Redis操作为什么是原子性的？","date":"2021-09-14T02:46:36.000Z","path":"wiki/Redis操作为什么是原子性的？/","text":"Redis原则性操作对Redis来说，执行get、set以及eval等API，都是一个一个的任务，这些任务都会由Redis的线程去负责执行，任务要么执行成功，要么 执行失败，这就是Redis的命令是原子性的原因。 Redis本身提供的所有API都是原子操作，Redis中的事务其实是要保证批量操作的原子性。 事务命令MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 Discard:Redis Discard 命令用于取消事务，放弃执行事务块内的所有命令。 123456如何开启一个事务watch key1 key2 . . . （监听相关key）multi (开启事务)// 对监听key的一些操作exec（执行事务）discard（取消事务） EXEC 命令负责触发并执行事务中的所有命令：如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 如果redis备份采用的是AOF的方式，事务执行一半被终止，会怎样？当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。 如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。 使用 redis-check-aof 程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。 如果是集群下，watch命令有没有什么局限性？有没有想过为什么监听多个落在不同节点上的key，不同槽位的也不可以，会不被允许？在单节点下，Redis单线程执行，能够保证原子性，但在不同节点下，就是多进程多线程的问题，Watch自然就不能用。 参考资料 知其所以然~redis的原子性 Redis实现原子操作的两种方式与商品入库出库解决方案","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[]},{"title":"大厂是如何进行【负载均衡】的","date":"2021-09-13T12:46:14.000Z","path":"wiki/大厂是如何进行【负载均衡】的/","text":"轮训算法 随机算法 平滑加权轮询算法 一致性哈希算法 最小活跃数算法 权重随机1、存到一个list中，权重高的存多份，权重低的，存少份，具体按照权重去比 2、把权重展示在坐标轴上，然后按照随机产生的数，在坐标轴上的分布来判断 curl -XPUT -u elastic ‘http://localhost:9200/_xpack/security/user/kibana/_password&#39; -H ‘Content-Type: application/json’ -d ‘{ “password” : “redeem_123456” }’ curl -u my_admin -XPUT ‘http://localhost:9200/_xpack/security/user/elastic/_password?pretty&#39; -H ‘Content-Type: application/json’ -d’ { “password” : “new_password” } ‘ 参考资料 2021年阿里巴巴JAVA面试100道","tags":[],"categories":[]},{"title":"来自TCP协议的灵魂拷问","date":"2021-09-10T12:05:58.000Z","path":"wiki/来自TCP协议的灵魂拷问/","text":"1. TCP 是用来解决什么问题？2. 为什么要 TCP，IP 层实现控制不行么？之所以要提取出一个 TCP 层来实现控制是因为 IP 层涉及到的设备更多，一条数据在网络上传输需要经过很多设备，而设备之间需要靠 IP 来寻址。 假设 IP 层实现了控制，那是不是涉及到的设备都需要关心很多事情？整体传输的效率是不是大打折扣了？ 3. 连接到底是什么？我们已经知道了为什么需要独立出 TCP 这一层，并且这一层主要是用来干嘛的，接下来就来看看它到底是怎么干的。 我们都知道 TCP 是面向连接的，那这个连接到底是个什么东西？真的是拉了一条线让端与端之间连起来了？ 所谓的连接其实只是双方都维护了一个状态，通过每一次通信来维护状态的变更，使得看起来好像有一条线关联了对方。 4. TCP报文头是什么样的 首先可以看到 TCP 包只有端口，没有 IP。 Seq 就是 Sequence Number 即序号，它是用来解决乱序问题的 ACK 就是 Acknowledgement Numer 即确认号，它是用来解决丢包情况的，告诉发送方这个包我收到啦 标志位就是 TCP flags 用来标记这个包是什么类型的，用来控制 TPC 的状态。 窗口就是滑动窗口，Sliding Window，用来流控。 5. 三次握手流程是怎么样的为什么要握手？ 其实主要就是为了初始化Seq Numer，SYN 的全称是 Synchronize Sequence Numbers，这个序号是用来保证之后传输数据的顺序性。 6. 第一次握手seq是怎么取值的不知道大家有没有想过 ISN 的值要设成什么？代码写死从零开始？ 想象一下如果写死一个值，比如 0 ，那么假设已经建立好连接了，client 也发了很多包比如已经第 20 个包了，然后网络断了之后 client 重新，端口号还是之前那个，然后序列号又从 0 开始，此时服务端返回第 20 个包的ack，客户端是不是傻了？ 所以 RFC793 中认为 ISN 要和一个假的时钟绑定在一起ISN 每四微秒加一，当超过 2 的 32 次方之后又从 0 开始，要四个半小时左右发生 ISN 回绕。 所以 ISN 变成一个递增值，真实的实现还需要加一些随机值在里面，防止被不法份子猜到 ISN。 7. SYN 超时了怎么处理？也就是 client 发送 SYN 至 server 然后就挂了，此时 server 发送 SYN+ACK 就一直得不到回复，怎么办？ 我脑海中一想到的就是重试，但是不能连续快速重试多次，你想一下，假设 client 掉线了，你总得给它点时间恢复吧，所以呢需要慢慢重试，阶梯性重试。 在 Linux 中就是默认重试 5 次，并且就是阶梯性的重试，间隔就是1s、2s、4s、8s、16s，再第五次发出之后还得等 32s 才能知道这次重试的结果，所以说总共等63s 才能断开连接。 8. SYN泛洪攻击是什么？你看到没 SYN 超时需要耗费服务端 63s 的时间断开连接，也就说 63s 内服务端需要保持这个资源，所以不法分子就可以构造出大量的 client 向 server 发 SYN 但就是不回 server。 使得 server 的 SYN 队列耗尽，无法处理正常的建连请求。 所以怎么办？如何解决SYN攻击呢？ 可以开启 tcp_syncookies，那就用不到 SYN 队列了。 SYN 队列满了之后 TCP 根据自己的 ip、端口、然后对方的 ip、端口，对方 SYN 的序号，时间戳等一波操作生成一个特殊的序号（即 cookie）发回去，如果对方是正常的 client 会把这个序号发回来，然后 server 根据这个序号建连。 或者调整 tcp_synack_retries 减少重试的次数，设置 tcp_max_syn_backlog 增加 SYN 队列数，设置 tcp_abort_on_overflow SYN 队列满了直接拒绝连接。 9. 什么是四次挥手？ 那么所有的断开链接都是上面锁描述的那样吗？ 可以看到👇双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN_WAIT_1 都进入到 CLOSING 这个过度状态然后再到 TIME_WAIT。 10. 挥手一定需要四次吗？假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。 等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。 那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？ 11. 为什么要有 TIME_WAIT?断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。 MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。 那么为什么要等 2MSL 呢？ 就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。 假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。 12. 等待 2MSL 会产生什么问题？如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。 如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？ 对于服务端来说就是资源得不到立即释放，对于客户端来说，就是端口不能立即释放来发起新的链接。 13. 超时重传机制是为了解决什么问题？前面我们提到 TCP 要提供可靠的传输，那么网络又是不稳定的如果传输的包对方没收到却又得保证可靠那么就必须重传。 TCP 的可靠性是靠确认号的，比如我发给你1、2、3、4这4个包，你告诉我你现在要 5 那说明前面四个包你都收到了，就是这么回事儿。 不过这里要注意，SeqNum 和 ACK 都是以字节数为单位的，也就是说假设你收到了1、2、4 但是 3 没有收到你不能 ACK 5，如果你回了 5 那么发送方就以为你5之前的都收到了。 所以只能回复确认最大连续收到包，也就是 3。 14. 为什么还需要快速重传机制？超时重传是按时间来驱动的，如果是网络状况真的不好的情况，超时重传没问题，但是如果网络状况好的时候，只是恰巧丢包了，那等这么长时间就没必要。 于是又引入了数据驱动的重传叫快速重传，什么意思呢？就是发送方如果连续三次收到对方相同的确认号，那么马上重传数据。 因为连续收到三次相同 ACK 证明当前网络状况是 ok 的，那么确认是丢包了，于是立马重发，没必要等这么久。 就是发送方收到接收方连续的ACK报文之后，马上重传报文。 14. SACK 的引入是为了解决什么问题？SACK 即 Selective Acknowledgment，它的引入就是为了解决发送方不知道该重传哪些数据的问题。 SACK 就是接收方会回传它已经接受到的数据，这样发送方就知道哪一些数据对方已经收到了，所以就可以选择性的发送丢失的数据。 如图，通过 ACK 告知我接下来要 5500 开始的数据，并一直更新 SACK，6000-6500 我收到了，6000-7000的数据我收到了，6000-7500的数据我收到了，发送方很明确的知道，5500-5999 的那一波数据应该是丢了，于是重传。 而且如果数据是多段不连续的， SACK 也可以发送，比如 SACK 0-500,1000-1500，2000-2500。就表明这几段已经收到了。 15. 滑动窗口是什么我们已经知道了 TCP 有序号，并且还有重传，但是这还不够，因为我们不是愣头青，还需要根据情况来控制一下发送速率，因为网络是复杂多变的，有时候就会阻塞住，而有时候又很通畅。 所以发送方需要知道接收方的情况，好控制一下发送的速率，不至于蒙着头一个劲儿的发然后接受方都接受不过来。 因此 TCP 就有个叫滑动窗口的东西来做流量控制，也就是接收方告诉发送方我还能接受多少数据，然后发送方就可以根据这个信息来进行数据的发送。 以下是发送方维护的窗口，就是黑色圈起来的。 图中的 #1 是已收到 ACK 的数据，#2 是已经发出去但是还没收到 ACK 的数据，#3 就是在窗口内可以发送但是还没发送的数据。#4 就是还不能发送的数据。 16. 如果接收方回复的窗口一直是 0 怎么办？上文已经说了发送方式根据接收方回应的 window 来控制能发多少数据，如果接收方一直回应 0，那发送方就杵着？ 你想一下，发送方发的数据都得到 ACK 了，但是呢回应的窗口都是 0 ，这发送方此时不敢发了啊，那也不能一直等着啊，这 Window 啥时候不变 0 啊？ 于是 TCP 有一个 Zero Window Probe 技术，发送方得知窗口是 0 之后，会去探测探测这个接收方到底行不行，也就是发送 ZWP 包给接收方。 17. 已经有滑动窗口了为什么还要拥塞控制？前面我们提到了重传，如果不管网络整体的情况，肯定就是对方没给 ACK ，那我就无脑重传。 如果此时网络状况很差，所有的连接都这样无脑重传，是不是网络情况就更差了，更加拥堵了？ 18. 拥塞控制怎么搞？主要有以下几个步骤来搞： 1、慢启动，探探路。 2、拥塞避免，感觉差不多了减速看看 3、拥塞发生快速重传/恢复 19. 什么是 TCP 半连接队列和全连接队列？在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是： 半连接队列，也称 SYN 队列； 全连接队列，也称 accepet 队列； 服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。 20. 如何查看全链接队列ss -lnt | grep port 查看全链接队列溢出情况： netstat -s | 参考资料 万字长文 | 23 个问题 TCP 疑难杂症全解析 TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[]},{"title":"详解网络IO","date":"2021-09-10T06:43:37.000Z","path":"wiki/详解网络IO/","text":"前言学习思路可以是网络IO的演变过程，从【阻塞io】到【非阻塞io】然后到【多路复用】，后续还有【异步io】 1. 阻塞io应用程序进行 recvfrom 系统调用时将阻塞在此调用，直到该套接字上有数据并且复制到用户空间缓冲区。该模式一般配合多线程使用， 应用进程每接收一个连接，为此连接创建一个线程来处理该连接上的读写以及业务处理。 缺点：如果套接字上没有数据，进程将一直阻塞。这时其他套接字上有数据也不能进行及时处理。 如果是多线程方式，除非连接关闭否则线程会一直存在，而线程的创建、维护和销毁非常消耗资源，所以能建立的连接数量非常有限。 2. 非阻塞io应用进程调用recefrom函数之后，不等待内核数据准备完成，而是不断轮训（注意这里是用户进程不断轮训，会有用户态到内核态的切换，性能损耗比较严重） 优点：代码编写相对简单，进程不会阻塞，可以在同一线程中处理所有连接。 缺点：需要频繁的轮询，比较耗CPU，在并发量很大的时候将花费大量时间在没有任何数据的连接上轮询。所以该模型只在专门提供某种功能的系统中才会出现。 3. io复用应用进程阻塞于 select/poll/epoll 等系统函数等待某个连接变成可读（有数据过来），再调用 recvfrom 从连接上读取数据。虽然此模式也会阻塞在 select/poll/epoll 上，但与阻塞IO 模型不同它阻塞在等待多个连接上有读（写）事件的发生，明显提高了效率且增加了单线程/单进程中并行处理多连接的可能。 优点：统一管理连接，不一定采用多线程的方式，同时也不需要轮询。只需要阻塞于 select 即可，可以同时管理多个连接。 缺点：当 select/poll/epoll 管理的连接数过少时，这种模型将退化成阻塞 IO 模型。并且还多了一次系统调用：一次 select/poll/epoll 一次 recvfrom。 3.1 select1、句柄上限- 默认打开的FD有限制,1024个。 2、重复初始化-每次调用 select()，需要把 fd 集合从用户态拷贝到内核态，内核进行遍历。 3、逐个排查所有FD状态效率不高。 3.2 pollpoll和select相比在本质上变化不大，只是poll没有了select方式的最大文件描述符数量的限制。 缺点：逐个排查所有FD状态效率不高。 3.3 epoll没有fd个数限制，用户态拷贝到内核态只需要一次，使用事件通知机制来触发。通过epoll_ctl注册fd，一旦fd就绪就会通过callback回调机制来激活对应fd，进行相关的I/O操作。 epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序可以不立即 处理该事件`。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序必须立即处 理该事件`。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 缺点： 跨平台，Linux 支持最好。 底层实现复杂。 同步。 3.4 select/poll/epoll之间的区别 4. 信号驱动io应用进程创建 SIGIO 信号处理程序，此程序可处理连接上数据的读写和业务处理。并向操作系统安装此信号，进程可以往下执行。当内核数据准备好会向应用进程发送信号，触发信号处理程序的执 行。再在信号处理程序中进行 recvfrom 和业务处理。 优点：非阻塞 缺点：在前一个通知信号没被处理的情况下，后一个信号来了也不能被处理。所以在信号量大 的时候会导致后面的信号不能被及时感知。 5. 异步io应用进程通过 aio_read 告知内核启动某个操作，在整个操作完成之后内核再通知应用进程，包括把 数据从内核空间拷贝到用户空间。 信号驱动 IO 是内核通知我们何时可以启动一个 IO 操作，而异步 IO 模型是由内核通知我们 IO 操作何时完成。 注：前 4 种模型都是带有阻塞部分的，有的阻塞在等待数据准备好，有的阻塞在从内核空间拷贝数据到用户空间(信号量io)。而这种模型应用进程从调用 aio_read 到数据被拷贝到用户空间，不用任何阻塞，所以该种模式叫异步 IO 模型。 优点：没有任何阻塞，充分利用系统内核将 IO 操作与计算逻辑并行。 缺点：编程复杂、操作系统支持不好。目前只有 windows 下的 iocp 实现了真正的 AIO。linux 下在 2.6 版本中才引入，目前并不完善，所以 Linux 下一般采用多路复用模型。 6. Reactor 和 ProactorReactor 是非阻塞同步网络模式，感知的是就绪可读写事件。在每次 感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读 取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的， 读取完数据后应用进程才能处理数据。 Proactor 是异步网络模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据**，操作系统完成读写工作后，就会通知应用进程直接处理数据。 因此，Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 **Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。 可参见 https://mp.weixin.qq.com/s/px6-YnPEUCEqYIp_YHhDzg 7. 参考资料 「网络IO套路」当时就靠它追到女友 彻底理解 IO多路复用 看完这个，Java IO从此不在难 从操作系统层面理解Linux下的网络IO模型 五种IO模型介绍和对比 服务器网络编程之 IO 模型 网络编程与高效IO 高性能网络IO模式Reactor Linux IO模式及 select、poll、epoll详解 这次答应我，一举拿下 I/O 多路复用！ 原来 8 张图，就能学废 Reactor 和 Proactor 聊聊IO多路复用之select、poll、epoll详解 聊聊Linux 五种IO模型","tags":[{"name":"网络IO","slug":"网络IO","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9CIO/"},{"name":"epoll","slug":"epoll","permalink":"http://example.com/tags/epoll/"}],"categories":[]},{"title":"Netty基础","date":"2021-09-09T06:56:22.000Z","path":"wiki/Netty基础/","text":"常见问题1、什么是粘包和拆包，Netty如何解决这个问题https://www.cnblogs.com/rickiyang/p/12904552.html 这部分可能设计到的技术点 【TCP窗口滑动】【IO模型】【零拷贝】【socket缓冲区】【Nagle 算法】【MSS和MTU】 2、Netty中的空轮训bug是什么，如何解决这个问题？ 参考资料 Netty（Reactor线程模型/零拷贝/空轮询）Netty（Reactor线程模型/零拷贝/空轮询）","tags":[{"name":"NIO","slug":"NIO","permalink":"http://example.com/tags/NIO/"},{"name":"Netty","slug":"Netty","permalink":"http://example.com/tags/Netty/"}],"categories":[]},{"title":"零拷贝技术","date":"2021-09-09T06:12:33.000Z","path":"wiki/零拷贝技术/","text":"思考问题什么是零拷贝技术？ 为什么需要零拷贝？ 零拷贝有哪些应用场景？ 什么叫做用户态到内核态切换？为什么需要两种状态的切换？ 概述传统的数据拷贝技术没有DMA的数据拷贝流程如下 👇 用户发起read的系统调用，应用进程从用户态进入到内核态，CPU发送IO请求到磁盘，磁盘准备好数据之后发送中断信号。 之后CPU响应中断，讲磁盘缓冲区数据拷贝到内核缓冲区，数据拷贝完成之后，在把内核缓冲区的数据拷贝到应用进程的缓冲区中。 这个过程中CPU是一直占用的，不能进行其他的操作。最后，应用进程在从内核态切换到用户态。一共进行和4次数据拷贝和2次用户态/内核态的切换。 为了提高CPU 的执行效率，于是又了DMA技术。 什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。 1、用户进程调用read系统指令，进程从用户态切换到内核态，CPU发起IO请求，把读取磁盘数据的任务交给DMA 2、DMA发送IO请求到磁盘，磁盘准备好数据之后，发送中断信号，由DMA响应中断请求 3、DMA读取磁盘缓冲区的数据到内核缓冲区，数据读取完成之后，通知CPU进行处理 4、CPU把内核缓冲区的数据拷贝到应用缓冲期中，最后，进程从内核态切换到用户态。 为什么系统调用的时候需要 「用户态」和 「内核态」的切换这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。 这么设计是为了操作系统的安全考虑。 而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行 零拷贝的实现方式零拷贝有两种实现方式，mmap 和 sendFile 两种。 它们是如何减少「上下文切换」和「数据拷贝」的次数？ mmap实现零拷贝mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 我们可以得知，通过使用 mmap() 来代替 read()， 可以减少一次数据拷贝的过程。 但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。 sendfile实现零拷贝在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile()，函数形式如下： 123#include &lt;sys/socket.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);// 它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。 该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝 但是这还不是真正的零拷贝技术。因为CPU还有执行一次内核缓冲区到Socket缓冲区的拷贝。 于是，从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， sendfile() 系统调用的过程发生了点变化，具体过程如下： 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里； 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝； 零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。 参考资料 傻瓜三歪让我教他「零拷贝」 文件传输，零拷贝文件传输，零拷贝","tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[]},{"title":"SpringMVC-核心讲解","date":"2021-09-03T09:10:00.000Z","path":"wiki/SpringMVC-核心讲解/","text":"SpringMVC属于Spring那个模块 SpringMVC替我们做了哪些工作 SpringMVC如何简化工作的1、请求参数不需要在手动平装到对象上了。可以直接使用@RequestBody @RequestHeader 2、SpringMVC增强了对文件的处理 MultipartFile SpringMVC工作流程 用户发送请求 请求交由核心控制器处理 核心控制器找到映射器，映射器看看请求路径是什么 核心控制器再找到适配器，看看有哪些类实现了Controller接口或者对应的bean对象 将带过来的数据进行转换，格式化等等操作 找到我们的控制器Action，处理完业务之后返回一个ModelAndView对象 最后通过视图解析器来对ModelAndView进行解析 跳转到对应的JSP/html页面 参考资料 三歪肝出了期待已久的SpringMVC","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[]},{"title":"Spring-核心模块及其功能","date":"2021-09-03T06:58:39.000Z","path":"wiki/Spring-核心模块及其功能/","text":"Spring CoreIOC控制反转，SpringBean Spring-bean Spring-context Spring-core Spring-expression : 表达式相关 Spring Web主要是web开发相关，Spring MVC Spring DAOSpring 对jdbc操作的支持 【JdbcTemplate模板工具类】 Spring ORM整个Hibernate/Mybatis Spring AOP面向切面编程 Spring EEspring 对javaEE其他模块的支持 参考资料 Spring入门这一篇就够了","tags":[],"categories":[]},{"title":"Spring-什么是AOP","date":"2021-09-03T06:20:06.000Z","path":"wiki/Spring-什么是AOP/","text":"AOP概述Aop： aspect object programming 面向切面编程 功能： 让关注点代码与业务代码分离！ 面向切面编程就是指： 对很多功能都有的重复的代码抽取，再在运行的时候往业务方法上动态植入“切面类代码”。 举个例子，比如我们需要监控一个方法的执行时长，方法结束时间 - 进入方法的时间。 如果就几个方法，我们完全可以在这几个方法上实现，进入方法是获取当前时间，退出时时间戳做一下差就完事了。 但是如果很多方法或者接口需要监控，怎么办，AOP就体现出来了。 12345678910111213141516171819202122232425@Aspect@Order(1)@Component@Lazy(false)public class TimeAspect &#123; private Logger logger = LoggerFactory.getLogger(getClass()); // 切入点表达式主要就是来配置拦截哪些类的哪些方法 @Pointcut(&quot;@annotation(com.yj.common.app.annotation.Time)&quot;) public void timePointCut() &#123; // 好处是避免before，after等每个方法都写一遍，这样只写一遍就好了 &#125; // 切入点的使用， @Around(&quot;timePointCut()&quot;) public Object around(ProceedingJoinPoint point) throws Throwable &#123; Method method = ((MethodSignature) point.getSignature()).getMethod(); Time time = method.getAnnotation(Time.class); long beginTime = System.currentTimeMillis(); Object object = point.proceed(); logger.info(&quot;&#123;&#125; : execute_totalTime: &#123;&#125; ms&quot;, time.value(), (System.currentTimeMillis() - beginTime)); return object; &#125;&#125; 这样，我们仅仅需要在需要监控的方法上添加@Time就可以了，当然还有一个参数，备注一些信息。 AOP注解 @Aspect 指定一个类为切面类 @Pointcut(“execution(* cn.itcast.e_aop_anno.*.*(..))”) 指定切入点表达式 @Before(“pointCut_()”) 前置通知: 目标方法之前执行 @After(“pointCut_()”) 后置通知：目标方法之后执行（始终执行） @AfterReturning(“pointCut_()”) 返回后通知： 执行方法结束前执行(异常不执行) @AfterThrowing(“pointCut_()”) 异常通知: 出现异常时候执行 @Around(“pointCut_()”) 环绕通知： 环绕目标方法执行 动态代理静态代理需要实现目标对象的相同接口，那么可能会导致代理类会非常非常多，所以会产生动态代理。 JDK自带的动态代理JDK Proxy在实现的时候有一个限制，代理的对象一定是要有接口的。没有接口的话不能实现动态代理。 看一下JDK的API就明白了 123public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 所以JDK的动态代理就存在一定的局限性。 而cglib则比较灵活，cglib代理也叫子类代理，从内存中构建出一个子类来扩展目标对象的功能！ CGLIB是一个强大的高性能的代码生成包，它可以在运行期扩展Java类与实现Java接口。它广泛的被许多AOP的框架使用，例如Spring AOP和dynaop，为他们提供方法的interception（拦截）。 在Spring的动态代理中，如果代理对象是有接口的，代理的实现是JDK的Proxy，如果代理对象不是接口的，代理的实现是通过cglib。 Spring AOP就是这么简单 三歪红着眼睛总结了Spring知识点","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[]},{"title":"Spring-面试官问题什么是IOC","date":"2021-09-03T05:56:20.000Z","path":"wiki/Spring-面试官问题什么是IOC/","text":"1、思路1、首先明确什么是IOC，明确什么是DI，IOC和DI有什么关系 2、说明什么是IOC之后，明确为什么需要 IOC，或者IOC的优点 3、然后是IOC是如何实现的 2、概述什么是IOCSpring IOC 解决的是 对象管理和对象依赖的问题。Spring AOP 解决的是 非业务代码抽取的问题。 Spring IOC 解决的是对象管理和对象依赖的问题。本来我们的对象都是new出来的，而我们如果使用Spring 则把对象交给「IOC容器」来管理。 「IOC容器」是什么？我们可以理解为是一个「工厂」，我们把对象都交由这个「工厂」来管理，包括对象的创建和对象之间的依赖关系等等。等我们要用到对象的时候，就从这个「工厂」里边取出来。 「控制反转」指的就是：本来是「由我们自己」new出来的对象，现在交给了IOC容器。把这个对象的「控制权」给「他方」了。「控制反转」更多的是一种思想或者说是设计模式，把原有由自己掌控的事交给「别人」来处理。 「依赖注入」更多指的是「控制反转」这个思想的实现方式：对象无需自行创建或管理它们的依赖关系，依赖关系将被「自动注入」到需要它们的对象当中去。 最简单理解「依赖注入」和「控制反转」：本来我们的对象都是「由我们自己」new出来的，现在我们把这个对象的创建权限和对象之间的依赖关系交由「IOC容器」来管理。 使用IOC有什么好处 将对象集中统一管理，便于修改 降低耦合度（调用方无需自己组装，也无需关心对象的实现，直接从「IOC容器」取就好了） IOC如何设计实现的既然已经说到，IOC可以帮助我们管理对象，那么我们的对象是如何交给IOC的呢？ Spring提供了四种方式： 注解 XML JavaConfig 基于Groovy DSL配置 我们以XML配置+注解来装配Bean比较多，其中注解这种方式占大部分。 依赖注入Spring使用依赖注入来实现对象之间的依赖关系, 在创建完对象之后，对象的关系处理就是依赖注入 Spring提供了好几种的方式来给属性赋值 1) 通过构造函数 2) 通过set方法给属性注入值 3) p名称空间 4) 自动装配 5) 注解 参考资料三歪讲Spring 工厂模式理解了没有 Spring依赖注入就是这么简单","tags":[],"categories":[]},{"title":"LeetCode-比较版本号(165)","date":"2021-09-02T06:08:13.000Z","path":"wiki/LeetCode-比较版本号-165/","text":"https://leetcode-cn.com/problems/compare-version-numbers/ 12345678910111213141516171819202122232425262728293031323334class Solution &#123; public int compareVersion(String version1, String version2) &#123; String [] sp1 = version1.split(&quot;\\\\.&quot;); String [] sp2 = version2.split(&quot;\\\\.&quot;); int a = 0,b = 0; while(a &lt; sp1.length &amp;&amp; b &lt; sp2.length)&#123; if (Integer.valueOf(sp1[a]) &gt; Integer.valueOf(sp2[b]))&#123; return 1; &#125; if (Integer.valueOf(sp1[a]) &lt; Integer.valueOf(sp2[b]))&#123; return -1; &#125; a++; b++; &#125; if(a == sp1.length)&#123; for(int x = b; x &lt; sp2.length;x++)&#123; if(Integer.valueOf(sp2[x]) &gt; 0)&#123; return -1; &#125; &#125; &#125; if(b == sp2.length)&#123; for(int y = a; y &lt; sp1.length; y++)&#123; if(Integer.valueOf(sp1[y]) &gt; 0)&#123; return 1; &#125; &#125; &#125; return 0; &#125;&#125; 注意点⚠️ 1、a++;,b++; while里面是两次条件判断，所以 if (Integer.valueOf(sp1[a]) &gt; Integer.valueOf(sp2[b]))不能写成 if (Integer.valueOf(sp1[a++]) &gt; Integer.valueOf(sp2[b++])) 2、先判断相同长度部分，如果相同长度部分能分出结果就直接返回，如果不能，就比较长度比较长的版本剩余的部分是否比0大就完事了。","tags":[{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://example.com/tags/LeetCode/"},{"name":"中等","slug":"中等","permalink":"http://example.com/tags/%E4%B8%AD%E7%AD%89/"}],"categories":[]},{"title":"LeetCode-热度Top100🔥","date":"2021-08-31T06:06:04.000Z","path":"wiki/LeetCode-热度Top100🔥/","text":"挑战LeetCode热度Top100 👇https://leetcode-cn.com/problem-list/2cktkvj/ 1. 两数之和https://leetcode-cn.com/problems/two-sum/submissions/ 123456789101112131415161718192021class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; if(nums == null || nums.length == 0)&#123; return res; &#125; int [] arr = new int[2]; for(int i = 0; i&lt;nums.length -1 ; i++)&#123; int temp = target - nums[i]; for(int j = i+1;j&lt; nums.length; j++)&#123; if(temp == nums[j])&#123; arr[0] = i; arr[1] = j; return arr; &#125; &#125; &#125; arr[0] = 0; arr[1] = 0; return arr; &#125;&#125; 还有一种方式： 12345678910111213141516public int[] twoSum(int[] nums, int target) &#123; int[] res = new int[2]; if(nums == null || nums.length == 0)&#123; return res; &#125; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for(int i = 0; i &lt; nums.length; i++)&#123; int temp = target - nums[i]; if(map.containsKey(temp))&#123; res[1] = i; res[0] = map.get(temp); &#125; map.put(nums[i], i); &#125; return res;&#125; 2. 两数相加 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode() &#123;&#125; * ListNode(int val) &#123; this.val = val; &#125; * ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode result = new ListNode(0); ListNode index = result; if(l1 == null &amp;&amp; l2 == null)&#123; return result; &#125; int pre = 0; while(l1 != null || l2 != null || pre != 0)&#123; int a = l1 != null ? l1.val : 0; int b = l2 != null ? l2.val : 0; int c = a + b + pre; if(c &gt; 9)&#123; c = c - 10; pre = 1; &#125;else&#123; pre = 0; &#125; result.next = new ListNode(c); result = result.next; l1 = l1 == null ? l1 : l1.next; l2 = l2 == null ? l2 : l2.next; &#125; return index.next; &#125;&#125; index指向result的头节点，result不断的往next添加值，最终返回index.next. ⚠️ 注意临界条件 while(l1 != null || l2 != null || pre != 0) pre != 0 是会出现最高位是1，但是l1和l2都是null的情况，所以最后需要判断一下pre的位置有没有值。 3. 无重复字符的最长子串4. 寻找两个正序数组的中位数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; int [] arr = combineArr(nums1,nums2); return getResult(arr); &#125; double getResult(int [] nums)&#123; int c = 0; int d = 0; if(nums.length % 2 == 0)&#123; c = nums.length / 2; d = c - 1; &#125;else&#123; c = (nums.length - 1) / 2; d = c; &#125; return (double)(nums[c] + nums[d] ) / 2; &#125; int [] combineArr(int [] nums1, int [] nums2)&#123; int [] target = new int[nums1.length + nums2.length]; int i = 0,j = 0; int offset = 0; while(i &lt; nums1.length &amp;&amp; j &lt; nums2.length)&#123; if(nums1[i] &lt;= nums2[j])&#123; target[offset] = nums1[i]; i++; &#125;else&#123; target[offset] = nums2[j]; j++; &#125; offset++; &#125; if(i != nums1.length)&#123; for(int x = i; x &lt; nums1.length;x++)&#123; target[offset] = nums1[x]; offset++; &#125; &#125;else&#123; for(int x = j; x &lt; nums2.length;x++)&#123; target[offset] = nums2[x]; offset++; &#125; &#125; return target; &#125;&#125; 上面的思路比较清晰，合并两个有序数组，然后取中位数。 5. 最长回文子串 1234567891011121314151617181920212223242526272829303132333435363738public static String longestPalindrome(String s) &#123; if (s == null || s.length() == 0) &#123; return null; &#125; int left = 0; int right = 0; int maxLength = 0; int i = 0; while (i &lt; s.length()) &#123; int step = 1; while ((i - step) &gt;= 0 &amp;&amp; (i + step) &lt;= (s.length() - 1)) &#123; if (s.charAt(i - step) == s.charAt(i + step)) &#123; if (step &gt;= maxLength &amp;&amp; (i - step) != (i + step)) &#123; left = Math.max((i - step), 0); right = (i + step); &#125; step++; maxLength = Math.max(maxLength, step); continue; &#125; if (s.charAt(i) == s.charAt(i + step)) &#123; if (step &gt;= maxLength &amp;&amp; step != 0) &#123; left = Math.max((i), 0); right = (i + step); &#125; step++; maxLength = Math.max(maxLength, step); continue; &#125; step++; &#125; i++; &#125; return s.substring(left, right + 1); &#125; ❌ 上面这种写法在 bb bbbb bbbbbb这些用例下不兼容 有没有人帮忙调整一下！ 1234567891011121314151617181920public String longestPalindrome(String s) &#123; int[] arr = new int[2]; char[] chars = s.toCharArray(); int n = s.length(); for (int i = 0; i &lt; n; i++) &#123; int high = i, low = i; while (high &lt; n - 1 &amp;&amp; chars[i] == chars[high + 1]) &#123; high++; &#125; while (low - 1 &gt;= 0 &amp;&amp; high + 1 &lt; n &amp;&amp; chars[low - 1] == chars[high + 1]) &#123; high++; low--; &#125; if (high - low &gt; arr[1] - arr[0]) &#123; arr[0] = low; arr[1] = high; &#125; &#125; return s.substring(arr[0], arr[1] + 1); &#125; 这种写法和上面👆第一种写法思想类似。 7. 整数反转12345678910class Solution &#123; public int reverse(int x) &#123; long n = 0; while(x != 0) &#123; n = n*10 + x%10; x = x/10; &#125; return (int)n==n ? (int)n:0; &#125;&#125; 还有一种简单的方法，字符串反转。如果出现Integer.parseInt()异常，就返回0； 15. 三数之和最先想到的肯定是暴力求解法，先对数组排个序，三层循环，判断重复组合，简单易懂。 1234567891011121314151617181920212223242526272829303132class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123; if(nums == null || nums.length &lt; 3)&#123; return new ArrayList(); &#125; Arrays.sort(nums); List list = new ArrayList(); HashSet set = new HashSet(); for(int i = 0;i&lt;nums.length - 2;i++)&#123; for(int j = i + 1; j &lt; nums.length - 1; j++)&#123; for(int t = j + 1; t &lt; nums.length; t++)&#123; if(nums[i] + nums[j] + nums[t] == 0)&#123; List temp = new ArrayList(); temp.add(nums[i]); temp.add(nums[j]); temp.add(nums[t]); String flag = nums[i] + &quot;&quot; + nums[j] + &quot;&quot; +nums[t] ; if(set.contains(flag))&#123; continue; &#125; set.add(flag); list.add(temp); &#125; &#125; &#125; &#125; return list; &#125;&#125; 但是这种方式肯定是比较差的，leetcode上也是直接执行超时了。 优化 👇： 解题方案 首先对数组进行排序，排序后固定一个数 nums[i]nums[i]，再使用左右指针指向 nums[i]nums[i]后面的两端，数字分别为 nums[L]nums[L] 和 nums[R]nums[R]，计算三个数的和 sumsum 判断是否满足为 00，满足则添加进结果集如果 nums[i]nums[i]大于 00，则三数之和必然无法等于 00，结束循环如果 nums[i]nums[i] == nums[i-1]nums[i−1]，则说明该数字重复，会导致结果重复，所以应该跳过当 sumsum == 00 时，nums[L]nums[L] == nums[L+1]nums[L+1] 则会导致结果重复，应该跳过，L++L++当 sumsum == 00 时，nums[R]nums[R] == nums[R-1]nums[R−1] 则会导致结果重复，应该跳过，R–R−−时间复杂度：O(n^2)O(n 2 )，nn 为数组长度 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList(); if(nums == null || nums.length &lt; 3)&#123; return new ArrayList(); &#125; Arrays.sort(nums); // -4,-1,-1,0,1,2 for(int i=0; i&lt; nums.length; i++) &#123; int l = i + 1; int r = nums.length - 1; if(nums[i] &gt; 0) &#123; continue; &#125; if(i &gt; 0 &amp;&amp; nums[i] == nums[i-1] )&#123; continue; &#125; while(l &lt; r)&#123; int count = nums[i] + nums[l] + nums[r]; if (count == 0) &#123; List list = new ArrayList(); list.add(nums[i]); list.add(nums[l]); list.add(nums[r]); result.add(list); while(l &lt; r &amp;&amp; nums[l] == nums[l+1])&#123; l++; &#125; while(l &lt; r &amp;&amp; nums[r] == nums[r-1])&#123; r--; &#125; l++; r--; continue; &#125; if(count &gt; 0)&#123; r--; &#125; if(count &lt; 0)&#123; l++; &#125; &#125; &#125; return result; &#125;&#125; 19. 删除链表的倒数第 N 个结点 思路： 快慢指针先找到倒数第K个节点，然后把这个节点的前节点设置成这个节点的后节点。 https://leetcode-cn.com/problems/lian-biao-zhong-dao-shu-di-kge-jie-dian-lcof/ 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode() &#123;&#125; * ListNode(int val) &#123; this.val = val; &#125; * ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode l = head; ListNode r = head; ListNode result = new ListNode(); ListNode node = result; while(r != null &amp;&amp; n &gt; 0)&#123; r = r.next; n--; &#125; while(r != null)&#123; r = r.next; result.next = l; l = l.next; result = result.next; &#125; result.next = l.next; return node.next; &#125;&#125; 24. 两两交换链表中的节点给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。 你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。 解题思路： 【猿来绘（逻辑清晰，简单易懂）- 24. 两两交换链表中的节点】 非递归： 1234567891011121314151617181920class Solution &#123; public static ListNode swapPairs(ListNode head) &#123; if(head == null || head.next == null)&#123; return head; &#125; ListNode result = new ListNode(); result.next = head; ListNode curr = result; while(curr.next != null &amp;&amp; curr.next.next != null)&#123; ListNode temp = curr; ListNode first = curr.next; ListNode second = first.next; temp.next = second; first.next = second.next; second.next = first; curr = curr.next.next; &#125; return result.next; &#125;&#125; 递归： 12345678910111213141516class Solution &#123; public ListNode swapPairs(ListNode head) &#123; if(head == null || head.next == null)&#123; return head; &#125; ListNode next = head.next; head.next = swapPairs(next.next); next.next = head; return next; &#125;&#125;作者：guanpengchn链接：https://leetcode-cn.com/problems/swap-nodes-in-pairs/solution/hua-jie-suan-fa-24-liang-liang-jiao-huan-lian-biao/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 61. 旋转链表给你一个链表的头节点 head ，旋转链表，将链表每个节点向右移动 k 个位置。 解题思路参见：👇 https://leetcode-cn.com/problems/rotate-list/solution/dong-hua-yan-shi-kuai-man-zhi-zhen-61-xu-7bp0/ 向右移动k个位置就相当于倒数第k个节点做头节点，把前面的部分拼后后面就完事了，思路很简单，就这一句话。 这种思路最核心的点在于寻找倒数第k个点，这个和上面19题差不多。 快慢指针寻找倒数第k个点，快的走两步，慢的走一步，步数走完或者节点为空了，慢节点就是倒数第k个点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode() &#123;&#125; * ListNode(int val) &#123; this.val = val; &#125; * ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123; public ListNode rotateRight(ListNode head, int k) &#123; if(k == 0 || head == null || head.next == null)&#123; return head; &#125; int length = 0; ListNode node = head; while(node != null)&#123; length++; node = node.next; &#125; int target = k % length; if(target == 0)&#123; return head; &#125; ListNode origin = head; ListNode first = head; ListNode second = head; int num = target + 1; while(first != null &amp;&amp; num &gt; 0)&#123; first = first.next; num--; &#125; while(first != null)&#123; first = first.next; second = second.next; &#125; ListNode newNode = second.next; second.next = null; ListNode tail = newNode; while(tail.next != null)&#123; tail = tail.next; &#125; tail.next = origin; return newNode; &#125;&#125;","tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"http://example.com/tags/LeetCode/"},{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"}],"categories":[]},{"title":"Redis-布隆过滤器","date":"2021-08-31T03:46:19.000Z","path":"wiki/Redis-布隆过滤器/","text":"参考资料","tags":[],"categories":[]},{"title":"Spring-事务传播行为","date":"2021-08-30T12:53:54.000Z","path":"wiki/Spring-事务传播行为/","text":"https://mp.weixin.qq.com/s/IglQITCkmx7Lpz60QOW7HA","tags":[],"categories":[]},{"title":"Redis-如何实现高可用","date":"2021-08-30T07:54:00.000Z","path":"wiki/Redis-如何实现高可用/","text":"https://juejin.cn/post/7002011542145204261?utm_source=gold_browser_extension","tags":[],"categories":[]},{"title":"Redis-持久化机制","date":"2021-08-30T07:49:04.000Z","path":"wiki/Redis-持久化机制/","text":"Redis启动加载 优先加载AOF 其次加载RDB 如果都没有，直接启动成功，如果加载失败，则报错 https://juejin.cn/post/7002011542145204261?utm_source=gold_browser_extension","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[]},{"title":"Redis-过期策略和内存淘汰策略","date":"2021-08-30T07:23:03.000Z","path":"wiki/Redis-过期策略和内存淘汰策略/","text":"过期策略和内存淘汰策略我们在set key的时候，可以给它设置一个过期时间，比如expire key 60。指定这key60s后过期，60s后，redis是如何处理的嘛？我们先来介绍几种过期策略： 1. 过期策略1.1 定时过期每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即对key进行清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 1.2 惰性过期只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 1.3 定期过期每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。 Redis中同时使用了惰性过期和定期过期两种过期策略。 假设Redis当前存放30万个key，并且都设置了过期时间，如果你每隔100ms就去检查这全部的key，CPU负载会特别高，最后可能会挂掉。 因此，redis采取的是定期过期，每隔100ms就随机抽取一定数量的key来检查和删除的。 但是呢，最后可能会有很多已经过期的key没被删除。这时候，redis采用惰性删除。在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间并且已经过期了，此时就会删除。 但是呀，如果定期删除漏掉了很多过期的key，然后也没走惰性删除。就会有很多过期key积在内存内存，直接会导致内存爆的。或者有些时候，业务量大起来了，redis的key被大量使用，内存直接不够了，运维小哥哥也忘记加大内存了。难道redis直接这样挂掉？不会的！Redis用8种内存淘汰策略保护自己~ Redis 内存淘汰策略volatile-lru：从设置了过期时间的key中使用LRU（最近最少使用）算法进行淘汰； allkeys-lru：从所有key中使用LRU（最近最少使用）算法进行淘汰。 volatile-lfu：4.0版本新增，在过期的key中，使用LFU算法进行删除key。 allkeys-lfu：4.0版本新增，从所有key中使用LFU算法进行淘汰； volatile-random：从设置了过期时间的key中，随机淘汰数据；。 allkeys-random：从所有key中随机淘汰数据。 volatile-ttl：在设置了过期时间的key中，根据过期时间进行淘汰，越早过期的优先被淘汰； noeviction：默认策略，当内存不足以容纳新写入数据时，新写入操作会报错。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[]},{"title":"MySQL-ShardingJDBC分库分表实战","date":"2021-08-27T11:18:15.000Z","path":"wiki/MySQL-ShardingJDBC分库分表实战/","text":"分库分表Demo项目地址 https://github.com/geekibli/sharding-demos/ Springboot + Sharding jdbcmaven依赖123456789101112131415161718192021222324252627282930313233343536&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.0.0-RC1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 数据源配置1234567891011121314151617181920212223242526272829@Configurationpublic class DuridConfig &#123; @Bean public Filter statFilter() &#123; StatFilter filter = new StatFilter(); filter.setSlowSqlMillis(5000); filter.setLogSlowSql(true); filter.setMergeSql(true); return filter; &#125; @Bean public ServletRegistrationBean statViewServlet() &#123; //创建servlet注册实体 ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean(new StatViewServlet(), &quot;/druid/*&quot;); //设置ip白名单 servletRegistrationBean.addInitParameter(&quot;allow&quot;, &quot;127.0.0.1&quot;); //设置ip黑名单，如果allow与deny共同存在时,deny优先于allow //servletRegistrationBean.addInitParameter(&quot;deny&quot;,&quot;192.168.0.19&quot;); //设置控制台管理用户 servletRegistrationBean.addInitParameter(&quot;loginUsername&quot;, &quot;root&quot;); servletRegistrationBean.addInitParameter(&quot;loginPassword&quot;, &quot;123456&quot;); //是否可以重置数据 servletRegistrationBean.addInitParameter(&quot;resetEnable&quot;, &quot;false&quot;); return servletRegistrationBean; &#125;&#125; 配置文件123456789101112131415161718192021222324252627282930313233server.port=8071# mybatis 配置mybatis.mapper-locations=classpath:mapping/*.xmlmybatis.type-aliases-package=com.ibli.sharding.simple.domainspring.shardingsphere.datasource.names=ds0,ds1spring.shardingsphere.datasource.ds0.type=com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.ds0.driver-class-name=com.mysql.jdbc.Driverspring.shardingsphere.datasource.ds0.url=jdbc:mysql://localhost:3331/ds0?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;allowMultiQueries=true&amp;useSSL=falsespring.shardingsphere.datasource.ds0.username=rootspring.shardingsphere.datasource.ds0.password=123456spring.shardingsphere.datasource.ds1.type=com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.ds1.driver-class-name=com.mysql.jdbc.Driverspring.shardingsphere.datasource.ds1.url=jdbc:mysql://localhost:3331/ds1?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;allowMultiQueries=true&amp;useSSL=falsespring.shardingsphere.datasource.ds1.username=rootspring.shardingsphere.datasource.ds1.password=123456# 分库配置 ， 根据member_id分库spring.shardingsphere.sharding.default-database-strategy.inline.sharding-column=member_idspring.shardingsphere.sharding.default-database-strategy.inline.algorithm-expression=ds$-&gt;&#123;member_id % 2&#125;# 分表配置，根据member_id分表spring.shardingsphere.sharding.tables.member.actual-data-nodes=ds$-&gt;&#123;0..1&#125;.memberspring.shardingsphere.sharding.tables.member.table-strategy.inline.sharding-column=member_idspring.shardingsphere.sharding.tables.member.table-strategy.inline.algorithm-expression=member$-&gt;&#123;member_id % 2&#125;spring.shardingsphere.sharding.tables.member.key-generator.column=member_idspring.shardingsphere.sharding.tables.member.key-generator.type=SNOWFLAKEspring.shardingsphere.props.sql.show=true 插入数据12345678910111213@RequestMapping(&quot;/add&quot;) public Member add()&#123; Member member = new Member(); //不用手动设置主键id，新增时，sharding-jdbc会自动赋值，因为在配置文件中配置了该列使用SNOWFLAKE算法生成值// member.setMemberId(IdWorker.getLongId()); member.setNickName(&quot;nickname&quot;); member.setAccountNo(new Date().hashCode()); member.setPassword(UUID.randomUUID().toString()); member.setAge(10); member.setDelFlag(UUID.randomUUID().toString()); memberServiceImpl.insert(member); return member; &#125; SQL文件12345678910111213141516171819202122CREATE TABLE `member0` ( `member_id` bigint(20) NOT NULL AUTO_INCREMENT, `nick_name` varchar(255) DEFAULT NULL, `account_no` int(11) DEFAULT NULL, `password` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, `birthday` varchar(255) DEFAULT NULL, `del_flag` varchar(255) DEFAULT NULL, PRIMARY KEY (`member_id`)) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8mb4;CREATE TABLE `member1` ( `member_id` bigint(20) NOT NULL AUTO_INCREMENT, `nick_name` varchar(255) DEFAULT NULL, `account_no` int(11) DEFAULT NULL, `password` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, `birthday` varchar(255) DEFAULT NULL, `del_flag` varchar(255) DEFAULT NULL, PRIMARY KEY (`member_id`)) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8mb4; 启动项目访问创建用户接口curl -X GET http://localhost:8071/member/add 查看数据库 参考资料https://juejin.cn/post/6844903773383426061 https://juejin.cn/post/6890772387000762382 https://shardingsphere.apache.org/document/current/cn/quick-start/shardingsphere-proxy-quick-start/ https://www.pianshen.com/article/7996383507/","tags":[],"categories":[]},{"title":"MySQL-主从同步原理以及实现","date":"2021-08-27T08:53:05.000Z","path":"wiki/MySQL-主从同步原理以及实现/","text":"主从复制的原理我们在平时工作中，使用最多的数据库就是 MySQL 了，随着业务的增加，如果单单靠一台服务器的话，负载过重，就容易造成宕机。 这样我们保存在 MySQL 数据库的数据就会丢失，那么该怎么解决呢？ 其实在 MySQL 本身就自带有一个主从复制的功能，可以帮助我们实现负载均衡和读写分离。 对于主服务器（Master）来说，主要负责写，从服务器（Slave）主要负责读，这样的话，就会大大减轻压力，从而提高效率。 MySQL 的主从复制工作过程大致如下： 从库生成两个线程，一个 I/O 线程，一个 SQL 线程； I/O 线程去请求主库的 binlog，并将得到的 binlog 日志写到 relay log(中继日志) 文件中； 主库会生成一个 log dump 线程，用来给从库 I/O 线程传 binlog； SQL 线程会读取 relay log 文件中的日志，并解析成具体操作，来实现主从的操作一致，而最终数据一致； 请求流程MySQL 建立请求的主从的详细流程如下： 当从服务器连接主服务器时，主服务器会创建一个 log dump 线程，用于发送 binlog 的内容。在读取 binlog 的内容的操作中，会对象主节点上的 binlog 加锁，当读取完成并发送给从服务器后解锁。 当从节点上执行 start slave 命令之后，从节点会创建一个 IO 线程用来连接主节点，请求主库中更新binlog。IO 线程接收主节点 binlog dump 进程发来的更新之后，保存到 relay-log 中。 从节点 SQL 线程负责读取 realy-log 中的内容，解析成具体的操作执行，最终保证主从数据的一致性。 主从同步实现我这里实现了一主一从。数据库版本都是用的MySQL5.8。 设置主库设置配置文件123log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义 创建同步用户12345# 创建用户create user &#x27;repl&#x27;@&#x27;*&#x27; identified by &#x27;repl&#x27;;# 授权，只授予复制和客户端访问权限grant replication slave on *.* to &#x27;repl&#x27;@&#x27;*&#x27;;#分配权限 查看binlog状态SHOW MASTER STATUS; Position一开始同步之前是在 2729 位置。 配置从库Slave修改配置文件123log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=2 设置同步信息123456CHANGE MASTER TO MASTER_HOST=&#x27;123.56.77.177&#x27;, MASTER_USER=&#x27;repl&#x27;, MASTER_PASSWORD=&#x27;repl&#x27;, MASTER_LOG_FILE=&#x27;binlog.000001&#x27;, MASTER_LOG_POS=2729, Master_Port = 3306; 执行这个SQL必须是在 slave not running 状态下可以运行，可以 stop slave命令来停止slave。 启动slavemysql&gt;start slave; 查看slave状态mysql&gt; show slave status\\G ⚠️ 只有当 Slave_IO_Running: Yes 和 Slave_SQL_Running: Yes 同时是 yes的时候，才可以进行同步。 Slave_IO_Running: No 可能的原因1、端口不同 ping ip 如果是docker启动的mysql的话，需要退出docker来测试端口。 telnet ip port 2、没有权限 grant replication slave, replication client on *.* to &#39;repl&#39;@&#39;%&#39;; 3、binlog文件写错 可以查看 Last_IO_Error 的错误提示，我这里遇到一个错误，主库binlog文件是binlog.000001 ，但是我直接粘贴别人博客的时候，没有注意，博客上面写的是mysql-bin.000001。 同步效果测试分三个步骤测试1、测试数据库 2、测试表 3、测试表数据 参考资料主从同步的类型 （异步 同步 半同步 延迟） https://juejin.cn/post/6967224081410162696","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"Docker安装MySQL","date":"2021-08-27T06:16:47.000Z","path":"wiki/Docker安装MySQL/","text":"linux安装dockeryum updateyum install docker 结果启动docker时报错了 👇 12Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.Error: open /proc/self/uid_map: no such file or directory 解决办法1，卸载podman软件（可以使用rpm -qa|grep docker）yum remove docker2,下载docker-ce源curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo3，安装docker-ceyum install docker-ce -y 问题原因分析：Centos 8使用yum install docker -y时，默认安装的是podman-docker软件 查看docker状态 启动dockersystemctl start docker 安装MySQldocker search mysql 选择你要安装的版本docker pull centos/mysql-57-centos7 查看安装的镜像docker images 启动mysqldocker run --name mysqlserver -v /etc/mysql/my.conf:/etc/my.conf -e MYSQL_ROOT_PASSWORD=123456 -d -i -p 3306:3306 centos/mysql-57-centos7","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}],"categories":[]},{"title":"算法-KMP算法以及扩展","date":"2021-08-26T14:27:32.000Z","path":"wiki/算法-KMP算法以及扩展/","text":"题目1 “123456” 和 “345612”是不是互为旋转串 思路：“123456123456” 判断 “345612” 是不是子串就可以了 题目2 ![image-20210826225714858](/Users/gaolei/Library/Application Support/typora-user-images/image-20210826225714858.png) 荷兰国旗问题？？？ Bfprt 算法","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"KMP","slug":"KMP","permalink":"http://example.com/tags/KMP/"}],"categories":[]},{"title":"一文搞懂高并发性能指标：QPS、TPS、RT、吞吐量","date":"2021-08-26T11:41:56.000Z","path":"wiki/一文搞懂高并发性能指标：QPS、TPS、RT、吞吐量/","text":"QPSQueries Per Second意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。 互联网中，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。 原理：每天80%的访问集中在20%的时间里，这20%时间叫做峰值时间。 公式：( 总PV数 * 80% ) / ( 每天秒数 * 20% ) = 峰值时间每秒请求数(QPS) 。 PV（page view）即页面浏览量，通常是衡量一个网络新闻频道或网站甚至一条网络新闻的主要指标。网页浏览数是评价网站流量最常用的指标之一，简称为PV。 机器：峰值时间每秒QPS / 单台机器的QPS = 需要的机器 。 每天300w PV 的在单台机器上，这台机器需要多少QPS？ ( 3000000 * 0.8 ) / (86400 * 0.2 ) = 139 (QPS)。 一般需要达到139QPS，因为是峰值。(200万pv才有100峰值qps) TPSTPS：Transactions Per Second（每秒传输的事务处理个数），即服务器每秒处理的事务数。 TPS包括一条消息入和一条消息出，加上一次用户数据库访问。（业务TPS = CAPS × 每个呼叫平均TPS） 一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。 一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力最低模块的TPS值。 RT（响应时长）响应时间：执行一个请求从开始到最后收到响应数据所花费的总体时间,即从客户端发起请求到收到服务器响应结果的时间。 响应时间RT(Response-time)，是一个系统最重要的指标之一，它的数值大小直接反应了系统的快慢。 直观上看，这个指标与人对软件性能的主观感受是非常一致的，因为它完整地记录了整个计算机系统处理请求的时间。 由于一个系统通常会提供许多功能，而不同功能的处理逻辑也千差万别，因而不同功能的响应时间也不尽相同，甚至同一功能在不同输入数据的情况下响应时间也不相同。 所以，在讨论一个系统的响应时间时，人们通常是指该系统所有功能的平均时间或者所有功能的最大响应时间。当然，往往也需要对每个或每组功能讨论其平均响应时间和最大响应时间。 对于单机的没有并发操作的应用系统而言，人们普遍认为响应时间是一个合理且准确的性能指标。需要指出的是，响应时间的绝对值并不能直接反映软件的性能的高低，软件性能的高低实际上取决于用户对该响应时间的接受程度。 对于一个游戏软件来说，响应时间小于100毫秒应该是不错的，响应时间在1秒左右可能属于勉强可以接受，如果响应时间达到3秒就完全难以接受了。而对于编译系统来说，完整编译一个较大规模软件的源代码可能需要几十分钟甚至更长时间，但这些响应时间对于用户来说都是可以接受的 Load（系统负载）Linux的Load(系统负载)，是一个让新手不太容易了解的概念。load的就是一定时间内计算机有多少个active_tasks，也就是说是计算机的任务执行队列的长度，cpu计算的队列。 top/uptime等工具默认会显示1分钟、5分钟、15分钟的平均Load。 具体来说，平均Load是指，在特定的一段时间内统计的正在CPU中运行的(R状态)、正在等待CPU运行的、处于不可中断睡眠的(D状态)的任务数量的平均值。 最后，说一下CPU使用率和Load的关系吧。如果主要是CPU密集型的程序在运行(If CPU utilization is near 100 percent (user + nice + system), the workload sampled is CPU-bound.)， 那么CPU利用率高，Load一般也会比较高。而I/O密集型的程序在运行， 可能看到CPU的%user, %system都不高，%iowait可能会有点高，这时的Load通常比较高。 同理，程序读写慢速I/O设备(如磁盘、NFS)比较多时，Load可能会比较高，而CPU利用率不一定高。这种情况，还经常发生在系统内存不足并开始使用swap的时候，Load一般会比较高，而CPU使用率并不高。 PV页面访问次数：Page View UV访客数（去重复）：Unique Visitor 并发数并发数是指系统同时能处理的请求数量，这个也是反应了系统的负载能力。 吞吐量系统的吞吐量（承压能力）与request对CPU的消耗、外部接口、IO等等紧密关联。单个request 对CPU消耗越高，外部系统接口、IO速度越慢，系统吞吐能力越低，反之越高。 系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间。 QPS（TPS）：（Query Per Second）每秒钟request/事务 数量 并发数： 系统同时处理的request/事务数 响应时间： 一般取平均响应时间 理解了上面三个要素的意义之后，就能推算出它们之间的关系： QPS（TPS）= 并发数/平均响应时间 并发数 = QPS*平均响应时间 最佳线程数、QPS、RT1、单线程QPS公式：QPS=1000ms/RT对同一个系统而言，支持的线程数越多，QPS越高。假设一个RT是80ms,则可以很容易的计算出QPS,QPS = 1000/80 = 12.5多线程场景，如果把服务端的线程数提升到2，那么整个系统的QPS则为 2*（1000/80） = 25, 可见QPS随着线程的增加而线性增长，那QPS上不去就加线程呗，听起来很有道理，公司也说的通，但是往往现实并非如此。 2、最佳线程数量刚好消耗完服务器的瓶颈资源的临界线程数，公式如下最佳线程数量=（（线程等待时间+线程cpu时间）/ 线程cpu时间）* cpu数量特性： 在达到最佳线程数的时候，线程数量继续递增，则QPS不变，而响应时间变长，持续递增线程数量，则QPS开始下降。 每个系统都有其最佳线程数量，但是不同状态下，最佳线程数量是会变化的。 瓶颈资源可以是CPU,可以是内存，可以是锁资源，IO资源：超过最佳线程数-导致资源的竞争，超过最佳线程数-响应时间递增。","tags":[{"name":"高并发","slug":"高并发","permalink":"http://example.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"性能指标","slug":"性能指标","permalink":"http://example.com/tags/%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"}],"categories":[]},{"title":"MySQL-基本架构及原理","date":"2021-08-26T03:51:47.000Z","path":"wiki/MySQL-基本架构及原理/","text":"","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"用了那么久MySQL，我竟然不知道ICP","date":"2021-08-26T03:37:16.000Z","path":"wiki/MySQL-索引条件下推ICP/","text":"用了那么久MySQL，我竟然不知道ICP概述开启ICP，如果部分WHERE条件能使用索引中的字段，MySQL Server 会把这部分下推到存储引擎层，存储引擎通过索引过滤，把满足的行从表中读取出。ICP能减少引擎层访问基表的次数和MySQL Server 访问存储引擎的次数。 ICP 优化的全称是 Index Condition Pushdown Optimization 。 ICP 优化适用于 MySQL 利用索引从表里检索数据的场景。 经过学习了ICP之后我大概对ICP有了初步的理解 👇 关闭ICP情况下的查询流程禁用ICP，存储引擎会通过遍历索引定位基表中的行，然后返回给MySQL Server层，再去为这些数据行进行WHERE后的条件的过滤。 开启ICP之后的执行流程 一开始我对于 ICP只能使用于二级索引，而不能用于主键索引这一限制不太理解。那是因为我没有该明白ICP到底是干什么的？ 我们这边用一些别人的图方便理解。 ICP能减少引擎层访问基表的次数和MySQL Server 访问存储引擎的次数。 这是ICP的关键目的，为什么能减少引擎对于基表的访问呢？因为ICP在引擎层执行了额外的过滤和筛选，使得大量的无效数据查询基表。 因为是二级索引，需要根据主键id去获取到整行的数据。 ICP 原理以 InnoDB 表为例。 在不启用 ICP 的情况下利用二级索引查找数据的过程： 用二级索引查找数据的主键； 用主键回表读取完整的行记录； 利用 where 语句的条件对行记录进行过滤。 启用 ICP 的情况下利用二级索引查找数据的过程为： 用二级索引查找数据的主键； 如果二级索引记录的元组里的列出现在 where 条件里，那么对元组进行过滤； 对索引元组的主键回表读取完整的行记录； 利用 where 语句的剩余条件对行记录进行过滤； ICP 适用的一个隐含前提是二级索引必须是组合索引、且在使用索引进行扫描时只能采用最左前缀匹配原则。组合索引后面的列出现在 where 条件里，因此可以先过滤索引元组、从而减少回表读的数量。 举例对于组合索引 INDEX (zipcode, lastname, firstname)，下面的 SQL 根据最左前缀原则，只能使用到索引的第一列 zipcode，索引的另一列 lastname 出现在 where 条件里，可以采用 ICP 对索引的元组进行过滤，即应用 lastname LIKE &#39;%etrunia%&#39; 条件；然后再回表读取完成的行记录，再对行记录应用 address LIKE &#39;%Main Street%&#39; 条件。 1234SELECT * FROM peopleWHERE zipcode=&#x27;95054&#x27;AND lastname LIKE &#x27;%etrunia%&#x27;AND address LIKE &#x27;%Main Street%&#x27;; ICP使用限制1 当sql需要全表访问时,ICP的优化策略可用于range, ref, eq_ref, ref_or_null 类型的访问数据方法 。 2 支持InnoDB和MyISAM表。 3 ICP只能用于二级索引，不能用于主索引。 4 并非全部where条件都可以用ICP筛选。 如果where条件的字段不在索引列中,还是要读取整表的记录到server端做where过滤。 5 ICP的加速效果取决于在存储引擎内通过ICP筛选掉的数据的比例。 6 5.6 版本的不支持分表的ICP 功能，5.7 版本的开始支持。 7 当sql 使用覆盖索引时，不支持ICP 优化方法。 如何查看是否开启了ICPshow variables like &#39;optimizer_switch&#39;; 1index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,engine_condition_pushdown=on,index_condition_pushdown=on,mrr=on,mrr_cost_based=on,block_nested_loop=on,batched_key_access=off,materialization=on,semijoin=on,loosescan=on,firstmatch=on,duplicateweedout=on,subquery_materialization_cost_based=on,use_index_extensions=on,condition_fanout_filter=on,derived_merge=on,use_invisible_indexes=off,skip_scan=on,hash_join=on,subquery_to_derived=off,prefer_ordering_index=on,hypergraph_optimizer=off,derived_condition_pushdown=on 什么是二级索引1、一级索引索引和数据存储在一起，都存储在同一个B+tree中的叶子节点。一般主键索引都是一级索引。 2、二级索引二级索引树的叶子节点存储的是主键而不是数据。也就是说，在找到索引后，得到对应的主键，再回到一级索引中找主键对应的数据记录。 参考文档https://mp.weixin.qq.com/s/ygvuP35B_sJAlBHuuEJhfg https://mp.weixin.qq.com/s/Vv1gNLh1aLCLDJfEYXvr-g https://blog.51cto.com/qhd2004/1870996?source=drt 二级索引检索过程 https://zhuanlan.zhihu.com/p/137647823","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"MySQL-InnoDB架构","date":"2021-08-26T02:12:37.000Z","path":"wiki/MySQL-InnoDB存储引擎底层结构级原理/","text":"Innodb架构存储引擎是MySQL非常重要的组成部分，它直接影响了一个数据库的性能，是MySQL的绝对核心。 下面是InnoDB的结构图 👇 从上面第二张图可以看到，InnoDB 主要分为两大块： InnoDB In-Memory Structures InnoDB On-Disk Structures 内存和磁盘，让我们先从内存开始。 In-Memory Structures内存模块Buffer Pool正如之前提到的，MySQL 不会直接去修改磁盘的数据，因为这样做太慢了，MySQL 会先改内存，然后记录 redo log，等有空了再刷磁盘，如果内存里没有数据，就去磁盘 load。 而这些数据存放的地方，就是 Buffer Pool。 我们平时开发时，会用 redis 来做缓存，缓解数据库压力，其实 MySQL 自己也做了一层类似缓存的东西。 MySQL 是以「页」（page）为单位从磁盘读取数据的，Buffer Pool 里的数据也是如此，实际上，Buffer Pool 是a linked list of pages，一个以页为元素的链表。 为什么是链表？因为和缓存一样，它也需要一套淘汰算法来管理数据。Buffer Pool 采用基于 LRU（least recently used） 的算法来管理内存。 B+树中一个节点为一页或页的倍数最为合适。为什么呢？ 因为如果一个节点的大小小于1页，那么读取这个节点的时候其实也会读出1页，造成资源的浪费。 如果一个节点的大小大于1页，比如1.2页，那么读取这个节点的时候会读出2页，也会造成资源的浪费。 所以为了不造成浪费，所以最后把一个节点的大小控制在1页、2页、3页、4页等倍数页大小最为合适。 各个数据页可以组成一个双向链表 而每个数据页中的记录又可以组成一个单向链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 所以说，如果我们写 select * from user where username=’小明’ 这样没有进行任何优化的sql语句，默认会这样做： 定位到记录所在的页 需要遍历双向链表，找到所在的页 从所在的页内中查找相应的记录 由于不是根据主键查询，只能遍历所在页的单链表了 Change Bufferchange buffer存放在Buffer Pool中。 上面提到过，如果内存里没有对应「页」的数据，MySQL 就会去把数据从磁盘里 load 出来，如果每次需要的「页」都不同，或者不是相邻的「页」，那么每次 MySQL 都要去 load，这样就很慢了。 于是如果 MySQL 发现你要修改的页，不在内存里，就把你要对页的修改，先记到一个叫 Change Buffer 的地方，同时记录 redo log，然后再慢慢把数据 load 到内存，load 过来后，再把 Change Buffer 里记录的修改，应用到内存（Buffer Pool）中，这个动作叫做 merge；而把内存数据刷到磁盘的动作，叫 purge： merge：Change Buffer -&gt; Buffer Pool purge：Buffer Pool -&gt; Disk Adaptive Hash IndexMySQL 索引，不管是在磁盘里，还是被 load 到内存后，都是 B+ 树，B+ 树的查找次数取决于树的深度。你看，数据都已经放到内存了，还不能“一下子”就找到它，还要“几下子”，这空间牺牲的是不是不太值得？ 尤其是那些频繁被访问的数据，每次过来都要走 B+ 树来查询，这时就会想到，我用一个指针把数据的位置记录下来不就好了？ 这就是「自适应哈希索引」（Adaptive Hash Index）。自适应，顾名思义，MySQL 会自动评估使用自适应索引是否值得，如果观察到建立哈希索引可以提升速度，则建立。 Redo log bufferRedo log buffer里面存储了数据修改所产生的redo log。 On-Disk Structures 磁盘模块磁盘里有什么呢？除了表结构定义和索引，还有一些为了高性能和高可靠而设计的角色，比如 redo log、undo log、Change Buffer，以及 Doublewrite Buffer 等等. Tablespaces表空间Tablespaces 分为五种： The System Tablespace File-Per-Table Tablespaces General Tablespace Undo Tablespaces Temporary Tablespaces 其中，我们平时创建的表的数据，可以存放到 The System Tablespace 、File-Per-Table Tablespaces、General Tablespace 三者中的任意一个地方，具体取决于你的配置和创建表时的 sql 语句。 Doublewrite Buffer如果说 Change Buffer 是提升性能，那么 Doublewrite Buffer 就是保证数据页的可靠性。 怎么理解呢？ 前面提到过，MySQL 以「页」为读取和写入单位，一个「页」里面有多行数据，写入数据时，MySQL 会先写内存中的页，然后再刷新到磁盘中的页。 这时问题来了，假设在某一次从内存刷新到磁盘的过程中，一个「页」刷了一半，突然操作系统或者 MySQL 进程奔溃了，这时候，内存里的页数据被清除了，而磁盘里的页数据，刷了一半，处于一个中间状态，不尴不尬，可以说是一个「不完整」，甚至是「坏掉的」的页。 有同学说，不是有 Redo Log 么？其实这个时候 Redo Log 也已经无力回天，Redo Log 是要在磁盘中的页数据是正常的、没有损坏的情况下，才能把磁盘里页数据 load 到内存，然后应用 Redo Log。而如果磁盘中的页数据已经损坏，是无法应用 Redo Log 的。 所以，MySQL 在刷数据到磁盘之前，要先把数据写到另外一个地方，也就是 Doublewrite Buffer，写完后，再开始写磁盘。Doublewrite Buffer 可以理解为是一个备份（recovery），万一真的发生 crash，就可以利用 Doublewrite Buffer 来修复磁盘里的数据。 Insert Buffer对于主键顺序插入的数据，插入速度很快，因为数据页的存放是按照主键顺序存放的。但是对于非聚集的且不是唯一的索引，数据的插入不是连续的，所以需要离散的访问非聚集索引页，随机读取的存在会导致插入操作性能下降。 MySQL 的插入缓冲，在非聚集索引的插入或更新时，不直接插入到索引页，而是先判断插入的非聚集索引页是否在缓冲池中，若在，则直接插入；如果不在，不会去读数据，而是先放入到一个insert buffer对象中，然后再以一定的频率和情况进行 insert buffer 和 辅助索引页子节点合并操作，这是通常能将多个插入合并到一个操作中，这就大大提高了对于非聚集索引插入的性能。 使用Insert Buffer的场景1、索引是辅助索引 2、索引不是唯一的 因为在插入时，数据库并不去查找插入记录的唯一性，否则就需要离散的读取数据，这使 insert buffer 失去了意义。 风险点如果数据库宕机时还有大量的缓存没合并到实际的索引中去，恢复这些数据可能需要很长的时间 flush neighbor page当刷新一个脏页时，Innodb存储引擎会 检测该页所在区的所有页，如果是脏页，那么一起进行刷新 innodb_fast_shutdown参数innodb_fast_shutdown参数有三个值 如下👇 0 表示mysql数据库关闭时，innodb需要完成所有的full purge 和 merge insert buffer，并将所有的脏页刷新会磁盘。耗时长 1 默认值，表示不需要完成 full purge 和 merge insert buffer 操作，但是在缓冲池中的一些数据还是会刷新会磁盘。 2 表示不完成 full purge 和 merge insert buffer 操作，也不刷新脏页，而是将日志都写入日志。这样不会有任何的事务丢失，但是下次数据库启动时，需要进行恢复操作 后台线程InnoDB 是多线程模型，不同的后台线程处理不同的任务。 Master Thread核心线程，负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲、Undo页的回收。 IO ThreadInnoDB 使用AIO处理IO请求，提高数据库性能，IO Thread负责这些请求的回调。 IO Thread 包括write/read/insert buffer/log io thread Purge Thread事务被提交后，其使用的undolog可能不再需要，需要 purge thread来回收已经使用并分配的undo页 Page Cleaner Thread减轻原Master Thrad 的工作及对于用户查询线程的阻塞，进一步提供InnoDB存储引擎的性能 checkpoint什么是checkpoint页面操作先在内存缓冲区，再刷新到磁盘，如果刷新磁盘时发生的宕机，那么数据将丢失。为了解决这个文件，当前事务数据库普遍使用 write ahead log策略，即事务提交时，先写重做日志，再修改页。这样即使宕机，也可以通过重做日志来完成数据的恢复。但是重做日志没有redis的redo功能，对于运行时间较长或者提交较大的重做日志恢复是非常耗时的，所以需要 CheckPoint 解决 Checkpoint 解决的问题1、缓冲池不够用时 ，将脏页刷新到磁盘 2、重做日志不可用时，刷新脏页 3、数据库只需要到checkpoint后的日志进行恢复，缩短数据库的恢复时间","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"MySQL-索引","date":"2021-08-25T06:48:18.000Z","path":"wiki/MySQL-索引/","text":"MySQL中支持的索引之前问过存储引擎是数据库层面的还是数据表层面的？ 那么现在也同样问一个问题，索引是存储引擎层面的还是服务器层面实现的？答案是存储引擎层面的。 什么是索引官方介绍索引是帮助MySQL高效获取数据的数据结构。更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度。 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往是存储在磁盘上的文件中的（可能存储在单独的索引文件中，也可能和数据一起存储在数据文件中）。 我们通常所说的索引，包括聚集索引、覆盖索引、组合索引、前缀索引、唯一索引等，没有特别说明，默认都是使用B+树结构组织（多路搜索树，并不一定是二叉的）的索引。 索引的优缺点优势：1、可以提高数据检索的效率，降低数据库的IO成本，类似于书的目录。 2、通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗。 3、被索引的列会自动进行排序，包括【单列索引】和【组合索引】，只是组合索引的排序要复杂一些。如果按照索引列的顺序进行排序，对应order by语句来说，效率就会提高很多。 劣势：1、索引会占据磁盘空间 2、索引虽然会提高查询效率，但是会降低更新表的效率。比如每次对表进行增删改操作，MySQL不仅要保存数据，还有保存或者更新对应的索引文件。 一个表最多可以创建多少个索引？ 16个 索引的类型主键索引索引列中的值必须是唯一的，不允许有空值。InnoDB下强烈推荐使用数值类型的自增主键。这个和b+数的排序有关。 普通索引MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和空值。 唯一索引索引列中的值必须是唯一的，但是允许为空值。 全文索引只能在文本类型CHAR,VARCHAR,TEXT类型字段上创建全文索引。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引。MyISAM和InnoDB中都可以使用全文索引。 空间索引MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。 前缀索引在文本类型如CHAR,VARCHAR,TEXT类列上创建索引时，可以指定索引列的长度，但是数值类型不能指定。 组合索引组合索引的使用，需要遵循最左前缀匹配原则（最左匹配原则）。一般情况下在条件允许的情况下使用组合索引替代多个单列索引使用。 聚簇索引（聚集索引）在MyISAM存储引擎中，主键索引上存储的是数据在磁盘上的地址。 在InnoDB存储引擎中，主键索引上存储的就是整行数据。 辅助索引在MyISAM存储引擎中，辅助索引其实和主键索引一样，也是存储的数据的磁盘地址。区别在主键索引的键值必须唯一，而辅助索引的键值可以重复。 在InnoDB存储引擎中，辅助索引存储的是数据的主键。查到主键之后，在根据主键值查找主键索引上存储的数据。 这会发生一种现象叫回表，会增加性能消耗，解决这种问题的方法就是 覆盖索引，也就是把要查询的字段也添加到索引中。 跳跃索引一般情况下，如果表users有复合索引idx_status_create_time，我们都知道，单独用create_time去查询，MySQL优化器是不走索引，所以还需要再创建一个单列索引idx_create_time。用过Oracle的同学都知道，是可以走索引跳跃扫描（Index Skip Scan），在MySQL 8.0也实现Oracle类似的索引跳跃扫描，在优化器选项也可以看到skip_scan=on。 1| optimizer_switch |use_invisible_indexes=off,skip_scan=on,hash_join=on | 适合复合索引前导列唯一值少，后导列唯一值多的情况，如果前导列唯一值变多了，则MySQL CBO不会选择索引跳跃扫描，取决于索引列的数据分表情况。 12345mysql&gt; explain select id, user_id，status, phone from users where create_time &gt;=&#x27;2021-01-02 23:01:00&#x27; and create_time &lt;= &#x27;2021-01-03 23:01:00&#x27;;+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+----| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+----| 1 | SIMPLE | users | NULL | range | idx_status_create_time | idx_status_create_time | NULL | NULL | 15636 | 11.11 | Using where; Using index for skip scan| 也可以通过 optimizer_switch=&#39;skip_scan=off&#39;来关闭索引跳跃扫描特性。 B TreeB树有什么特点呢？ 1、所有的键值分布在整棵树上面 2、搜索有可能在非叶子节点结束，在关键字全集内做一次查找，性能接近于二分查找 3、每个节点最多拥有m个子树，根节点至少有2个子树，分支节点至少拥有m / 2个子树（除根节点和叶子节点都是分支节点） 4、所有叶子节点都在同一层，每个节点最多可以有m - 1个key，并且按照升序排序 B Tree 指的是 Balance Tree，也就是平衡树，平衡树是一颗查找树，并且所有叶子节点位于同一层。 图例说明1、每个节点占据一个磁盘块，一个节点上有两个升序排序的关键字K和三个指向子树节点的指针P，指针P存储子节点所在的磁盘块的地址。 2、两个关键字K划分三个范围域对应三个指针指向子树的数据范围 查找关键字的过程1、根据根节点查找磁盘块1，读入到内存（第一次磁盘IO） 2、比较关键字K22在（K25，K50）区间，找到磁盘块1的P2指针 3、根据磁盘块1的P2指针找到磁盘块3，读到内存（第二次磁盘IO） 4、继续比较K32,锁定在K30,K38之间，找到磁盘3的P2指针 5、根据磁盘块2的P2指针锁定磁盘块8，读到内存（第三次磁盘IO） 6、在磁盘块8的关键字列表中找到关键字K32 B树的缺点1、每个节点都存储key和数据，而数据一般又会占用相对较多的空间，这样导致每个节点锁存储的关键字数量较少，最终导致树的高度比较高 2、当数据量较大的时候，由于树的高度比较深，造成查询的性能极差 B+Tree思考一下问题🤔 为什么MySQL使用B+Tree这种数据结构来作为索引结构呢？ 为什么不使用二叉树呢？为什么不使用B Tree呢？ B+树的特点B+ Tree 是 B 树的一种变形，它是基于 B Tree 和叶子节点顺序访问指针进行实现，通常用于数据库和操作系统的文件系统中。 1、B+ 树有两种类型的节点：内部节点（也称索引节点）和叶子节点，内部节点就是非叶子节点，内部节点不存储数据，只存储索引，相对于B树，每个节点可以存储更多的节点。这样可以大大降低树的高度，同时把数据范围数据都存在叶子节点。 2、内部节点中的 key 都按照从小到大的顺序排列，对于内部节点中的一个 key，左子树中的所有 key 都小于它，右子树中的 key 都大于等于它，叶子节点的记录也是按照从小到大排列的。 3、叶子节点两两指针互相链接（符合磁盘的预读特性），顺序查询性能更高。 B+树的优点 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。 哈希索引哈希索引能以 O(1) 时间进行查找，但是失去了有序性： 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。 查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 索引的优化1、索引不支持表达式 2、在需要使用多个字段作为查询条件的时候，可以使用多列索引，注意最左匹配原则 3、索引列的顺序要注意，区分度大的列写在索引的前面，效率会比较高 4、前缀索引，对于Blog，Text之类的大字段，必须使用前缀索引，只索引开始的部分字符，前缀长度的选取需要根据索引选择性来确定。 5、覆盖索引，查询的字段尽量保存在索引，避免回表。 索引的优点 大大减少了服务器需要扫描的数据行数。 帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。 索引的使用条件 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效； 对于中到大型的表，索引就非常有效； 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 为什么对于非常小的表，大部分情况下简单的全表扫描比建立索引更高效？ 如果一个表比较小，那么显然直接遍历表比走索引要快（因为需要回表）。 ⚠️注：首先，要注意这个答案隐含的条件是查询的数据不是索引的构成部分，否也不需要回表操作。其次，查询条件也不是主键，否则可以直接从聚簇索引中拿到数据。 https://mp.weixin.qq.com/s/faOaXRQM8p0kwseSHaMCbg","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"MySQL-存储引擎","date":"2021-08-25T06:19:10.000Z","path":"wiki/MySQL-存储引擎/","text":"存储引擎MySQL支持很多种存储引擎 👇 有一个面试题：存储引擎是数据库级别的还是数据表级别的？ 看上图应该就知道了吧！ 数据表级别的 InnoDBInnoDB 是 MySQL 默认的事务型存储引擎，只要在需要它不支持的特性时，才考虑使用其他存储引擎。 InnoDB 采用 MVCC 来支持高并发，并且实现了四个标准隔离级别(未提交读、提交读、可重复读、可串行化)。其默认级别时可重复读（REPEATABLE READ），在可重复读级别下，通过 MVCC + Next-Key Locking 防止幻读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对主键查询有很高的性能。 InnoDB 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读（局部性原理），能够自动在内存中创建 hash 索引以加速读操作的自适应哈希索引，以及能够加速插入操作的插入缓冲区等。 InnoDB 支持真正的在线热备份，MySQL 其他的存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合的场景中，停止写入可能也意味着停止读取。 支持行级锁，间隙锁。 MyISAM设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 InnoDB 和 MyISAM 的比较 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。 外键：InnoDB 支持外键。 备份：InnoDB 支持在线热备份。 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 索引：InnoDB支持聚簇索引，MyISAM支持非聚簇索引 其它特性：MyISAM 支持压缩表和空间数据索引。 由上图可以看出，InnoDB的叶子节点存储的是数据，而MyISAM的叶子节点存放的是数据所在的地址。 InnoDB是通过B+树结构对主键创建索引，然后叶子节点中存储记录，如果没有主键，则会选择一个唯一键，如果也没有唯一键，则会自动生成一个6位数的row_id作为主键 如果创建索引的键是其他字段，那么叶子节点中存储的是该记录的主键，通过主键索引找到对应的记录，此过程称之为回表。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"MySQL系列合集","date":"2021-08-25T06:18:51.000Z","path":"wiki/MySQL系列合集/","text":"技术目录MySQL基础知识 MySQL系统架构 QueryCache是什么，为什么MySQL8废弃了？ MySQL支持的存储引擎（InnoDB&amp;MyISAM） InnoDB底层结构以及原理 MySQL事务级别以及实现原理 MySQL性能调优 SQL调优总结 MySQL支持的索引 MySQL中的各种锁你都知道吗 为什么有MVCC，解决了什么问题 数据库主从原理以及实现 分库分表原理以及实战 参考文章https://github.com/AobingJava/JavaFamily http://simpleframework.net/news/list?t=mysql 优美结构图https://www.processon.com/mindmap/6127055f6376896024deffae https://www.processon.com/view/5e4a00f7e4b0369b9163d2c8?fromnew=1 https://www.processon.com/view/60af54f21e08531e9c899e74?fromnew=1 https://www.processon.com/view/60c167ac0e3e7468f4c4de93?fromnew=1 https://www.processon.com/view/609bd4041e08533e429e6a42?fromnew=1","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"Redis-Key过期监听器","date":"2021-08-25T03:40:30.000Z","path":"wiki/Redis-Key过期监听器/","text":"背景有一个需求，我在某平台发布了一片文章，需要判断这片文章在发布之后，10min，30min，1h，3h，1d，3d时间点的点赞数量和关注数量，但是呢，平台没有提供信息统计的功能，那么只能我定期去查看。 那么如何实现这个功能或者需求呢？ 当时首先想到的是定时任务轮训，这种方式其实比较简单，就是搂数据库，判断时间就完事了，同时记录这片文章定时任务执行了多少次，超过一定次数之后，设置标志位，那么下次就不需要筛选这些文章了。 但是这种方式的缺点很明显，首先定时任务执行的频率改如何设置呢，应该是最小时间10min。也就是每10min搂一次库，查出来的数据，再去执行业务逻辑。当数据量很大的时候，这个定时任务就会显得比较重了。 于是我想到了基于事件触发的方式去解决这个问题，比如延时队列，redis过期策略啊等等，应该有很多。 这里说到延时队列，为什么我没有用JDK自带的DelayQueue呢，毕竟这些数据都是放在内存中，还是解决不了内存的问题。 还有通过redis的sort set数据结果来做的方式，score存的是时间戳，这种方式其实要比直接搂数据库要好的多。 最后我选择使用redis过期监听策略来实现这个需求，各位大佬们有什么别的方案呢？ redis过期监听首先设置一下redis的通知事件需要设置redis配置文件 notify-keyspace-events Ex 12345678910111213141516171819202122# For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo# K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# t Stream commands# d Module key type events# m Key-miss events (Note: It is not included in the &#x27;A&#x27; class)# A Alias for g$lshzxetd, so that the &quot;AKE&quot; string means all the events# (Except key-miss events which are excluded from &#x27;A&#x27; due to their# unique nature). 或者使用命令 CONFIG set notify-keyspace-events Ex Springboot 集成redis12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Configurationpublic class RedisConfig &#123; @Bean @Primary public &lt;T&gt; RedisTemplate&lt;String, T&gt; getRedisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;String, T&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new GenericFastJsonRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setHashValueSerializer(new GenericFastJsonRedisSerializer()); return redisTemplate; &#125; @Bean public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;Object, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new GenericFastJsonRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setHashValueSerializer(new GenericFastJsonRedisSerializer()); return redisTemplate; &#125; @Bean public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; StringRedisTemplate stringRedisTemplate = new StringRedisTemplate(); stringRedisTemplate.setConnectionFactory(redisConnectionFactory); return stringRedisTemplate; &#125; @Bean public RedisScript&lt;Boolean&gt; hitMaxScript() &#123; DefaultRedisScript&lt;Boolean&gt; redisScript = new DefaultRedisScript&lt;&gt;(); redisScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(&quot;scripts/hitmax.lua&quot;))); redisScript.setResultType(Boolean.class); return redisScript; &#125; @Bean RedisMessageListenerContainer container(RedisConnectionFactory redisConnectionFactory) &#123; RedisMessageListenerContainer container = new RedisMessageListenerContainer(); container.setConnectionFactory(redisConnectionFactory); container.setTaskExecutor(executor()); Topic topic = new PatternTopic(RedisKeyExpirationListener.LISTENER_PATTERN); container.addMessageListener(new RedisKeyExpirationListener(), topic); return container; &#125; @Bean public Executor executor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setMaxPoolSize(20); executor.setQueueCapacity(100); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(&quot;V-Thread&quot;); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.initialize(); return executor; &#125;&#125; 自定义监听器重写onMessage方法123456789101112@Componentpublic class RedisKeyExpirationListener implements MessageListener &#123; public static final String LISTENER_PATTERN = &quot;__key*@*__:*&quot;; @Override public void onMessage(Message message, byte[] pattern) &#123; System.err.println(&quot;触发监听器。。。。。。&quot;); String body = new String(message.getBody()); String channel = new String(message.getChannel()); System.out.println(&quot;onMessage &gt;&gt; &quot;+String.format(&quot;channel: %s, body: %s, bytes: %s&quot;,channel,body,new String(pattern))); &#125;&#125; 测试 项目控制台： ⚠️ 监听key过期时间是不能获取key的value的，因为这个时间是key过期才触发的，所以我们把关键信息放到key上就行了","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[]},{"title":"Web开发基础-Servlet","date":"2021-08-24T08:00:44.000Z","path":"wiki/Web开发基础-Servlet/","text":"1、什么是servlet什么是Serlvet？ Servlet其实就是一个遵循Servlet开发的java类。Serlvet是由服务器调用的，运行在服务器端。 我们编写java程序想要在网上实现 聊天、发帖、这样一些的交互功能，普通的java技术是非常难完成的。sun公司就提供了Serlvet这种技术供我们使用。 2、servlet生命周期Servlet生命周期可分为5个步骤 加载Servlet。当Tomcat第一次访问Servlet的时候，Tomcat会负责创建Servlet的实例 初始化。当Servlet被实例化后，Tomcat会调用init()方法初始化这个对象 处理服务。当浏览器访问Servlet的时候，Servlet 会调用service()方法处理请求 销毁。当Tomcat关闭时或者检测到Servlet要从Tomcat删除的时候会自动调用destroy()方法，让该实例释放掉所占的资源。一个Servlet如果长时间不被使用的话，也会被Tomcat自动销毁 卸载。当Servlet调用完destroy()方法后，等待垃圾回收。如果有需要再次使用这个Servlet，会重新调用init()方法进行初始化操作。 简单总结：只要访问Servlet，service()就会被调用。init()只有第一次访问Servlet的时候才会被调用。 destroy()只有在Tomcat关闭的时候才会被调用。 3、servlet调用流程 4、Servlet是单例的4.1 为什么Servlet是单例的浏览器多次对Servlet的请求，一般情况下，服务器只创建一个Servlet对象，也就是说，Servlet对象一旦创建了，就会驻留在内存中，为后续的请求做服务，直到服务器关闭。 4.2 每次访问请求对象和响应对象都是新的对于每次访问请求，Servlet引擎都会创建一个新的HttpServletRequest请求对象和一个新的HttpServletResponse响应对象，然后将这两个对象作为参数传递给它调用的Servlet的service()方法，service方法再根据请求方式分别调用doXXX方法。 4.3 线程安全问题当多个用户访问Servlet的时候，服务器会为每个用户创建一个线程。当多个用户并发访问Servlet共享资源的时候就会出现线程安全问题。 原则： 如果一个变量需要多个用户共享，则应当在访问该变量的时候，加同步机制synchronized (对象){} 如果一个变量不需要共享，则直接在 doGet() 或者 doPost()定义.这样不会存在线程安全问题 5、ServletConfig对象5.1 ServletConfig对象有什么用？ 通过此对象可以读取web.xml中配置的初始化参数。 现在问题来了，为什么我们要把参数信息放到web.xml文件中呢？我们可以直接在程序中都可以定义参数信息，搞到web.xml文件中又有什么好处呢？ 好处就是：能够让你的程序更加灵活【更换需求，更改配置文件web.xml即可，程序代码不用改】 5.2 获取web.xml文件配置的参数信息为Demo1这个Servlet配置一个参数，参数名是name，值是zhongfucheng 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;servlet&gt; &lt;servlet-name&gt;Demo1&lt;/servlet-name&gt; &lt;servlet-class&gt;zhongfucheng.web.Demo1&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;name&lt;/param-name&gt; &lt;param-value&gt;zhongfucheng&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;Demo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/Demo1&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 在Servlet中获取ServletConfig对象，通过ServletConfig对象获取在web.xml文件配置的参数 12ServletConfig config = this.getServletConfig();String name = config.getInitParameter(&quot;name&quot;); 6、ServletContext对象6.1 什么是ServletContext对象？当Tomcat启动的时候，就会创建一个ServletContext对象。它代表着当前web站点 6.2 ServletContext有什么用？ ServletContext既然代表着当前web站点，那么所有Servlet都共享着一个ServletContext对象，所以Servlet之间可以通过ServletContext实现通讯。 ServletConfig获取的是配置的是单个Servlet的参数信息，ServletContext可以获取的是配置整个web站点的参数信息 利用ServletContext读取web站点的资源文件 实现Servlet的转发【用ServletContext转发不多，主要用request转发】 7、什么是会话技术 基本概念: 指用户开一个浏览器，访问一个网站,只要不关闭该浏览器，不管该用户点击多少个超链接，访问多少资源，直到用户关闭浏览器，整个这个过程我们称为一次会话. 7.1 什么是Cookie Cookie是由W3C组织提出，最早由netscape社区发展的一种机制 网页之间的交互是通过HTTP协议传输数据的，而Http协议是无状态的协议。无状态的协议是什么意思呢？一旦数据提交完后，浏览器和服务器的连接就会关闭，再次交互的时候需要重新建立新的连接。 服务器无法确认用户的信息，于是乎，W3C就提出了：给每一个用户都发一个通行证，无论谁访问的时候都需要携带通行证，这样服务器就可以从通行证上确认用户的信息。通行证就是Cookie 7.11 Cookie的流程浏览器访问服务器，如果服务器需要记录该用户的状态，就使用response向浏览器发送一个Cookie，浏览器会把Cookie保存起来。当浏览器再次访问服务器的时候，浏览器会把请求的网址连同Cookie一同交给服务器。 7.1.2 Cookie API Cookie类用于创建一个Cookie对象 response接口中定义了一个addCookie方法，它用于在其响应头中增加一个相应的Set-Cookie头字段 request接口中定义了一个getCookies方法，它用于获取客户端提交的Cookie 常用的Cookie方法： public Cookie(String name,String value) setValue与getValue方法 setMaxAge与getMaxAge方法 setPath与getPath方法 setDomain与getDomain方法 getName方法 7.1.3 cookie使用方式12345response.setContentType(&quot;text/html;charset=UTF-8&quot;);Cookie cookie = new Cookie(&quot;username&quot;, &quot;zhongfucheng&quot;);cookie.setMaxAge(1000);response.addCookie(cookie);response.getWriter().write(&quot;我已经向浏览器发送了一个Cookie&quot;); 7.1.4 Cookie不可跨域名性 很多人在初学的时候可能有一个疑问：在访问Servlet的时候浏览器是不是把所有的Cookie都带过去给服务器，会不会修改了别的网站的Cookie 答案是否定的。Cookie具有不可跨域名性。浏览器判断一个网站是否能操作另一个网站的Cookie的依据是域名。所以一般来说，当我访问baidu的时候，浏览器只会把baidu颁发的Cookie带过去，而不会带上google的Cookie。 7.1.5 Cookie的有效期Cookie的有效期是通过setMaxAge()来设置的。 如果MaxAge为正数，浏览器会把Cookie写到硬盘中，只要还在MaxAge秒之前，登陆网站时该Cookie就有效【不论关闭了浏览器还是电脑】 如果MaxAge为负数，Cookie是临时性的，仅在本浏览器内有效，关闭浏览器Cookie就失效了，Cookie不会写到硬盘中。Cookie默认值就是-1。这也就为什么在我第一个例子中，如果我没设置Cookie的有效期，在硬盘中就找不到对应的文件。 如果MaxAge为0，则表示删除该Cookie。Cookie机制没有提供删除Cookie对应的方法，把MaxAge设置为0等同于删除Cookie 7.1.6 Cookie的域名Cookie的domain属性决定运行访问Cookie的域名。domain的值规定为“.域名” Cookie的隐私安全机制决定Cookie是不可跨域名的。也就是说www.baidu.com和www.google.com之间的Cookie是互不交接的。**即使是同一级域名，不同二级域名也不能交接**。 7.2 什么是SessionSession 是另一种记录浏览器状态的机制。不同的是Cookie保存在浏览器中，Session保存在服务器中。用户使用浏览器访问服务器的时候，服务器把用户的信息以某种的形式记录在服务器，这就是Session。 7.2.1 为什么要使用Session技术？Session比Cookie使用方便，Session可以解决Cookie解决不了的事情【Session可以存储对象，Cookie只能存储字符串。】。 7.2.2 Session API long getCreationTime();【获取Session被创建时间】 String getId();【获取Session的id】 long getLastAccessedTime();【返回Session最后活跃的时间】 ServletContext getServletContext();【获取ServletContext对象】 void setMaxInactiveInterval(int var1);【设置Session超时时间】 int getMaxInactiveInterval();【获取Session超时时间】 Object getAttribute(String var1);【获取Session属性】 Enumeration getAttributeNames();【获取Session所有的属性名】 void setAttribute(String var1, Object var2);【设置Session属性】 void removeAttribute(String var1);【移除Session属性】 void invalidate();【销毁该Session】 boolean isNew();【该Session是否为新的】 7.2.3 session作为域对象Session有着request和ServletContext类似的方法。其实Session也是一个域对象。Session作为一种记录浏览器状态的机制，只要Session对象没有被销毁，Servlet之间就可以通过Session对象实现通讯 一般来讲，当我们要存进的是用户级别的数据就用Session，那什么是用户级别呢？只要浏览器不关闭，希望数据还在，就使用Session来保存。 7.2.4 session生命周期Session的生命周期和有效期 Session在用户第一次访问服务器Servlet，jsp等动态资源就会被自动创建，Session对象保存在内存里，这也就为什么上面的例子可以直接使用request对象获取得到Session对象。 如果访问HTML,IMAGE等静态资源Session不会被创建。 Session生成后，只要用户继续访问，服务器就会更新Session的最后访问时间，无论是否对Session进行读写，服务器都会认为Session活跃了一次。 由于会有越来越多的用户访问服务器，因此Session也会越来越多。为了防止内存溢出，服务器会把长时间没有活跃的Session从内存中删除，这个时间也就是Session的超时时间。 Session的超时时间默认是30分钟，有三种方式可以对Session的超时时间进行修改 1、修改tomcat的web.xml 2、修改项目的web.xml 3、httpSession.setMaxInactiveInterval(60); 7.3 Session和Cookie的区别 从存储方式上比较 Cookie只能存储字符串，如果要存储非ASCII字符串还要对其编码。 Session可以存储任何类型的数据，可以把Session看成是一个容器 从隐私安全上比较 Cookie存储在浏览器中，对客户端是可见的。信息容易泄露出去。如果使用Cookie，最好将Cookie加密 Session存储在服务器上，对客户端是透明的。不存在敏感信息泄露问题。 从有效期上比较 Cookie保存在硬盘中，只需要设置maxAge属性为比较大的正整数，即使关闭浏览器，Cookie还是存在的 Session的保存在服务器中，设置maxInactiveInterval属性值来确定Session的有效期。并且Session依赖于名为JSESSIONID的Cookie，该Cookie默认的maxAge属性为-1。如果关闭了浏览器，该Session虽然没有从服务器中消亡，但也就失效了。 从对服务器的负担比较 Session是保存在服务器的，每个用户都会产生一个Session，如果是并发访问的用户非常多，是不能使用Session的，Session会消耗大量的内存。 Cookie是保存在客户端的。不占用服务器的资源。像baidu、Sina这样的大型网站，一般都是使用Cookie来进行会话跟踪。 从浏览器的支持上比较 如果浏览器禁用了Cookie，那么Cookie是无用的了！ 如果浏览器禁用了Cookie，Session可以通过URL地址重写来进行会话跟踪。 从跨域名上比较 Cookie可以设置domain属性来实现跨域名 Session只在当前的域名内有效，不可跨域名 8、forward和redirect的区别8.1 实际发生位置不同，地址栏不同1、转发是发生在服务器的2、转发是由服务器进行跳转的，细心的朋友会发现，在转发的时候，浏览器的地址栏是没有发生变化的，在我访问Servlet111的时候，即使跳转到了Servlet222的页面，浏览器的地址还是Servlet111的。也就是说浏览器是不知道该跳转的动作，转发是对浏览器透明的。通过上面的转发时序图我们也可以发现，实现转发只是一次的http请求，一次转发中request和response对象都是同一个。这也解释了，为什么可以使用request作为域对象进行Servlet之间的通讯。3、重定向是发生在浏览器的 - 重定向是由浏览器进行跳转的，进行重定向跳转的时候，浏览器的地址会发生变化的。曾经介绍过：实现重定向的原理是由response的状态码和Location头组合而实现的。这是由浏览器进行的页面跳转实现重定向会发出两个http请求，request域对象是无效的，因为它不是同一个request对象 8.2 用法不同很多人都搞不清楚转发和重定向的时候，资源地址究竟怎么写。有的时候要把应用名写上，有的时候不用把应用名写上。很容易把人搞晕。记住一个原则： 给服务器用的直接从资源名开始写，给浏览器用的要把应用名写上 request.getRequestDispatcher(&quot;/资源名 URI&quot;).forward(request,response)转发时”/“代表的是本应用程序的根目录【zhongfucheng】response.send(&quot;/web应用/资源名 URI&quot;); 重定向时”/“代表的是webapps目录 8.3 能够去往的URL的范围不一样转发是服务器跳转只能去往当前web应用的资源重定向是服务器跳转，可以去往任何的资源 8.4 传递数据的类型不同转发的request对象可以传递各种类型的数据，包括对象重定向只能传递字符串 8.5 跳转的时间不同转发时：执行到跳转语句时就会立刻跳转重定向：整个页面执行完之后才执行跳转 那么转发(forward)和重定向(redirect)使用哪一个？ 根据上面说明了转发和重定向的区别也可以很容易概括出来。转发是带着转发前的请求的参数的。重定向是新的请求。 8.6 典型的应用场景： 转发: 访问 Servlet 处理业务逻辑，然后 forward 到 jsp 显示处理结果，浏览器里 URL 不变 重定向: 提交表单，处理成功后 redirect 到另一个 jsp，防止表单重复提交，浏览器里 URL 变了 Servlet第六篇【Session介绍、API、生命周期、应用、与Cookie区别】(修订版)","tags":[],"categories":[]},{"title":"服务发现的基本原理","date":"2021-08-24T06:30:25.000Z","path":"wiki/服务发现的基本原理/","text":"参考资料https://juejin.cn/post/6844903580999090183 使用Consul做服务发现的若干姿势 注册中心篇（三）：Consul 服务发现的底层实现","tags":[{"name":"服务发现","slug":"服务发现","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/"}],"categories":[]},{"title":"JVM-性能调优","date":"2021-08-23T02:13:43.000Z","path":"wiki/JVM-性能调优/","text":"1、 JVM性能调优的6大步骤，及关键调优参数详解","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"Arthas-Java诊断神器","date":"2021-08-20T08:28:32.000Z","path":"wiki/Arthas-Java诊断神器/","text":"Arthas-Java诊断神器 官方文档地址 👉 https://arthas.aliyun.com/doc/index.html# 1. 安装arthaswget https://arthas.aliyun.com/arthas-boot.jar java -jar arthas-boot.jar --target-ip 0.0.0.0 如果你的机器没有任何java进程在运行，会提示如下错误 👇 正常启动如下： ⚠️ 由于我们仅仅启动了一个java进程，所有这里就只有一个。输入1回车即可。 42423就是我们的java进程号 2. 查看JVM信息2.1 syspropsysprop 可以打印所有的System Properties信息。 也可以指定单个key： sysprop java.version 也可以通过grep来过滤： sysprop | grep user 可以设置新的value： sysprop testKey testValue 2.2 sysenvsysenv 命令可以获取到环境变量。和sysprop命令类似。 2.3 jvmjvm 命令会打印出JVM的各种详细信息。 2.4 dashboarddashboard 命令可以查看当前系统的实时数据面板。 输入 Q 或者 Ctrl+C 可以退出dashboard命令。 3. 查看线程相关3.1 查看线程列表thread 3.2 查看线程栈信息thread 18 3.3 查看5秒内的CPU使用率top n线程栈thread -n 3 -i 5000 3.4 查找线程是否有阻塞thread -b 4. sc/sm 查看已加载的类下面介绍Arthas里查找已加载类的命令。 4.1 sc 查找到所有JVM已经加载到的类如果搜索的是接口，还会搜索所有的实现类。比如查看所有的Filter实现类： sc javax.servlet.Filter 通过-d参数，可以打印出类加载的具体信息，很方便查找类加载问题。 sc -d javax.servlet.Filter sc支持通配，比如搜索所有的StringUtils： sc *StringUtils 4.2 sm 查找类的具体函数sm java.math.RoundingMode 通过-d参数可以打印函数的具体属性： sm -d java.math.RoundingMode 也可以查找特定的函数，比如查找构造函数： sm java.math.RoundingMode &lt;init&gt; 5. Jad反编译可以通过 jad 命令来反编译代码： jad com.example.demo.arthas.user.UserController 通过--source-only参数可以只打印出在反编译的源代码： jad --source-only com.example.demo.arthas.user.UserController 6. Ognl动态代码在Arthas里，有一个单独的ognl命令，可以动态执行代码。 6.1 调用static函数ognl &#39;@java.lang.System@out.println(&quot;hello ognl&quot;)&#39; 可以检查Terminal里的进程输出，可以发现打印出了hello ognl。 6.2 查找UserController的ClassLoader123sc -d com.example.demo.arthas.user.UserController | grep classLoaderHash$ sc -d com.example.demo.arthas.user.UserController | grep classLoaderHash classLoaderHash 1be6f5c3 注意hashcode是变化的，需要先查看当前的ClassLoader信息，提取对应ClassLoader的hashcode。 如果你使用-c，你需要手动输入hashcode：-c &lt;hashcode&gt; $ ognl -c 1be6f5c3 @com.example.demo.arthas.user.UserController@logger 对于只有唯一实例的ClassLoader可以通过--classLoaderClass指定class name，使用起来更加方便： 123456$ ognl --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader @org.springframework.boot.SpringApplication@logger@Slf4jLocationAwareLog[ FQCN=@String[org.apache.commons.logging.LogAdapter$Slf4jLocationAwareLog], name=@String[org.springframework.boot.SpringApplication], logger=@Logger[Logger[org.springframework.boot.SpringApplication]],] --classLoaderClass 的值是ClassLoader的类名，只有匹配到唯一的ClassLoader实例时才能工作，目的是方便输入通用命令，而-c &lt;hashcode&gt;是动态变化的。 6.3 获取静态类的静态字段获取UserController类里的logger字段： 1ognl --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader @com.example.demo.arthas.user.UserController@logger 还可以通过-x参数控制返回值的展开层数。比如： 1ognl --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader -x 2 @com.example.demo.arthas.user.UserController@logger 6.4 执行多行表达式，赋值给临时变量，返回一个List123456ognl &#x27;#value1=@System@getProperty(&quot;java.home&quot;), #value2=@System@getProperty(&quot;java.runtime.name&quot;), &#123;#value1, #value2&#125;&#x27;$ ognl &#x27;#value1=@System@getProperty(&quot;java.home&quot;), #value2=@System@getProperty(&quot;java.runtime.name&quot;), &#123;#value1, #value2&#125;&#x27;@ArrayList[ @String[/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre], @String[Java(TM) SE Runtime Environment],] 6.5 更多在Arthas里ognl表达式是很重要的功能，在很多命令里都可以使用ognl表达式。 一些更复杂的用法，可以参考： OGNL特殊用法请参考：https://github.com/alibaba/arthas/issues/71 OGNL表达式官方指南：https://commons.apache.org/proper/commons-ognl/language-guide.html 7. Watch查看命令7.1 如何使用watch com.example.demo.arthas.user.UserController * &#39;&#123;params, throwExp&#125;&#39; 执行完之后，会阻塞，此时如果有请求进来，发生一场的话，就会看到异常信息。 如果想把获取到的结果展开，可以用-x参数： watch com.example.demo.arthas.user.UserController * &#39;&#123;params, throwExp&#125;&#39; -x 2 7.2 返回值表达式在上面的例子里，第三个参数是返回值表达式，它实际上是一个ognl表达式，它支持一些内置对象： loader clazz method target params returnObj throwExp isBefore isThrow isReturn 你可以利用这些内置对象来组成不同的表达式。比如返回一个数组： 1watch com.example.demo.arthas.user.UserController * &#x27;&#123;params[0], target, returnObj&#125;&#x27; 更多参考： https://arthas.aliyun.com/doc/advice-class.html 7.3 条件表达式watch命令支持在第4个参数里写条件表达式，比如： watch com.example.demo.arthas.user.UserController * returnObj &#39;params[0] &gt; 100&#39; 当访问 localhost:80/user/1时，watch命令没有输出 当访问localhost:80/user/101时，watch会打印出结果。 7.4 当异常时捕获watch命令支持-e选项，表示只捕获抛出异常时的请求： watch com.example.demo.arthas.user.UserController * &quot;&#123;params[0],throwExp&#125;&quot; -e 7.5 按照耗时进行过滤watch命令支持按请求耗时进行过滤，比如： watch com.example.demo.arthas.user.UserController * &#39;&#123;params, returnObj&#125;&#39; &#39;#cost&gt;200&#39; 8. 热更新代码下面介绍通过jad/mc/redefine 命令实现动态更新代码的功能。 目前，访问 http://localhost/user/0 ，会返回500异常： 12curl http://localhost/user/0&#123;&quot;timestamp&quot;:1550223186170,&quot;status&quot;:500,&quot;error&quot;:&quot;Internal Server Error&quot;,&quot;exception&quot;:&quot;java.lang.IllegalArgumentException&quot;,&quot;message&quot;:&quot;id &lt; 1&quot;,&quot;path&quot;:&quot;/user/0&quot;&#125; 下面通过热更新代码，修改这个逻辑。 8.1 jad反编译UserController在arthas中执行jad命令 👇 jad --source-only com.example.demo.arthas.user.UserController &gt; /tmp/UserController.java jad反编译的结果保存在 /tmp/UserController.java文件里了。 在【 机器 】上然后用vim来编辑/tmp/UserController.java： vim /tmp/UserController.java 比如当 user id 小于1时，也正常返回，不抛出异常： 123456789@GetMapping(value=&#123;&quot;/user/&#123;id&#125;&quot;&#125;)public User findUserById(@PathVariable Integer id) &#123; logger.info(&quot;id: &#123;&#125;&quot;, (Object)id); if (id != null &amp;&amp; id &lt; 1) &#123; return new User(id, &quot;name&quot; + id); // throw new IllegalArgumentException(&quot;id &lt; 1&quot;); &#125; return new User(id.intValue(), &quot;name&quot; + id);&#125; 8.2 sc查找加载UserController的ClassLoader123sc -d *UserController | grep classLoaderHash$ sc -d *UserController | grep classLoaderHash classLoaderHash 1be6f5c3 可以发现是 springbootLaunchedURLClassLoader@1be6f5c3 加载的。 记下classLoaderHash，后面需要使用它。在这里，它是 1be6f5c3。 8.3 mc编译java文件编译java文件，类似于javac。 保存好/tmp/UserController.java之后，使用mc(Memory Compiler)命令来编译，并且通过-c或者--classLoaderClass参数指定ClassLoader： 12345mc --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader /tmp/UserController.java -d /tmp$ mc --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader /tmp/UserController.java -d /tmpMemory compiler output:/tmp/com/example/demo/arthas/user/UserController.classAffect(row-cnt:1) cost in 346 ms 也可以通过mc -c &lt;classLoaderHash&gt; /tmp/UserController.java -d /tmp，使用-c参数指定ClassLoaderHash: 1$ mc -c 1be6f5c3 /tmp/UserController.java -d /tmp 8.4 redefine加载class文件再使用redefine命令重新加载新编译好的UserController.class： 123redefine /tmp/com/example/demo/arthas/user/UserController.class$ redefine /tmp/com/example/demo/arthas/user/UserController.classredefine success, size: 1 8.5 热修改代码结果redefine成功之后，再次访问 localhost:80/user/0 ，结果是： 1234&#123; &quot;id&quot;: 0, &quot;name&quot;: &quot;name0&quot;&#125; 9. Exit/Stop9.1 resetArthas在 watch/trace 等命令时，实际上是修改了应用的字节码，插入增强的代码。显式执行 reset 命令，可以清除掉这些增强代码。 9.2 退出Arthas用 exit 或者 quit 命令可以退出Arthas。 退出Arthas之后，还可以再次用 java -jar arthas-boot.jar 来连接。 9.3 彻底退出Arthasexit/quit命令只是退出当前session，arthas server还在目标进程中运行。 想完全退出Arthas，可以执行 stop 命令。 10. arthas-boot支持的参数arthas-boot.jar 支持很多参数，可以执行 java -jar arthas-boot.jar -h 来查看。 10.1 允许外部访问默认情况下， arthas server侦听的是 127.0.0.1 这个IP，如果希望远程可以访问，可以使用--target-ip的参数。 java -jar arthas-boot.jar --target-ip 10.2 列出所有的版本java -jar arthas-boot.jar --versions 使用指定版本： java -jar arthas-boot.jar --use-version 3.1.0 10.3 只侦听Telnet端口，不侦听HTTP端口java -jar arthas-boot.jar --telnet-port 9999 --http-port -1 10.4 打印运行的详情java -jar arthas-boot.jar -v 11. Web Console","tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}],"categories":[]},{"title":"LeetCode-只出现一次的数字","date":"2021-08-19T13:12:54.000Z","path":"wiki/LeetCode-只出现一次的数字/","text":"只出现一次的数字https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x21ib6/ 给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。 说明： 你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 示例 1: 输入: [2,2,1]输出: 1示例 2: 输入: [4,1,2,1,2]输出: 4相关标签位运算 分析仅仅出现一个，很显然，这个数和前后都不一样，然后特殊判断一下头部和尾部就可以了。 代码1234567891011121314151617181920212223class Solution &#123; public int singleNumber(int[] nums) &#123; if(nums == null || nums.length == 0)&#123; return 0; &#125; if(nums.length == 1)&#123; return nums[0]; &#125; Arrays.sort(nums); for(int i = 2; i &lt; nums.length; i++)&#123; if(nums[i - 2] != nums[i - 1] &amp;&amp; nums[i - 1] != nums[i])&#123; return nums[i - 1]; &#125; if(i == nums.length - 1 &amp;&amp; nums[i] != nums[i - 1])&#123; return nums[i]; &#125; if(i == 2 &amp;&amp; nums[i - 2] != nums[i - 1])&#123; return nums[i - 2]; &#125; &#125; return 0; &#125;&#125; 分析二使用异或运算，将所有值进行异或异或运算，相异为真，相同为假，所以 a^a = 0 ;0^a = a因为异或运算 满足交换律 a^b^a = a^a^b = b 所以数组经过异或运算，单独的值就剩下了 https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x21ib6/?discussion=Mo9fKT 代码二123456789class Solution &#123; public int singleNumber(int[] nums) &#123; int reduce = 0; for (int num : nums) &#123; reduce = reduce ^ num; &#125; return reduce; &#125;&#125;","tags":[{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"位运算","slug":"位运算","permalink":"http://example.com/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}],"categories":[]},{"title":"LeetCode-存在重复元素","date":"2021-08-19T12:57:35.000Z","path":"wiki/LeetCode-存在重复元素/","text":"存在重复元素https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x248f5/ 给定一个整数数组，判断是否存在重复元素。 如果存在一值在数组中出现至少两次，函数返回 true 。如果数组中每个元素都不相同，则返回 false 。 示例 1: 输入: [1,2,3,1]输出: true示例 2: 输入: [1,2,3,4]输出: false示例 3: 输入: [1,1,1,3,3,4,3,2,4,2]输出: true 代码1、双重循环是很容易想到，但是会超出时间限制 123456789101112131415class Solution &#123; public boolean containsDuplicate(int[] nums) &#123; if(nums == null || nums.length == 0)&#123; return false; &#125; for(int i = 0; i &lt; nums.length; i++)&#123; for(int j = i + 1; j &lt; nums.length ; j++)&#123; if(nums[i] == nums[j])&#123; return true; &#125; &#125; &#125; return false; &#125;&#125; 2、先排序，然后判断相邻两个元素是否相等 1234567891011class Solution &#123; public boolean containsDuplicate(int[] nums) &#123; Arrays.sort(nums); for(int i = 1; i &lt; nums.length; i++) &#123; if (nums[i] == nums[i - 1]) &#123; return true; &#125; &#125; return false; &#125;&#125;","tags":[{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"哈希表","slug":"哈希表","permalink":"http://example.com/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"排序","slug":"排序","permalink":"http://example.com/tags/%E6%8E%92%E5%BA%8F/"}],"categories":[]},{"title":"LeetCode-买卖股票的最佳时机II","date":"2021-08-19T12:35:49.000Z","path":"wiki/LeetCode-买卖股票的最佳时机II/","text":"买卖股票的最佳时机IIhttps://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x2zsx1/ 给定一个数组 prices ，其中 prices[i] 是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: prices = [7,1,5,3,6,4]输出: 7解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。示例 2: 输入: prices = [1,2,3,4,5]输出: 4解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。示例 3: 输入: prices = [7,6,4,3,1]输出: 0解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 ⏰提示： 1 &lt;= prices.length &lt;= 3 * 1040 &lt;= prices[i] &lt;= 104 分析[7,1,5,3,6,4][1,2,3,4,5] 其实规律很简单，就比较当天和前一天的大小关系就好了，一次循环就下来了。 一开始想的比较复杂，用的指针滑动，比如1，2，3，4，5这种，一开始想的是 5 - 1 ，其实 1 + 1 + 1 + 1就行了。 代码12345678910111213141516171819202122class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices == null || prices.length == 0)&#123; return 0; &#125; if(prices.length == 1)&#123; return 0; &#125; int count = 0; int index = 0; // [7,1,5,3,6,4] // [1,2,3,4,5] for(int i = 1; i &lt; prices.length ; i++)&#123; if(prices[i] &gt; prices[i - 1])&#123; count += prices[i] - prices[i - 1]; &#125; &#125; return count; &#125;&#125;","tags":[{"name":"贪心","slug":"贪心","permalink":"http://example.com/tags/%E8%B4%AA%E5%BF%83/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"动态规划","slug":"动态规划","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"}],"categories":[]},{"title":"LeetCode-删除排序数组中的重复项","date":"2021-08-19T11:28:03.000Z","path":"wiki/LeetCode-删除排序数组中的重复项/","text":"删除排序数组中的重复项https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x2gy9m/ 给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。 不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。 说明: 为什么返回数值是整数，但输出的答案是数组呢? 请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。 你可以想象内部操作如下: 1234567// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝int len = removeDuplicates(nums);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; 示例 1： 输入：nums = [1,1,2]输出：2, nums = [1,2]解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。示例 2： 输入：nums = [0,0,1,1,1,2,2,3,3,4]输出：5, nums = [0,1,2,3,4]解释：函数应该返回新的长度 5 ， 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4 。不需要考虑数组中超出新长度后面的元素。 提示： 0 &lt;= nums.length &lt;= 3 * 104-104 &lt;= nums[i] &lt;= 104nums 已按升序排列 思路假如输入 [0,0,1,1,1,2,2,3,3,4] 最终的结果应该是[0,1,2,3,4,2,2,3,3,4] count 变量用来存储不重复数组的个数 offset 作为当前游标记录判断之后不重复的数据的位置 解法123456789101112131415161718class Solution &#123; public int removeDuplicates(int[] nums) &#123; if (nums == null || nums.length == 0)&#123; return 0; &#125; int result = 1; int offset = 1; for (int i = 1;i&lt; nums.length;i++)&#123; if (nums[i] == nums[i - 1])&#123; continue; &#125; nums[offset] = nums[i]; offset++; result++; &#125; return result; &#125;&#125;","tags":[{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"双指针","slug":"双指针","permalink":"http://example.com/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"}],"categories":[]},{"title":"MySQL中的锁🔒","date":"2021-08-19T07:17:31.000Z","path":"wiki/MySQL中的锁🔒/","text":"乐观锁和悲观锁 表锁和行锁 表锁： 开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低 表锁按照数据操作可以分成两种： 表读锁（Table Read Lock） 表写锁（Table Write Lock 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁 读写阻塞：当前用户在读数据，其他的用户不能修改当前用户读的数据，会加锁！ 写写阻塞：当前用户在修改数据，其他的用户不能修改当前用户正在修改的数据，会加锁！ 行锁： 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高 InnoDB实现了以下两种类型的行锁。 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 也叫做读锁：读锁是共享的，多个客户可以同时读取同一个资源，但不允许其他客户修改。 排他锁（X锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 也叫做写锁：写锁是排他的，写锁会阻塞其他的写锁和读锁。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁也是数据库隐式帮我们做了，不需要程序员操心！ ⚠️ InnoDB行锁和表锁都支持！MyISAM只支持表锁！ innoDB什么时候会使用到行锁？InnoDB只有通过索引条件检索数据才使用行级锁，否则，InnoDB将使用表锁 参考资料数据库两大神器【索引和锁】 基于数据表乐观锁实现分布式锁","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"MySQL性能调优","date":"2021-08-19T06:06:45.000Z","path":"wiki/MySQL性能调优/","text":"1、使用【 覆盖索引 】避免回表锁造成的时间消耗1、查询语句的时候避免使用select * 2、创建索引添加适当的列避免回表 2、使用【 联合索引 】区分度比较高的列放到前面注意联合索引的最左匹配原则 3、对索引进行函数计算或者表达式计算会导致索引失效 🔒4、利用子查询优化超多分页场景5、explain命令查询执行计划 show profile查询执行的性能消耗面试前必须知道的MySQL命令【expalin】 -3y 6、在事务开始后，事务内尽可能只操作数据库，减少锁持有时间7、尽量避免字符串查询，如果允许的话可以使用Elasticsearch8、如果优化都做了还是查询的很慢，可以做一些聚合表，线上的业务直接查聚合之后的数据9、读写瓶颈问题9.1 如果是单库的情况下，可以考虑读写分离，提升读/写的性能 9.2 主从结构下还是存在瓶颈的话，可以考虑分库分表 注意 分库分表下的id尽量保证递增","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"kafka-基础知识","date":"2021-08-18T12:05:29.000Z","path":"wiki/kafka-基础知识/","text":"kafka是什么kafka运行时架构kafka为什么能承载高并发kafka的确认机制是什么kafka如何保证消息准确kafka会丢消息吗？ kafka会重复消费消息吗 幂等性 消息顺序消费问题参考资料kafka基础知识 - yyy","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[]},{"title":"Spring-事务隔离","date":"2021-08-18T03:10:21.000Z","path":"wiki/Spring-事务隔离/","text":"如何使用Spring来对程序进行事务控制和管理？ 事务控制一般是在哪一层（controller? Service? Dao?） service 事务控制有几种方式？ 事务控制分类编程式事务自己手动控制事务，就叫做编程式事务控制。 Jdbc代码： Conn.setAutoCommite(false); // 设置手动控制事务 Hibernate代码： Session.beginTransaction(); // 开启一个事务 【细粒度的事务控制： 可以对指定的方法、指定的方法的某几行添加事务控制】 (比较灵活，但开发起来比较繁琐： 每次都要开启、提交、回滚.) 人为控制容易产生难以控制的问题 声明式事务TransactionDefinition事务属性事务传播类型1、TransactionDefinition.PROPAGATION_REQUIRED使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。也就是说： 如果外部方法没有开启事务的话，Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务并且被Propagation.REQUIRED的话，所有Propagation.REQUIRED修饰的内部方法和外部方法均属于同一事务 ，只要一个方法回滚，整个事务均回滚。 如果内部方法的事务也开启Propagation.REQUIRED, 并且抛出异常，不管外层方法是否catch异常，整个外层方法都会回滚 2、TransactionDefinition.PROPAGATION_REQUIRES_NEW创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 3、TransactionDefinition.PROPAGATION_NESTED:如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。也就是说： 在外部方法未开启事务的情况下Propagation.NESTED和Propagation.REQUIRED作用相同，修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务的话，Propagation.NESTED修饰的内部方法属于外部事务的子事务，外部主事务回滚的话，子事务也会回滚，而内部子事务可以单独回滚而不影响外部主事务和其他子事务。 4、TransactionDefinition.PROPAGATION_MANDATORY如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 若是错误的配置以下 3 种事务传播行为，事务将不会发生回滚，这里不对照案例讲解了，使用的很少。 **5、TransactionDefinition.PROPAGATION_SUPPORTS如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 6、TransactionDefinition.PROPAGATION_NOT_SUPPORTED以非事务方式运行，如果当前存在事务，则把当前事务挂起。 7、TransactionDefinition.PROPAGATION_NEVER以非事务方式运行，如果当前存在事务，则抛出异常。 Isolation事务隔离级别12345678910111213141516171819202122public enum Isolation &#123; DEFAULT(TransactionDefinition.ISOLATION_DEFAULT), READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED), READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED), REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ), SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); private final int value; Isolation(int value) &#123; this.value = value; &#125; public int value() &#123; return this.value; &#125;&#125; TransactionDefinition.ISOLATION_DEFAULT :使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED :最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED : 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ : 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE : 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务超时时间所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒，默认值为-1。 事务只读属性12345678910package org.springframework.transaction;import org.springframework.lang.Nullable;public interface TransactionDefinition &#123; ...... // 返回是否为只读事务，默认值为 false boolean isReadOnly();&#125; 对于只有读取数据查询的事务，可以指定事务类型为 readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。 很多人就会疑问了，为什么我一个数据查询操作还要启用事务支持呢？ 拿 MySQL 的 innodb 举例子，根据官网 dev.mysql.com/doc/refman/… 描述： “ MySQL 默认对每一个新建立的连接都启用了autocommit模式。在该模式下，每一个发送到 MySQL 服务器的sql语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务。 但是，如果你给方法加上了Transactional注解的话，这个方法执行的所有sql会被放在一个事务中。如果声明了只读事务的话，数据库就会去优化它的执行，并不会带来其他的什么收益。 如果不加Transactional，每条sql会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。 分享一下关于事务只读属性，其他人的解答： 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持 SQL 执行期间的读一致性； 如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询 SQL 必须保证整体的读一致性，否则，在前条 SQL 查询之后，后条 SQL 查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持 rollback事务回滚规则@Transactional(rollbackFor= MyException.class) 发生如下异常时，触发事务回滚。 参考资料Spring DAO模块知识 https://mp.weixin.qq.com/s/IglQITCkmx7Lpz60QOW7HA https://juejin.cn/post/6844904160005996552","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[]},{"title":"Flink-物理分区函数","date":"2021-08-12T08:38:31.000Z","path":"wiki/Flink-物理分区函数/","text":"Flink提供的8种分区函数GlobalPartitioner该分区器会将所有的数据都发送到下游的某个算子实例(subtask id = 0) 12345678910111213// 数据会被分发到下游算子的第一个实例中进行处理 public static void global() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setMaxParallelism(8); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); dataStream.flatMap(new RichFlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String s, Collector&lt;String&gt; collector) throws Exception &#123; collector.collect(s + &quot;_**&quot;); &#125; &#125;).setParallelism(2).global().print(&quot;global : &quot;); env.execute(); &#125; ShufflePartitioner随机选择一个下游算子实例进行发送 12345678//数据会被随机分发到下游算子的每一个实例中进行 public static void shuffle() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); DataStream&lt;String&gt; broadcast = dataStream.shuffle(); broadcast.print(&quot;shuffle : &quot;); env.execute(); &#125; BroadcastPartitioner发送到下游所有的算子实例 12345678//广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做Jion的场景 public static void broadcast() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); DataStream&lt;String&gt; broadcast = dataStream.broadcast(); broadcast.print(&quot;broadcast : &quot;); env.execute(); &#125; RebalancePartitioner通过循环的方式依次发送到下游的task 123456789101112public static void rebalance() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setParallelism(4); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); dataStream.map(new RichMapFunction&lt;String, String&gt;() &#123; @Override public String map(String s) throws Exception &#123; return s + &quot;_**&quot;; &#125; &#125;).setParallelism(1).rebalance().print(&quot;rebalance : &quot;); env.execute(); &#125; RescalePartitioner123456789101112131415/** * 这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为 2，编号为 A 和 B。下游并行度为 4，编号为 1，2，3，4。那么 A 则把数据循环发送给 1 和 2，B 则把数据循环发送给 3 和 4。假设上游并行度为 4，编号为 A，B，C，D。下游并行度为 2，编号为 1，2。那么 A 和 B 则把数据发送给 1，C 和 D 则把数据发送给 2。 */ public static void rescale() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setParallelism(4); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); dataStream.map(new RichMapFunction&lt;String, String&gt;() &#123; @Override public String map(String s) throws Exception &#123; return s + &quot;_**&quot;; &#125; &#125;).setParallelism(1).rescale().print(&quot;rescale : &quot;); env.execute(); &#125; ForwardPartitioner发送到下游对应的第一个task，保证上下游算子并行度一致，即上有算子与下游算子是1:1的关系 12345678//用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。 public static void forward() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setParallelism(1); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); DataStream&lt;String&gt; broadcast = dataStream.shuffle(); broadcast.print(&quot;shuffle : &quot;); env.execute(); &#125; ⚠️ 在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner，对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常。 KeyByPartitioner根据key的分组索引选择发送到相对应的下游subtask 123456789101112public static void keyBy() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setMaxParallelism(8); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;hhh&quot;, &quot;hhh&quot;, &quot;hhh&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;www&quot;); dataStream.flatMap(new RichFlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String s, Collector&lt;String&gt; collector) throws Exception &#123; collector.collect(s + &quot;_**&quot;); &#125; &#125;).keyBy(String::toString).print(&quot;keyBy : &quot;); env.execute(); &#125; CustomPartitionerWrapper通过Partitioner实例的partition方法(自定义的)将记录输出到下游。 1234567891011121314151617181920212223public static void custom() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setMaxParallelism(8); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhhh&quot;, &quot;hhhss&quot;, &quot;hhh&quot;, &quot;hhh&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;www&quot;); dataStream.flatMap(new RichFlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String s, Collector&lt;String&gt; collector) throws Exception &#123; collector.collect(s + &quot;_**&quot;); &#125; &#125;).partitionCustom(new CustomPartitioner(),String::toString) .print(&quot;custom :&quot;); env.execute(); &#125; public static class CustomPartitioner implements Partitioner&lt;String&gt; &#123; // key: 根据key的值来分区 // numPartitions: 下游算子并行度 @Override public int partition(String key, int numPartitions) &#123; return key.length() % numPartitions;//在此处定义分区策略 &#125; &#125; Flink的八种分区策略源码解读 Apache Flink 中文文档","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-UDF函数类","date":"2021-08-12T06:36:09.000Z","path":"wiki/Flink-UDF函数类/","text":"函数类比如说我们常用的MapFunction，FilterFunction，ProcessFunction等，每一步操作都基本上都对应一个Function。 12345678910public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125;&#125; ⚠️ 简单滚动聚合函数，比如sum，max是不需要Function。 好处： 1、通用型强，可复用 2、可抽象方法，代码简洁 匿名函数不需要单独定义Function，直接在Stream的操作中直接实现，效果和上面👆的完全一样。 1234567DataStream&lt;SensorReading&gt; dataStream = unionStream.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Double(strings[1]), new Double(strings[2])); &#125;&#125;); 富函数‘‘富函数’’是DataStream API 提供的一个函数类的接口，所有Flink函数类都有其Rich版本，它与常规函数的不同在于，可以获取运行环境的上下文，并包含一些声明周期方法，所以可以实现更加复杂的功能。 RichMapFunction，RichFlatMapFunction等等 Rich Function有一个生命周期的概念，典型的生命周期方法有 👇 open() 方法是rich function的初始化方法，当一个算子比如map被调用之前被调用。 close()方法是生命周期中最后一个被调用的方法，做一些清理工作。 如果有多个分区的话，每个分区的open方法和close方法都会执行一次 getRuntimeContext()获取运行时上下文。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class RichFunctionDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; dataSource = env.socketTextStream(&quot;localhost&quot;, 9999); DataStream&lt;SensorReading&gt; dataStream = dataSource.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Long(strings[1]), new Double(strings[2])); &#125; &#125;); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; stream = dataStream.map(new MyMapFunction()); stream.print(); env.execute(); &#125; // 富函数是抽象类，这里要用继承 public static class MyMapFunction extends RichMapFunction&lt;SensorReading, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void open(Configuration parameters) throws Exception &#123; System.err.println(&quot;invoke open&quot;); // 一般定义状态，或者链接数据库操作 super.open(parameters); &#125; @Override public Tuple2&lt;String, Integer&gt; map(SensorReading sensorReading) throws Exception &#123; RuntimeContext runtimeContext = this.getRuntimeContext(); System.err.println(&quot;runtimeContext.getTaskName() : &quot; + runtimeContext.getTaskName()); return new Tuple2&lt;&gt;(sensorReading.getSersorId(), runtimeContext.getIndexOfThisSubtask()); &#125; @Override public void close() throws Exception &#123; super.close(); System.err.println(&quot;invoke close method&quot;); &#125; &#125;&#125;","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-到底支持多少种数据类型","date":"2021-08-11T11:50:41.000Z","path":"wiki/Flink-到底支持多少中数据类型/","text":"Flink支持所有的Java和Scala基础数据类型以及其包装类型 支持Tuple元组类型，Flink在Java API中定义了很多Tuple的实现类，从Tuple0 ~ Tuple25类型 Scala样例类 case class，对应Java中的POJO类对象(必须提供无参构造方法 get/set) 其他，比如 Arrays , Lists, Maps, Enums等都是支持的","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-如何读取数据源（集合｜文件｜自定义｜Kafka等）","date":"2021-08-11T11:50:11.000Z","path":"wiki/Flink-如何读取数据源（集合｜文件｜自定义｜Kafka等）/","text":"读取文件这里是以txt文件为例，实现WordCount，其他文件类型同理。 12345678910111213141516171819202122232425 public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125;&#125; 实现自定义数据源需要自己写一个类，实现SourceFunction接口的run方法和cancle方法，注意⚠️，SourceFunction的泛型类型必须要写上，不然会报错的。 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(1); DataStreamSource dataStreamSource = env.addSource(new MySourceFunction()); dataStreamSource.print(); env.execute(); &#125; // 实现自定义的source public static class MySourceFunction implements SourceFunction&lt;SensorReading&gt; &#123; // 定义标识位 控制数据产生 private boolean running = true; public void run(SourceContext ctx) throws Exception &#123; // 定义各个随机数生成器 HashMap&lt;String, Double&gt; sensorMap = new HashMap&lt;String, Double&gt;(10); for (int i = 0; i &lt; 10; i++) &#123; sensorMap.put(&quot;sensor_&quot; + (i + 1), 60 + new Random().nextGaussian() * 20); &#125; while (running) &#123; for (String sensor : sensorMap.keySet()) &#123; double newtemp = sensorMap.get(sensor) + new Random().nextGaussian(); sensorMap.put(sensor, newtemp); ctx.collect(new SensorReading(sensor, System.currentTimeMillis(), newtemp)); &#125; Thread.sleep(10000); &#125; &#125; public void cancel() &#123; running = false; &#125; &#125;","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-你所知道的算子都在这","date":"2021-08-11T11:49:20.000Z","path":"wiki/Flink-你所知道的算子都在这/","text":"好了，看到这的话，Apache Flink基础概念啥的都应该了解差不多了吧，我们几天就See一下，平时用到的StreamApi中各式各样的算子都有什么，然后，我们搞点Demo试一下。 📒 我也是边学边实现一些Demo,这样呢可以方便自己理解，形成体系以后也应该能帮到别人快速学习吧。 这就是地址了👉 https://github.com/geekibli/flink-study 欢迎star！ 下面的Demo都是可以直接运行的 如果是通过socket获取数据的话，确认现开启端口啊，我用的Mac系统，可以使用以下命令 nc -lk 9999 提供一个全局获取环境的方法我们一个静态方法getEnv(), 不然每次还要new，挺麻烦的； 123private static StreamExecutionEnvironment getEnv() &#123; return StreamExecutionEnvironment.getExecutionEnvironment();&#125; POJO类12345public class SensorReading implements Serializable &#123; private String sersorId; private double timestamp; private double newtemp;&#125; map12345678910111213141516public static void mapTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); ArrayList&lt;Integer&gt; nums = Lists.newArrayList(); nums.add(1); nums.add(2); nums.add(3); DataStreamSource&lt;Integer&gt; source = env.fromCollection(nums); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer integer) throws Exception &#123; return integer * integer; &#125; &#125;); map.print(); env.execute();&#125; keyBy123456789101112131415161718192021public static void keyByTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements( new Tuple2&lt;String, Integer&gt;(&quot;age&quot;, 1), new Tuple2&lt;String, Integer&gt;(&quot;name&quot;, 2), new Tuple2&lt;String, Integer&gt;(&quot;name&quot;, 3), new Tuple2&lt;String, Integer&gt;(&quot;name&quot;, 3)); source.map( new MapFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; map(Tuple2&lt;String, Integer&gt; stringIntegerTuple2) throws Exception &#123; Integer f1 = stringIntegerTuple2.f1; stringIntegerTuple2.setField(f1 + 10, 1); return stringIntegerTuple2; &#125; &#125;) .keyBy(1) .print(); env.execute(); &#125; reduce12345678910111213141516171819public static void reduceTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); env.fromElements( Tuple2.of(2L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(2L, 4L), Tuple2.of(1L, 5L)) .keyBy(1) .reduce(new ReduceFunction&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123; @Override public Tuple2&lt;Long, Long&gt; reduce(Tuple2&lt;Long, Long&gt; longLongTuple2, Tuple2&lt;Long, Long&gt; t1) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(t1.f0, longLongTuple2.f1 + t1.f1); &#125; &#125;) .print(); env.execute();&#125; 还有一个栗子🌰 123456789101112131415161718192021222324public static void reduce() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataSource = env.socketTextStream(&quot;localhost&quot;, 9999); DataStream&lt;SensorReading&gt; dataStream = dataSource.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Long(strings[1]), new Double(strings[2])); &#125; &#125;); DataStream&lt;SensorReading&gt; sersorId = dataStream.keyBy(&quot;sersorId&quot;) .reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; @Override public SensorReading reduce(SensorReading sensorReading, SensorReading t1) throws Exception &#123; String id = t1.getSersorId(); Double time = t1.getTimestamp(); return new SensorReading(id, time, Math.max(sensorReading.getNewtemp(), t1.getNewtemp())); &#125; &#125;); sersorId.print(); env.execute();&#125; split|select12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public static void splitTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataSource = env.socketTextStream(&quot;localhost&quot;, 9999); DataStream&lt;SensorReading&gt; dataStream = dataSource.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Double(strings[1]), new Double(strings[2])); &#125; &#125;); SplitStream&lt;SensorReading&gt; split = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading value) &#123; return (value.getNewtemp() &gt; 30) ? Collections.singleton(&quot;high&quot;) : Collections.singleton(&quot;low&quot;); &#125; &#125;); DataStream&lt;SensorReading&gt; low = split.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; high = split.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; all = split.select(&quot;high&quot;, &quot;low&quot;); // connect DataStream&lt;Tuple2&lt;String, Double&gt;&gt; highStream = high.map(new MapFunction&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(SensorReading sensorReading) throws Exception &#123; return new Tuple2&lt;&gt;(sensorReading.getSersorId(), sensorReading.getNewtemp()); &#125; &#125;); // 链接之后的stream ConnectedStreams&lt;Tuple2&lt;String, Double&gt;, SensorReading&gt; connect = highStream.connect(low); SingleOutputStreamOperator&lt;Object&gt; resultStream = connect.map(new CoMapFunction&lt;Tuple2&lt;String, Double&gt;, SensorReading, Object&gt;() &#123; @Override public Object map1(Tuple2&lt;String, Double&gt; stringDoubleTuple2) throws Exception &#123; return new Tuple3&lt;&gt;(stringDoubleTuple2.f0, stringDoubleTuple2.f0, &quot;high temp warning&quot;); &#125; @Override public Object map2(SensorReading sensorReading) throws Exception &#123; return new Tuple2&lt;&gt;(sensorReading.getSersorId(), &quot;normal temp&quot;); &#125; &#125;); resultStream.print(); env.execute(); &#125; connect | coMap如上split方法下面我们是有操作connect的api的 union12345678910111213141516171819public static void unionTest() throws Exception &#123; // 必须是数据类型相同 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; firstStream = env.socketTextStream(&quot;localhost&quot;, 9999); DataStreamSource&lt;String&gt; secondStream = env.socketTextStream(&quot;localhost&quot;, 7777); DataStream&lt;String&gt; unionStream = firstStream.union(secondStream); DataStream&lt;SensorReading&gt; dataStream = unionStream.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Double(strings[1]), new Double(strings[2])); &#125; &#125;); dataStream.print(); env.execute();&#125; // TODO 不断学习 不断补充","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-核心之Windows窗口","date":"2021-08-10T08:55:40.000Z","path":"wiki/Flink-核心之Windows窗口/","text":"什么是窗口 窗口分配器有几种 窗口如何实现增量计算和全量计算","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"设计模式之美-门面模式","date":"2021-08-07T12:27:47.000Z","path":"wiki/设计模式之美-门面模式/","text":"门面模式门面模式又叫做外观模式，提供统一的一个接口，用来访问子系统中的一群接口；门面模式定义了一个高层接口，让子系统更容易使用；门面模式属于结构型模式； 类图 Facade 门面类12345678910111213141516public class Facade &#123; // 继承各个子系统功能，进行封装，一定程度上不遵循单一职责原则 SubSystemA subSystemA = new SubSystemA(); SubSystemB subSystemB = new SubSystemB(); SubSystemC subSystemC = new SubSystemC(); public void doA()&#123; subSystemA.doA(); &#125; public void doB()&#123; subSystemB.doB(); &#125; public void doC()&#123; subSystemC.doC(); &#125;&#125; 子系统A123public class SubSystemA &#123; public void doA()&#123;&#125;&#125; 子系统B123public class SubSystemB &#123; public void doB()&#123;&#125;&#125; 子系统C123public class SubSystemC &#123; public void doC()&#123;&#125;&#125; 以上Facade类集成了三个子系统的类，在自己定义的方法中，并不是Facade自己实现的逻辑，而是调用了对应子系统的方法，这种实现方式叫做门面模式；是不是很简单； 看到这是不是有点似曾相识呢，没错，我们天天都在写的Controller,Service,Dao不就是门面模式吗，没错，只不过把这种方式形成方法论，也就有了所谓的门面模式！ 举个栗子一些商业博客会有一个功能，就是发表文章或者评论点赞会获得一些积分啊，虚拟币啊,然后会有积分商城，在里面可以免费的兑换商品，其实很难凑的够积分，不够费劲的… 好了，结合伪代码来体验门面模式👇👇👇： 下面是一些演示所需要的类： 关系图如下 PaymentService 支付服务123456public class PaymentService &#123; public boolean pay(GiftInfo giftInfo)&#123; System.out.println(&quot;扣减&quot; + giftInfo.getName() + &quot;积分成功！&quot;); return true; &#125;&#125; QualityService 库存服务123456public class QualityService &#123; public boolean isAvailable(GiftInfo giftInfo)&#123; System.out.println(&quot;校验&quot; + giftInfo.getName() + &quot;积分通过，库存充足！&quot;); return true; &#125;&#125; ShipService 物流服务123456public class ShipService &#123; public String doShip(GiftInfo giftInfo)&#123; System.out.println(giftInfo.getName() + &quot;生成物流订单&quot;); return String.valueOf(System.currentTimeMillis()); &#125;&#125; 客户端 非门面模式写法12345678910111213141516171819public static void main(String[] args) &#123; QualityService qualityService = new QualityService(); PaymentService paymentService = new PaymentService(); ShipService shipService = new ShipService(); GiftInfo giftInfo = new GiftInfo(&quot; 《Java编程思想》 &quot;); if (!qualityService.isAvailable(giftInfo))&#123; System.err.println(&quot;Quality not enough!&quot;); &#125; if (!paymentService.pay(giftInfo))&#123; System.err.println(&quot;Pay error!&quot;); &#125; String shipNo = shipService.doShip(giftInfo); System.err.println(&quot;Order shipNo:&quot; + shipNo);&#125; 这种写法会将库存，支付和物流等服务都暴露给调用方，是很不安全的，而且造成客户端依赖严重，代码臃肿； 门面模式写法123456public static void main(String[] args) &#123; FacadeService facadeService = new FacadeService(); GiftInfo giftInfo = new GiftInfo(&quot; 《Java编程思想》 &quot;); String shipNo = facadeService.doOrder(giftInfo); System.err.println(&quot;Order shipNo:&quot; + shipNo); &#125; 上面这种就是门面模式的写法👆 ， 相信大家应该很熟悉吧，这样的话，暴露给客户端就一个订单服务就可以了！ 应用场景 子系统越来越复杂，增加门面模式提供简单的接口，给用户使用； 构建多层的系统接口，利用门面对象作为每层的入口，简化层之间的调用 应用Spring JdbcUtilsMybatis configurationTomcat requestFacade responseFacade 优点 简化了调用过程，无需深入了解子系统，以防止给子系统带来风险 根据上面礼品兑换的逻辑，用户根本不care你底层的兑换逻辑，什么库存啊，支付状态啊，生成订单逻辑等等，对于用户来说，我只需要一步下单即可； 减少系统依赖，松耦合 这一点也是相对客户端来说，客户端只关心的的订单服务就好了，其他的库存，供应链等都不关系； 更好的划分访问层次，提高了安全性 合理的划分层次，减少底层系统的暴露，仅仅暴露一些必要的状态和接口，这一点大家应该都知道的，像service层调用Dao层，而不能在service层直接访问数据库； 遵循迪米特法则 缺点 当子系统的功能需要扩展或者修改的时候，上层封装可能要面临修改的风险，这样增加了后期的维护成本，也不遵循开闭原则 可能会违背单一职责原则 门面模式和代理模式的区别简单来说，门面模式就是一种代理模式，是属于静态代理的模式；但是和静态代理又有一些区别，门面模式的侧重点在于对底层的封装，而静态代理则终于对代理对象的增强，除了调用受委托对象的方法之外，可以扩展额外的功能；很多时候会把门面模式注入成单例，比如一些全局的Util,还有我们常见的一些Controller等等；","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-适配器模式","date":"2021-08-07T12:23:59.000Z","path":"wiki/设计模式之美-适配器模式/","text":"适配器模式适配器模式（Adapter Pattern）又叫做变压器模式，它的功能是将一个类的接口变成客户端所期望的另一个接口，从而使得原本因接口不匹配而导致无法在一起工作的两个类能够在一起工作，属于结构性设计模式的一种； 在软件开发的过程中，基本上任何问题都可以通过一个中间层解决。适配器模式其实就是一个中间层，适配器模式起着转化/委托的作用，将一种接口转化为两一种服务功能或需求的接口。 下面举个例子来分析一下👇 类图 如图是一个适配器模式类图所在👆 代码实现一个文件上传的功能，我们有很多中选择，亚马逊的AWS,阿里的OSS等等吧，但是不同的厂商有自己的标准或者API,但是在我们系统中体现的就是一个putObject方法，所以需要定义一个CloudSDK标准，然后不同的厂商来适配我们自己的标准，在我们的putObject和三方的SDK中间加一层适配层；对于客户端来说，仅仅是完成文件上传的动作，至于你服务端到底使用亚马逊的服务还是阿里的服务，它是不care的，这也体现的策略模式； 所以这个例子兼备工厂模式+策略模式+适配器模式； CloudController12345678910111213141516171819public class CloudController &#123; // 如果是Spring项目这里直接注入就可以了，没必要这么麻烦 public CloudController(CloudService cloudService) &#123; this.cloudService = cloudService; &#125; private CloudService cloudService; public void uploadFile(String fileName)&#123; System.out.println(&quot;invoke upload file service!&quot; + fileName); cloudService.uploadFile(fileName); &#125; public static void main(String[] args) &#123; CloudController cloudController = new CloudController(new CloudService(&quot;ali&quot;)); cloudController.uploadFile(&quot;think in java&quot;); &#125;&#125; CloudService1234567891011121314public class CloudService &#123; private CloudSDK cloudSDK; public CloudService(String cloudStorage) &#123; // 使用工厂来创建具体的SDK this.cloudSDK = CloudFactory.create(cloudStorage); &#125; public void uploadFile(String fileName) &#123; cloudSDK.putObject(fileName); &#125;&#125; CloudSDK 定义SDK标准123public interface CloudSDK &#123; void putObject(String fileName);&#125; AWSSDKAdapter 适配CloudSDK标准123456789101112public class AWSSDKAdapter implements CloudSDK &#123; private AWSSDK awssdk; public AWSSDKAdapter() &#123; this.awssdk = new AWSSDK(); &#125; @Override public void putObject(String fileName) &#123; awssdk.putObject(fileName); &#125;&#125; AliSDKAdapter适配CloudSDK标准1234567891011121314public class AliSDKAdapter implements CloudSDK &#123; private AliSDK aliSDK; public AliSDKAdapter() &#123; this.aliSDK = new AliSDK(); &#125; // 这个方法就是适配器的标准 不管你的三方服务需要多少接口，是要实现云上传，统一通过putObject这个接口实现就可以 @Override public void putObject(String fileName) &#123; aliSDK.setBucket(); aliSDK.uploadFile(fileName); &#125;&#125; AWSSDK 三方SDK123456public class AWSSDK &#123; // 一般以JAR的方式引入到项目中，我们也不可能去修改三方的SDK,这是各个厂商制定的自己的标准 public void putObject(String fileName)&#123; System.out.println(&quot;aws upload file &quot; + fileName); &#125;&#125; AliSDK 三方SDK12345678910public class AliSDK &#123; // 一般以JAR的方式引入到项目中，我们也不可能去修改三方的SDK,这是各个厂商制定的自己的标准 public void setBucket() &#123; System.out.println(&quot;Ali oss set bucket!&quot;); &#125; public void uploadFile(String fileName) &#123; System.out.println(&quot;Ali oss upload file!&quot; + fileName); &#125;&#125; CloudFactory12345678910111213141516public class CloudFactory &#123; // 这里是写死了 生产环境中可以通过配置文件的方式或者启动加载等等方式实现，有很多 private static final Map&lt;String, CloudSDK&gt; sdkMap = new HashMap&lt;&gt;(); // 总之要符合开闭原则 对修改关闭 static &#123; sdkMap.put(&quot;ali&quot;, new AliSDKAdapter()); sdkMap.put(&quot;aws&quot;, new AWSSDKAdapter()); &#125; // 通过不同的策略生成具体的SDK实例 public static CloudSDK create(String storage) &#123; return sdkMap.get(storage); &#125;&#125; 适配器和装饰器模式的区别适配器模式和装饰器模式都是包装器模式，装饰器模式其实是一种特殊的代理模式； 对比维度 适配器模式 装饰器模式 形式 没有层级关系 一种非常特别的代理模式，具有层级关系 定义 适配器和被适配者没有必然的联系，通常采用继承或代理的方式进行包装 装饰器和被装饰者都实现同一个接口，主要目的是扩展之后依旧保留OOP关系 关系 没有满足has-a的关系 满足is-a关系 功能 注重兼容、转换 注重覆盖、扩展 设计 后置考虑 前置考虑 适配器功能的优点 ✅ 能提高类的透明性和复用，但现有的类复用不需要改变 适配器类和原角色解耦，投稿程序的扩展性 在很多业务中符合开闭原则 适配器的缺点 适配器编写过程需要结合业务场景综合考虑，可能会增加系统的复杂性 增加代码阅读难度，降低代码可读性，过多的适配器会使得系统代码变得紊乱","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-责任链模式","date":"2021-08-07T12:21:01.000Z","path":"wiki/设计模式之美-责任链模式/","text":"责任链模式责任链模式（Chain of Responsibility Patter）是将链中的每一个节点看成是一个对象，每个节点处理的请求均不相同，并且内部自动维护下一个节点对象。当一个请求从链的头部发出时，会沿着链的路径一次传递给每一个节点对象，直到有节点处理这个请求为止；责任链模式属于行为型模式 类图 Handler 责任链抽象类 链式结构12345678910public abstract class Handler &#123; protected Handler nextHandler; public void setNextHandler(Handler nextHandler) &#123; this.nextHandler = nextHandler; &#125; public abstract void handleRequest(String request);&#125; ConcreteHandlerA 节点对象A1234567891011121314public class ConcreteHandlerA extends Handler &#123; @Override public void handleRequest(String request) &#123; if (&quot;reqA&quot;.equals(request)) &#123; System.err.println(this.getClass().getSimpleName() + &quot;deal this request &quot; + request); return; &#125; if (this.nextHandler != null) &#123; this.nextHandler.handleRequest(request); &#125; &#125;&#125; ConcreteHandlerB 节点对象B123456789101112public class ConcreteHandlerB extends Handler &#123; @Override public void handleRequest(String request) &#123; if (&quot;reqB&quot;.equals(request)) &#123; System.err.println(this.getClass().getSimpleName() + &quot;deal this request &quot; + request); return; &#125; if (this.nextHandler != null) &#123; this.nextHandler.handleRequest(request); &#125; &#125;&#125; DemoTest 测试类12345678910public class DemoTest &#123; public static void main(String[] args) &#123; Handler handlerA = new ConcreteHandlerA(); Handler handlerB = new ConcreteHandlerB(); handlerA.setNextHandler(handlerB); handlerA.handleRequest(&quot;reqB&quot;); &#125;&#125;// 输出结果：//ConcreteHandlerBdeal this request reqB 业务下面我们举一个业务场景的例子🌰来展示一下什么是责任链模式：我们登录时，肯定要校验用户名和密码有没有传参，如果参数都是空的话，那也没有必要执行后面的登录部分，直接返回了；如果参数是合法的，那我们就要校验数据库是否存在这个用户了，如果不存在该用户，那就直接返回；如果存在，那就要校验用户有没有权限访问正在请求的资源，如果有权限，则可以正常访问资源，如果没有权限，则抛出异常； 非责任链模式写法12345678910111213141516// 以下是伪代码public void login(String userName,String pwd)&#123; if (StringUtils.isEmpty(userName) || StringUtils.isEmpty(pwd))&#123; throw new IllegalArgumentException(); &#125; Member member = selectUser(userName,pwd); if (Objects.isNull(member))&#123; throw new IllegalArgumentException(); &#125; if (!validateRoot(member))&#123; throw new IllegalArgumentException(); &#125; System.err.println(&quot;Login success!&quot;); &#125; 这种写法虽然简单，但是将所有的操作全部杂糅在一起了，结构是十分混乱的；下面👇看一下如果使用责任链模式该如何实现： Handler 抽象责任链处理器12345678910public abstract class Handler &#123; protected Handler next; public void next(Handler next)&#123; this.next = next; &#125; public abstract void doHandler(Member member);&#125; ValidateHandler 参数校验处理器1234567891011public class ValidateHandler extends Handler&#123; @Override public void doHandler(Member member) &#123; if (StringUtils.isEmpty(member.getLoginName()) || StringUtils.isEmpty(member.getLoginPass()))&#123; System.err.println(&quot;登录名或者密码为空！&quot;); return; &#125; System.err.println(&quot;参数校验成功！&quot;); next.doHandler(member); &#125;&#125; 登录处理器12345678public class LoginHandler extends Handler &#123; @Override public void doHandler(Member member) &#123; System.err.println(&quot;login success!&quot;); member.setRoleName(&quot;root&quot;); next.doHandler(member); &#125;&#125; 鉴权处理器12345678910public class AuthHandler extends Handler&#123; @Override public void doHandler(Member member) &#123; if (&quot;root&quot;.equals(member.getRoleName()))&#123; System.err.println(&quot;Auth success!&quot;); return; &#125; System.err.println(&quot;Auth failed!&quot;); &#125;&#125; Member成员类12345678910111213public class Member &#123; private String loginName; private String loginPass; private String roleName; public Member(String loginName, String loginPass) &#123; this.loginName = loginName; this.loginPass = loginPass; &#125; // GETTER SETTER ....&#125; DemoTest 测试类123456789101112public class DemoTest &#123; // 首先创建所有的处理器，设置后它们的先后顺序，设置next节点，完成之后在执行链路操作； public static void main(String[] args) &#123; ValidateHandler validateHandler = new ValidateHandler(); LoginHandler loginHandler = new LoginHandler(); validateHandler.next(loginHandler); AuthHandler authHandler = new AuthHandler(); loginHandler.next(authHandler); Member member = new Member(&quot;xiaoming&quot;, &quot;222&quot;); validateHandler.doHandler(member); &#125;&#125; 上面的写法，看起来有没有比非责任链写法要B格高一些，但是，这样每次创建节点对象，好像又显得不太爽，有些臃肿的意思！而且调整节点之间的顺序时也比较复杂，容易改错！ 那我们就进一步优化以下吧！ 责任链模式+建造者模式 优化添加建造器builder12345678910111213141516171819202122232425262728293031public abstract class Handler&lt;T&gt; &#123; protected Handler next; public void next(Handler next) &#123; this.next = next; &#125; public abstract void doHandler(Member member); // 暴露建造器，将节点对象以链表的形式串起来 public static class Builder&lt;T&gt; &#123; private Handler&lt;T&gt; head; private Handler&lt;T&gt; tail; public Handler&lt;T&gt; build() &#123; return this.head; &#125; public Builder&lt;T&gt; addHandler(Handler&lt;T&gt; handler) &#123; if (this.head == null) &#123; this.head = this.tail = handler; return this; &#125; this.tail.next(handler); this.tail = handler; return this; &#125; &#125;&#125; 测试123456789public static void main(String[] args) &#123; Member member = new Member(&quot;xiaoming&quot;, null); Handler.Builder&lt;Handler&gt; builder = new Handler.Builder(); builder.addHandler(new ValidateHandler()) .addHandler(new LoginHandler()) .addHandler(new AuthHandler()) .build() .doHandler(member); &#125; 通过添加建造器之后，责任链的组装与调用是不是显得很清晰，每个节点对象各司其职，如果需要本节点执行，则执行，如果不是，则交给下一个节点继续！ 责任链适用的场景 多个对象处理同一个请求，但具体由那个对象处理则在运动时动态决定 在不明确🈯指定接受者的情况下，向多个对象的一个提交请求 可以动态指定一组对象处理请求 举一些实际的例子：javax.servlet.Filter的doFilter方法就是使用的责任链模式; org.springframework.web.filter.CompositeFilter#doFilter123public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; (new CompositeFilter.VirtualFilterChain(chain, this.filters)).doFilter(request, response); &#125; 这是Spring框架中的一个实现(javax.servlet.Filter),下面展开具体的代码： org.springframework.web.filter.CompositeFilter12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class CompositeFilter implements Filter &#123; //Filter 这里的Filter相当于处理节点，存放在List中，和我们上面的Handler存放在建造者的链式结构中，异曲同工。 private List&lt;? extends Filter&gt; filters = new ArrayList(); public CompositeFilter() &#123; &#125; public void setFilters(List&lt;? extends Filter&gt; filters) &#123; this.filters = new ArrayList(filters); &#125; public void init(FilterConfig config) throws ServletException &#123; Iterator var2 = this.filters.iterator(); while(var2.hasNext()) &#123; Filter filter = (Filter)var2.next(); filter.init(config); &#125; &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; (new CompositeFilter.VirtualFilterChain(chain, this.filters)).doFilter(request, response); &#125; public void destroy() &#123; int i = this.filters.size(); while(i-- &gt; 0) &#123; Filter filter = (Filter)this.filters.get(i); filter.destroy(); &#125; &#125; private static class VirtualFilterChain implements FilterChain &#123; private final FilterChain originalChain; private final List&lt;? extends Filter&gt; additionalFilters; private int currentPosition = 0; public VirtualFilterChain(FilterChain chain, List&lt;? extends Filter&gt; additionalFilters) &#123; this.originalChain = chain; this.additionalFilters = additionalFilters; &#125; public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException &#123; if (this.currentPosition == this.additionalFilters.size()) &#123; this.originalChain.doFilter(request, response); // 通过currentPosition的移动，转移到链路上的不同处理节点，这就是责任链模式的体现 &#125; else &#123; ++this.currentPosition; Filter nextFilter = (Filter)this.additionalFilters.get(this.currentPosition - 1); nextFilter.doFilter(request, response, this); &#125; &#125; &#125;&#125; 像上面的例子还有很多很多，比如Spring Security和Shiro中的拦截器就是很典型的责任链模式，还有Netty中的 io.netty.channel.ChannelPipeline等等，都是责任链模式； 责任链模式的优点👍👍👍 将请求与处理解耦 请求处理者（节点对象）只需要关注自己感兴趣的请求进行处理即可，对于不感兴趣的请求，直接转发给下一个节点对象 具备链式传递处理请求功能，请求发送者无需知晓链路结构，只需等待处理请求结果 链路结构灵活，可以通过改变链路结果动态的新增和删除责任 易于扩展新的请求节点（符合开闭原则） 责任链模式的缺点 责任链太长或者处理时间太长，导致系统性能下降 如果节点对象存在循环♻️引用时，会造成死循环，导致系统崩溃，所以这个在设计的时候一定要注意不能形成闭环⚠️⚠️⚠️","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-观察者模式","date":"2021-08-07T12:18:40.000Z","path":"wiki/设计模式之美-观察者模式/","text":"观察者模式观察者模式指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。 属于行为型模式 类图 ISubject12345678public interface ISubject&lt;E&gt; &#123; boolean attach(IObserver&lt;E&gt; observer); boolean detach(IObserver&lt;E&gt; observer); void notify(E event);&#125; ConcreteSubject12345678910111213141516171819202122232425public class ConcreteSubject&lt;E&gt; implements ISubject&lt;E&gt; &#123; private List&lt;IObserver&lt;E&gt;&gt; observers = new ArrayList&lt;&gt;(); @Override public boolean attach(IObserver&lt;E&gt; observer) &#123; if (this.observers.contains(observer)) &#123; return false; &#125; observers.add(observer); return true; &#125; @Override public boolean detach(IObserver&lt;E&gt; observer) &#123; return this.observers.remove(observer); &#125; @Override public void notify(E event) &#123; for (IObserver observer : observers) &#123; observer.update(event); &#125; &#125;&#125; IObserver1234public interface IObserver&lt;E&gt; &#123; void update(E event);&#125; ConcreteObserver123456public class ConcreteObserver&lt;E&gt; implements IObserver&lt;E&gt; &#123; @Override public void update(E event) &#123; System.err.println(&quot;receive event &quot; + event); &#125;&#125; DemoTest12345678910public class DemoTest &#123; public static void main(String[] args) &#123; ISubject&lt;String&gt; subject = new ConcreteSubject&lt;String&gt;(); subject.attach(new ConcreteObserver&lt;String&gt;()); subject.attach(new ConcreteObserver&lt;String&gt;()); subject.attach(new ConcreteObserver&lt;String&gt;()); subject.notify(&quot;test&quot;); &#125;&#125; JDK实现观察者模式 比如有一个博客系统提供了问题社区，一个人提出问题，会有其他人收到这个消息；我们使用JDK来模拟一个简单的场景； Blog 被观察对象12345678910111213141516171819public class Blog extends Observable &#123; private Blog() &#123; &#125; private static final Blog blog = new Blog(); private String name = &quot;IBLi Blog&quot;; public static Blog getInstance() &#123; return blog; &#125; public void question(Question question) &#123; System.out.println(question.getUserName() + &quot; 在 blog 提交了问题 ： &quot; + question.getContent()); setChanged(); notifyObservers(question); &#125;&#125; Question1234567891011121314151617181920212223242526public class Question &#123; private String userName; private String content; public Question(String userName, String content) &#123; this.userName = userName; this.content = content; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125;&#125; Student 学生类 被通知对象123456789101112131415161718192021222324252627282930313233public class Student implements Observer &#123; private String name; public Student(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; /** * This method is called whenever the observed object is changed. An * application calls an &lt;tt&gt;Observable&lt;/tt&gt; object&#x27;s * &lt;code&gt;notifyObservers&lt;/code&gt; method to have all the object&#x27;s * observers notified of the change. * * @param o the observable object. * @param arg an argument passed to the &lt;code&gt;notifyObservers&lt;/code&gt; */ @Override public void update(Observable o, Object arg) &#123; Blog blog = (Blog) o; Question question = (Question) arg; System.out.println(&quot;******************&quot;); System.out.println(this.name + &quot; 看到了 &quot; + question.getUserName() + &quot; 的问题 \\n&quot; + &quot;问题是： &quot; + question.getContent()); &#125;&#125; Client 测试1234567891011public class Client &#123; public static void main(String[] args) &#123; Blog blog = Blog.getInstance(); blog.addObserver(new Student(&quot;Tom&quot;)); blog.addObserver(new Student(&quot;John&quot;)); Question question = new Question(&quot;小明&quot;,&quot;观察者模式是什么？&quot;); blog.question(question); &#125;&#125; 测试结果1234567小明 在 blog 提交了问题 ： 观察者模式是什么？******************John看到了 小明 的问题 问题是： 观察者模式是什么？******************Tom看到了 小明 的问题 问题是： 观察者模式是什么？ 基于Guava实现观察者模式Student 被观察对象123456789101112131415161718192021222324252627public class Student &#123; private String name; public Student(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; @Subscribe public void Observer(Student str) &#123; System.err.println(&quot;student invoke method + &quot; + str.toString()); &#125;&#125; Teacher 被通知对象12345678910111213141516171819202122232425262728public class Teacher &#123; private String name; public Teacher(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Teacher&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; @Subscribe public void Observer(Teacher str) &#123; System.err.println(&quot;teacher invoke method + &quot; + str.toString()); &#125;&#125; Client 测试类123456789101112public class Client &#123; public static void main(String[] args) &#123; EventBus eventBus = new EventBus(); Student student = new Student(&quot;Ibli&quot;); Teacher teacher = new Teacher(&quot;laoshi&quot;); eventBus.register(student); eventBus.register(teacher); eventBus.post(new Student(&quot;xuesheng&quot;)); eventBus.post(new Teacher(&quot;teacher&quot;)); &#125;&#125; 测试结果12student invoke method + Student&#123;name=&#x27;xuesheng&#x27;&#125;teacher invoke method + Teacher&#123;name=&#x27;teacher&#x27;&#125; 观察者模式使用场景1、当一个抽象模型包含两个方面内容，其中一个方面依赖另一个方面2、其他一个或多个对象的变化依赖另一个对象的变化3、实现类似广播机制的功能，无需知道具体收听者，只需要广播。系统中感兴趣的对象会自动接口该广播4、多层级嵌套机制，形成一种链式出发机制，是的时间具备跨域（跨越两种观察者类型）通知 观察者模式优点✅1、降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。2、目标与观察者之间建立了一套触发机制。 观察者模式缺点目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-装饰器模式","date":"2021-08-07T12:15:15.000Z","path":"wiki/设计模式之美-装饰器模式/","text":"装饰器模式（Decorator Pattern）装饰器模式也叫做包装模式，是指在不改变原有对象的基础上，将功能附加到对象上，提供比继承更有弹性的替代方案（扩展原有对象的功能），强调一点是基于已有对象的功能增强；装饰器模式属于结构性模式; 装饰器类图 Component 抽象构件 定义一个抽象接口以规范准备接受附加责任的对象 123public abstract class Component &#123; public abstract void operation();&#125; ConcreteComponent 具体的构件 实现抽象构件,通过装饰角色为其添加一些职责 123456public class ConcreteComponent extends Component&#123; @Override public void operation() &#123; System.err.println(&quot;Do some biz event!&quot;); &#125;&#125; Decorator 抽象装饰角色 继承抽象构件,并包含具体构件的实例,可以通过其子类扩展具体构件的功能 123456789101112131415public abstract class Decorator extends Component&#123; //持有组件对象 protected Component component; public Decorator(Component component) &#123; this.component = component; &#125; @Override public void operation() &#123; //转发请求给组件对象,可以在转发前后执行一些附加动作 component.operation(); &#125;&#125; ConcreteDecoratorA 实现抽象装饰的相关方法,并给具体构件对象添加附加的责任 123456789101112131415161718192021public class ConcreteDecoratorA extends Decorator &#123; public ConcreteDecoratorA(Component component) &#123; super(component); &#125; public void operationFirst() &#123; System.err.println(&quot;ConcreteDecoratorA first!&quot;); &#125; public void operationLast() &#123; System.err.println(&quot;ConcreteDecoratorA last!&quot;); &#125; @Override public void operation() &#123; operationFirst(); super.operation(); operationLast(); &#125;&#125; Client 测试类1234567public class Client &#123; public static void main(String[] args) &#123; Component component = new ConcreteComponent(); Decorator decoratorA = new ConcreteDecoratorA(component); decoratorA.operation(); &#125;&#125; 举个例子买煎饼的时候,我们可以直接买一个煎饼,但有的时候觉得味道单一或者吃不饱,我们可以加一些东西,比如烤肠,鸡蛋,鸡排等等; 先看看非装饰器模式的写法 BatterCake 就一个普通的煎饼类； 123456public class BatterCake &#123; protected String getMsg()&#123;return &quot;煎饼&quot;;&#125; public int getPrice()&#123; return 10; &#125;&#125; BatterCakeWithEgg 添加一个鸡蛋的煎饼； 123456789public class BatterCakeWithEgg extends BatterCake&#123; protected String getMsg()&#123; return super.getMsg() + &quot; 加一个鸡蛋&quot;; &#125; public int getPrice()&#123; return super.getPrice() + 1; &#125;&#125; BatterCakeWithEggAndSauage 加一个鸡蛋 再加一跟烤肠的煎饼； 123456789public class BatterCakeWithEggAndSauage extends BatterCakeWithEgg&#123; protected String getMsg()&#123; return super.getMsg() + &quot; 加一个烤肠&quot;; &#125; public int getPrice()&#123; return super.getPrice() + 2; &#125;&#125; Client 客户123456789101112public class Client &#123; public static void main(String[] args) &#123; BatterCake batterCake = new BatterCake(); System.err.println(batterCake.getMsg() + &quot;价格 &quot; + batterCake.getPrice()); BatterCakeWithEgg batterCakeWithEgg = new BatterCakeWithEgg(); System.err.println(batterCakeWithEgg.getMsg() + &quot;价格 &quot; + batterCakeWithEgg.getPrice()); BatterCakeWithEggAndSauage batterCakeWithEggAndSauage = new BatterCakeWithEggAndSauage(); System.err.println(batterCakeWithEggAndSauage.getMsg() + &quot;价格 &quot; + batterCakeWithEggAndSauage.getPrice()); &#125;&#125; 看到这种写法，应该能看出它的弊端了，就是实现很简单，适合于需求固定的业务和多样性比较简单的业务；一旦客户需要一个鸡蛋，两根烤肠，那是不是还要在BatterCakeWithEggAndSauage类上继续扩展呢，这其实是很low的设计；而且非常不灵活，比如客户只需要一跟烤肠的煎饼。这个时候怎么解决呢； 使用装饰器模式优化 BatterCake 抽象组件1234public abstract class BatterCake &#123; protected abstract String getMsg(); protected abstract int getPrice();&#125; 基础类 实现抽象接口1234567891011public class BaseBatterCake extends BatterCake&#123; @Override protected String getMsg() &#123; return &quot;煎饼&quot;; &#125; @Override protected int getPrice() &#123; return 5; &#125;&#125; BatterCakeDecorator 装饰器12345678910111213141516171819public class BatterCakeDecorator extends BatterCake&#123; private BatterCake batterCake; public BatterCakeDecorator(BatterCake batterCake) &#123; this.batterCake = batterCake; &#125; @Override protected String getMsg() &#123; return this.batterCake.getMsg(); &#125; @Override protected int getPrice() &#123; return this.batterCake.getPrice(); &#125;&#125; EggDecorator 具体的装饰器对象12345678910111213141516public class EggDecorator extends BatterCakeDecorator&#123; public EggDecorator(BatterCake batterCake) &#123; super(batterCake); &#125; @Override protected String getMsg() &#123; return super.getMsg() + &quot; 加一个鸡蛋&quot;; &#125; @Override protected int getPrice() &#123; return super.getPrice() + 1; &#125;&#125; SauageDecorator 具体的装饰对象12345678910111213141516public class SauageDecorator extends BatterCakeDecorator&#123; public SauageDecorator(BatterCake batterCake) &#123; super(batterCake); &#125; @Override protected String getMsg() &#123; return super.getMsg() + &quot; 加一个烤肠&quot;; &#125; @Override protected int getPrice() &#123; return super.getPrice() + 2; &#125;&#125; 客户 测试类123456789101112131415161718public class Client &#123; public static void main(String[] args) &#123; BatterCake batterCake = new BaseBatterCake(); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); batterCake = new EggDecorator(batterCake); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); batterCake = new SauageDecorator(batterCake); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); batterCake = new EggDecorator(batterCake); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); &#125;&#125; 上面这种写法是运用了装饰器模式的写法，这样会增加程序的灵活性，EggDecorator返回一个BatterCake，在EggDecorator中的getMsg方法和getPrice方法中添加关于Egg的逻辑来实现对BatterCake的增强，同理SauageDecorator也是，在它自己的getMsg方法和getPrice方法中添加自己的逻辑，当然，都是基于调用super方法的基础上添加自己的逻辑，同时具体的装饰对象返回父类类型的对象； 1234@Override protected int getPrice() &#123; return super.getPrice() + 2; &#125; 这里进行一下总结：1、具体装饰对象(EggDecorator)一定是继承自装饰组件(BatterCakeDecorator)2、为了实现对象增强，子类中的方法一定是基于super方法的基础上，添加自己的逻辑的 使用场景 用于扩展一个类的功能或给一个类添加附加职责 动态地给一个对象添加功能,这些功能可以在动态的撤销 实际应用Spring TransactionAwareCacheManagerJDK FileInputStream 装饰器模式与代理模式 装饰器模式是一种特殊的代理模式 装饰器模式强调自身的功能扩展,透明的,动态的扩展与增强 透明指的是功能的扩展由客户端控制 代理模式强调代理过程的控制 装饰器模式的优点1、装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用2、通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果3、装饰器模式完全遵守开闭原则 装饰器模式的缺点1、增加了一些子类，系统代码会显得臃肿。2、组合方式容易出错，代码可读性比较差。 参考文档1、包装模式就是这么简单啦2、装饰器模式（装饰设计模式）详解3、java中的装饰设计模式，浅谈与继承之间的区别4、JDK IO中的适配器模式和装饰者模式","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-策略模式","date":"2021-08-07T12:09:10.000Z","path":"wiki/设计模式之美-策略模式/","text":"策略模式策略模式（Strategy Pattern）也叫做政策模式（Policy Pattern）,它是将定义的算法封装起来，让它们之间可以相互替换，从而让算法的变化不影响到使用算法的用户；它属于行为型模式。可以在一定程度上规避if-else/switch等策略模式使用的面向对象的继承和多态机制，从而实现同一行为在不同的场景下具备不同的实现； 应用策略模式在实际应用场景中有很多的应用，凡是设计到选择的场景基本都可以使用策略模式来实现；比如购买一个商品选择支付方式，是选择银行卡还是快捷支付，微信？支付宝等； 假如一个系统中有很多类，而它们的区别仅仅在与它们的行为不同； 一个系统需要动态的从几种算法中选择一种；一些平台型产品中肯定会用到的 需要屏蔽算法规则； 策略模式类图 通过上图可以看到，策略模式主要包含3个角色： 上下文角色（Context）: 用来操作策略的上下文环境，屏蔽高层模块（客户端）对策略/算法的直接访问，封装可能存在的变化 抽象策略角色（IStrategy）: 规定策略或算法的行为 具体的策略角色（ConcreteStrategy）: 具体的策略逻辑或算法 这里的上下文角色仅仅是一个称谓，大家只要知道他在策略模式中的作用就可以了，就是桥接客户端和策略/方法的作用； 举个例子 背景： 电商场景下，用户购买一个商品，支付时可以选择一种优惠策略，如果没有优惠，则使用默认的策略，即无任何优惠； 1、上面两层是定义的一个营销策略的接口，然后提供不同的策略来实现；2、第三层是策略生成所使用的工厂和客户端调用策略中间的上下文，承接上下，桥接模式；3、最下层的Demo可以看成是客户端； 定义策略接口 制定标准方法123public interface IPromotionStrategy &#123; void doPromote(); &#125; CashBackStrategy 现金折返策略123456public class CashBackStrategy implements IPromotionStrategy&#123; @Override public void doPromote() &#123; System.out.println(&quot;直接返现&quot;); &#125;&#125; CouponStrategy 优惠券策略1234567public class CouponStrategy implements IPromotionStrategy&#123; // 优惠券策略 @Override public void doPromote() &#123; System.out.println(&quot;使用优惠券抵扣&quot;); &#125;&#125; GroupBuyStrategy 团购优惠策略123456public class GroupBuyStrategy implements IPromotionStrategy&#123; @Override public void doPromote() &#123; System.out.println(&quot;团购 5人成团&quot;); &#125;&#125; EmptyStrategy 默认策略123456public class EmptyStrategy implements IPromotionStrategy&#123; @Override public void doPromote() &#123; System.out.println(&quot;无任何优惠&quot;); &#125;&#125; PromoteActivity 客户端和具体算法之间的上下文，用于桥接123456789101112public class PromoteActivity &#123; private IPromotionStrategy promotionStrategy; public PromoteActivity(IPromotionStrategy promotionStrategy) &#123; this.promotionStrategy = promotionStrategy; &#125; public void execute()&#123; promotionStrategy.doPromote(); &#125;&#125; PromoteStrategyFactory 策略工厂123456789101112131415161718192021222324252627282930313233343536public class PromoteStrategyFactory &#123; private PromoteStrategyFactory() &#123; &#125; // 策略容器 private static Map&lt;String, IPromotionStrategy&gt; promotionStrategyMap = new HashMap&lt;&gt;(); // 默认空策略-没有任何优惠 private static IPromotionStrategy emptyStrategy = new EmptyStrategy(); // 这个可以放到配置文件或者放到数据库，在项目启动的时候加载到服务中就可以了 static &#123; promotionStrategyMap.put(PromoteKey.CASH_BACK, new CashBackStrategy()); promotionStrategyMap.put(PromoteKey.COUPON, new CouponStrategy()); promotionStrategyMap.put(PromoteKey.CROUP_BUY, new GroupBuyStrategy()); &#125; // 简单工厂创建营销策略 public static IPromotionStrategy createPromoteStrategy(String key) &#123; IPromotionStrategy promotionStrategy = promotionStrategyMap.get(key); if (promotionStrategy == null) &#123; return emptyStrategy; &#125; return promotionStrategy; &#125; // 所有定义好的策略 private interface PromoteKey &#123; String COUPON = &quot;coupon&quot;; String CROUP_BUY = &quot;groupBuy&quot;; String CASH_BACK = &quot;cashBack&quot;; &#125; // 因为策略模式需要用户知道所有可用的策略，所以这个方法暴露给客户端 public static Set&lt;String&gt; getPromoteKeySet() &#123; return promotionStrategyMap.keySet(); &#125;&#125; 策略模式下客户端写法 12345public static void main(String[] args) &#123; String strategy = PromoteStrategyFactory.getPromoteKeySet().stream().findAny().get(); IPromotionStrategy promoteStrategy = PromoteStrategyFactory.createPromoteStrategy(strategy); promoteStrategy.doPromote();&#125; 非策略模式下客户端写法1234567891011121314public static void main(String[] args) &#123; String strategy = &quot;客户选择的策略&quot;; IPromotionStrategy promotionStrategy; if (&quot;团购&quot;.equals(strategy))&#123; promotionStrategy = new GroupBuyStrategy(); &#125;else if (&quot;优惠券&quot;.equals(strategy))&#123; promotionStrategy = new CouponStrategy(); &#125;else if (&quot;现金折返&quot;.equals(strategy))&#123; promotionStrategy = new CashBackStrategy(); &#125;else &#123; promotionStrategy = new EmptyStrategy(); &#125; promotionStrategy.doPromote();&#125; 以上的对比，哪种方式比较优雅，立见高下了吧😄😄😄 策略模式的优点 策略模式是符合开闭原则的 避免使用多重条件转移语句 if-else语句、switch语句等 使用策略模式可以提高算法的保密性和安全性（客户端通过上下文组建来调用具体的算法-桥接） 策略模式的缺点 客户端必须要知道全部可用的策略，然后由用户决定使用那个策略，决定权在于客户 代码中会产生很多的策略类，增加代码量和系统的维护难度 👤个人体会1、结合业务场景来设计，体会设计模式的思路，学设计模式最重要的是思想；2、设计模式一般在框架中体现的比较多，大家可以多学习一些框架源码的设计理念，如果你是初级，可能一下子理解不了，慢慢体会吧，当你工作一段时间之后，见过一些工业生产的项目，看过一些优秀的框架源码的时候，会有一个 “柳暗花明又一村”的阶段；3、举个例子，Controller我们都写过的，一个项目中的接口路径是唯一的，项目启动的时候，由Spring加载到容器中，当由请求进来时是，Spring是根据path来返回具体的控制器，也就是Controller具体的方法；4、还有Comparator比较器，不同的容器内部做数据排序的时候由不同的实现，这也是策略模式的体现；5、本文中的例子虽然简单，但是包含了简单工厂模式，桥接模式，策略模式。任何一种设计模式都难以独立存在，这个大家要注意；","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-桥接模式","date":"2021-08-07T11:59:32.000Z","path":"wiki/设计模式之美-桥接模式/","text":"桥接模式（Bridge Pattern）桥接模式也称为桥梁模式，接口（Integer）模式或者柄体（Handle and Body）模式，是将抽象部分和它的具体实现部分分离，是它们都可以独立地变化； 通过组合的方式建立两个类之间的联系，而不是继承； 桥接模式属于结构型模式； 继承一般来作为多继承的备用方案； 桥接模式的结构 桥接模式主要包括一下几个角色： 1、抽象角色（Abstraction）： 定义抽象类，并包含一个对实现化对象的引用，正是这个引用，起着桥梁性的作用；2、扩展抽象化（Refined Abstraction）角色： Abstraction123456789101112public abstract class Abstraction &#123; protected IImplementor iImplementor; public Abstraction(IImplementor iImplementor) &#123; this.iImplementor = iImplementor; &#125; public void operation()&#123; this.iImplementor.operation(); &#125;&#125; IImplementor123public interface IImplementor &#123; void operation();&#125; ConcreteImplementA1234567public class ConcreteImplementA implements IImplementor&#123; @Override public void operation() &#123; &#125;&#125; ConcreteImplementB1234567public class ConcreteImplementB implements IImplementor&#123; @Override public void operation() &#123; &#125;&#125; RefinedAbstraction1234567891011public class RefinedAbstraction extends Abstraction&#123; public RefinedAbstraction(IImplementor iImplementor) &#123; super(iImplementor); &#125; @Override public void operation() &#123; super.operation(); &#125;&#125; 举个例子 AbstractMessage123456789101112public abstract class AbstractMessage &#123; private IMessage message; public AbstractMessage(IMessage message) &#123; this.message = message; &#125; void sendMsg(String msg,String toUser)&#123; this.message.send(msg,toUser); &#125;&#125; IMessage123public interface IMessage &#123; void send(String msg,String toUser);&#125; SmsMessage123456public class SmsMessage implements IMessage&#123; @Override public void send(String msg, String toUser) &#123; System.err.println(&quot;使用短信消息发送 &quot; + msg + &quot; 发送给 &quot; + toUser); &#125;&#125; EmailMessage123456public class EmailMessage implements IMessage&#123; @Override public void send(String msg, String toUser) &#123; System.err.println(&quot;使用邮件消息发送 &quot; + msg + &quot; 发送给 &quot; + toUser); &#125;&#125; NormalMessage1234567891011public class NormalMessage extends AbstractMessage&#123; public NormalMessage(IMessage message) &#123; super(message); &#125; @Override void sendMsg(String msg, String toUser) &#123; super.sendMsg(msg, toUser); &#125;&#125; UegencyMessage12345678910public class UegencyMessage extends AbstractMessage&#123; void sendMsg(String msg,String toUser)&#123; msg = &quot;[ 加急]&quot; + msg; super.sendMsg(msg,toUser); &#125; public UegencyMessage(IMessage message) &#123; super(message); &#125;&#125; DemoTest123456789101112public class DemoTest &#123; public static void main(String[] args) &#123; IMessage message = new SmsMessage(); AbstractMessage abstractMessage = new NormalMessage(message); abstractMessage.sendMsg(&quot;加班申请&quot;,&quot;老大&quot;); message = new EmailMessage(); abstractMessage = new UegencyMessage(message); abstractMessage.sendMsg(&quot;调休申请&quot;,&quot;老总&quot;); &#125;&#125; 12使用短信消息发送 加班申请 发送给 老大使用邮件消息发送 [ 加急]调休申请 发送给 老总 桥接模式的适用场景1、在抽象和具体实现之间需要增加更多的灵活性的场景；2、一个类在两个或者多个独立变化的维度，而这两个或多个维度都需要独立的进行拓展3、不希望使用继承，或者因为多层继承导致系统类的个数剧增； 桥接模式的优点1、遵循软件设计原则，分离抽象部分和具体实现部分2、提高了系统的扩展性3、符合开闭原则4、符合合成复用原则 不使用继承而使用组合 桥接模式1、增加了系统代码可读性和复杂性2、需要正确识别交接的不同维度","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-建造者模式","date":"2021-08-07T11:55:36.000Z","path":"wiki/设计模式之美-建造者模式/","text":"建造者模式 建造者模式（Builder Pattern）将一个复杂的对象的构建过程与它的表示，使得同样的构建过程可以创建出不同的表示。 建造者模式属于创建型模式； 对于用户而言，使用建造者模式只需要指定需要创建的类型就可以获取对象，创建的过程以及细节不需要了解，根据建造者模式的定义，可以简单的理解为两层含义：1、构建与表示分离；构建代表对象创建，表示代表对象行为，方法，也就是将对象的创建与行为分离；（对应到Java代码，其实就是使用接口规范行为，然后由具体的实现进行构建）2、创建不同的表示；也就是具备同样的行为，但是却由于构建的行为顺序不同或其他原因可以创建出不同的表示； Course1234567891011@Data@Getter@Setterpublic class Course &#123; private String name; private String ppt; private String video; private String note; private String homework; &#125; CourseBuilder123456789101112131415161718192021222324252627282930public class CourseBuilder &#123; private Course course = new Course(); public CourseBuilder addName(String name)&#123; course.setName(name); return this; &#125; public CourseBuilder addHomework(String homework)&#123; course.setHomework(homework); return this; &#125; public CourseBuilder addVideo(String video)&#123; course.setVideo(video); return this; &#125; public CourseBuilder addPpt(String ppt)&#123; course.setPpt(ppt); return this; &#125; public CourseBuilder addNote(String note)&#123; course.setNote(note); return this; &#125; public Course builder()&#123; return course; &#125; &#125; DomoTest 测试123456789public class DomoTest &#123; public static void main(String[] args) &#123; CourseBuilder courseBuilder = new CourseBuilder(); courseBuilder.addHomework(&quot;课后作业111&quot;) .addName(&quot;设计模式&quot;) .addNote(&quot;课堂笔记&quot;); System.out.println(&quot;courseBuilder = &quot; + courseBuilder.builder()); &#125;&#125; 输出结果： 1courseBuilder = Course(name=设计模式, ppt=null, video=null, note=课堂笔记, homework=课后作业111) 建造者在JDK中的应用StringBuilder 123public final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125; java.lang.StringBuilder.append(java.lang.CharSequence)12345@Overridepublic StringBuilder append(CharSequence s) &#123; super.append(s); return this;&#125; 这里的StringBuilder就是一个实现构造器，只不过它的上级还有一个抽象的构造器AbstractStringBuilder;看到这源码是不是和我们上面举的例子类似呢，这就是JDK源码中很典型的建造者模式的应用；处理上面的StringBuilder之外，还有像Mybatis框架中的CacheBuilder缓存构造器，还有像SqlSessionFactory装载时的openSession方法都是建造者模式； 建造者模式的使用场景 适用于创建对象需要很多步骤，但是步骤的顺序不一定是固定的 如果一个对象有非常复杂的内部结构（有很多的成员变量或属性） 把复杂对象的创建和使用分离 建造者模式的优点 封装行很好，使得创建过程和使用分离开 扩展性好，建造类之间独立，一定程度上解耦 便于控制细节，建造者可以对创建过程逐步细化，而不对其他模块产生任何影响 建造者模式的缺点 产生了对于的Builder对象，造成了类的冗余 如果产品内部发生变化，建造者都要修改，维护成本比较大；不适合经常变动的对象，这样也是不符合开闭原则的 建造者模式和工厂模式的对比 建造者模式更加注重方法的调用顺序，工厂模式注重于对象的创建 创建对象的力度不同，建造者模式创建复杂的对象，由各种复杂的组件组成，工厂模式创建出来对象的都一样 关注点不同，工厂模式只需要吧对象创建出来就可以了，而建造者模式中不仅要创建出这个对象，还要知道这个对象由那些组件组成 建造者模式根据建造过程中的顺序不一样，最终的对象不见组成也不一样，对象的每个部件的设置都是很灵活的","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-工厂模式","date":"2021-08-07T11:40:56.000Z","path":"wiki/设计模式之美-工厂模式/","text":"1. 简单工厂模式简单工厂模式是指有一个工厂对象决定创建出哪一个产品类的实例；属于创建型模式，但它不属于GOF 23种设计模式。 UML类图 组成要素1、一个抽象产品类2、具体产品类3、一个工厂 肥宅喜爱的各种快乐水（产品接口） 123public interface Kls &#123; String name();&#125; 肥宅快乐水-可乐（具体产品） 123456public class Coke implements Kls &#123; @Override public String name() &#123; return &quot;肥宅快乐水-可乐&quot;; &#125;&#125; 快乐水-雪碧（具体产品） 123456public class Sprite implements Kls &#123; @Override public String name() &#123; return &quot;快乐水-雪碧&quot;; &#125;&#125; 快乐水生产工厂(工厂类) 1234567891011121314public class KlsFactory &#123; public static Kls getFzs(String type) throws Exception &#123; Kls fzs = null; if (&quot;coke&quot;.equalsIgnoreCase(type)) &#123; fzs = new Coke(); &#125; else if (&quot;sprite&quot;.equalsIgnoreCase(type)) &#123; fzs = new Sprite(); &#125; if (Objects.isNull(fzs)) &#123; throw new RuntimeException(&quot;没找到快乐水~&quot;); &#125; return fzs; &#125;&#125; 客户端使用 12345678910111213public class Fz &#123; @Test public void drink() throws Exception &#123; // 制造可乐 Kls coke = KlsFactory.getFzs(&quot;coke&quot;); System.out.println(&quot;肥宅开始喝：&quot; + coke.name()); // 制造雪碧 Kls sprite = KlsFactory.getFzs(&quot;sprite&quot;); System.out.println(&quot;肥宅开始喝：&quot; + sprite.name()); &#125;&#125; 1.1 优点 对客户端隐藏的具体的实现，客户端只需要知道要创建什么对象即可，不用关心对象是怎么创建的 解耦，客户端不需要通过new来创建对象，如果后期产品类需要修改，则只需要修改工厂类即可，不用整个项目遍地去寻找哪里有new 1.2 缺点 由一个专门的工厂负责生产，如果业务变得复杂，这个类将变得十分臃肿 工厂类生产什么产品都是写死在工厂中的，如果增加新的产品，还要修改工厂类的生产逻辑 2. 工厂方法模式工厂方法模式（Factory Method）是简单工厂的仅一步深化， 在工厂方法模式中，我们不再提供一个统一的工厂类来创建所有的对象，而是针对不同的对象提供不同的工厂。也就是说每个对象都有一个与之对应的工厂。 组成要素1、一个抽象产品类2、多个具体产品类3、一个抽象工厂4、多个具体工厂 - 每一个具体产品对应一个具体工厂5、符合 - OCP开放封闭原则 接着上面快乐水的例子。将 快乐水工厂 （KlsFactory） 抽象出共有方法，再分别实现具体的快乐水生产工厂。 快乐水总工厂 12345678public interface Factory &#123; /** * 制造快乐水 * * @return Kls */ Kls create();&#125; 可乐工厂 123456public class CokeFactory implements Factory &#123; @Override public Kls create() &#123; return new Coke(); &#125;&#125; 雪碧工厂 123456public class SpriteFactory implements Factory &#123; @Override public Kls create() &#123; return new Sprite(); &#125;&#125; 肥宅(客户端) 1234567891011121314public class Fz &#123; @Test public void drink() throws Exception &#123; // 制造可乐 CokeFactory cokeFactory = new CokeFactory(); Kls coke = cokeFactory.create(); System.out.println(&quot;肥宅开始喝：&quot; + coke.name()); // 制造雪碧 SpriteFactory spriteFactory = new SpriteFactory(); Kls sprite = spriteFactory.create(); System.out.println(&quot;肥宅开始喝：&quot; + sprite.name()); &#125;&#125; 扩展芬达快乐水 123456public class Fanta implements Kls &#123; @Override public String name() &#123; return &quot;快乐水-芬达&quot;; &#125;&#125; 复制代码芬达工厂 123456public class FantaFactory implements Factory &#123; @Override public Kls create() &#123; return new Fanta(); &#125;&#125; 肥宅使用添加 123FantaFactory fantaFactory = new FantaFactory();Kls fanta = fantaFactory.create();System.out.println(&quot;肥宅开始喝：&quot; + fanta.name()); 2.1 优点1、降低了代码耦合度，对象的生成交给子类去完成（这里的耦合度是相对于简单工厂模式的工厂类比较的）2、降低了代码耦合度，对象的生成交给子类去完成 2.2 缺点1、增加了代码量，每个具体产品都需要一个具体工厂（在具体的业务中可能会产生大量的重复代码）2、当增加抽象产品 也就是添加一个其他产品族 需要修改工厂 违背OCP 3. 抽象工厂模式 组成要素1、多个抽象产品类2、具体产品类3、抽象工厂类 - 声明(一组)返回抽象产品的方法4、具体工厂类 - 生成(一组)具体产品 一个抽象产品类和两个具体的产品（可乐） 1234567891011121314151617public abstract class Coke &#123; public abstract void doCreate();&#125;public class LocalCoke extends Coke&#123; @Override public void doCreate() &#123; System.err.println(&quot;生产本土可乐&quot;); &#125;&#125;public class ForeignCoke extends Coke&#123; @Override public void doCreate() &#123; System.out.println(&quot;生产外国可乐&quot;); &#125;&#125; 一个抽象产品类和两个具体的产品（雪碧） 1234567891011121314151617public abstract class Sprite &#123; public abstract void doCreate();&#125;public class LocalSprite extends Sprite&#123; @Override public void doCreate() &#123; System.out.println(&quot;生产本地雪碧&quot;); &#125;&#125;public class ForeignSprite extends Sprite&#123; @Override public void doCreate() &#123; System.err.println(&quot;生产外国雪碧&quot;); &#125;&#125; 一个抽象工厂和两个具体的工厂 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public interface IAbstractFactory &#123; /** * 成产可乐 * * @return 声明(一组)返回抽象产品的方法 */ Coke createCoke(); /** * 生产雪碧 * * @return 声明(一组)返回抽象产品的方法 */ Sprite createSprite();&#125;public class LocalFactory implements IAbstractFactory&#123; /** * 成产可乐 * * @return 生成(一组)具体产品 */ @Override public LocalCoke createCoke() &#123; return new LocalCoke(); &#125; /** * 生产雪碧 * * @return 生成(一组)具体产品 */ @Override public LocalSprite createSprite() &#123; return new LocalSprite(); &#125;&#125;public class ForeignFactory implements IAbstractFactory&#123; /** * 成产可乐 * * @return 生成(一组)具体产品 */ @Override public ForeignCoke createCoke() &#123; return new ForeignCoke(); &#125; /** * 生产雪碧 * * @return 生成(一组)具体产品 */ @Override public ForeignSprite createSprite() &#123; return new ForeignSprite(); &#125;&#125; 3.1 抽象工厂的使用1234567891011public class FactoryTest &#123; public static void main(String[] args) &#123; LocalFactory localFactory = new LocalFactory(); LocalCoke localCoke = localFactory.createCoke(); localCoke.doCreate(); ForeignFactory foreignFactory = new ForeignFactory(); ForeignSprite foreignSprite = foreignFactory.createSprite(); foreignSprite.doCreate(); &#125;&#125; 3.2 优点1、代码解耦2、很好的满足OCP开放封闭原则3、抽象工厂模式中我们可以定义实现不止一个接口，一个工厂也可以生成不止一个产品类 对于复杂对象的生产相当灵活易扩展(相对于工厂方法模式的优化) 3.3 缺点1、扩展产品是需要修改所有工厂，违反违反OCP原则2、整体实现也比较复杂 参考文章 设计模式-简单工厂、工厂方法模式、抽象工厂模式 设计模式 —— 工厂模式 还有多少个十年 继续做热血少年","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-原型模式","date":"2021-08-07T11:31:43.000Z","path":"wiki/设计模式之美-原型模式/","text":"原型模式原型模式（Prototype Pattern）指原型实例指定创建对象的种类，并且通过复制这些原型创建新的对象，属于创建型模式； 原型模式的核心在于复制原型对象。以系统中已存在的一个对象为原型，直接基于内存二进制流进行复制，不需要再精力耗时的对象初始化过程（不调用构造函数），性能提升很多。当对象的构造过程比较耗时时，可以把当前系统已存在的对象作为原型，对其进行复制（一般是基于二进制流的复制），躲避初始化过程，使得新对象的创建时间大大缩短； 原型模式类图 IPrototype 定义克隆的方法 类似于JDK自带的Cloneable123public interface IPrototype&lt;T&gt; &#123; T clone();&#125; ConcretePrototype 具体的要克隆的对象1234567891011121314151617181920212223242526272829303132333435363738public class ConcretePrototype implements IPrototype &#123; private int age; private String name; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; // 可能我们平时都这么去复制对象 @Override public ConcretePrototype clone() &#123; ConcretePrototype concretePrototype = new ConcretePrototype(); concretePrototype.setAge(this.age); concretePrototype.setName(this.name); return concretePrototype; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; Client 客户端 测试123456789101112public static void main(String[] args) &#123; //创建原型对象 ConcretePrototype prototype = new ConcretePrototype(); prototype.setAge(18); prototype.setName(&quot;Tom&quot;); System.out.println(prototype); //拷贝原型对象 ConcretePrototype cloneType = prototype.clone(); System.out.println(cloneType); System.err.println(cloneType == prototype); &#125; 运行结果： 123ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;&#125;ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;&#125;false 实现JDK Cloneable的克隆对象写法1234567891011121314151617181920212223242526272829303132333435363738394041public class ConcretePrototype1 implements Cloneable &#123; private int age; private String name; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public ConcretePrototype1 clone() &#123; Object clone = null; try &#123; clone = super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); System.err.println(&quot;Clone Error!&quot;); &#125; return (ConcretePrototype1) clone; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 输出结果同上；以上的两个属性都是基本数据类型和String，并没有引用类型，下面我们添加一个引用类型的属性测试以下👇👇 ConcretePrototype添加一个List类型属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class ConcretePrototype implements IPrototype &#123; private int age; private String name; private List&lt;String&gt; hobbies; public List&lt;String&gt; getHobbies() &#123; return hobbies; &#125; public void setHobbies(List&lt;String&gt; hobbies) &#123; this.hobbies = hobbies; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public ConcretePrototype clone() &#123; ConcretePrototype concretePrototype = new ConcretePrototype(); concretePrototype.setAge(this.age); concretePrototype.setName(this.name); concretePrototype.setHobbies(this.hobbies); return concretePrototype; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, hobbies=&quot; + hobbies + &#x27;&#125;&#x27;; &#125;&#125; SETTER方法实现浅克隆 浅克隆也叫浅拷贝：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 123456789101112131415161718192021222324public static void main(String[] args) &#123; //创建原型对象 ConcretePrototype1 prototype = new ConcretePrototype1(); prototype.setAge(18); prototype.setName(&quot;Tom&quot;); ArrayList&lt;String&gt; strings = Lists.newArrayList(&quot;数学&quot;, &quot;英语&quot;); prototype.setHobbies(strings); System.out.println(prototype); //拷贝原型对象 ConcretePrototype1 cloneType = prototype.clone(); cloneType.getHobbies().add(&quot;语文&quot;); System.out.println(cloneType); System.out.println(prototype); System.err.println(cloneType == prototype); System.err.println(cloneType.getHobbies() == prototype.getHobbies()); &#125;输出结果：ConcretePrototype1&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语]&#125;ConcretePrototype1&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语, 语文]&#125;ConcretePrototype1&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语, 语文]&#125;false// 这里为什么结果是true看到上面画的图应该可以看懂吧，set方法其实是将list的引用设置过去，并不是创建一个新的list再赋值！true 通过内存字节流”克隆”对象，实现深克隆 深克隆也叫深拷贝：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class ConcretePrototypeDeep implements Cloneable,Serializable &#123; private int age; private String name; private List&lt;String&gt; hobbies; public List&lt;String&gt; getHobbies() &#123; return hobbies; &#125; public void setHobbies(List&lt;String&gt; hobbies) &#123; this.hobbies = hobbies; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; // 基于内存字节流生成对象 public ConcretePrototypeDeep deepClone() &#123; try &#123; //将当前对象信息写到内存 ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream); objectOutputStream.writeObject(this); //从内存中读取对象信息，强制转换成当前类型 ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray()); ObjectInputStream objectInputStream = new ObjectInputStream(byteArrayInputStream); return (ConcretePrototypeDeep) objectInputStream.readObject(); &#125; catch (IOException ioException) &#123; ioException.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, hobbies=&quot; + hobbies + &#x27;&#125;&#x27;; &#125;&#125; 输出结果： 12345ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语]&#125;ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语, 语文]&#125;ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语]&#125;falsefalse 在Java语言中，如果需要实现深克隆，可以通过覆盖Object类的clone()方法实现，也可以通过序列化(Serialization)等方式来实现。 要实现深拷贝，必须实现Cloneable接口，去重写clone方法，否则会抛出CloneNotSupportedException异常，但是如果对象中包含很多引用类型的属性，这样去覆盖clone方法其实是很麻烦的，可以优先使用序列化的方式实现！ 克隆破坏单例模式如果我们克隆的目标的对象是单例对象，这便意味着，深克隆会破坏单例。解决以上问题的思路： 禁止深克隆 在单例对象的getInstance方法，返回当前对象，而不是去新创建一个对象或者通过内存字节流等方法生成对象 原型模式在Java中的应用ArrayList底层是基于数组结果的，它的动态扩容过程是创建一个新的数组，并把数组中的元素拷贝过去，用新的数组来继续存放元素；在创建新数组的过程中便使用了原型模式。 java.util.ArrayList.clone1234567891011public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // this shouldn&#x27;t happen, since we are Cloneable throw new InternalError(e); &#125;&#125; java.util.Arrays.copyOf(T[], int)123public static &lt;T&gt; T[] copyOf(T[] original, int newLength) &#123; return (T[]) copyOf(original, newLength, original.getClass()); &#125; java.util.Arrays.copyOf(U[], int, java.lang.Class&lt;? extends T[]&gt;)123456789public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; &#125; 最终还是System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength));这个方法来完成数组的拷贝！像我们使用的 com.alibaba.fastjson.JSON#parseObject org.springframework.beans.BeanUtils#copyProperties(java.lang.Object, java.lang.Object)` 等都是原型模式； 原型模式适用场景 类初始化消耗资源过多 new一个对象需要很多繁琐的过程（数据准备，访问权限等） 构造函数比较复杂 循环体内产生大量对象时 原型模式的优点 Java自带的原型模式是基于内存二进制流的复制，在性能上比直接创建一个对象更加优良 可以使用深克隆的方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用，可辅助实现撤销操作。 原型模式的缺点 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-单例模式","date":"2021-08-07T11:22:11.000Z","path":"wiki/设计模式之美-单例模式/","text":"单例模式概念单例模式：指一个类在任何情况下都绝对只有一个实例，并提供一个全局访问点（getInstance方法）。大概实现就是隐藏其构造方法，单例模式属于创建型模式。一些实际的应用场景比如，DBpool, ServletContext,ServletConfig等 单例模式写法饿汉式单例在单例类首次加载时创建实例； 123456789public class HungrySingleton &#123; private static final HungrySingleton hungrySingleton = new HungrySingleton(); private HungrySingleton()&#123;&#125; public static HungrySingleton getInstance()&#123; return hungrySingleton; &#125;&#125; 优点执行效率高，没有加任何锁 缺点类加载的时候就初始化，在某些情况下，可能会造成内存浪费；如果出现类的数量很多的时候，会初始化很多类，占用大量内存； 局限性Spring就不能使用，Spring启动的时候，会有大量的类加载。 饿汉式的第二种写法 1234567891011121314public class HungryStaticSingleton &#123; private static final HungryStaticSingleton hungrySingleton ; static &#123; hungrySingleton = new HungryStaticSingleton(); &#125; private HungryStaticSingleton()&#123;&#125; public static HungryStaticSingleton getInstance()&#123; return hungrySingleton; &#125;&#125; 区别仅仅实在与类加载的顺序不同。👇 懒汉式单例被外部类调用时才创建实例； 123456789101112131415public class LazySingleton &#123; private static LazySingleton instance = null; private LazySingleton() &#123; &#125; public static LazySingleton getInstance() &#123; if (instance == null) &#123; instance = new LazySingleton(); &#125; return instance; &#125;&#125; 优点节省了内存 缺点不能保证真实单例，线程不安全 线程不安全的原因 后面的线程覆盖掉前面线程创建的实例 同时进入判断条件，按顺序返回，没有覆盖的时候，就返回实例 解决方法： 123456public static synchorized LazySingleton getInstance() &#123; if (instance == null) &#123; instance = new LazySingleton(); &#125; return instance;&#125; 但是getInstance方法添加上锁之后，性能下降，如果有很多请求访问，除了获得锁的线程之外，其他线程都要等待。 如何优化？ 12345678910111213141516public class LazyDclSingleton &#123; private static LazyDclSingleton instance = null; private LazyDclSingleton() &#123; &#125; // 后面的线程覆盖掉前面线程创建的实例 public static LazyDclSingleton getInstance() &#123; if (instance == null) &#123; synchronized (LazyDclSingleton.class) &#123; instance = new LazyDclSingleton(); &#125; &#125; return instance; &#125;&#125; 双重检查锁1234567891011121314151617181920public class LazyDclSingleton &#123; private static volatile LazyDclSingleton instance = null; private LazyDclSingleton() &#123; &#125; public static LazyDclSingleton getInstance() &#123; // 检查是否要阻塞 if (instance == null) &#123; synchronized (LazyDclSingleton.class) &#123; // 检查是否要创建实例 if (instance == null) &#123; instance = new LazyDclSingleton(); &#125; &#125; &#125; return instance; &#125;&#125; 局限性会出现指令重排序的问题，有可能返回一个不完整的实例解决方案：private static volatile LazyDclSingleton instance = null;（volatile禁止指令重排序） 优点性能高，能保证线程安全 缺点代码可读性查，不够美观，代码不够优雅 静态内部类写法12345678910111213141516171819/** * 静态内部类 * 静态内部类在使用时才进行构建 * classPath: ../LazyInnerClassSingleton.class * ../LazyInnerClassSingleton$LazyHolder.class * * 优点：写法优雅，利用了java语言语法，性能也高，避免内存的浪费 * 缺点：能够被反射破坏 */public class LazyInnerClassSingleton &#123; private LazyInnerClassSingleton()&#123;&#125; public static LazyInnerClassSingleton getInstance()&#123; return LazyHolder.instance; &#125; private static class LazyHolder&#123; private static final LazyInnerClassSingleton instance = new LazyInnerClassSingleton(); &#125;&#125; 优点1、写法优雅，利用了java语言语法2、性能也高3、避免内存的浪费 缺点1、能够被反射破坏单例 12345678910111213/** * 反射破坏单例模式 */public class ReflectTest &#123; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Class&lt;?&gt; clazz = LazyInnerClassSingleton.class; Constructor&lt;?&gt; declaredConstructor = clazz.getDeclaredConstructor(null); declaredConstructor.setAccessible(true); Object instance = declaredConstructor.newInstance(); System.err.println(instance); &#125;&#125;// 打印结果 com.ibli.javaBase.pattern.singleton.LazyInnerClassSingleton@38af3868 解决办法： 在构造器中添加一个判断，如果实例已经创建，则直接抛出异常终止创建； 12345private LazyInnerClassSingleton()&#123; if (LazyHolder.instance != null)&#123; throw new IllegalArgumentException(); &#125; &#125; 注册式单例 将每一个实例都缓存到一个容器中，使用唯一标志获取实例 枚举写法123456789101112131415161718public enum EnumSingleton &#123; INSTANCE; private Object object; public Object getObject() &#123; return object; &#125; public void setObject(Object object) &#123; this.object = object; &#125; public static EnumSingleton getInstance()&#123; return INSTANCE; &#125;&#125; 使用与测试 123456789101112131415public class EnumSingletonTest &#123; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; EnumSingleton enumSingleton = EnumSingleton.getInstance(); enumSingleton.setObject(new Object()); // 尝试使用反射破坏 Class&lt;?&gt; clazz = EnumSingleton.class; Constructor&lt;?&gt; declaredConstructor = clazz.getDeclaredConstructor(String.class,int.class); System.err.println(declaredConstructor); declaredConstructor.setAccessible(true); Object object = declaredConstructor.newInstance(); System.err.println(object); &#125;&#125; 测试结果： 1234private com.ibli.javaBase.pattern.singleton.EnumSingleton(java.lang.String,int)Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Cannot reflectively create enum objects at java.lang.reflect.Constructor.newInstance(Constructor.java:417) at com.ibli.javaBase.pattern.singleton.EnumSingletonTest.main(EnumSingletonTest.java:17) 原因在JDK底层源码中已经做了限制 12345678910111213141516171819202122@CallerSensitive public T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, null, modifiers); &#125; &#125; // 不能创建枚举类型 if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException(&quot;Cannot reflectively create enum objects&quot;); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) &#123; ca = acquireConstructorAccessor(); &#125; @SuppressWarnings(&quot;unchecked&quot;) T inst = (T) ca.newInstance(initargs); return inst; &#125; 优点写法优雅，使用方便 缺点和饿汉式一样，在某些情况下会造成大量内存浪费 容器式单例写法12345678910111213141516171819202122public class ContainerSingleton &#123; private ContainerSingleton() &#123; &#125; private static Map&lt;String, Object&gt; ioc = new ConcurrentHashMap&lt;&gt;(); public static Object getInstance(String className) &#123; Object instance = null; if (!ioc.containsKey(className)) &#123; try &#123; instance = Class.forName(className).newInstance(); ioc.put(className, instance); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return instance; &#125; else &#123; return ioc.get(className); &#125; &#125;&#125; 测试类： 1234567public class ContainerSingletonTest &#123; public static void main(String[] args) &#123; Object o1 = ContainerSingleton.getInstance(&quot;com.ibli.javaBase.pattern.singleton.Pojo&quot;); Object o2 = ContainerSingleton.getInstance(&quot;com.ibli.javaBase.pattern.singleton.Pojo&quot;); System.err.println(o1 == o2); // true &#125;&#125; 容器式单例写法适合创建大量单例实例的场景，类似与Spring的IOC容器。当然上面的写法也会存在一个线程安全问题 序列化破坏单例模式123456789101112131415/** * 序列化：把内存中对象的状态转换为字节码的形式，然后在把字节码以IO输出流写到磁盘上 * 反序列化： 将持久化的字节码内容，通过IO流的方式读取到内存中，然后在转换成Java对象 */public class SerializableSingleton implements Serializable &#123; private static final SerializableSingleton serializableSingleton = new SerializableSingleton(); private SerializableSingleton() &#123; &#125; public static SerializableSingleton getInstance() &#123; return serializableSingleton; &#125;&#125; 测试类： 123456789101112131415161718192021222324252627public class SerializableSingletonTest &#123; public static void main(String[] args) &#123; SerializableSingleton s1; SerializableSingleton s2 = SerializableSingleton.getInstance(); FileOutputStream fos; try &#123; fos = new FileOutputStream(&quot;SerializableSingleton.obj&quot;); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(s2); oos.flush(); oos.close(); FileInputStream fis = new FileInputStream(&quot;SerializableSingleton.obj&quot;); ObjectInputStream ois = new ObjectInputStream(fis); s1 = (SerializableSingleton) ois.readObject(); ois.close(); System.out.println(s1); System.out.println(s2); System.out.println(s1 == s2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 结果： 123com.ibli.javaBase.pattern.singleton.SerializableSingleton@20ad9418com.ibli.javaBase.pattern.singleton.SerializableSingleton@681a9515false 解决方法：在SerializableSingleton中添加一个方法 1234// 桥接模式private Object readResolve() &#123; return serializableSingleton;&#125; 结果: 123com.ibli.javaBase.pattern.singleton.SerializableSingleton@681a9515com.ibli.javaBase.pattern.singleton.SerializableSingleton@681a9515true 原因：ois.readObject();方法底层有对readResolve方法的判断，如果不存在这个方法，会利用反射生成一个新的实例； ThreadLocal单例下面介绍一种比较少见的一种单例模式 12345678910111213141516public class ThreadLocalSingleton &#123; private static final ThreadLocal&lt;ThreadLocalSingleton&gt; threadLocalSingleton = new ThreadLocal&lt;ThreadLocalSingleton&gt;() &#123; @Override protected ThreadLocalSingleton initialValue() &#123; return new ThreadLocalSingleton(); &#125; &#125;; private ThreadLocalSingleton()&#123;&#125; public static ThreadLocalSingleton getInstance()&#123; return threadLocalSingleton.get(); &#125;&#125; 123456public class ThreadLocalExector implements Runnable&#123; @Override public void run() &#123; System.err.println(ThreadLocalSingleton.getInstance()); &#125;&#125; 测试： 123456789101112public class ThreadLocalSingletonTest &#123; public static void main(String[] args) &#123; System.out.println(ThreadLocalSingleton.getInstance()); System.out.println(ThreadLocalSingleton.getInstance()); Thread thread1 = new Thread(new ThreadLocalExector()); Thread thread2 = new Thread(new ThreadLocalExector()); thread1.start(); thread2.start();; &#125;&#125; 结果： 12345com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@38af3868 1com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@38af3868 2com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@10c69a60 3com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@2f1eeb2f 4 以上3和4的结果虽然不一样，但是其实也是实现了【单例】的效果。 山脚太拥挤 我们更高处见。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-享元模式","date":"2021-08-07T11:16:35.000Z","path":"wiki/设计模式之美-享元模式/","text":"享元模式（Flyweight Pattern）前言在面向对象程序设计过程中，有时会面临要创建大量相同或相似对象实例的问题。创建那么多的对象将会耗费很多的系统资源，它是系统性能提高的一个瓶颈。 定义享元模式，又称为轻量级模式，运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。它是对象池的一种实现，类似于线程池，线程池可以避免不停的创建个销毁多个对象，消耗性能（用户态和内核态）；【宗旨】：共享细粒度对象，将多个对同一对象的访问集中起来；属于结构性模式； 内部状态和外部状态在共享对象的过程中，又两种状态：内部状态： 内部状态指对象共享出来的信息，存储在享元信息内部，并且不回随环境的改变而改变； 就是对象所共有的信息，比如火车票，多有从北京-上海的对象可以共用一个，而不是有1000张票，创建1000个对象； 外部状态： 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享； 也是不叫好理解的，当火车票被你购买之后，就和你的身份证号（唯一）绑定了，这属于外部状态； 使用场景1、常常应用于系统底层的开发，以便解决系统的性能问题；2、系统有大量相似对象，需要缓冲池的地方； 举个例子🌰享元实体类123456789101112131415161718192021222324252627282930313233public class Examination &#123; private String name; private String phone; private String subject; public Examination(String subject) &#123; super(); this.subject = subject; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPhone() &#123; return phone; &#125; public void setPhone(String phone) &#123; this.phone = phone; &#125; @Override public String toString() &#123; return &quot;Examination [name=&quot; + name + &quot;, phone=&quot; + phone + &quot;, subject=&quot; + subject + &quot;]&quot;; &#125;&#125; 享元工厂1234567891011121314151617181920212223public class ExaminationFactory &#123; // 对象池 private static Map&lt;String, Examination&gt; pool = new HashMap&lt;&gt;(); public static Examination getexExamination(String name, String phone, String subject) &#123; // 通过学科判断是否存在这个对象 if (pool.containsKey(subject)) &#123; Examination examination = pool.get(subject); examination.setName(name); examination.setPhone(phone); System.out.println(&quot;在缓存池取用对象:&quot; + examination.toString()); return examination; &#125; else &#123; Examination examination = new Examination(subject); pool.put(subject, examination); examination.setName(name); examination.setPhone(phone); System.out.println(&quot;新建对象:&quot; + examination.toString()); return examination; &#125; &#125;&#125; 测试类123456789101112public class Client &#123; public static void main(String[] args) &#123; Examination A = ExaminationFactory.getexExamination(&quot;A&quot;, &quot;13812345678&quot;, &quot;软件工程&quot;); Examination B = ExaminationFactory.getexExamination(&quot;B&quot;, &quot;13812341234&quot;, &quot;软件工程&quot;); Examination C = ExaminationFactory.getexExamination(&quot;C&quot;, &quot;13812341328&quot;, &quot;电子信息工程&quot;); Examination D = ExaminationFactory.getexExamination(&quot;D&quot;, &quot;13812345111&quot;, &quot;桥梁工程工程&quot;); Examination E = ExaminationFactory.getexExamination(&quot;E&quot;, &quot;13812345444&quot;, &quot;软件工程&quot;); System.err.println(A.hashCode()); System.err.println(B.hashCode()); &#125;&#125; 测试结果1234567新建对象:Examination [name=A, phone=13812345678, subject=软件工程]在缓存池取用对象:Examination [name=B, phone=13812341234, subject=软件工程]新建对象:Examination [name=C, phone=13812341328, subject=电子信息工程]新建对象:Examination [name=D, phone=13812345111, subject=桥梁工程工程]在缓存池取用对象:Examination [name=E, phone=13812345444, subject=软件工程]15289025771528902577 实际应用1、JDK – String 123456789101112131415161718192021222324252627282930313233public static void main(String[] args) &#123; // &quot;hello&quot;是编译器常量， String s1是运行时，把常量地址赋值给他&lt;br&gt; String s1 = &quot;hello&quot;; String s2 = &quot;hello&quot;; // &quot;he&quot; + &quot;llo&quot; 两个常量相加，会在编译期处理&lt;br&gt; String s3 = &quot;he&quot; + &quot;llo&quot;; // &quot;hel&quot; &quot;lo&quot; new String 共建了3个空间，然后拼接起来是一个新的空间（why?） String s4 = &quot;hel&quot; + new String(&quot;lo&quot;); String s5 = new String(&quot;hello&quot;); // s5存放的是堆中中间 String s6 = s5.intern(); //拿到常量中的地址， String s7 = &quot;h&quot;; String s8 = &quot;ello&quot;; String s9 = s7 + s8; //为什么这个不一样，因为是变量相加所以编译期没有做优化 System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;); System.out.println(&quot;s1 &quot; + System.identityHashCode(s1)); System.out.println(&quot;s2 &quot; + System.identityHashCode(s2)); System.out.println(&quot;s3 &quot; + System.identityHashCode(s3)); System.out.println(&quot;s4 &quot; + System.identityHashCode(s4)); System.out.println(&quot;s5 &quot; + System.identityHashCode(s5)); //s6为s5.intern()拿到的是常量池里的“hello” System.out.println(&quot;s6 &quot; + System.identityHashCode(s6)); System.out.println(&quot;s7 &quot; + System.identityHashCode(s7)); System.out.println(&quot;s8 &quot; + System.identityHashCode(s8)); System.out.println(&quot;s9 &quot; + System.identityHashCode(s9)); System.out.print(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;); System.out.println(s1==s2);//true System.out.println(s1==s3);//true System.out.println(s1==s4);//false System.out.println(s1==s9);//false System.out.println(s4==s5);//false System.out.println(s1==s6);//true &#125; 2、JDK – Integer 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125; &#125; 3、线程池4、Tomcat连接池 享元模式的优点✅1、减少对象的创建，降低内存中对象的数量，降低系统的内存使用，提升效率；2、减少内存之外的其他资源占用，IO,带宽等； 享元模式的缺点1、关注内部，外部状态，关注程序线程安全问题；2、使系统、程序的逻辑变得复杂； 参考资料1、[设计模式] - 享元模式2、设计模式-享元模式3、设计模式-享元设计模式","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"Java并发编程之ConcurrentHashMap实现原理","date":"2021-08-05T07:35:46.000Z","path":"wiki/Java并发编程之ConcurrentHashMap实现原理/","text":"ConcurrentHashMapMap应该是我们平时开发过程中除了List使用的第二频繁的数据结构了吧，我们都知道HashMap无法在多线程环境下保证安全，那我们可以使用什么来代替HashMap呢，有两个选择，HashTable和ConcurrentHashMap,由于HashTable的性能相对比较低，我们一般都使用ConcurrentHashMap来代替HashMap。 前言HashTable&amp;HashMapHashTable的put, get,remove等方法是通过synchronized来修饰保证其线程安全性的。 HashTable是 不允许key跟value为null的。 问题是synchronized是个关键字级别的==重量锁==，在get数据的时候任何写入操作都不允许。相对来说性能不好。 ConcurrentHashMap概述在ConcurrentHashMap中通过一个Node&lt;K,V&gt;[]数组来保存添加到map中的键值对，而在同一个数组位置是通过链表和红黑树的形式来保存的。但是这个数组只有在第一次添加元素的时候才会初始化，否则只是初始化一个ConcurrentHashMap对象的话，只是设定了一个sizeCtl变量，这个变量用来判断对象的一些状态和是否需要扩容，后面会详细解释。 第一次添加元素的时候，默认初期长度为16，当往map中继续添加元素的时候，通过hash值跟数组长度取与来决定放在数组的哪个位置，如果出现放在同一个位置的时候，优先以链表的形式存放，在同一个位置的个数又达到了8个以上，如果数组的长度还小于64的时候，则会扩容数组。如果数组的长度大于等于64了的话，在会将该节点的链表转换成树。 通过扩容数组的方式来把这些节点给分散开。然后将这些元素复制到扩容后的新的数组中，同一个链表中的元素通过hash值的数组长度位来区分，是还是放在原来的位置还是放到扩容的长度的相同位置去 。在扩容完成之后，如果某个节点的是树，同时现在该节点的个数又小于等于6个了，则会将该树转为链表。 取元素的时候，相对来说比较简单，通过计算hash来确定该元素在数组的哪个位置，然后在通过遍历链表或树来判断key和key的hash，取出value值。 ConcurrentHashMap 原理解析ConcurrentHashMap属性123456789101112131415161718private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;private static final int DEFAULT_CAPACITY = 16;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;static final int MOVED = -1; // 表示正在转移static final int TREEBIN = -2; // 表示已经转换成树static final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashtransient volatile Node&lt;K,V&gt;[] table;//默认没初始化的数组，用来保存元素private transient volatile Node&lt;K,V&gt;[] nextTable;//转移的时候用的数组/** * 用来控制表初始化和扩容的，默认值为0，当在初始化的时候指定了大小，这会将这个大小保存在sizeCtl中，大小为数组的0.75 * 当为负的时候，说明表正在初始化或扩张， * -1表示初始化 * -(1+n) n:表示活动的扩张线程 */private transient volatile int sizeCtl; Node&lt;K,V&gt;,这是构成每个元素的基本类。12345678910111213141516static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //key的hash值 final K key; //key volatile V val; //value volatile Node&lt;K,V&gt; next; //表示链表中的下一个节点 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125;&#125; TreeNode，构造树的节点12345678910111213static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125;&#125; TreeBin 用作树的头结点只存储root和first节点，不存储节点的key、value值。 12345678910static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock&#125; ForwardingNode在转移的时候放在头部的节点，是一个空节点 1234567static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; ConcurrentHashMap几个重要方法123456789101112131415161718192021/* * 用来返回节点数组的指定位置的节点的原子操作 */@SuppressWarnings(&quot;unchecked&quot;)static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;/* * cas原子操作，在指定位置设定值 */static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;/* * 原子操作，在指定位置设定值 */static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; ConcurrentHashMap的初始化先看一下ConcurrentHashMap的构造器 1234567891011121314151617//空的构造public ConcurrentHashMap() &#123;&#125;//如果在实例化对象的时候指定了容量，则初始化sizeCtlpublic ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125;//当出入一个Map的时候，先设定sizeCtl为默认容量，在添加元素public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125; 可以看到，在任何一个构造方法中，都没有对存储Map元素Node的table变量进行初始化。而是在第一次put操作的时候在进行初始化。 下面来看看数组的初始化方法initTable 1234567891011121314151617181920212223242526272829/** * 初始化数组table， * 如果sizeCtl小于0，说明别的数组正在进行初始化，则让出执行权 * 如果sizeCtl大于0的话，则初始化一个大小为sizeCtl的数组 * 否则的话初始化一个默认大小(16)的数组 * 然后设置sizeCtl的值为数组长度的3/4 */private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //第一次put的时候，table还没被初始化，进入while if ((sc = sizeCtl) &lt; 0) //sizeCtl初始值为0，当小于0的时候表示在别的线程在初始化表或扩展表 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //SIZECTL：表示当前对象的内存偏移量，sc表示期望值，-1表示要替换的值，设定为-1表示要初始化表了 try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; //指定了大小的时候就创建指定大小的Node数组，否则创建指定大小(16)的Node数组 @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; //初始化后，sizeCtl长度为数组长度的3/4 &#125; break; &#125; &#125; return tab;&#125; ConcurrentHashMap的put操作详解👇下面是put方法的源码： 12345678/* * 单纯的额调用putVal方法，并且putVal的第三个参数设置为false * 当设置为false的时候表示这个value一定会设置 * true的时候，只有当这个key的value为空的时候才会设置 */ public V put(K key, V value) &#123; return putVal(key, value, false); &#125; 下面看一下putVal方法实现： 当添加一对键值对的时候，首先会去判断保存这些键值对的数组是不是初始化了，如果没有的话就初始化数组, 然后通过计算hash值来确定放在数组的哪个位置。 如果这个位置为空则直接添加，如果不为空的话，则取出这个节点来，取出来的节点的hash值是MOVED(-1)的话，则表示当前正在对这个数组进行扩容，复制到新的数组，则当前线程也去帮助复制。 如果这个节点，不为空，也不在扩容，则通过synchronized来加锁，进行添加操作。 然后判断当前取出的节点位置存放的是链表还是树，如果是链表的话，则遍历整个链表，直到取出来的节点的key来个要放的key进行比较，如果key相等，并且key的hash值也相等的话，则说明是同一个key，则覆盖掉value，否则的话则添加到链表的末尾。如果是树的话，则调用putTreeVal方法把这个元素添加到树中去。 最后在添加完成之后，会判断在该节点处共有多少个节点（注意是添加前的个数），如果达到8个以上了的话，则调用treeifyBin方法来尝试将处的链表转为树，或者扩容数组。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293final V putVal (K key, V value,boolean onlyIfAbsent)&#123; if (key == null || value == null) throw new NullPointerException(); //取得key的hash值 int hash = spread(key.hashCode()); //用来计算在这个节点总共有多少个元素，用来控制扩容或者转移为树 int binCount = 0; for (Node&lt;K, V&gt;[] tab = table; ; ) &#123; Node&lt;K, V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) //第一次put的时候table没有初始化，则初始化table tab = initTable(); //通过哈希计算出一个表中的位置因为n是数组的长度，所以(n-1)&amp;hash肯定不会出现数组越界 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果这个位置没有元素的话，则通过cas的方式尝试添加，注意这个时候是没有加锁的 if (casTabAt(tab, i, null, //创建一个Node添加到数组中区，null表示的是下一个节点为空 new Node&lt;K, V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; /* * 如果检测到某个节点的hash值是MOVED，则表示正在进行数组扩张的数据复制阶段， * 则当前线程也会参与去复制，通过允许多线程复制的功能，一次来减少数组的复制所带来的性能损失 */ else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; /* * 如果在这个位置有元素的话，就采用synchronized的方式加锁， * 如果是链表的话(hash大于0)，就对这个链表的所有元素进行遍历， * 如果找到了key和key的hash值都一样的节点，则把它的值替换到 * 如果没找到的话，则添加在链表的最后面 * 否则，是树的话，则调用putTreeVal方法添加到树中去 * * 在添加完之后，会对该节点上关联的的数目进行判断， * 如果在8个以上的话，则会调用treeifyBin方法，来尝试转化为树，或者是扩容 */ V oldVal = null; synchronized (f) &#123; //再次取出要存储的位置的元素，跟前面取出来的比较 if (tabAt(tab, i) == f) &#123; //取出来的元素的hash值大于0，当转换为树之后，hash值为-2 if (fh &gt;= 0) &#123; binCount = 1; //遍历这个链表 for (Node&lt;K, V&gt; e = f; ; ++binCount) &#123; K ek; //要存的元素的hash，key跟要存储的位置的节点的相同的时候，替换掉该节点的value即可 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; //当使用putIfAbsent的时候，只有在这个key没有设置值得时候才设置 if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K, V&gt; pred = e; //如果不是同样的hash，同样的key的时候，则判断该节点的下一个节点是否为空， if ((e = e.next) == null) &#123; //为空的话把这个要加入的节点设置为当前节点的下一个节点 pred.next = new Node&lt;K, V&gt;(hash, key, value, null); break; &#125; &#125; //表示已经转化成红黑树类型了 &#125; else if (f instanceof TreeBin) &#123; Node&lt;K, V&gt; p; binCount = 2; //调用putTreeVal方法，将该元素添加到树中去 if ((p = ((TreeBin&lt;K, V&gt;) f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //当在同一个节点的数目达到8个的时候，则扩张数组或将给节点的数据转为tree if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); //计数 return null;&#125; ConcurrentHashMap的扩容详解在put方法的详解中，我们可以看到，在同一个节点的个数超过8个的时候，会调用treeifyBin方法来看看是扩容还是转化为一棵树，同时在每次添加完元素的addCount方法中，也会判断当前数组中的元素是否达到了sizeCtl的量，如果达到了的话，则会进入transfer方法去扩容。 12345678910111213141516171819202122232425262728293031/** * Replaces all linked nodes in bin at given index unless table is * too small, in which case resizes instead. * 当数组长度小于64的时候，扩张数组长度一倍，否则的话把链表转为树 */private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; System.out.println(&quot;treeifyBin方\\t==&gt;数组长：&quot;+tab.length); if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //MIN_TREEIFY_CAPACITY 64 tryPresize(n &lt;&lt; 1); // 数组扩容 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; //使用synchronized同步器，将该节点出的链表转为树 if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //hd：树的头(head) for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) //把Node组成的链表，转化为TreeNode的链表，头结点任然放在相同的位置 hd = p; //设置head else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd));//把TreeNode的链表放入容器TreeBin中 &#125; &#125; &#125; &#125;&#125; 可以看到当需要扩容的时候，调用的时候tryPresize方法，看看trePresize的源码 trePresize的源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * 扩容表为指可以容纳指定个数的大小（总是2的N次方） * 假设原来的数组长度为16，则在调用tryPresize的时候，size参数的值为16&lt;&lt;1(32)，此时sizeCtl的值为12 * 计算出来c的值为64,则要扩容到sizeCtl≥为止 * 第一次扩容之后 数组长：32 sizeCtl：24 * 第二次扩容之后 数组长：64 sizeCtl：48 * 第二次扩容之后 数组长：128 sizeCtl：94 --&gt; 这个时候才会退出扩容 */private final void tryPresize(int size) &#123; /* * MAXIMUM_CAPACITY = 1 &lt;&lt; 30 * 如果给定的大小大于等于数组容量的一半，则直接使用最大容量， * 否则使用tableSizeFor算出来 * 后面table一直要扩容到这个值小于等于sizeCtrl(数组长度的3/4)才退出扩容 */ int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // printTable(tab); 调试用的 /* * 如果数组table还没有被初始化，则初始化一个大小为sizeCtrl和刚刚算出来的c中较大的一个大小的数组 * 初始化的时候，设置sizeCtrl为-1，初始化完成之后把sizeCtrl设置为数组长度的3/4 * 为什么要在扩张的地方来初始化数组呢？这是因为如果第一次put的时候不是put单个元素， * 而是调用putAll方法直接put一个map的话，在putALl方法中没有调用initTable方法去初始化table， * 而是直接调用了tryPresize方法，所以这里需要做一个是不是需要初始化table的判断 */ if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //初始化tab的时候，把sizeCtl设为-1 try &#123; if (table == tab) &#123; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; /* * 一直扩容到的c小于等于sizeCtl或者数组长度大于最大长度的时候，则退出 * 所以在一次扩容之后，不是原来长度的两倍，而是2的n次方倍 */ else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) &#123; break; //退出扩张 &#125; else if (tab == table) &#123; int rs = resizeStamp(n); /* * 如果正在扩容Table的话，则帮助扩容 * 否则的话，开始新的扩容 * 在transfer操作，将第一个参数的table中的元素，移动到第二个元素的table中去， * 虽然此时第二个参数设置的是null，但是，在transfer方法中，当第二个参数为null的时候， * 会创建一个两倍大小的table */ if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; /* * transfer的线程数加一,该线程将进行transfer的帮忙 * 在transfer的时候，sc表示在transfer工作的线程数 */ if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; /* * 没有在初始化或扩容，则开始扩容 */ else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) &#123; transfer(tab, null); &#125; &#125; &#125;&#125; 在tryPresize方法中，并没有加锁，允许多个线程进入，如果数组正在扩张，则当前线程也去帮助扩容。 transfer方法数组扩容的主要方法就是transfer方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. * 把数组中的节点复制到新的数组的相同位置，或者移动到扩张部分的相同位置 * 在这里首先会计算一个步长，表示一个线程处理的数组长度，用来控制对CPU的使用， * 每个CPU最少处理16个长度的数组元素,也就是说，如果一个数组的长度只有16，那只有一个线程会对其进行扩容的复制移动操作 * 扩容的时候会一直遍历，知道复制完所有节点，没处理一个节点的时候会在链表的头部设置一个fwd节点，这样其他线程就会跳过他， * 复制后在新数组中的链表不是绝对的反序的 */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) //MIN_TRANSFER_STRIDE 用来控制不要占用太多CPU stride = MIN_TRANSFER_STRIDE; // subdivide range //MIN_TRANSFER_STRIDE=16 /* * 如果复制的目标nextTab为null的话，则初始化一个table两倍长的nextTab * 此时nextTable被设置值了(在初始情况下是为null的) * 因为如果有一个线程开始了表的扩张的时候，其他线程也会进来帮忙扩张， * 而只是第一个开始扩张的线程需要初始化下目标数组 */ if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; /* * 创建一个fwd节点，这个是用来控制并发的，当一个节点为空或已经被转移之后，就设置为fwd节点 * 这是一个空的标志节点 */ ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; //是否继续向前查找的标志位 boolean finishing = false; // to ensure sweep(清扫) before committing nextTab,在完成之前重新在扫描一遍数组，看看有没完成的没 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) &#123; advance = false; &#125; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; //已经完成转移 nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //设置sizeCtl为扩容后的0.75 return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) &#123; return; &#125; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) //数组中把null的元素设置为ForwardingNode节点(hash值为MOVED[-1]) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; synchronized (f) &#123; //加锁操作 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; //该节点的hash值大于等于0，说明是一个Node节点 /* * 因为n的值为数组的长度，且是power(2,x)的，所以，在&amp;操作的结果只可能是0或者n * 根据这个规则 * 0--&gt; 放在新表的相同位置 * n--&gt; 放在新表的（n+原来位置） */ int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; /* * lastRun 表示的是需要复制的最后一个节点 * 每当新节点的hash&amp;n -&gt; b 发生变化的时候，就把runBit设置为这个结果b * 这样for循环之后，runBit的值就是最后不变的hash&amp;n的值 * 而lastRun的值就是最后一次导致hash&amp;n 发生变化的节点(假设为p节点) * 为什么要这么做呢？因为p节点后面的节点的hash&amp;n 值跟p节点是一样的， * 所以在复制到新的table的时候，它肯定还是跟p节点在同一个位置 * 在复制完p节点之后，p节点的next节点还是指向它原来的节点，就不需要进行复制了，自己就被带过去了 * 这也就导致了一个问题就是复制后的链表的顺序并不一定是原来的倒序 */ for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; //n的值为扩张前的数组的长度 if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; /* * 构造两个链表，顺序大部分和原来是反的 * 分别放到原来的位置和新增加的长度的相同位置(i/n+i) */ for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) /* * 假设runBit的值为0， * 则第一次进入这个设置的时候相当于把旧的序列的最后一次发生hash变化的节点(该节点后面可能还有hash计算后同为0的节点)设置到旧的table的第一个hash计算后为0的节点下一个节点 * 并且把自己返回，然后在下次进来的时候把它自己设置为后面节点的下一个节点 */ ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else /* * 假设runBit的值不为0， * 则第一次进入这个设置的时候相当于把旧的序列的最后一次发生hash变化的节点(该节点后面可能还有hash计算后同不为0的节点)设置到旧的table的第一个hash计算后不为0的节点下一个节点 * 并且把自己返回，然后在下次进来的时候把它自己设置为后面节点的下一个节点 */ hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; //否则的话是一个树节点 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; /* * 在复制完树节点之后，判断该节点处构成的树还有几个节点， * 如果≤6个的话，就转回为一个链表 */ ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 到这里，ConcurrentHashMap的put操作和扩容都介绍的差不多了， 下面的两点一定要注意： 1、复制之后的新链表不是旧链表的绝对倒序 2、在扩容的时候每个线程都有处理的步长，最少为16，在这个步长范围内的数组节点只有自己一个线程来处理 ConcurrentHashMap的get操作详解1234567891011121314151617181920212223242526/* * 相比put方法，get就很单纯了，支持并发操作， * 当key为null的时候回抛出NullPointerException的异常 * get操作通过首先计算key的hash值来确定该元素放在数组的哪个位置 * 然后遍历该位置的所有节点 * 如果不存在的话返回null */public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 链表转为红黑树的过程前面在讲解tryifyBin的源码的时候讲到过，如果在当个bin上的元素超过了8个的时候，就会尝试去扩容数组或者是将链表转为红黑树。 源码：👇 12345678910111213141516171819202122232425262728private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //MIN_TREEIFY_CAPACITY 64 tryPresize(n &lt;&lt; 1); // 数组扩容 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; //使用synchronized同步器，将该节点出的链表转为树 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //hd：树的头(head) for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); //把Node组成的链表，转化为TreeNode的链表，头结点任然放在相同的位置 if ((p.prev = tl) == null) hd = p; //设置head else tl.next = p; tl = p; &#125; //把TreeNode的链表放入容器TreeBin setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd));中 &#125; &#125; &#125; &#125;&#125; 首先将Node的链表转化为一个TreeNode的链表，然后将TreeNode链表的头结点来构造一个TreeBin。下面是TreeBin构造方法的源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748TreeBin(TreeNode&lt;K,V&gt; b) &#123; //创建的TreeBin是一个空节点，hash值为TREEBIN（-2） super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125;// else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; //x代表的是转换为树之前的顺序遍历到链表的位置的节点，r代表的是根节点 for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) // dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) //当key不可以比较，或者相等的时候采取的一种排序措施 dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; //在这里判断要放的left/right是否为空，不为空继续用left/right节点来判断 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; //每次插入一个元素的时候都调用balanceInsertion来保持红黑树的平衡 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root);&#125; ConcurrentHashMap的同步机制前面分析了下ConcurrentHashMap的源码，那么，对于一个映射集合来说，ConcurrentHashMap是如果来做到并发安全，又是如何做到高效的并发的呢？ 首先是读操作，从源码中可以看出来，在get操作中，根本没有使用同步机制，也没有使用unsafe方法，所以读操作是支持并发操作的。那么写操作呢？ 分析这个之前，先看看什么情况下会引起数组的扩容，扩容是通过transfer方法来进行的。而调用transfer方法的只有trePresize、helpTransfer和addCount三个方法。 这三个方法又是分别在什么情况下进行调用的呢？ tryPresize是在treeIfybin和putAll方法中调用，treeIfybin主要是在put添加元素完之后，判断该数组节点相关元素是不是已经超过8个的时候，如果超过则会调用这个方法来扩容数组或者把链表转为树。 helpTransfer是在当一个线程要对table中元素进行操作的时候，如果检测到节点的HASH值为MOVED的时候，就会调用helpTransfer方法，在helpTransfer中再调用transfer方法来帮助完成数组的扩容 addCount是在当对数组进行操作，使得数组中存储的元素个数发生了变化的时候会调用的方法。 所以引起数组扩容的情况如下： 1、只有在往map中添加元素的时候，在某一个节点的数目已经超过了8个，同时数组的长度又小于64的时候，才会触发数组的扩容。 2、当数组中元素达到了sizeCtl的数量的时候，则会调用transfer方法来进行扩容 那么在扩容的时候，可以不可以对数组进行读写操作呢？ 事实上是可以的。当在进行数组扩容的时候，如果当前节点还没有被处理（也就是说还没有设置为fwd节点），那就可以进行设置操作。如果该节点已经被处理了，则当前线程也会加入到扩容的操作中去。 那么，多个线程又是如何同步处理的呢？ 在ConcurrentHashMap中，同步处理主要是通过Synchronized和unsafe两种方式来完成的。 1、在取得sizeCtl、某个位置的Node的时候，使用的都是unsafe的方法，来达到并发安全的目的 2、当需要在某个位置设置节点的时候，则会通过Synchronized的同步机制来锁定该位置的节点。 3、在数组扩容的时候，则通过处理的步长和fwd节点来达到并发安全的目的，通过设置hash值为MOVED 4、当把某个位置的节点复制到扩张后的table的时候，也通过Synchronized的同步机制来保证线程安全 参考资料https://juejin.cn/search?query=concurrenthashmap&amp;utm_source=gold_browser_extension&amp;utm_medium=search https://juejin.cn/post/6844904136937308168 https://www.cnblogs.com/zerotomax/p/8687425.html#go0 https://juejin.cn/post/6844903520957644808 https://processon.com/view/6049998ae401fd39d6fffbaa?fromnew=1 https://juejin.cn/post/6871793103020556295 https://juejin.cn/post/6844903641866846222 https://juejin.cn/post/6844903602423595015 ConcurrentHashMap基于JDK1.8","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之CountDownLatch工具","date":"2021-08-04T09:00:07.000Z","path":"wiki/Java并发编程之CountDownLatch工具/","text":"CountDownLatch你要问什么是CountDownLatch? 那我可有的说了。 之前干活的时候，有很多处理数据的任务，但是呢，数据量很大，写的java脚本执行下来肯定会比较慢，那怎么办呢，想起来刚毕业那会，有个同事写了一个并发调用的工具，当时感觉碉堡了。 当我查看这个工具的具体实现时，发现它是基于CountDownLatch来封装的，咱当时也没用过CountDownLatch，感觉应该挺难，就直接用了那个工具。 后来发现那个工具使用起来有些繁琐，就比如我刷数据这个事，CountDownLatch直接干是最简单的。 CountDownLatch是什么A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. 按照官方API文档上的介绍呢，CountDownLatch就是一个同步机制，用来实现一个或多个线程一直wait知道另一个线程完成一系列动作。 CountDownLatch使用1234567891011121314151617181920212223242526272829public class CountDownLatchDemo &#123; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(2); new Thread(()-&gt;&#123; try &#123; Thread.sleep(1000); latch.countDown(); System.out.println(Thread.currentThread().getName() + &quot; execute 111&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(()-&gt;&#123; try &#123; Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + &quot; execute 222&quot;); latch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); System.out.println(&quot;main thread invoke await&quot;); latch.await(); System.out.println(&quot;subThread execute end&quot;); &#125;&#125; 执行结果如下： 1234main thread invoke awaitThread-0 execute 111Thread-1 execute 222subThread execute end 下面我们就从countDown和await两个方法解析CountDownLatch的运行机制吧 CountDownLatch实现原理和ReentrantLock实现独占锁不同的是，CountDownLatch是典型的共享锁。 值得注意的是，CountDownLatch的静态内部类Sync继承了AbstractQueuedSynchronizer并实现了tryAcquireShared方法和tryReleaseShared方法。 下面先从构造方法入手开始学习 👇 1234public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); this.sync = new Sync(count);&#125; 初始化count字段，其值是设置在AQS的state字段上面的，当每个线程执行了countDown()之后，state = state - 1 当state = 0 时，唤醒之前await的线程。 await()下面是await方法： 123public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125; AQS#acquireSharedInterruptibly(int arg) 12345678public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) // 获取中断标志，把中断标志复位，然后把中断异常往上层抛 throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125; tryAcquireShared(arg)这个方法和之前学习ReentrantLock时是一样的，这是AQS提供的模版方法。 AQS提供模版方法，有每个子类自己去实现逻辑，然后再由AQS本身调用。 CountDownLatch#tryAcquireShared(int acquires) 123protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1;&#125; getState()返回的是AQS的state值，第一个线程获取是肯定不是0 如果getState()方法返回-1的话，会执行下面的方法： AQS#doAcquireSharedInterruptibly 1234567891011121314151617181920212223242526272829private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 表示aqs state = 0 // 需要把当前线程设置成头节点，并向下传播 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 避免一直空转，将前一个节点状态设置成SIGNAL,然后挂起当前线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 如果线程中断，则直接抛出异常 throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 当countDownLatch的count变成0的时候，主线程await完成，然后被唤醒，继续执行。 setHeadAndPropagate(Node node, int propagate) 123456789101112131415161718192021222324252627private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus either before * or after setHead) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don&#x27;t know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) // 如果后续节点是shard节点，释放 doReleaseShared(); &#125;&#125; countDown()123public void countDown() &#123; sync.releaseShared(1);&#125; AQS#releaseShared(int arg) 释放共享锁 12345678public final boolean releaseShared(int arg) &#123; // 有子类实现 if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; CountDownLatch#tryReleaseShared(int releases) 这个方法比较简单，每执行一次countDown(), state = state - 1 最后返回state是否等于0 如果不等于0 说明共享锁不能释放 1234567891011protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; doReleaseShared() 释放共享锁方法 12345678910111213141516171819202122232425262728293031private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; // 获取头结点的等待状态 int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 释放后继结点 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; unparkSuccessor()执行线程唤醒的方法 1234567891011121314151617181920212223242526private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 参考文章https://www.jianshu.com/p/128476015902 https://segmentfault.com/a/1190000015807573","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之搞懂线程池","date":"2021-08-04T08:43:19.000Z","path":"wiki/Java并发编程之搞懂线程池-1/","text":"ThreadPool1. 为什么存在线程池1.1 降低资源消耗通过复用已存在的线程和降低线程关闭的次数来尽可能降低系统性能损耗；（享元模式） 1.2 提升系统响应速度通过复用线程，省去创建线程的过程，因此整体上提升了系统的响应速度； 1.3 提高线程的可管理性线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，因此，需要使用线程池来管理线程。 至于为什么不允许手动创建线程池，请参见https://dayarch.top/p/why-we-need-to-use-threadpool.html 2. 线程池的工作流程线程池顾名思义，就是由很多线程构成的池子，来一个任务，就从池子中取一个线程，处理这个任务。这是一个简单的理解，实际上线程池的实现和运转是一个非常复杂的过程。 例如线程池肯定不会无限扩大的，否则资源会耗尽；当线程数到达一个阶段，提交的任务会被暂时存储在一个队列中，如果队列内容可以不断扩大，极端下也会耗尽资源，那选择什么类型的队列，当队列满如何处理任务，都有涉及很多内容。线程池总体的工作过程如下图： 线程池内的线程数的大小相关的概念有两个，一个是核心池大小，还有最大池大小。如果当前的线程个数比核心池个数小，当任务到来，会优先创建一个新的线程并执行任务。当已经到达核心池大小，则把任务放入队列，为了资源不被耗尽，队列的最大容量可能也是有上限的，如果达到队列上限则考虑继续创建新线程执行任务，如果此刻线程的个数已经到达最大池上限，则考虑把任务丢弃。 在 java.util.concurrent 包中，提供了 ThreadPoolExecutor 的实现。 12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;&#125; 3. 线程池参数既然有了刚刚对线程池工作原理对概述，这些参数就很容易理解了： 3.1 corePoolSize 核心池大小，既然如前原理部分所述。需要注意的是在初创建线程池时线程不会立即启动，直到有任务提交才开始启动线程并逐渐时线程数目达到corePoolSize。若想一开始就创建所有核心线程需调用prestartAllCoreThreads方法。 3.2 maximumPoolSize池中允许的最大线程数。需要注意的是当核心线程满且阻塞队列也满时才会判断当前线程数是否小于最大线程数，并决定是否创建新线程。 3.3 keepAliveTime当线程数大于核心时，多于的空闲线程最多存活时间 3.4 unitkeepAliveTime 参数的时间单位。 3.5 workQueue当线程数目超过核心线程数时用于保存任务的队列。主要有3种类型的BlockingQueue可供选择：无界队列，有界队列和同步移交。将在下文中详细阐述。从参数中可以看到，此队列仅保存实现Runnable接口的任务。 别看这个参数位置很靠后，但是真的很重要，因为楼主的坑就因这个参数而起，这些细节有必要仔细了解清楚。 3.6 threadFactory执行程序创建新线程时使用的工厂。 3.7 handler阻塞队列已满且线程数达到最大值时所采取的饱和策略。java默认提供了4种饱和策略的实现方式：中止、抛弃、抛弃最旧的、调用者运行。将在下文中详细阐述。 4. 可选择的阻塞队列BlockingQueue详解再重复一下新任务进入时线程池的执行策略：1、如果运行的线程少于corePoolSize，则 Executor始终首选添加新的线程，而不进行排队。（如果当前运行的线程小于corePoolSize，则任务根本不会存入queue中，而是直接运行） 2、如果运行的线程大于等于 corePoolSize，则 Executor始终首选将请求加入队列，而不添加新的线程。如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。主要有3种类型的BlockingQueue： 4.1 无界队列队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而楼主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 4.2 有界队列常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue与有界的LinkedBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 在我们的修复方案中，选择的就是这个类型的队列，虽然会有部分任务被丢失，但是我们线上是排序日志搜集任务，所以对部分对丢失是可以容忍的。 4.3 同步移交队列如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 5. 可选择的饱和策略RejectedExecutionHandler详解JDK主要提供了4种饱和策略供选择。4种策略都做为静态内部类在ThreadPoolExcutor中进行实现。 5.1 AbortPolicy中止策略该策略是默认饱和策略。 12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125; 使用该策略时在饱和时会抛出RejectedExecutionException（继承自RuntimeException），调用者可捕获该异常自行处理。 5.2 DiscardPolicy抛弃策略12public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;&#125; 如上所示，什么都不做。 5.3 DiscardOldestPolicy抛弃旧任务策略123456public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125;&#125; 如代码，先将阻塞队列中的头元素出队抛弃，再尝试提交任务。如果此时阻塞队列使用PriorityBlockingQueue优先级队列，将会导致优先级最高的任务被抛弃，因此不建议将该种策略配合优先级队列使用。 5.4 CallerRunsPolicy调用者运行12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125;&#125; 既不抛弃任务也不抛出异常，直接运行任务的run方法，换言之将任务回退给调用者来直接运行。使用该策略时线程池饱和后将由调用线程池的主线程自己来执行任务，因此在执行任务的这段时间里主线程无法再提交新任务，从而使线程池中工作线程有时间将正在处理的任务处理完成。 6. Java提供的四种常用线程池解析既然楼主踩坑就是使用了 JDK 的默认实现，那么再来看看这些默认实现到底干了什么，封装了哪些参数。简而言之 Executors 工厂方法Executors.newCachedThreadPool() 提供了无界线程池，可以进行自动线程回收；Executors.newFixedThreadPool(int) 提供了固定大小线程池，内部使用无界队列；Executors.newSingleThreadExecutor() 提供了单个后台线程。 详细介绍一下上述四种线程池。 6.1 newCachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 在newCachedThreadPool中如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。初看该构造函数时我有这样的疑惑：核心线程池为0，那按照前面所讲的线程池策略新任务来临时无法进入核心线程池，只能进入 SynchronousQueue中进行等待，而SynchronousQueue的大小为1，那岂不是第一个任务到达时只能等待在队列中，直到第二个任务到达发现无法进入队列才能创建第一个线程？这个问题的答案在上面讲SynchronousQueue时其实已经给出了，要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。因此即便SynchronousQueue一开始为空且大小为1，第一个任务也无法放入其中，因为没有线程在等待从SynchronousQueue中取走元素。因此第一个任务到达时便会创建一个新线程执行该任务。 6.2 newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 看代码一目了然了，线程数量固定，使用无限大的队列。 6.3 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 123public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 在来看看ScheduledThreadPoolExecutor（）的构造函数 1234public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); &#125; ScheduledThreadPoolExecutor的父类即ThreadPoolExecutor，因此这里各参数含义和上面一样。值得关心的是DelayedWorkQueue这个阻塞对列，在上面没有介绍，它作为静态内部类就在ScheduledThreadPoolExecutor中进行了实现。简单的说，DelayedWorkQueue是一个无界队列，它能按一定的顺序对工作队列中的元素进行排列。 6.4 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 1234public static ScheduledExecutorService newSingleThreadScheduledExecutor() &#123; return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); &#125; 首先new了一个线程数目为 1 的ScheduledThreadPoolExecutor，再把该对象传入DelegatedScheduledExecutorService中，看看DelegatedScheduledExecutorService的实现代码： 1234DelegatedScheduledExecutorService(ScheduledExecutorService executor) &#123; super(executor); e = executor;&#125; 在看看它的父类 123DelegatedExecutorService(ExecutorService executor) &#123; e = executor; &#125; 其实就是使用装饰模式增强了ScheduledExecutorService（1）的功能，不仅确保只有一个线程顺序执行任务，也保证线程意外终止后会重新创建一个线程继续执行任务。 7. 为什么禁止使用 Executors 创建线程池? 7.1 实验证明Executors缺陷12345678910111213141516171819202122public class ExecutorsDemo &#123; private static ExecutorService executor = Executors.newFixedThreadPool(15); public static void main(String[] args) &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; executor.execute(new SubThread()); &#125; &#125; &#125; class SubThread implements Runnable &#123; @Override public void run() &#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; //do nothing &#125; &#125; &#125; &#125; 通过指定JVM参数:-Xmx8m -Xms8m运行以上代码，会抛出OOM: 1234Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceededat java.util.concurrent.LinkedBlockingQueue.offer(LinkedBlockingQueue. java:416)at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor. java:1371)at com.hollis.ExecutorsDemo.main(ExecutorsDemo.java:16) 以上代码指出，ExecutorsDemo.java 的第 16 行，就是代码中的 execu- tor.execute(new SubThread());。 7.2 Executors 为什么存在缺陷通过上面的例子，我们知道了 Executors 创建的线程池存在 OOM 的风险，那 么到底是什么原因导致的呢?我们需要深入 Executors 的源码来分析一下。 其实，在上面的报错信息中，我们是可以看出蛛丝马迹的，在以上的代码中其实 已经说了，真正的导致 OOM 的其实是 LinkedBlockingQueue.offer 方法。 如果读者翻看代码的话，也可以发现，其实底层确实是通过 LinkedBlock- ingQueue 实现的: 123public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); 如果读者对 Java 中的阻塞队列有所了解的话，看到这里或许就能够明白原因了。 Java 中 的 BlockingQueue 主 要 有 两 种 实 现， 分 别 是 ArrayBlockingQ- ueue 和 LinkedBlockingQueue。 ArrayBlockingQueue 是一个用数组实现的有界阻塞队列，必须设置容量。 LinkedBlockingQueue 是一个用链表实现的有界阻塞队列，容量可以选择 进行设置，不设置的话，将是一个无边界的阻塞队列，最大长度为 Integer.MAX_ VALUE。 这里的问题就出在:不设置的话，将是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE。也就是说，如果我们不设置 LinkedBlockingQueue 的 容量的话，其默认容量将会是 Integer.MAX_VALUE。 而 newFixedThreadPool 中创建 LinkedBlockingQueue 时，并未指定容 量。此时，LinkedBlockingQueue 就是一个无边界队列，对于一个无边界队列 来说，是可以不断的向队列中加入任务的，这种情况下就有可能因为任务过多而导 致内存溢出问题。 上面提到的问题主要体现在newFixedThreadPool 和 newSingleThreadExecutor 两个工厂方法上，并不是说 newCachedThreadPool 和 newScheduledThreadPool 这两个方法就安全了，这两种方式创建的最大线程数可能是 Integer.MAX_VALUE，而创建这么多线程，必然就有可能导致 OOM。 7.3 创建线程池的正确姿势避免使用 Executors 创建线程池，主要是避免使用其中的默认实现，那么我们 可以自己直接调用 ThreadPoolExecutor 的构造函数来自己创建线程池。在创建的 同时，给 BlockQueue 指定容量就可以了。 12private static ExecutorService executor = new ThreadPoolExecutor(10, 10, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue(10)); 这种情况下，一旦提交的线程数超过当前可用线程数时，就会抛出 java.util. concurrent.RejectedExecutionException，这是因为当前线程池使用的队列 是有边界队列，队列已经满了便无法继续处理新的请求。但是异常(Exception)总比 发生错误(Error)要好。 除了自己定义 ThreadPoolExecutor 外。还有其他方法。这个时候第一时间 就应该想到开源类库，如 apache 和 guava 等。 作者推荐使用 guava 提供的 ThreadFactoryBuilder 来创建线程池。 12345678910public class ExecutorsDemo &#123; private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor. AbortPolicy()); public static void main(String[] args) &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; pool.execute(new SubThread()); &#125; &#125;&#125; 通过上述方式创建线程时，不仅可以避免 OOM 的问题，还可以自定义线程名 称，更加方便的出错的时候溯源。 8、ThreadPoolExecutor源码解析8.1 ThreadPoolExecutor类重要属性1234567891011121314151617//这个属性是用来存放 当前运行的worker数量以及线程池状态的//int是32位的，这里把int的高3位拿来记录线程池状态的标志位,后29位拿来记录当前运行worker的数量private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//存放任务的阻塞队列private final BlockingQueue&lt;Runnable&gt; workQueue;//worker的集合,用set来存放private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();//历史达到的worker数最大值private int largestPoolSize;//当队列满了并且worker的数量达到maxSize的时候,执行具体的拒绝策略private volatile RejectedExecutionHandler handler;//超出coreSize的worker的生存时间private volatile long keepAliveTime;//常驻worker的数量private volatile int corePoolSize;//最大worker的数量,一般当workQueue满了才会用到这个参数private volatile int maximumPoolSize; 8.2 ThreadPoolExecutor定义的内部状态12345678910111213141516private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 其中AtomicInteger变量ctl的功能非常强大: 利用低29位表示线程池中线程数，通过高3位表示线程池的运行状态: RUNNING: -1 &lt;&lt; COUNT_BITS，即高3位为111，该状态的线程池会接收新任务，并处理阻塞队列中的任务； SHUTDOWN: 0 &lt;&lt; COUNT_BITS，即高3位为000，该状态的线程池不会接收新任务，但会处理阻塞队列中的任务； STOP : 1 &lt;&lt; COUNT_BITS，即高3位为001，该状态的线程不会接收新任务，也不会处理阻塞队列中的任务，而且会中断正在运行的任务； TIDYING : 2 &lt;&lt; COUNT_BITS，即高3位为010, 所有的任务都已经终止； TERMINATED: 3 &lt;&lt; COUNT_BITS，即高3位为011, terminated()方法已经执行完成 8.3 execute源码解析1234567891011121314151617181920212223242526public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加任务队列并创建核心线程，然后在执行 if (addWorker(command, true)) return; c = ctl.get(); &#125; // 线程池是运行状态并且任务成功添加到队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // recheck and if necessary 回滚到入队操作前，即倘若线程池shutdown状态，就remove(command) //如果线程池没有RUNNING，成功从阻塞队列中删除任务，执行reject方法处理任务 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) // 使用普通线程运行任务 addWorker(null, false); &#125; // 往线程池中创建新的线程失败，则reject任务 else if (!addWorker(command, false)) reject(command);&#125; 思考🤔 为什么需要double check线程池的状态? 在多线程环境下，线程池的状态时刻在变化，而ctl.get()是非原子操作，很有可能刚获取了线程池状态后线程池状态就改变了。判断是否将command加入workque是线程池之前的状态。倘若没有double check，万一线程池处于非running状态(在多线程环境下很有可能发生)，那么command永远不会执行。 addWorker方法 先看看addWorker方法的注释，方便我们理解源码 1234567891011121314151617181920212223242526272829303132Checks if a new worker can be added with respect to current * pool state and the given bound (either core or maximum). If so, * the worker count is adjusted accordingly, and, if possible, a * new worker is created and started, running firstTask as its * first task. This method returns false if the pool is stopped or * eligible to shut down. It also returns false if the thread * factory fails to create a thread when asked. If the thread * creation fails, either due to the thread factory returning * null, or due to an exception (typically OutOfMemoryError in * Thread.start()), we roll back cleanly. * // 大概翻译如下： //1、首先先检查线程池的状态和线程数量是否超过界限 //2、如果可以创建的话，需要更新任务的数量，然后运行任务 //3、有两种情况这个方法会返回false，线程池stop状态或者shut down状态 //还有一种情况是共创创建线程失败 4、不管是发生什么异常，例如线程工厂返回null或者是发生了OOM,直接回滚 * @param firstTask the task the new thread should run first (or * null if none). Workers are created with an initial first task * (in method execute()) to bypass queuing when there are fewer * than corePoolSize threads (in which case we always start one), * or when the queue is full (in which case we must bypass queue). * Initially idle threads are usually created via * prestartCoreThread or to replace other dying workers. * * @param core if true use corePoolSize as bound, else * maximumPoolSize. (A boolean indicator is used here rather than a * value to ensure reads of fresh values after checking other pool * state). * @return true if successful 下面是源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // cas修改c的值 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); // largestPoolSize记录的最大workers长度 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 线程启动，执行任务(Worker.thread(firstTask).start()); t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 线程start之后会执行如下run方法： 1234 /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this);&#125; 下面是runWorker方法 12345678910111213141516171819202122232425262728293031323334353637383940414243final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125; &#125; 通过getTask方法从阻塞队列中获取等待的任务，如果队列中没有任务，getTask方法会被阻塞并挂起，不会占用cpu资源； getTask()方法源码如下 下面来看一下getTask()方法，这里面涉及到keepAliveTime的使用，从这个方法我们可以看出先吃池是怎么让超过corePoolSize的那部分worker销毁的。 12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; allowCoreThreadTimeOut为false，线程即使空闲也不会被销毁；倘若为ture，在keepAliveTime内仍空闲则会被销毁。 如果线程允许空闲等待而不被销毁timed == false，workQueue.take任务: 如果阻塞队列为空，当前线程会被挂起等待；当队列中有任务加入时，线程被唤醒，take方法返回任务，并执行； 如果线程不允许无休止空闲timed == true, workQueue.poll任务: 如果在keepAliveTime时间内，阻塞队列还是没有任务，则返回null；","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之Blocking Queue","date":"2021-08-03T08:48:32.000Z","path":"wiki/Java并发编程之Blocking-Queue/","text":"Blocking QueueA blocking queue is a queue that blocks when you try to dequeue from it and the queue is empty, or if you try to enqueue items to it and the queue is already full. A thread trying to dequeue from an empty queue is blocked until some other thread inserts an item into the queue. A thread trying to enqueue an item in a full queue is blocked until some other thread makes space in the queue, either by dequeuing one or more items or clearing the queue completely. 阻塞队列两大特性： 当队列满时，如果生产者线程向队列 put 元素，队列会一直阻塞生产者线程，直到队列可用或者响应中断退出 当队列为空时，如果消费者线程 从队列里面 take 元素，队列会阻塞消费者线程，直到队列不为空 阻塞队列最常使用在生产者和消费者模型中，生产者生产数据，将数据存放在队列中，消费者消费数据，在队列中取出数据。 阻塞队列在不可用时，下面是各种处理操作的结果：👇 方法/处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e, time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 add 抛出异常IllegalStateException12345678public class ArrayBlockingQueueDemo &#123; public static void main(String[] args) &#123; ArrayBlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(1); queue.add(&quot;a&quot;); queue.add(&quot;b&quot;); System.err.println(&quot;queue size -&gt; &quot; + queue.size()); &#125;&#125; 异常信息： 1234Exception in thread &quot;main&quot; java.lang.IllegalStateException: Queue full at java.util.AbstractQueue.add(AbstractQueue.java:98) at java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:312) at com.ibli.note.ArrayBlockingQueueDemo.main(ArrayBlockingQueueDemo.java:15) element抛出异常NoSuchElementException1234567public class ArrayBlockingQueueDemo &#123; public static void main(String[] args) &#123; ArrayBlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(1); System.err.println(&quot;queue size -&gt; &quot; + queue.size()); queue.element(); &#125;&#125; 异常信息： 1234queue size -&gt; 0Exception in thread &quot;main&quot; java.util.NoSuchElementException at java.util.AbstractQueue.element(AbstractQueue.java:136) at com.ibli.note.ArrayBlockingQueueDemo.main(ArrayBlockingQueueDemo.java:14) ArrayBlockingQueue底层由数组实现的有界的阻塞队列，它的容量在创建的时候就已经确认了，并且不能修改。 12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125; 默认情况下，ArrayBlockingQueue是不保证线程公平访问队列的，这里所谓的公平与否是指，阻塞的线程能否按照阻塞的先后顺序访问队列，先阻塞先访问，后阻塞后访问。 思考为什么默认情况下是非公平的方式访问呢？ 🤔 这个是为了增加系统资源利用率，在不保证公平的情况下，多线程之间之间执行的效率要比公平模式下高的多。 ArrayBlovkingQueue#put方法 下面是put方法源码： 12345678910111213141516public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; // 加锁 lock.lockInterruptibly(); try &#123; while (count == items.length) // 队列满了之后，阻塞 notFull.await(); // 向队列中添加元素 enqueue(e); &#125; finally &#123; // 执行完最后释放锁 lock.unlock(); &#125;&#125; 下面是添加数据的方法源码： 1234567891011private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; // 数据添加完之后，唤醒等待队列中的线程到同步队列 notEmpty.signal();&#125; ‼️唤醒的线程能够抢到锁是不确定的，signal会添加节点到同步队列中等待获取锁。这个可以看一下Condition那篇文章。 ArrayBlockingQueue更多详细细节以及原理跳转链接https://www.jianshu.com/p/a636b3d83911 LinkedBlockingQueueLinkedBlockingQueue是用链表实现的有界阻塞队列，同样满足FIFO的特性，与ArrayBlockingQueue相比起来具有更高的吞吐量，为了防止LinkedBlockingQueue容量迅速增，损耗大量内存。通常在创建LinkedBlockingQueue对象时，会指定其大小，如果未指定，容量等于Integer.MAX_VALUE; Executors.newFixedThreadPool 阿里巴巴禁止使用Executors来创建线程池 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 属性信息12345678910111213141516171819202122232425262728293031323334353637/** * 节点类，用于存储数据 */static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125;&#125;/** 阻塞队列的大小，默认为Integer.MAX_VALUE */private final int capacity;/** 当前阻塞队列中的元素个数 */private final AtomicInteger count = new AtomicInteger();/** * 阻塞队列的头结点 */transient Node&lt;E&gt; head;/** * 阻塞队列的尾节点 */private transient Node&lt;E&gt; last;/** 获取并移除元素时使用的锁，如take, poll, etc */private final ReentrantLock takeLock = new ReentrantLock();/** notEmpty条件对象，当队列没有数据时用于挂起执行删除的线程 */private final Condition notEmpty = takeLock.newCondition();/** 添加元素时使用的锁如 put, offer, etc */private final ReentrantLock putLock = new ReentrantLock();/** notFull条件对象，当队列数据已满时用于挂起执行添加的线程 */private final Condition notFull = putLock.newCondition(); 构造函数123456789101112131415161718192021222324252627282930public LinkedBlockingQueue() &#123; // 默认大小为Integer.MAX_VALUE this(Integer.MAX_VALUE);&#125;public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125;public LinkedBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this(Integer.MAX_VALUE); final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; int n = 0; for (E e : c) &#123; if (e == null) throw new NullPointerException(); if (n == capacity) throw new IllegalStateException(&quot;Queue full&quot;); enqueue(new Node&lt;E&gt;(e)); ++n; &#125; count.set(n); &#125; finally &#123; putLock.unlock(); &#125;&#125; LinkedBlockingQueue#put方法1234567891011121314151617181920212223242526public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; // 获取锁中断 putLock.lockInterruptibly(); try &#123; //判断队列是否已满，如果已满阻塞等待 while (count.get() == capacity) &#123; notFull.await(); &#125; // 把node放入队列中 enqueue(node); c = count.getAndIncrement(); // 再次判断队列是否有可用空间，如果有唤醒下一个线程进行添加操作 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; // 如果队列中有一条数据，唤醒消费线程进行消费 if (c == 0) signalNotEmpty();&#125; 队列已满，阻塞等待。 队列未满，创建一个node节点放入队列中，如果放完以后队列还有剩余空间，继续唤醒下一个添加线程进行添加。如果放之前队列中没有元素，放完以后要唤醒消费线程进行消费。 ArrayBlockingQueue与LinkedBlockingQueue的比较相同点：ArrayBlockingQueue和LinkedBlockingQueue都是通过condition通知机制来实现可阻塞式插入和删除元素，并满足线程安全的特性； 不同点： 1、ArrayBlockingQueue底层是采用的数组进行实现，而LinkedBlockingQueue则是采用链表数据结构； 2、ArrayBlockingQueue插入和删除数据，只采用了一个lock，而LinkedBlockingQueue则是在插入和删除分别采用了putLock和takeLock，这样可以降低线程由于线程无法获取到lock而进入WAITING状态的可能性，从而提高了线程并发执行的效率 更多LinkedBlockingQueue的实现细节参见https://blog.csdn.net/tonywu1992/article/details/83419448 PriorityBlockingQueuePriorityBlockingQueue是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现compareTo()方法来指定元素排序规则，或者初始化时通过构造器参数Comparator来指定排序规则。 123456789public PriorityBlockingQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator) &#123; if (initialCapacity &lt; 1) throw new IllegalArgumentException(); this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); this.comparator = comparator; this.queue = new Object[initialCapacity]; &#125; 使用优先级队列需要注意的点： 1、队列中不允许出现null值，也不允许出现不能排序的元素。 2、队列容量是没有上限的，但是如果插入的元素超过负载，有可能会引起OutOfMemory异常。 当我们使用无界队列是都应该注意的点，不能在队列中无限存放数据 3、PriorityBlockingQueue由于是无界的，所以put方法是非阻塞的。 123public void put(E e) &#123; offer(e); // never need to block 请自行对照上面表格&#125; 可以给定初始容量，这个容量会按照一定的算法自动扩充 123456// Default array capacity.private static final int DEFAULT_INITIAL_CAPACITY = 11;public PriorityBlockingQueue() &#123; this(DEFAULT_INITIAL_CAPACITY, null);&#125; 这里默认的容量是 11，由于也是基于数组。 4、内部只有一个Lock，所以生产消费者不能同时作业 详情可以参照https://www.cnblogs.com/wyq1995/p/12289462.html DelayQueueDelayQueue顾名思义，具有延时作用的队列。 记得第一次接触延时队列的时候是在看分布式任务调度时看到底层有关延时队列的实现。 DelayQueue 也是一个无界阻塞队列，使用时要注意OOM。 只有delay时间小于0的元素才能够被取出。 生产者消费者模型创建一个类，实现Delayed方法，重写getDelay方法和compareTo方法； 12345678910111213141516171819202122232425262728293031323334353637public class DelayData implements Delayed &#123; private long second; private String val; public DelayData(long second, String val) &#123; long l = System.currentTimeMillis(); System.err.println(second + &quot; &quot; + l); this.second = second + l; this.val = val; &#125; public long getSecond() &#123; return second; &#125; @Override public long getDelay(TimeUnit unit) &#123; long diffTime = second - System.currentTimeMillis(); return unit.convert(diffTime,TimeUnit.MILLISECONDS); &#125; @Override public int compareTo(Delayed o) &#123; DelayData tmp = (DelayData) o; long result = second - tmp.getSecond() ; return (int) result; &#125; @Override public String toString() &#123; return &quot;DelayData&#123;&quot; + &quot;second=&quot; + second + &quot;, val=&#x27;&quot; + val + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 然后创建两个线程模拟生产者和消费者 123456789101112131415161718192021222324252627282930313233public class DelayQueueDemo &#123; public static void main(String[] args) &#123; DelayQueue&lt;DelayData&gt; delayQueue = new DelayQueue&lt;DelayData&gt;(); new Thread(() -&gt; &#123; delayQueue.put(new DelayData(5000, &quot;a&quot;)); delayQueue.put(new DelayData(10000, &quot;b&quot;)); delayQueue.put(new DelayData(15000, &quot;c&quot;)); &#125;).start(); new Thread(() -&gt; &#123; boolean flag = true; while (true &amp;&amp; flag) &#123; try &#123; Thread.sleep(1000); System.err.println(&quot;执行一次循环 队列长度&quot; + delayQueue.size()); DelayData poll = delayQueue.take(); if (poll != null)&#123; System.err.println(poll.toString()); &#125; if (delayQueue.size() == 0)&#123; flag = false; break; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125; SynchronousQueueSynchronousQueue实际上它不是一个真正的队列，因为它不会维护队列中元素的存储空间，与其他队列不同的是，它维护一组线程，这些线程在等待把元素加入或移除队列。适用于生产者少消费者多的情况。 SynchronousQueue是生产者直接把数据给消费者（消费者直接从生产者这里拿数据）。换句话说，每一个插入操作必须等待一个线程对应的移除操作。SynchronousQueue又有两种模式： 1、公平模式 采用公平锁，并配合一个FIFO队列（Queue）来管理多余的生产者和消费者 2、非公平模式 采用非公平锁，并配合一个LIFO栈（Stack）来管理多余的生产者和消费者，这也是SynchronousQueue默认的模式 构造方法123456 public SynchronousQueue() &#123; this(false); &#125;public SynchronousQueue(boolean fair) &#123; transferer = fair ? new TransferQueue() : new TransferStack();&#125; transferer 是一个内部类用于在生产者和消费者之间传递数据 实现生产者消费者下面模拟一个生产者生产数据，两个消费者消费数据。 123456789101112131415161718192021222324252627282930313233343536373839404142public class SynchronousQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; SynchronousQueue queue = new SynchronousQueue(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(2000L); while (true) &#123; Thread.sleep(100); long l = System.currentTimeMillis(); queue.put(l); System.out.println(Thread.currentThread().getName() + &quot; 生产者生产数据 :&quot; + l); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); while (true) &#123; Thread.sleep(300); System.out.println(Thread.currentThread().getName() + &quot;消费者消费数据 ： &quot; + queue.take()); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); while (true) &#123; Thread.sleep(300); System.out.println(Thread.currentThread().getName() + &quot;消费者消费数据 ： &quot; + queue.take()); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 代码运行结果： 1234567Thread-0 生产者生产数据 :1628055947404Thread-1消费者消费数据 ： 1628055947404Thread-0 生产者生产数据 :1628055947506Thread-2消费者消费数据 ： 1628055947506Thread-0 生产者生产数据 :1628055947608Thread-2消费者消费数据 ： 1628055947608Thread-0 生产者生产数据 :1628055947713 SynchronousQueue详细实现细节参见https://blog.csdn.net/yanyan19880509/article/details/52562039 参考资料https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html http://tutorials.jenkov.com/java-concurrency/index.html https://www.baeldung.com/java-blocking-queue https://blog.csdn.net/tonywu1992/article/details/83419448","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之中断机制","date":"2021-08-03T07:56:37.000Z","path":"wiki/Java并发编程之中断机制/","text":"中断机制Java语言提供一种机制来试图“终止”一些特殊的线程，比如一下空转的线程一直消耗系统资源，可以使用中断的方式来停止这一类的线程，这就是Java中断机制。 1、中断注意的地方1、Java中线程间是协作式，而非抢占式. 调用一个线程的interrupt() 方法中断一个线程，并不是强行关闭这个线程，只是跟这个线程打个招呼，将线程的中断标志位置为true，线程是否中断，由线程本身决定。 2、isInterrupted() 判定当前线程是否处于中断状态。 3、静态方法 interrupted() 判定当前线程是否处于中断状态，同时中断标志位改为 false。 4、如果方法里如果抛出中断异常 InterruptedException，则线程的中断标志位会被复位成false，如果确实是需要中断线程，要求我们自己在catch语句块里再次调用interrupt()。 5、Java 中所有的阻塞方法都会抛出 InterruptedException，比如wait(), join(),sleep()。 2、Java中断提供的方法在Java中提供了3个有关中断的方法： Thread.currentThread().isInterrupted() 判断当前的线程是否被中断 thread.interrupt(); 中断一个线程，将中断标志设置成true 1234567891011121314public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; Thread.interrupted()123public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; 判断线程是否被中断，并清除中断标志，改成false； 验证一下就可以了 👇 12345678public static void main(String[] args) &#123; System.err.println(Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); System.err.println(Thread.currentThread().isInterrupted()); boolean interrupted = Thread.interrupted(); System.err.println(&quot;interrupted &quot; + interrupted); System.err.println(Thread.currentThread().isInterrupted());&#125; 3、中断例子1234567891011121314151617181920212223242526272829public class InterrupterDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (true &amp;&amp; !Thread.currentThread().isInterrupted())&#123; System.err.println(1); System.err.println(Thread.interrupted()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; System.err.println(&quot;after sleep &quot;+ Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); boolean interrupted = Thread.interrupted(); System.err.println(&quot;interrupted &quot;+ Thread.currentThread().isInterrupted() + &quot;/ &quot; + interrupted); Thread.currentThread().interrupt(); System.err.println(&quot;final sleep &quot;+ Thread.currentThread().isInterrupted()); break; &#125; System.err.println(2); &#125; &#125;); thread.start(); Thread.sleep(3000); thread.interrupt(); &#125;&#125; 注意，中断一场不要【 吞掉 】，要不在程序中相应中断一场，进行相应的逻辑处理，或者将一场继续向上抛，由上层处理。 参考资料https://dayarch.top/p/java-concurrency-interrupt-mechnism.html","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之Condition机制底层","date":"2021-08-03T04:00:16.000Z","path":"wiki/Java并发编程之Condition机制底层/","text":"Lock框架中的Condition机制还是看一下之前ReentrantLock中调用condition方法的流程图 👇 任何一个java对象都天然继承于Object类，在线程间实现通信的往往会应用到Object的几个方法，比如wait(),wait(long timeout),wait(long timeout, int nanos)与notify(),notifyAll()几个方法实现等待/通知机制，同样的， 在java Lock体系下依然会有同样的方法实现等待/通知机制。 从整体上来看Object的wait和notify/notify是与对象监视器配合完成线程间的等待/通知机制，而Condition与Lock配合完成等待通知机制，前者是java底层级别的，后者是语言级别的，具有更高的可控制性和扩展性。两者除了在使用方式上不同外，在功能特性上还是有很多的不同： Condition能够支持不响应中断，而通过使用Object方式不支持； Condition能够支持多个等待队列（new 多个Condition对象），而Object方式只能支持一个； Condition能够支持超时时间的设置，而Object不支持 1. Condition接口提供的方法1.1 await方法void await() throws InterruptedException 当前线程进入等待状态，如果其他线程调用condition的signal或者signalAll方法并且当前线程获取Lock从await方法返回，如果在等待状态中被中断会抛出被中断异常； long awaitNanos(long nanosTimeout) 当前线程进入等待状态直到被通知，中断或者超时； boolean await(long time, TimeUnit unit)throws InterruptedException 同第二种，支持自定义时间单位 boolean awaitUntil(Date deadline) throws InterruptedException 当前线程进入等待状态直到被通知，中断或者到了某个时间 1.2 signal方法void signal() 唤醒一个等待在condition上的线程，将该线程从等待队列中转移到同步队列中，如果在同步队列中能够竞争到Lock则可以从等待方法中返回。 void signalAll() 与1的区别在于能够唤醒所有等待在condition上的线程。 2. Condition在ReentrantLock中的使用下面先通过一个例子看一下Condition的使用 👇 1、大致流程就是线程1先获取lock之后，执行线程1的方法，然后调用condition.await();方法阻塞当前线程；同时加入Condition等待队列 2、线程1释放lock之后，线程2而已经在同步队列中了，线程2获取lock执行权，执行condition.signal()方法唤醒线程1 3、线程1被唤醒之后，node节点重新添加到同步队列中，等待获取执行权限，在线程2调用了unlock()方法之后，线程1重新获取到lock之后，执行后续流程。 123456789101112131415161718192021222324252627282930313233343536public class ReentrantLockDemo &#123; static Lock lock = new ReentrantLock(); public static void main(String[] args) &#123; Condition condition = lock.newCondition(); new Thread(()-&gt;&#123; System.err.println(&quot;enter thread 1 &quot;); lock.lock(); try &#123; try &#123; System.err.println(&quot;thread 1 invoke await&quot;); condition.await(); System.err.println(&quot;thread 1 invoked signal&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.err.println(&quot;exit thread 1 &quot;); &#125;finally &#123; lock.unlock(); &#125; &#125;).start(); new Thread(()-&gt;&#123; System.err.println(&quot;enter thread 2 &quot;); lock.lock(); try &#123; System.err.println(&quot;thread 2 invoke signal&quot;); condition.signal(); System.err.println(&quot;exit thread 2 &quot;); &#125;finally &#123; lock.unlock(); &#125; &#125;).start(); &#125;&#125; 上面代码的执行结果可以猜想一下 1234567enter thread 1 thread 1 invoke awaitenter thread 2 thread 2 invoke signalexit thread 2 thread 1 invoked signalexit thread 1 3. Condition等待/通知实现原理要想能够深入的掌握condition还是应该知道它的实现原理，现在我们一起来看看condiiton的源码。创建一个condition对象是通过lock.newCondition(),而这个方法实际上是会new出一个ConditionObject对象，该类是AQS的一个内部类，和Node类一样，非常重要。 condition是要和lock配合使用的也就是condition和Lock是绑定在一起的，而lock的实现原理又依赖于AQS，自然而然ConditionObject作为AQS的一个内部类无可厚非。 我们知道在锁机制的实现上，AQS内部维护了一个同步队列，如果是独占式锁的话，所有获取锁失败的线程的尾插入到同步队列，同样的，condition内部也是使用同样的方式，内部维护了一个 等待队列，所有调用condition.await方法的线程会加入到等待队列中，并且线程状态转换为等待状态。 另外注意到ConditionObject中有两个成员变量： private transient Node firstWaiter; private transient Node lastWaiter; 在AQS中condition队列可以存在多个如下所示，但是同步队列之可能是一个，值得注意的是，同步队列是一个双向链表队列，而等待队列是一个单向的队列。 下面从await方法入手来学习Condition的机制是如何运转的。 3.1 等待awaitpublic class ConditionObject implements Condition AQS#ConditionObject内部类实现了Condition接口的await方法： 1234567891011121314151617181920212223242526public final void await() throws InterruptedException &#123; // 判断线程是否中断 if (Thread.interrupted()) throw new InterruptedException(); // 将节点添加到等待队列 Node node = addConditionWaiter(); // 进入等待队列中的线程需要释放lock让给别的线程 int savedState = fullyRelease(node); int interruptMode = 0; // 如果节点不在同步队列，则挂起当前线程，知道进入同步队列或者被中断 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 调用await的线程会一直阻塞在上面的while循环，知道被唤醒或者相应中断，才会执行下面的方法 // 进入同步队列尝试获取lock，和之前一样，为了限制一直空转，会在第二次循环之后，park此节点，知道队列中轮到这个线程出队 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled // 清除掉取消的节点，踢出等待队列 unlinkCancelledWaiters(); //处理被中断的情况 if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; AQS#addConditionWaiter 添加节点到等待队列 123456789101112131415private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125; 这个方法应该比较好理解吧，就是添加一个节点，到等待队列。 ⚠️ 这里和把节点添加到同步队列还有点区别，不知道大家还有没有印象，在同步队列添加节点的时候，先判断tail是否为空，如果不是空，则直接添加；如果是空，则调用了enq(Node node)方法，先生成一个head节点，然后在把当前节点添加到后面，循环了两遍的。 这里是直接创建当前节点，然后将firstWaiter指针指向了node； AQS#fullyRelease 释放lock 123456789101112131415final int fullyRelease(Node node) &#123; boolean failed = true; try &#123; int savedState = getState(); if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 这个方法也不难，想一下，线程都已经调用await方法了，而且上一步就已经把节点添加到了等待队列中了，那么接下来要做什么呢？那肯定是释放锁lock了。对，这个方法就是做这个的。release方法之前已经介绍了，无非就是对state做一下减法，把对战线程清空一下，给新来的线程腾地方。 下面才是await的关键核心代码：‼️ 12345while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; isOnSyncQueue(node)判断当前节点是否在同步队列中，为什么要这个判断呢？原因很简单，当别的线程或者自己调用了signal方法之后，会把当前节点转移到同步队列中，在同步队列中说明什么呢，说明接下来这个线程要去竞争锁了，也就是被唤醒了，当竞争锁成功之后，这个线程就可以await后面的方法了。 (interruptMode = checkInterruptWhileWaiting(node)) != 0 如果当前线程被中断，则可以直接跳出循环，去竞争锁。 3.2 通知signal调用condition的signal或者signalAll方法可以将等待队列中等待时间最长的节点移动到同步队列中，使得该节点能够有机会获得lock。按照等待队列是先进先出（FIFO）的，所以等待队列的头节点必然会是等待时间最长的节点，也就是每次调用condition的signal方法是将头节点移动到同步队列中。signal方法源码为： 123456789public final void signal() &#123; //1. 先检测当前线程是否已经获取lock，如果没有获得锁，肯定是说不通的 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //2. 获取等待队列中第一个节点，之后的操作都是针对这个节点 Node first = firstWaiter; if (first != null) doSignal(first);&#125; signal方法首先会检测当前线程是否已经获取lock，如果没有获取lock会直接抛出异常，如果获取的话再得到等待队列的头指针引用的节点，之后的操作的doSignal方法也是基于该节点。下面我们来看看doSignal方法做了些什么事情。 AQS#doSignal 12345678910private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; //1. 将头结点从等待队列中移除 first.nextWaiter = null; //2. while中transferForSignal方法对头结点做真正的处理 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125; 具体逻辑请看注释，真正对头节点做处理的逻辑在transferForSignal放，该方法源码为： 12345678910111213141516171819202122final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ //1. 更新状态为0，加入同步队列的节点的初始状态是0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ //2.将该节点移入到同步队列中去 Node p = enq(node); int ws = p.waitStatus; // p节点是node的前置节点，需要将前驱节点的状态设置成Node.SIGNAL if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125; 关键逻辑请看注释，这段代码主要做了两件事情 1.将头结点的状态更改为CONDITION； 2.调用enq方法，将该节点尾插入到同步队列中，并且把前驱节点的状态设置成Node.SIGNAL 现在我们可以得出结论：调用condition的signal的前提条件是当前线程已经获取了lock，该方法会使得等待队列中的头节点即等待时间最长的那个节点移入到同步队列，而移入到同步队列后才有机会使得等待线程被唤醒，即从await方法中的LockSupport.park(this)方法中返回，从而才有机会使得调用await方法的线程成功退出。 signalAll方法通知所有等待线程 sigllAll与sigal方法的区别体现在doSignalAll方法上，前面我们已经知道doSignal方法只会对等待队列的头节点进行操作，而doSignalAll的源码为： 123456789private void doSignalAll(Node first) &#123; lastWaiter = firstWaiter = null; do &#123; Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; &#125; while (first != null);&#125; 该方法只不过时间等待队列中的每一个节点都移入到同步队列中，即“通知”当前调用condition.await()方法的每一个线程。 面试题 两个线程交替顺序打印1～10012345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.ibli.note;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class WaitNotifyDemo implements Runnable &#123; int count = 1; private Condition condition; private Lock lock; public WaitNotifyDemo(Condition condition, Lock lock) &#123; this.condition = condition; this.lock = lock; &#125; @Override public void run() &#123; while (true) &#123; lock.lock(); try &#123; condition.signal(); if (count &gt; 100) &#123; break; &#125; System.err.println(Thread.currentThread().getName() + &quot; =&gt; &quot; + count); count++; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;finally &#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; Lock lock = new ReentrantLock(); Condition condition = lock.newCondition(); WaitNotifyDemo waitNotifyDemo = new WaitNotifyDemo(condition, lock); new Thread(waitNotifyDemo).start(); new Thread(waitNotifyDemo).start(); &#125;&#125; 参考资料https://juejin.cn/post/6844903602419400718 https://juejin.cn/post/6844903654873382925","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之深入理解ReentrantLock","date":"2021-08-02T08:00:33.000Z","path":"wiki/Java并发编程之深入理解ReetrantLock/","text":"ReentrantLockReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞 加锁操作支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞 下面以非公平锁的lock方法为例，看一下ReentrantLock源码的实现 👇 首先是lock方法1、进入lock方法首先对调用compareAndSetState(0,1)去尝试获取锁，这一点正是体现了非公平锁 2、如果第一步没有获取到锁，然后执行第二步acquire(1) 1234567final void lock() &#123; // 非公平锁 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; lock方法首先会去cas修改AQS的state状态，独占锁模式下state增加1表示获取锁成功；state设置成功之后，需要将独占线程字段设置成当前线程：exclusiveOwnerThread = thread; AQS#acquire(1)如果没有抢占到锁，那么执行下面的acquire方法，这个方法定义在AQS类中 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; tryAcquire方法是在子类实现的，在这里我们看一下ReentrantLock的nonfairTryAcquire，也就是非公平锁的实现。 nonfairTryAcquire(int acquires)方法下面是ReentrantLock，非公平锁的lock实现代码： 1234567891011121314151617181920final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); // 获取锁 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; // 锁冲入 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; int c = getState() == 0 则表示没有线程占有锁，当前线程来加锁时，可以直接使用cas尝试获取锁。 current == getExclusiveOwnerThread() 表示当前线程已经持有线程锁了， int nextc = c + acquires;则表示支持锁重入，nextc的值则表示锁重入的次数； 以上如果没有加锁成功，则返回false，然后执行AQS的acquireQueue方法，首先将当前节点封装成addWaiter(Node.EXCLUSIVE), arg) 添加到同步队列，同时判断头节点是否获取锁成功，如果成功了，将当前节点添加到头上； AQS#addWaiter(Node mode)添加节点到队列中，Node.EXCLUSIVE独占锁，这里采用的是尾插法，在队列的队尾添加新的节点。 12345678910111213141516private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // 如果队列不是空的，则直接添加到队尾 if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 如果是空的，则调用enq方法，创建队列，并添加到队尾 enq(node); return node;&#125; AQS#enq(final Node node)第一个线程获取锁的时候，肯定是无锁的状态，根本走不到这一步，最早走到这里的是第二个去获取锁的线程。 当第二个线程执行到该方法是需要执行两次循环： 1、t == null时，需要初始化队列 2、执行下一次循环，将node添加到tail,由于这个方法还是处在并发环境下的，所以，设置队尾的时候还是需要cas操作。 123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; AQS#acquireQueued(final Node node, int arg)这个方法绝对是绝对的AQS核心方法 ‼️ 这个方法主要有3个重要操作： 1、判断前置节点是不是head，如果是的话，去尝试获取锁； 2、如果前置节点不是head，要把前置节点的waitState设置成SIGNAL，同时park当前线程，避免一直空转，因为这里是用的 for (;;) {} 3、如果获取锁和park都失败了，则把当前节点设置成cancel状态。 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; AQS.cancelAcquire(Node node)这个方法比较难理解，总结一下就干了下面几个事： 1、执行到这个方法的node肯定是要取消的，那个需要thread设置成null 2、查看当前节点之前的节点有没有是取消状态的，一起踢出队列 3、把当前节点设置成Node.CANCELLED状态 4、判断node在队列中的位置，如果是队尾的话，把tail指向node的前置节点，并且把前驱节点的next指向null 5、如果不是tail节点，那么判断是不是head，如果不是head，那么，将node的前驱节点的状态设置成Node.SIGNAL，并且把node的前驱节点node的next节点 6、如果node是head节点，那么直接unpark此线程去执行acquire 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private void cancelAcquire(Node node) &#123; // Ignore if node doesn&#x27;t exist if (node == null) return; node.thread = null; // Skip cancelled predecessors Node pred = node.prev; while (pred.waitStatus &gt; 0) //cancelled node.prev = pred = pred.prev; // predNext is the apparent node to unsplice. CASes below will // fail if not, in which case, we lost race vs another cancel // or signal, so no further action is necessary. Node predNext = pred.next; // Can use unconditional write instead of CAS here. // After this atomic step, other Nodes can skip past us. // Before, we are free of interference from other threads. node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // If successor needs signal, try to set pred&#x27;s next-link // so it will get one. Otherwise wake it up to propagate. int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; // if执行的逻辑是把前置节点设置成Node.SIGNAL Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) // 把node的前置前置节点的下一个节点指向node的下一个节点，因为上面node已经是Node.CANCELLED状态了，需要踢出队列 compareAndSetNext(pred, predNext, next); &#125; else &#123; // 前置节点是head，此时没有被人竞争锁资源，直接唤醒当前节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125; &#125; 上面是以ReentrantLock的非公平锁为例学习了一下ReentrantLock加锁的过程。那么思考一下公平锁和非公平锁的有什么区别呢？🤔 理解了上面的流程之后，下面直接比较源码遍很好理解两者之间的区别！ 公平锁和非公平锁如何制定ReentarntLock的公平锁和非公平锁？ 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 上面看了NonfairSync#lock的实现，下面看一下FairSync#lock的实现：👇 12345678910111213141516171819202122232425262728293031static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; final void lock() &#123; acquire(1); &#125; /** * Fair version of tryAcquire. Don&#x27;t grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125;&#125; FairSync和NonfairSync都是ReentrantLock的静态内部类，在FairSync的lock方法中，没有下面的代码： 12if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); 每一个线程都直接调用AQS#acquire(1)方法，而且在ReentrantLock#FairSync#FairSync(int acquires)的实现中，添加了一个判断 12345if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; 也就是hasQueuedPredecessors方法，这个方法的作用是判断队列中是否有节点在等待，如果有的话，ReentrantLock#FairSync#FairSync(int acquires)直接返回false，当前节点智能进入到队列中。这两点就是公平锁和非公平锁的明显区别体现。 释放锁操作 unlock()123public void unlock() &#123; sync.release(1);&#125; AQS#release(int arg)123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease的具体实现仍是有具体的子类来实现的。 ReentrantLock#tryRelease(int releases)方法1、释放锁的逻辑应该比较好理解，是将state做减法。 2、判断state == 0 , 则表示无锁状态，如果不是0，则表示还在线程重入的状态下，同时设置state 123456789101112protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 这里注意一点，设置state的时候是直接赋值的，而没有使用cas，为什么？ 123protected final void setState(int newState) &#123; state = newState;&#125; 其实考虑到上下文就很简单了，此时设置state的时候，有两种状态，无锁和重入锁，肯定不会是多线程的场景。所以不需要cas操作。 接着分析上面的AQS#release方法: 当state设置成功之后，需要判断head节点，然后唤醒head的后驱节点的线程，如果存在的话。 12345678910111213141516171819202122232425262728private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; // if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 这里是共享锁，在ReentarntLock先跳过 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; tryLock(long time, TimeUnit unit)方法描述如下：在给定的等待时间内并且线程没有被中断以及锁可用的情况下，去获取锁。如果锁可用，方法会直接返回。如果锁不可用，则当前线程将会处于不可用状态以达到线程调度目的，并且休眠直到下面三个事件中的一个发生：①、当前线程获取到锁②、其他线程中断当前线程③、指定的等待时间已过假如当前线程：在该方法的条目上设置其中断状态或在获取锁时中断，并且支持锁获取中断时，将抛出中断异常，当前线程中断状态会被清除。假如给定的等待时间已过，将会返回false。 下面具体阅读源码实现,方法的入参指定了等待时间，和时间的单位，有NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS…等单位。 下面具体阅读源码实现,方法的入参指定了等待时间，和时间的单位，有NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS…等单位。 1234public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(timeout));&#125; 方法的内部调用了Sync的tryAcquireNanos，继续往下 123456789public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; //判断中断状态并决定是否抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //尝试获取锁，如果成功则返回true，失败则调用doAcquireNanos进行等待 return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout);&#125; tryAcqure和之前分析的是同一个方法，不再赘述。接下来是doAcquireNanos方法 12345678910111213141516171819202122232425262728293031323334353637383940private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; //如果给定的时间值小于等于0，则直接返回false if (nanosTimeout &lt;= 0L) return false; //根据给定参数计算截止时间 final long deadline = System.nanoTime() + nanosTimeout; //将当前线程添加到CLH等待队列 final Node node = addWaiter(Node.EXCLUSIVE); //初始失败标志 boolean failed = true; try &#123; //在给定时间内循环/自旋尝试获取锁 for (;;) &#123; //取出前置节点 final Node p = node.predecessor(); //如果前置节点为首节点，并且当前线程能够成功获取锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC 前首节点出队，帮助GC failed = false; return true; &#125; //判断是否等待超时，如果超时，则返回false nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) return false; //这里判断是否可以阻塞线程并做相应操作，跟之前分析的几个方法不一样的是，这里的阻塞多了一个判断，并且是在有限时间内阻塞，类似于sleep if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); //判断中断状态，并决定是否抛出异常 if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; doAcquireNanos的阻塞是有时间限制的，所以能在给定的时间内，返回获取锁的操作结果。 参考资料https://juejin.cn/post/6870099231361728525 https://www.processon.com/view/5f047c16f346fb1ae598b4dd?fromnew=1 https://www.imooc.com/article/51118","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之AQS底层实现与原理","date":"2021-08-02T07:17:07.000Z","path":"wiki/Java并发编程之AQS底层实现与原理/","text":"AQS锁限时等待是如何实现的？ 公平锁与非公平锁流程是怎样的？ 独占锁&amp;共享锁独占锁即只允许一个线程获取同步状态，当这个线程还没有释放同步状态时，其他线程是获取不了的，只能加入到同步队列，进行等待。 公平锁&amp;非公平锁公平锁公平策略：在多个线程争用锁的情况下，公平策略倾向于将访问权授予等待时间最长的线程。也就是说，相当于有一个线程等待队列，先进入等待队列的线程后续会先获得锁，这样按照“先来后到”的原则，对于每一个等待线程都是公平的。 非公平锁在多个线程争用锁的情况下，能够最终获得锁的线程是随机的（由底层OS调度）。 注意：一般情况下，使用公平策略的程序在多线程访问时，总体吞吐量（即速度很慢，常常极其慢）比较低，因为此时在线程调度上面的开销比较大。 AQS是什么同步器是用来构建锁和其他同步组件的基础框架，它的实现主要依赖于一个int类型的成员变量来表示同步状态以及一个FIFO队列构建等待队列。它的子类必须重写AQS定义的几个protected修饰的用来改变同步状态的方法，其他方法主要是用来实现排队和阻塞机制的。 同步器是实现锁的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义，可以这样理解两者的关系： 锁是面向使用者的，它定义了使用者和锁交互的接口，隐藏了实现的细节，同步器是面向锁的实现者，它简化了锁的实现方式，屏蔽了同步状态的管理，线程的排队，等待和唤醒等底层操作。 AQS的设计是使用模版方法设计模式，它将一个方法开放给子类重写，而同步器给同步组件所提供的模版方法又会重新调用子类所重写的方法。 AQS核心思想如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。 如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 1、AQS使用一个int成员变量来表示同步状态 2、使用Node实现FIFO队列，可以用于构建锁或者其他同步装置 AQS资源共享方式：独占Exclusive（排它锁模式）和共享Share（共享锁模式） AQS它的所有子类中，要么实现并使用了它的独占功能的api，要么使用了共享锁的功能，而不会同时使用两套api，即便是最有名的子类ReentrantReadWriteLock也是通过两个内部类读锁和写锁分别实现了两套api来实现的 state状态state状态使用volatile int类型的变量，表示当前同步状态。state的访问方式有三种: getState() setState() compareAndSetState() Node内部类Node类是AQS的绝对核心类，AQS基于Node来构建同步队列和Condition队列； 源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /** waitStatus value to indicate successor&#x27;s thread needs unparking */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; volatile Node next; volatile Thread thread; Node nextWaiter; /** * Returns true if node is waiting in shared mode. */ final boolean isShared() &#123; return nextWaiter == SHARED; &#125; // 获取前置节点 final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125; &#125; CANCELLED waitStatus值为1时表示该线程节点已释放（超时、中断），已取消的节点不会再阻塞。 SIGNAL waitStatus为-1时表示该线程的后续线程需要阻塞，即只要前置节点释放锁，就会通知标识为 SIGNAL 状态的后续节点的线程 CONDITION waitStatus为-2时，表示该线程在condition队列中阻塞（Condition有使用） PROPAGATE waitStatus为-3时，表示该线程以及后续线程进行无条件传播（CountDownLatch中有使用）共享模式下， PROPAGATE 状态的线程处于可运行状态 AQS之独占+非公平获取锁acquire ReentrantLock是AQS独占模式的经典实现，ReentrantLock在构造实例是可以指定是否是fair lock。 123456789/** * Creates an instance of &#123;@code ReentrantLock&#125; with the * given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; acquire方法获取许可下面我们就从锁的获取入手开始解读AQS： 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; tryAcquire抽象方法tryAcquire是个protected方法，具体是实现在对应的子类中，这个方法的功能就是尝试去修改state的状态值 123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; nonfairTryAcquire非公平锁获取许可1234567// ReentrantLock 非公平锁进来就开始抢占锁，体现非公平性final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; 以ReentrantLock方法的实现为例，看一下源码： 123456789101112131415161718192021222324final boolean nonfairTryAcquire(int acquires) &#123; // 获取当前线程 final Thread current = Thread.currentThread(); // getState()返回的就是AQS类中的state字段的值 int c = getState(); // c == 0 说明当前锁没有被任何线程占有 if (c == 0) &#123; // 使用cas去修改state的值，独占模式下acquires = 1 if (compareAndSetState(0, acquires)) &#123; // 修改state成功之后，将独占线程设置成当前线程，并且返回true，表示抢占锁成功 setExclusiveOwnerThread(current); return true; &#125; &#125; // 如果state ！= 0 并且独占线程就是当前线程，表示当前线程持有对象的锁，此时，需要锁重入，state继续累加 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; AQS的acquire(int arg)方法中还有一部分就是 acquireQueued(addWaiter(Node.EXCLUSIVE), arg) addWaiter添加等待队列我们先看一下addWaiter方法，java.util.concurrent.locks.AbstractQueuedSynchronizer#addWaiter 如果tryAcquire返回FALSE（获取同步状态失败），则调用该方法将当前线程加入到CLH同步队列尾部。 1234567891011121314151617private Node addWaiter(Node mode) &#123; // 首先创建一个Node节点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // cas 将当前节点设置到同步队列的队尾 if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 如果上面cas设置没有成功，则通过enq方法将节点添加到队尾 enq(node); return node; &#125; java.util.concurrent.locks.AbstractQueuedSynchronizer#enq 12345678910111213141516private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize 初始化头节点 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125; 通过自旋，最终将node节点添加到同步队列中。 acquireQueued获取许可节点添加到同步队列之中，然后是一个非常重要的方法 ‼️ acquireQueued方法为一个自旋的过程，也就是说当前线程（Node）进入同步队列后，就会进入一个自旋的过程，每个节点都会自省地观察，当条件满足，获取到同步状态后，就可以从这个自旋过程中退出，否则会一直执行下去。 1234567891011121314151617181920212223242526final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 如果node节点是队列中第二个节点（因为第一个正在执行状态）肯定要队列中从第二个节点开始尝试获取锁 final Node p = node.predecessor(); // 第二个节点调用tryAcquire方法 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //把当前节点设置成队列头节点 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; //判断是否需要挂起队列中后续的节点 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; // 如果获取锁失败 if (failed) cancelAcquire(node); &#125; &#125; shouldParkAfterFailedAcquire方法shouldParkAfterFailedAcquire将队列后续节点挂起 12345678910111213141516171819private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 如果前一个节点的waitStatus == Node.SIGNAL 则直接返回true // 因为前一个节点状态是Node.SIGNAL时，才会通知后续节点进行park或者unpark return true; if (ws &gt; 0) &#123; // static final int CANCELLED = 1; // 取消状态的节点直接在等待队列中去除 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 将前一个节点的waitStatus设置成Node.SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false; &#125; 12345private final boolean parkAndCheckInterrupt() &#123; // 挂起当前线程，走到这肯定是没有拿到执行权的，线程需要挂起等待其他线程释放锁 LockSupport.park(this); return Thread.interrupted();&#125; 最后如果获取失败的话，会调用下面这个方法： cancelAcquire取消获取123456789101112131415161718192021222324252627282930313233343536373839private void cancelAcquire(Node node) &#123; // 如果节点为空，直接返回 if (node == null) return; // 由于线程要被取消了，所以将 thread 线程清掉 node.thread = null; // 下面这步表示将 node 的 pre 指向之前第一个非取消状态的结点（即跳过所有取消状态的结点）,waitStatus &gt; 0 表示当前结点状态为取消状态 Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 获取经过过滤后的 pre 的 next 结点，这一步主要用在后面的 CAS 设置 pre 的 next 节点上 Node predNext = pred.next; // 将当前结点设置为取消状态 node.waitStatus = Node.CANCELLED; // 如果当前取消结点为尾结点，使用 CAS 则将尾结点设置为其前驱节点，如果设置成功，则尾结点的 next 指针设置为空 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // 这一步看得有点绕，我们想想，如果当前节点取消了，那是不是要把当前节点的前驱节点指向当前节点的后继节点 // 但是我们之前也说了，要唤醒或阻塞结点，须在其前驱节点的状态为 SIGNAL 的条件才能操作， //所以在设置 pre 的 next 节点时要保证 pre 结点的状态为 SIGNAL，想通了这一点相信你不难理解以下代码。 int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; // 如果 pre 为 head，或者 pre 的状态设置 SIGNAL 失败，则直接唤醒后继结点去竞争锁，之前我们说过， SIGNAL 的结点取消（或释放锁）后可以唤醒后继结点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 释放锁release123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease方法也是由子类来实现的。 1234567891011121314protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; // 判断当前线程 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; // 将独占线程设置成null，下一个线程获取到锁时会设置成自己的 setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; 下面是执行unparkSuccessor(h);方法了，当前线程释放了锁之后，需要唤醒等待队列中的第二个节点对应的线程。这里注意一点的是，要执行的Node节点的waitStatus肯定是0；？？ 1234567891011121314151617private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) //置零当前线程所在的结点状态，允许失败 compareAndSetWaitStatus(node, ws, 0);// 从第二个节点开始往后找waitStatus&lt;=0的节点，然后执行unpark Node s = node.next; // 找到下一个需要唤醒的结点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; Condition在AQS中的实现上面已经介绍了AQS所提供的核心功能，当然它还有很多其他的特性，这里我们来继续说下Condition这个组件。 Condition是在java 1.5中才出现的，它用来替代传统的Object的wait()、notify()实现线程间的协作，相比使用Object的wait()、notify()，使用Condition中的await()、signal()这种方式实现线程间协作更加安全和高效。因此通常来说比较推荐使用`Condition 其中AbstractQueueSynchronizer中实现了Condition中的方法，主要对外提供awaite(Object.wait())和signal(Object.notify())调用。 Condition在java代码中的应用12345678910111213141516171819202122232425262728293031323334public class ReentrantLockDemo &#123; static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) &#123; Condition condition = lock.newCondition(); new Thread(() -&gt; &#123; lock.lock(); try &#123; System.out.println(&quot;线程一加锁成功&quot;); System.out.println(&quot;线程一执行await被挂起&quot;); condition.await(); System.out.println(&quot;线程一被唤醒成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); System.out.println(&quot;线程一释放锁成功&quot;); &#125; &#125;).start(); new Thread(() -&gt; &#123; lock.lock(); try &#123; System.out.println(&quot;线程二加锁成功&quot;); condition.signal(); System.out.println(&quot;线程二唤醒线程一&quot;); &#125; finally &#123; lock.unlock(); System.out.println(&quot;线程二释放锁成功&quot;); &#125; &#125;).start(); &#125;&#125; 线程一调用了condition.await();之后，线程二才可以获取到锁并且执行自己的任务，线程二调用 condition.signal();之后唤醒线程一，但是还没有执行权限，只有在线程二执行完成之后调用lock.unlock();之后，线程一重新回去到锁，然后执行线程一后续的流程。 await方法await使当前线程释放锁，也就是执行许可，然后进入Condition队列，等待在某个时刻被某个线程唤醒。 12345678910111213141516171819202122232425public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 将当前线程封装成Node节点添加的Condition队列中 Node node = addConditionWaiter(); // 添加到Condition队列中的线程需要释放锁资源 int savedState = fullyRelease(node); int interruptMode = 0; // 查看当前节点是不是在同步队列中 while (!isOnSyncQueue(node)) &#123; // 当前节点不在同步队列中，那么直接park挂起 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 表明已经有的线程调用了signal唤醒当前线程， // 并且节点已经存放到了同步等待队列中，所以可以调用如果acquireQueued请求许可了 // savedState是获取许可的个数 这个要和之前释放的许可个数一致 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); &#125; addConditionWaiter方法添加一个节点到Condition队列中 java.util.concurrent.locks.AbstractQueuedSynchronizer.ConditionObject#addConditionWaiter 1234567891011121314151617private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; // 先检查一遍有没有取消状态的节点，如果有的话，清除掉 unlinkCancelledWaiters(); t = lastWaiter; &#125; // 将当前线程封装成Node添加到Condition队列中 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; &#125; signal方法唤醒一个线程 1234567 public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125; doSignal方法123456789private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; // 将Condition队列中的节点状态设置成SIGNAL，并将节点添加到同步队列中 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); &#125; transferForSignal方法1234567891011121314151617181920212223final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ // 将节点状态改成0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ // 把当前添加到同步队列中，并返回前一个节点 Node p = enq(node); int ws = p.waitStatus; // 设置前一个节点的状态为SIGNAL if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 唤醒当前节点的线程 LockSupport.unpark(node.thread); return true; &#125; 参考资料https://mp.weixin.qq.com/s/hB5ncpe7_tVovQj1sNlDRA https://mp.weixin.qq.com/s/iNz6sTen2CSOdLE0j7qu9A https://github.com/AobingJava/JavaFamily https://segmentfault.com/a/1190000015804888/ https://juejin.cn/post/6844903997438951437 https://juejin.cn/post/6870099231361728525","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程系列合集","date":"2021-08-02T03:22:49.000Z","path":"wiki/Java并发编程系列合集/","text":"Lock框架继承关系图 知识清单 Java多线程与并发基础 Java内存模型 深入理解同步锁-synchronized关键字 synchronized实现原理概述 深入理解volatile关键字 深入理解final关键字 深入理解并发编程之Blocking Queue 深入理解并发编程之AQS 深入理解ReentrantLock应用及实现 深入Condition机制的底层原理 Java多线程之中断机制 深入理解并发容器之concurrentHashMap 深入理解并发编程之ThreadLocal 深入理解Atomic底层及原理 如何构建一个安全可用的线程池 JUC-CountDownLatch JUC-CyclicBarrier LockSupport工具 并发编程之Unsafe类 学习文档https://www.codercc.com/backend/basic/juc/ https://segmentfault.com/a/1190000015558984 https://www.pdai.tech/md/java/thread/java-thread-x-juc-overview.html https://github.com/AobingJava/JavaFamily https://dayarch.top/categories/Coding/Java-Concurrency/ http://tutorials.jenkov.com/java-concurrency/blocking-queues.html","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"JVM-垃圾收集器","date":"2021-07-31T06:13:17.000Z","path":"wiki/JVM-垃圾收集器/","text":"1、什么是垃圾收集器如果说垃圾收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 JVM规范对于垃圾收集器的应该如何实现没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器差别较大，这里只看HotSpot虚拟机。 就像没有最好的算法一样，垃圾收集器也没有最好，只有最合适。我们能做的就是根据具体的应用场景选择最合适的垃圾收集器。 ​ 上图展示了7种作用于不同分代的收集器，其中用于回收新生代的收集器包括Serial、PraNew、Parallel Scavenge，回收老年代的收集器包括Serial Old、Parallel Old、CMS，还有用于回收整个Java堆的G1收集器。不同收集器之间的连线表示它们可以搭配使用。 2、串行，并行和并发2.1 串行计算机中的串行是用 Serial 表示。A 和 B 两个任务运行在一个 CPU 线程上，在 A 任务执行完之前不可以执行 B。即，在整个程序的运行过程中，仅存在一个运行上下文，即一个调用栈一个堆。程序会按顺序执行每个指令。 2.2 并行并行性指两个或两个以上事件或活动在同一时刻发生。在多道程序环境下，并行性使多个程序同一时刻可在不同 CPU 上同时执行。比如，A 和 B 两个任务可以同时运行在不同的 CPU 线程上，效率较高，但受限于 CPU 线程数，如果任务数量超过了 CPU 线程数，那么每个线程上的任务仍然是顺序执行的。 2.3 并发并发指多个线程在宏观(相对于较长的时间区间而言)上表现为同时执行，而实际上是轮流穿插着执行，并发的实质是一个物理 CPU 在若干道程序之间多路复用，其目的是提高有限物理资源的运行效率。 并发与并行串行并不是互斥的概念，如果是在一个CPU线程上启用并发，那么自然就还是串行的，而如果在多个线程上启用并发，那么程序的执行就可以是既并发又并行的。 2.4 JVM 垃圾收集中的串行、并行和并发在 JVM 垃圾收集器中也涉及到如上的三个概念。 串行（Serial）：使用单线程进行垃圾回收的回收器。 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 在了解了这些概念之后，我们开始具体介绍常用的垃圾收集器。 3、主流的垃圾收集器3.1 Serial收集器Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了（新生代采用复制算法，老生代采用标志整理算法）。大家看名字就知道这个收集器是一个单线程收集器了。 它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ：将用户正常工作的线程全部暂停掉），直到它收集结束。 上图中： 新生代采用复制算法，Stop-The-World 老年代采用标记-整理算法，Stop-The-World 当它进行GC工作的时候，虽然会造成Stop-The-World，正如每种算法都有存在的原因，该串行收集器也有存在的原因：因为简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，没有线程交互的开销，专心做GC，自然可以获得最高的单线程效率。 所以Serial收集器对于运行在client模式下的应用是一个很好的选择（到目前为止，它依然是虚拟机运行在client模式下的默认新生代收集器） 串行收集器的缺点很明显，虚拟机的开发者当然也是知道这个缺点的，所以一直都在缩减Stop The World的时间。 在后续的垃圾收集器设计中停顿时间在不断缩短（但是仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续） 3.1.1 特点 针对新生代的收集器； 采用复制算法； 单线程收集； 进行垃圾收集时，必须暂停所有工作线程，直到完成； 即会”Stop The World”； 3.1.2 应用场景 依然是HotSpot在Client模式下默认的新生代收集器； 也有优于其他收集器的地方： 简单高效（与其他收集器的单线程相比）； 对于限定单个CPU的环境来说，Serial收集器没有线程交互（切换）开销，可以获得最高的单线程收集效率； 在用户的桌面应用场景中，可用内存一般不大（几十M至一两百M），可以在较短时间内完成垃圾收集（几十MS至一百多MS）,只要不频繁发生，这是可以接受的 3.1.3 参数设置添加该参数来显式的使用串行垃圾收集器: “-XX:+UseSerialGC” 3.2 ParNew收集器 Serial收集器的多线程版本-使用多条线程进行GC ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，目前只有它能与CMS收集器配合工作。 CMS收集器是一个被认为具有划时代意义的并发收集器，因此如果有一个垃圾收集器能和它一起搭配使用让其更加完美，那这个收集器必然也是一个不可或缺的部分了。 收集器的运行过程如下图所示： 3.2.1 应用场景：在Server模式下，ParNew收集器是一个非常重要的收集器，因为除Serial外，目前只有它能与CMS收集器配合工作； 但在单个CPU环境中，不会比Serail收集器有更好的效果，因为存在线程交互开销。 3.2.2 设置参数指定使用CMS后，会默认使用ParNew作为新生代收集: “-XX:+UseConcMarkSweepGC” 强制指定使用ParNew:“-XX:+UseParNewGC” 指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相: “-XX:ParallelGCThreads” 3.2.3 为什么只有ParNew能与CMS收集器配合 CMS是HotSpot在JDK1.5推出的第一款真正意义上的并发（Concurrent）收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作； CMS作为老年代收集器，但却无法与JDK1.4已经存在的新生代收集器Parallel Scavenge配合工作； 因为Parallel Scavenge（以及G1）都没有使用传统的GC收集器代码框架，而另外独立实现；而其余几种收集器则共用了部分的框架代码； 3.3 Parallel Scavenge收集器Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器。 Parallel Scavenge收集器关注点是吞吐量（如何高效率的利用CPU）。 CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。 所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。（吞吐量：CPU用于用户代码的时间/CPU总消耗时间的比值，即=运行用户代码的时间/(运行用户代码时间+垃圾收集时间)。比如，虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。） 运行示意图： 3.3.1 特点 新生代收集器； 采用复制算法； 多线程收集； CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间；而Parallel Scavenge收集器的目标则是达一个可控制的吞吐量（Throughput）； 3.3.2 应用场景 高吞吐量为目标，即减少垃圾收集时间，让用户代码获得更长的运行时间； 当应用程序运行在具有多个CPU上，对暂停时间没有特别高的要求时，即程序主要在后台进行计算，而不需要与用户进行太多交互； 例如，那些执行批量处理、订单处理（对账等）、工资支付、科学计算的应用程序； 3.3.3 设置参数Parallel Scavenge收集器提供两个参数用于精确控制吞吐量： 控制最大垃圾收集停顿时间 “-XX:MaxGCPauseMillis” 控制最大垃圾收集停顿时间，大于0的毫秒数； MaxGCPauseMillis设置得稍小，停顿时间可能会缩短，但也可能会使得吞吐量下降；因为可能导致垃圾收集发生得更频繁； 设置垃圾收集时间占总时间的比率 “-XX:GCTimeRatio” 设置垃圾收集时间占总时间的比率，0 &lt; n &lt; 100的整数； GCTimeRatio相当于设置吞吐量大小； 垃圾收集执行时间占应用程序执行时间的比例的计算方法是： 1 / (1 + n) 。 例如，选项-XX:GCTimeRatio=19，设置了垃圾收集时间占总时间的5% = 1/(1+19)；默认值是1% = 1/(1+99)，即n=99； 垃圾收集所花费的时间是年轻一代和老年代收集的总时间； 如果没有满足吞吐量目标，则增加代的内存大小以尽量增加用户程序运行的时间； 3.4 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。 它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案 3.4.1 特点 针对老年代； 采用”标记-整理-压缩”算法（Mark-Sweep-Compact）； 单线程收集； 3.4.2 应用场景 主要用于Client模式； 而在Server模式有两大用途：（A）、在JDK1.5及之前，与Parallel Scavenge收集器搭配使用（JDK1.6有Parallel Old收集器可搭配Parallel Scavenge收集器）；（B）、作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用； 3.5 Parallel Old收集器Parallel Scavenge收集器的老年代版本。 使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 在JDK1.6才有的。 3.5.1 特点 针对老年代； 采用”标记-整理-压缩”算法； 多线程收集； Parallel Scavenge/Parallel Old收集器运行示意图如下 3.5.2 应用场景 JDK1.6及之后用来代替老年代的Serial Old收集器； 特别是在Server模式，多CPU的情况下； 这样在注重吞吐量以及CPU资源敏感的场景，就有了Parallel Scavenge（新生代）加Parallel Old（老年代）收集器的”给力”应用组合； 3.5.3 设置参数指定使用Parallel Old收集器: “-XX:+UseParallelOldGC” 3.6 CMS（Concurrent Mark Sweep）收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常适合在注重用户体验的应用上使用。 3.6.1 特点 针对老年代 基于”标记-清除”算法(不进行压缩操作，会产生内存碎片) 以获取最短回收停顿时间为目标 并发收集、低停顿 需要更多的内存 CMS是HotSpot在JDK1.5推出的第一款真正意义上的并发（Concurrent）收集器； 第一次实现了让垃圾收集线程与用户线程（基本上）同时工作； 3.6.2 应用场景 与用户交互较多的场景；（如常见WEB、B/S-浏览器/服务器模式系统的服务器上的应用） 希望系统停顿时间最短，注重服务的响应速度； 以给用户带来较好的体验； 3.6.3 CMS收集器运作过程从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程可分为四个步骤： 初始标记： 暂停所有的其他线程，初始标记仅仅标记GC Roots能直接关联到的对象，速度很快； 并发标记 并发标记就是进行GC Roots Tracing的过程； 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方； 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录（采用多线程并行执行来提升效率）；需要”Stop The World”，且停顿时间比初始标记稍长，但远比并发标记短； 并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫，回收所有的垃圾对象； 由于整个过程耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作。 所以总体来说，CMS的内存回收是与用户线程一起“并发”执行的。 3.6.4参数设置指定使用CMS收集器 “-XX:+UseConcMarkSweepGC” 3.6.5 CMS收集器缺点3.6.5.1 对CPU资源敏感面向并发设计的程序都对CPU资源比较敏感（并发程序的特点）。在并发阶段，它虽然不会导致用户线程停顿，但会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。（在对账系统中，不适合使用CMS收集器）。 CMS的默认收集线程数量是=(CPU数量+3)/4； 当CPU数量越多，回收的线程占用CPU就少。 也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，对用户程序影响可能较大；不足4个时，影响更大，可能无法接受。（比如 CPU=2时，那么就启动一个线程回收，占了50%的CPU资源。） （一个回收线程会在回收期间一直占用CPU资源） 针对这种情况，曾出现了”增量式并发收集器”（Incremental Concurrent Mark Sweep/i-CMS）； 类似使用抢占式来模拟多任务机制的思想，让收集线程和用户线程交替运行，减少收集线程运行时间； 但效果并不理想，JDK1.6后就官方不再提倡用户使用。 3.6.5.2 无法处理浮动垃圾无法处理浮动垃圾,可能出现”Concurrent Mode Failure”失败 在并发清除时，用户线程新产生的垃圾，称为浮动垃圾； 解决办法： 这使得并发清除时需要预留一定的内存空间，不能像其他收集器在老年代几乎填满再进行收集； 也可以认为CMS所需要的空间比其他垃圾收集器大； 可以使用”-XX:CMSInitiatingOccupancyFraction”，设置CMS预留老年代内存空间； 3.6.5.3 产生大量内存碎片由于CMS是基于“标记+清除”算法来回收老年代对象的，因此长时间运行后会产生大量的空间碎片问题，可能导致新生代对象晋升到老生代失败。 由于碎片过多，将会给大对象的分配带来麻烦。因此会出现这样的情况，老年代还有很多剩余的空间，但是找不到连续的空间来分配当前对象，这样不得不提前触发一次Full GC。 解决办法 使用”-XX:+UseCMSCompactAtFullCollection”和”-XX:+CMSFullGCsBeforeCompaction”，需要结合使用。 UseCMSCompactAtFullCollection “-XX:+UseCMSCompactAtFullCollection” 为了解决空间碎片问题，CMS收集器提供−XX:+UseCMSCompactAlFullCollection标志，使得CMS出现上面这种情况时不进行Full GC，而开启内存碎片的合并整理过程； 但合并整理过程无法并发，停顿时间会变长； 默认开启（但不会进行，需要结合CMSFullGCsBeforeCompaction使用）； CMSFullGCsBeforeCompaction 由于合并整理是无法并发执行的，空间碎片问题没有了，但是有导致了连续的停顿。因此，可以使用另一个参数−XX:CMSFullGCsBeforeCompaction，表示在多少次不压缩的Full GC之后，对空间碎片进行压缩整理。 可以减少合并整理过程的停顿时间； 默认为0，也就是说每次都执行Full GC，不会进行压缩整理； 由于空间不再连续，CMS需要使用可用”空闲列表”内存分配方式，这比简单实用”碰撞指针”分配内存消耗大； 3.6.6 CMS&amp;Parallel Old总体来看，CMS与Parallel Old垃圾收集器相比，CMS减少了执行老年代垃圾收集时应用暂停的时间； 但却增加了新生代垃圾收集时应用暂停的时间、降低了吞吐量而且需要占用更大的堆空间； （原因：CMS不进行内存空间整理节省了时间，但是可用空间不再是连续的了，垃圾收集也不能简单的使用指针指向下一次可用来为对象分配内存的地址了。 相反，这种情况下，需要使用可用空间列表。即，会创建一个指向未分配区域的列表，每次为对象分配内存时，会从列表中找到一个合适大小的内存区域来为新对象分配内存。这样做的结果是，老年代上的内存的分配比简单实用碰撞指针分配内存消耗大。这也会增加年轻代垃圾收集的额外负担，因为老年代中的大部分对象是在新生代垃圾收集的时候从新生代提升为老年代的。） 当新生代对象无法分配过大对象，就会放到老年代进行分配。 3.7 G1收集器上一代的垃圾收集器(串行serial, 并行parallel, 以及CMS)都把堆内存划分为固定大小的三个部分: 年轻代(young generation), 年老代(old generation), 以及持久代(permanent generation)。 G1（Garbage-First）是JDK7-u4才推出商用的收集器；G1 (Garbage-First)是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器。以极高概率满足GC停顿时间要求的同时，还具备高吞吐量性能特征。被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。G1的使命是在未来替换CMS，并且在JDK1.9已经成为默认的收集器。 3.7.1 特点3.7.1.1 并行与并发G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 3.7.1.2 分代收集虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 能独立管理整个GC堆（新生代和老年代），而不需要与其他收集器搭配； 能够采用不同方式处理不同时期的对象； 虽然保留分代概念，但Java堆的内存布局有很大差别； 将整个堆划分为多个大小相等的独立区域（Region）； 新生代和老年代不再是物理隔离，它们都是一部分Region（不需要连续）的集合； 3.7.2 空间整合与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 从整体看，是基于标记-整理算法； 从局部（两个Region间）看，是基于复制算法；这是一种类似火车算法的实现；不会产生内存碎片，有利于长时间运行； （火车算法是分代收集器所用的算法，目的是在成熟对象空间中提供限定时间的渐进收集。在后面一篇中会专门介绍） 3.7.3 可预测的停顿这是G1相对于CMS的另一个大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型。可以明确指定M毫秒时间片内，垃圾收集消耗的时间不超过N毫秒。在低停顿的同时实现高吞吐量。 3.7.4 G1收集器延伸3.7.4.1 为什么G1可以实现可预测停顿 可以有计划地避免在Java堆的进行全区域的垃圾收集； G1收集器将内存分大小相等的独立区域（Region），新生代和老年代概念保留，但是已经不再物理隔离。 G1跟踪各个Region获得其收集价值大小，在后台维护一个优先列表； 每次根据允许的收集时间，优先回收价值最大的Region（名称Garbage-First的由来）； 这就保证了在有限的时间内可以获取尽可能高的收集效率； 3.7.4.2 一个对象被不同区域引用的问题一个Region不可能是孤立的，一个Region中的对象可能被其他任意Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题（而G1更突出）：回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率； 解决方法： 无论G1还是其他分代收集器，JVM都是使用Remembered Set来避免全局扫描： 每个Region都有一个对应的Remembered Set； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的Region（其他收集器：检查老年代对象是否引用了新生代对象）； 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在Region对应的Remembered Set中； 当进行垃圾收集时，在GC根节点的枚举范围加入Remembered Set； 就可以保证不进行全局扫描，也不会有遗漏。 3.7.5 应用场景 面向服务端应用，针对具有大内存、多处理器的机器； 最主要的应用是为需要低GC延迟，并具有大堆的应用程序提供解决方案； 如：在堆大小约6GB或更大时，可预测的暂停时间可以低于0.5秒； （实践：对账系统中将CMS垃圾收集器修改为G1，降低对账时间20秒以上） 具体什么情况下应用G1垃圾收集器比CMS好，可以参考以下几点（但不是绝对）： 超过50％的Java堆被活动数据占用； 对象分配频率或年代的提升频率变化很大； GC停顿时间过长（长于0.5至1秒）； 建议： 如果现在采用的收集器没有出现问题，不用急着去选择G1； 如果应用程序追求低停顿，可以尝试选择G1； 是否代替CMS只有需要实际场景测试才知道。（如果使用G1后发现性能还没有使用CMS好，那么还是选择CMS比较好） 3.7.6 设置参数可以通过下面的参数，来设置一些G1相关的配置。 指定使用G1收集器： “-XX:+UseG1GC” 当整个Java堆的占用率达到参数值时，开始并发标记阶段；默认为45： “-XX:InitiatingHeapOccupancyPercent” 为G1设置暂停时间目标，默认值为200毫秒： “-XX:MaxGCPauseMillis” 设置每个Region大小，范围1MB到32MB；目标是在最小Java堆时可以拥有约2048个Region: “-XX:G1HeapRegionSize” 新生代最小值，默认值5%: “-XX:G1NewSizePercent” 新生代最大值，默认值60%: “-XX:G1MaxNewSizePercent” 设置STW期间，并行GC线程数: “-XX:ParallelGCThreads” 设置并发标记阶段，并行执行的线程数: “-XX:ConcGCThreads” G1在标记过程中，每个区域的对象活性都被计算，在回收时候，就可以根据用户设置的停顿时间，选择活性较低的区域收集，这样既能保证垃圾回收，又能保证停顿时间，而且也不会降低太多的吞吐量。Remark（重新标记）阶段新算法的运用，以及收集过程中的压缩，都弥补了CMS不足。 引用Oracle官网的一句话：“G1 is planned as the long term replacement for the Concurrent Mark-Sweep Collector (CMS)”。 G1计划作为并发标记-清除收集器(CMS)的长期替代品 4、如何选择垃圾收集器垃圾收集器主要可以分为如下三大类： 串行收集器：Serial和Serial Old只能有一个垃圾回收线程执行，用户线程暂停。 适用于内存比较小的嵌入式设备 。 并行收集器[吞吐量优先]：Parallel Scanvenge和Parallel Old多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 适用于科学计算、后台处理等若交互场景 。 并发收集器[停顿时间优先]：CMS和G1。用户线程和垃圾收集线程同时执行(但并不一定是并行的，可能是交替执行的)，垃圾收集线程在执行的时候不会停顿用户线程的运行。 适用于对时间有要求的场景，比如Web应用。 参考资料https://zhuanlan.zhihu.com/p/142273073 https://juejin.cn/post/6844903877024677901 面试官：你对JVM垃圾收集器了解吗？13连问你是否抗的住！ https://juejin.cn/post/6874060477031579661#heading-32 https://juejin.cn/post/6844904041080684552 https://juejin.cn/post/6844904159817236494#heading-14 https://juejin.cn/post/6844903892774289421#heading-20","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-垃圾回收机制","date":"2021-07-30T11:49:33.000Z","path":"wiki/JVM-垃圾回收机制/","text":"垃圾回收总体思路： 1、什么是垃圾回收，为什么需要垃圾回收； 2、回收的到底是什么？由谁来回收谁？ 3、回收的判断标准是什么 4、什么时候回收，回收的种类和流程是怎样的 5、在哪些地方进行回收 1. 什么是垃圾回收任何语言在运行过程中都会创建对象，也就意味着需要在内存中为这些对象在内存中分配空间，在这些对象失去使用的意义的时候，需要释放掉这些内容，保证内存能够提供给新的对象使用。对于对象内存的释放就是垃圾回收机制，也叫做gc。 对于java开发者来说gc是一个双刃剑，一方面，java程序员在开发程序的时候不需要像开发C++那样手动分配对象的内存，还要在合适的时机手动释放，一定程度地降低了java程序员的开发难度。另一方面，虚拟机可以帮助程序开发人员管理内存，如果程序员不了解虚拟机垃圾回收的原理，很容易引起OOM，造成服务的崩溃和系统的宕机； 2、对象如何判活对象存活表示的是当前对象是否还在被使用，没有被使用的对象我们可以称其为已经“死亡”，如果对象依然在被使用，我们称其为“存活”状态，对象是否被使用则是通过对象的引用进行判断的。而垃圾回收机制就是负责将已经死亡的对象进行清理。 程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭，栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈的操作。每一个栈帧中分配多少内存基本上是在类结构确定下来时就已知的，因此这几个区域的内存分配和回收都具备确定性，在这几个区域不需要过多的考虑回收的问题，当方法结束或者线程结束的时候，内存自然就跟着回收了。 Java堆则和上述三种区域不同，Java中一个接口的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也不一样，而**只有当Java程序运行时我们才能知道哪些对象会被创建**，所以堆中的内存分配和回收都是动态进行的，因此垃圾收集器所关注的也是这部分的内存。 垃圾回收器在对堆进行回收前，第一件事情就是要判断堆中的对象哪些是依旧在使用的，哪些已经不可能再被使用了。这里的判断主要有两种方式，第一种是引用计数算法，第二种是可达性分析算法。 2.1 引用计数算法引用计数算法给对象添加一个引用计数器，每当有一个地方引用它时，计数器就加1，当引用失效时，计数器就减1，任何时刻计数器为0的对象就是不可能再被使用的。这种算法实现简单，判定效率也很高，但是它难以解决对象之间循环引用的问题，例如对象A和对象B相互引用了对方，而A和B都没有在被使用了，但这两个对象却也不会被垃圾回收器回收。 2.2 可达性分析主流的判断方法则是使用可达性分析算法来判断对象是否存活。这个算法需要选择一些对象作为“GC Roots”，每次都通过这些roots节点向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots不存在引用链的时候，则证明这个对象是不可用的。 在Java语言中，可作为GC Roots的对象包括下面几种： 1、虚拟机栈中引用的对象2、方法区中类静态属性引用的对象3、方法区中常量引用的对象4、本地方法栈中JNI（即Native方法）引用的对象 可达性分析算法中根据GC Roots找引用链，存在两个主要的问题。 一个是可作为GC Roots的节点主要在全局性的引用（例如常量或者类静态属性）于上下文(例如栈帧中的本地变量表）中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，将会消耗很多的时间。 还有一个问题是GC停顿，可达性分析必须确保在整个的分析过程中，执行系统就像被冻结在某个时间节点，整个分析过程中对象的引用关系不能发生变化，这样才能保证分析结果的准确性，因此在进行GC时，需要停顿所有的Java线程。（Stop The World） 3.3 对象两次标记判活即使在可达性分析算法中不可达的对象，也并非是”非死不可“的，这时候它们暂时处于”缓刑“阶段，要宣告一个对象死亡，至少要经历两次标记过程： 如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法(当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为”没有必要执行“)。 如果一个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里的执行是指由虚拟机去触发这个方法，但并不一定会等待该方法执行完毕（为了避免finalize方法中出现类似死循环都操作，导致内存无法被回收，同时导致F-Queue队列中的其他对象一直处于等待状态）。 当执行完finalze()方法后，GC将会对F-Queue中的对象进行第二次小规模的标记，如果对象在finalize()方法中又重新获得了引用，对象将会被移出对列并且继续存活，如果对象依旧存在于队列中并且被进行第二次标记，对象将被GC回收。 需要注意的是任何一个对象的finalize()方法只会执行一次，如果第一次通过finalize()方法救活了对象，那么第二次相同的方法就会失效。同时由于finalize()方法的运行代价高昂，不确定性大，无法保证各个对象的调用顺序，因此应当尽量避免使用finalize()方法。 3. 对象引用分类JDK1.2以前，Java中引用的定义很传统，如果reference类型的数据中存储的数值代表的是另一块内存的起始地址，就称这块内存代表着一个引用。这种定义下的对象只存在两种状态，被引用和未被引用状态。但有些对象我们希望当内存存够的时候能够保留这些对象，当内存不足的时候则能够对这些对象进行清理，这一类对象则无法使用这种传统的定义来表示。 JDK1.2之后，Java对引用进行了扩充，将引用分为强引用、软引用、弱引用和虚引用四种，这四种引用的强度依次逐渐减弱。 3.1 强引用就是指在程序代码中普遍存在的，类似 Object obj = new Object() 这类的引用，只要强引用还存在，垃圾收集器永远不会回收被引用的对象。即使内存不足时，垃圾回收器也不会回收强引用的对象，而是会直接抛出OutOfMemoryError异常。如果想让强引用对象被回收，可以手动设置obj = null;来实现。 3.2 软引用用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在内存足够的时候，是不会回收软引用的对象的，而在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果软引用回收后依然内存不足，则会抛出OutOfMemoryError异常。在JDK1.2之后，提供了SoftReference类来实现软引用。软引用可以用来实现缓存技术。 3.3 弱引用弱引用和软引用一样用来描述非必须的对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉被弱引用关联的对象。在JDK1.2之后，提供了WeakReference类来实现弱引用。 3.4 虚引用虚引用也被称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK1.2之后，提供了PhantomReference类来实现虚引用。 4.垃圾回收算法垃圾收集算法的目的是在已经明确了哪些内存块需要回收以后，如何高效的回收这些内存空间。 4.1 标记清除算法标记-清除算法采用从根集合进行扫描，对存活的对象对象标记，标记完毕后，再扫描整个空间中未被标记的对象，进行回收，如图所示。标记-清除算法不需要进行对象的移动，并且仅对不存活的对象进行处理，在存活对象比较多的情况下极为高效。 标记清除算法主要有两个不足之处：一个是效率问题，标记和清除两个过程的效率都不高；另一个问题是空间问题，标记清除之后会造成内存空间中存在大量的内存碎片，空间碎片太多时，当要分配一片大内存空间时可能会找不到合适的连续内存空间进行分配，从而触发另一次垃圾收集动作。 4.2 标记复制算法该算法的提出是为了克服句柄的开销和解决堆碎片的垃圾回收。建立在存活对象少，垃圾对象多的前提下。此算法每次只处理正在使用中的对象，因此复制成本比较小，同时复制过去后还能进行相应的内存整理，不会出现碎片问题。但缺点也是很明显，就是需要两倍内存空间。 它开始时把堆分成 一个对象面和多个空闲面， 程序从对象面为对象分配空间，当对象满了，基于copying算法的垃圾 收集就从根集中扫描活动对象，并将每个活动对象复制到空闲面(使得活动对象所占的内存之间没有空闲洞)，这样空闲面变成了对象面，原来的对象面变成了空闲面，程序会在新的对象面中分配内存。一种典型的基于coping算法的垃圾回收是stop-and-copy算法，它将堆分成对象面和空闲区域面，在对象面与空闲区域面的切换过程中，程序暂停执行。 现在的商业虚拟机都会采用这种算法来回收新生代，根据统计新生代中98%的对象都是“朝夕生死”的，因此对于新生代的回收不用按照1:1的比例来进行内存划分，可以将内存划分为一块Eden区域和两块Survivor空间，每次使用时都选择Eden区域和一块Survivor区域进行内存分配。当回收时，将Eden区域和Survivor区域中还存活的对象全部移动到另一块Survivor区域，然后清理掉Eden区域和刚刚使用的Survivor区域。 HotSpot虚拟机中Eden和Survivor的比例是1:8，即每次都有90%的内存空间在进行使用，只有10%的内存空间被浪费了。当然，如果每次内存都有98%被回收，那么每次被移动到另一块Survivor区域的内存只有2%，这样是没有任何问题的，但是如果移动到另一块Survivor区域的内存超过了10%，就需要依赖其他的内存（这里指老年代）进行分配担保了（将多出的对象分配到老年代）。 4.3 标记整理算法 此算法是结合了“标记-清除”和“复制算法”两个算法的优点。避免了“标记-清除”的碎片问题，同时也避免了“复制”算法的空间问题。 标记-整理算法的标记过程和标记-清除算法的标记过程一致，但是在标记完以后，标记-整理算法会将所有存活的对象都移动到一端，然后再进行清除。这种算法适用于老年代，因为老年代的对象存活率都会比较高，如果像之前一样进行复制移动，将会产生大量的复制操作导致效率变低，同时每次都会存活下大量对象导致需要很多的内存空间来进行分配担保。 4.4 分代收集算法 当前商业虚拟机的垃圾收集都采用“分代收集”算法，这种算法根据对象存活周期的不同将内存划分为几块，一般是把Java堆划分位新生代和老年代。新生代中每次都会有大批对象死去，只有少量对象存活，因此可以选用复制算法。 老年代每次都会有大量对象存活，因此选择标记-清理或者标记-整理算法来进行。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-对象创建","date":"2021-07-30T06:42:55.000Z","path":"wiki/JVM-对象创建/","text":"1.对象创建方式Java中有一下几种方式创建对象： 方式 实质 使用new关键 调用无参或有参构造器函数创建 使用Class的newInstance方法 调用无参或有参构造器函数创建，且需要是publi的构造函数 使用Constructor类的newInstance方法 调用有参和私有private构造器函数创建，实用性更广 使用Clone方法 不调用任何参构造器函数，且对象需要实现Cloneable接口并实现其定义的clone方法，且默认为浅复制 第三方库Objenesis 利用了asm字节码技术，动态生成Constructor对象 2、对象创建过程 2.1 类的加载虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 2.2 分配内存类的加载检查通过后，接下来是为新生对象分配内存。但类加载完成后所需的内存大小就已经完全确定，为对象分配空间的任务等同于把一块确定大小的内存从java堆中划分出来。分配内存有两种方式： 指针碰撞（Bump the Pointer）：假设java堆中内存是绝对规整的，所有用过得内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪到一段与对象大小相等的距离 空闲列表（Free List）：如果java堆中的内存并不是完整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 选择哪种分配方式由java堆是否完整决定，而java堆是否规整又由所采用的垃圾收集器是否带有压缩整理（标记-整理）功能决定。因此，在使用Serial、ParNew等带有Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。 还有一个问题需要考虑，在虚拟机中对象频繁的创建（即使是修改一个指针所指的位置），在并发情况下会带来线程安全的问题。作为虚拟机来说，必须保证线程安全，所有虚拟机采用两种方式保证线程安全： CAS+失败重试：CAS是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用CAS配上失败重试的方式保证更新操作的原子性 本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）：为每一个线程预先在Eden区分配一块内存，JVM在给线程中的对象分配内存时，首先在TLAB分配，当对象大于TLAB中剩余内存或TLAB的内存已用尽时，在采用上述的CAS进行内存分配 2.3 初始化零值内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这⼀步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使⽤，程序能访问到这些字段的数据类型所对应的零值。 2.4 设置对象头初始化零值完成之后，虚拟机要对对象进⾏必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运⾏状态的不同，如是否启⽤偏向锁等，对象头会有不同的设置⽅式。 2.5 执行Init方法在上⾯⼯作都完成之后，从虚拟机的视⻆来看，⼀个新的对象已经产⽣了，但从 Java 程序的视⻆来看，对象创建才刚开始， &lt;init&gt; ⽅法还没有执⾏，所有的字段都还为零。所以⼀般来说，执⾏ new 指令之后会接着执⾏ &lt;init&gt;⽅法，把对象按照程序员的意愿进⾏初始化，这样⼀个真正可⽤的对象才算完全产⽣出来。 3.对象在内存布局 对象头(Header)：包含两部分，第一部分用于存储对象自身的运行时数据，如哈希码、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等，32 位虚拟机占 32 bit，64 位虚拟机占 64 bit。官方称为 ‘Mark Word’。第二部分是类型指针，即对象指向它的类的元数据指针，虚拟机通过这个指针确定这个对象是哪个类的实例。另外，如果是 Java 数组，对象头中还必须有一块用于记录数组长度的数据，因为普通对象可以通过 Java 对象元数据确定大小，而数组对象不可以。 实例数据(Instance Data)：程序代码中所定义的各种类型的字段内容(包含父类继承下来的和子类中定义的)。 对齐填充(Padding)：不是必然需要，主要是占位，保证对象大小是某个字节的整数倍。 4. 对象访问建⽴对象就是为了使⽤对象，我们的Java程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问⽅式有虚拟机实现⽽定，⽬前主流的访问⽅式有使⽤句柄和直接指针两种： 4.1 句柄访问 如果使⽤句柄的话，那么Java堆中将会划分出⼀块内存来作为句柄池，reference 中存储的就是对象的句柄地址，⽽句柄中包含了对象实例数据与类型数据各⾃的具体地址信息； 4 .2 直接指针 如果使⽤直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，⽽reference 中存储的直接就是对象的地址。 这两种对象访问⽅式各有优势。使⽤句柄来访问的最⼤好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，⽽ reference 本身不需要修改。使⽤直接指针访问⽅式最⼤的好处就是速度快，它节省了⼀次指针定位的时间开销。 5、对象内存分配 Java内存体系中所提倡的自动内存管理最终可以归结为自动化地解决两个问题：给对象分配内存和回收分配给对象的内存。 对象的内存分配，往大方向讲，就是在堆上分配，对象主要分配在新生代的Eden区域，如果启动了本地线程分配缓冲，将按线程优先分配在TLAB上。少数情况下也可能直接分配在老年代中。具体的分配规则取决于垃圾收集器的类型以及虚拟机中参数的配置。但是有几条最普遍的内存分配规则如下： 5.1 对象优先在Eden分配大多数情况下，对象在新生代Eden区进行分配。当Eden区没有足够内存进行分配时，虚拟机将会发起一次Minor GC。 5.2 大对象直接进入老年代所谓大对象，是指需要大量连续存储空间的Java对象，最典型的大对象就是那种很长的字符串或者数组。大对象对虚拟机分配来说是一个坏消息，经常出现大对象会导致虚拟机需要经常调用GC来为这些大对象整理出足够的连续空间。 5.3长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应该放在新生代，哪些对象应该放在老年代。为了做到这一点，虚拟机给每一个对象定义了一个对象年龄计数器。如果对象在Eden出生并且经过了第一次Minor GC后仍然存活，并且能够被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设置为1。对象在Survivor区域中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁）对象将会被晋身到老年代中。 5.4 动态对象年龄判定为了能够更好的适应不同程序的内存状况，虚拟机并不是每次都要等到对象的年龄到达阈值才将对象移动到老年代。如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需等待年龄增长。 5.6 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代中最大可用的连续空间是否大于新生代所有对象空间综合，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次GC是有风险的；如果小于，或者设置不允许，那这时将改为进行一次Full GC。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-内存结构","date":"2021-07-30T03:53:42.000Z","path":"wiki/JVM-内存结构/","text":"作为程序员，最常接触到Java虚拟机的部分应该是内存结构这一部分了，同样这一部分的内容很多，面试也是最常被问到的。虽然JDK已经发布了16版本，但是国内大部分企业都还在使用JDK8。 今天学习一下虚拟机的运行时数据区的组成和各个组件的功能。 JDK8官方网站文档链接 – 》 JDK 运行时数据区Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些数据区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户的启动和结束而建立和销毁。 程序计数器程序计数器（Program Counter Register），它是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的模型概念中，字节码解释器的工作就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 如果线程正在执行一个 Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是 Native 方法，这个计数器的值则为 (Undefined)。 由于Java虚拟机的多线程是通过线程轮流切换处理器执行时间的方式实现的，在任何一个确定的时刻，一个处理器的一个核只会执行一条线程中的指令，因此，为了线程切换后能够恢复到正确的执行位置，每一条线程都需要拥有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，这类内存区域称为“线程私有”的内存，即如上图所示，每一个线程都会拥有自己的一块内存区域。 程序计数器在执行本地方法时（例如调用C语言代码）计数器值为空，其他时候则是指向正在执行的虚拟机字节码指令的地址。 程序计数器是在Java虚拟机规范中唯一一个没有规定任何OutOfMemoryError情况的区域，因为Java程序计数器它所需要存储的内容仅仅就是下一个需要待执行的命令的地址，其所需内存是创建时即可只晓的，不需要后期进行扩容等其他的操作。 Java虚拟机栈Java虚拟机栈（Java Virtual Machine Stacks），Java虚拟机栈也是线程私有的,它的生命周期与线程相同。Java每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至方法执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 虚拟机栈中局部变量表部分与Java对象内存分配关系密切，局部变量表存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，该类型可能是一个指向对象起始地址的引用指针，也可能是一个代表对象的句柄或其他于此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 局部变量表中，64位长度的long和double类型的数据会占用2个局部变量空间，其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况： 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态拓展，如果拓展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈作用类似，它们之间的区别是虚拟机栈为虚拟机执行Java方法，而本地方法栈则为虚拟机执行Native方法服务。有些虚拟机会将本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈也会抛出StackOverflowErro和OutOfMemoryError异常。 Java堆Java堆（Java Heap），对于大多数的应用来说，Java堆是虚拟机所管理的最大的一块内存。Java堆是被所有的线程所共享的，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都是在这里分配内存的（Java虚拟机规范中描述为所有的对象实例和数组都要在堆上分配内存）。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称为GC堆。从内存回收的角度来看，由于现在收集器基本都是采用分代算法收集器，所以Java堆中还可以细分为：新生代和老年代；再细致一点可以分为Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB）。 根据Java虚拟机规范，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，在实现时既可以是固定大小的，也可以是可拓展的，当前主流的虚拟机都是按照可拓展来实现的。如果在堆中没有内存完成实例分配，并且堆也无法再拓展时，将会抛出OutOfMemoryError异常。 在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。新生代 ( Young ) 又被划分为三个区域：Eden、**From Survivor(S0)、To Survivor(S1)**。如图所示： 这样划分的目的是为了使JVM能够更好的管理内存中的对象，包括内存的分配以及回收。 而新生代按eden和两个survivor的分法，是为了 有效空间增大，eden+1个survivor； 有利于对象代的计算，当一个对象在S0/S1中达到设置的XX:MaxTenuringThreshold值后，会将其挪到老年代中，即只需扫描其中一个survivor。如果没有S0/S1,直接分成两个区，该如何计算对象经过了多少次GC还没被释放。 两个Survivor区可解决内存碎片化 堆栈相关参数 参数 描述 -Xms 堆内存初始大小，单位m、g -Xmx 堆内存最大允许大小，一般不要大于物理内存的80% -Xmn 年轻代内存初始大小 -Xss 每个线程的堆栈大小，即JVM栈的大小 -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值 -XX:NewSzie(-Xns) 年轻代内存初始大小,可以缩写-Xns -XX:MaxNewSize(-Xmx) 年轻代内存最大允许大小，可以缩写-Xmx -XX:SurvivorRatio 年轻代中Eden区与Survivor区的容量比例值，默认为8，即8:1 -XX:MinHeapFreeRatio GC后，如果发现空闲堆内存占到整个预估堆内存的40%，则放大堆内存的预估最大值，但不超过固定最大值。 -XX:MaxHeapFreeRatio 预估堆内存是堆大小动态调控的重要选项之一。堆内存预估最大值一定小于或等于固定最大值(-Xmx指定的数值)。前者会根据使用情况动态调大或缩小，以提高GC回收的效率，默认70% -XX:MaxTenuringThreshold 垃圾最大年龄，设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代。对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率 -XX:InitialTenuringThreshold 可以设定老年代阀值的初始值 -XX:+PrintTenuringDistribution 查看每次minor GC后新的存活周期的阈值 Note： 每次GC 后会调整堆的大小，为了防止动态调整带来的性能损耗，一般设置-Xms、-Xmx 相等。 新生代的三个设置参数：-Xmn，-XX:NewSize，-XX:NewRatio的优先级： （1）.最高优先级： -XX:NewSize=1024m和-XX:MaxNewSize=1024m （2）.次高优先级： -Xmn1024m （默认等效效果是：-XX:NewSize==-XX:MaxNewSize==1024m） （3）.最低优先级：-XX:NewRatio=2 推荐使用的是-Xmn参数，原因是这个参数很简洁，相当于一次性设定NewSize和MaxNewSIze，而且两者相等。 方法区方法区（Method Area）与Java堆一样，是线程共享的，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。类加载的信息和数据就放在方法区。 Java虚拟机规范堆方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可拓展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说，这个区域的内存回收成绩比较令人难以满意，尤其时类型卸载，条件相当苛刻，但是这个区域的内存回收也是必要的。 根据Java虚拟机规范规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分，Class文件中除了类的版本、字段、方法、接口等描述信息以外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译器才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量池放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 直接内存直接内存（Direct Memory)并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范所定义的内存区域，但是这部分内存也被频繁的使用，而且也可能导致OutOfMemoryError异常出现。 在JDK1.4中新加入的NIO（New Input/Output）类，引入了一种基于通道与缓冲区的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，这样能在一些场景中显著提高性能，因为避免了在Java堆中和Native堆中来回复制数据。 直接内存虽然不会受到Java堆大小的限制，但是受到本机总内存大小以及处理器寻址空间的限制，如果忽略了直接内存，当各个区域内存总和大于服务器内存时，将会导致动态拓展时出现OutOfMemoryError异常。 参考资料https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-2.html#jvms-2.5 https://blog.csdn.net/qq_21122519/article/details/94408118 https://www.processon.com/view/5ec5d7c60791290fe0768668?fromnew=1","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-深入理解Java虚拟机","date":"2021-07-30T02:35:12.000Z","path":"wiki/JVM-深入理解Java虚拟机/","text":"JVM结构图 知识清单 Java内存结构 Java对象创建 Java内存模型 类加载机制 垃圾回收机制 垃圾收集器 虚拟机调优 学习资料JAVA虚拟机概述Java虚拟机（JVM）工作原理推荐收藏系列：一文理解JVM虚拟机（内存、垃圾回收、性能优化）解决面试中遇到问题Java虚拟机内存管理和性能调优重读 JVM【2021最新版】JVM面试题总结（87道题含答案解析）JVM 基础 - Java 类加载机制JVM知识点整理https://www.processon.com/view/5ec5d7c60791290fe0768668?fromnew=1","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"Java并发编程-深入理解volatile","date":"2021-07-29T08:25:05.000Z","path":"wiki/Java并发编程-深入理解volatile/","text":"volatile特性正确理解volatile 多级cache结构 -&gt; 缓存一致性协议（MESI）-&gt; store buffer和invalidate queue -&gt; 内存屏障 可见性volatile的可见性依赖于Java内存模型。 可以参见之前的文章 👉 Java内存模型 Java内存模型(JavaMemoryModel)描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量，存储到内存和从内存中读取变量这样的底层细节。 所有的共享变量都存储于主内存，这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。 每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。 线程对变量的所有的操作(读，取)都必须在工作内存中完成，而不能直接读写主内存中的变量。 不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。 volatile实现可见性 每个线程操作数据的时候会把数据从主内存读取到自己的工作内存，如果他操作了数据并且写会了，他其他已经读取的线程的变量副本就会失效了，需要都数据进行操作又要再次去主内存中读取了。 volatile保证不同线程对共享变量操作的可见性，也就是说一个线程修改了volatile修饰的变量，当修改写回主内存时，另外一个线程立即看到最新的值。 至于其他线程是如何更新缓存行中的数据以及其他线程的缓存行是如何失效的，可以参见之前的文章。 Java内存模型 嗅探机制在现代计算机中，CPU 的速度是极高的，如果 CPU 需要存取数据时都直接与内存打交道，在存取过程中，CPU 将一直空闲，这是一种极大的浪费，所以，为了提高处理速度，CPU 不直接和内存进行通信，而是在 CPU 与内存之间加入很多寄存器，多级缓存，它们比内存的存取速度高得多，这样就解决了 CPU 运算速度和内存读取速度不一致问题。 由于 CPU 与内存之间加入了缓存，在进行数据操作时，先将数据从内存拷贝到缓存中，CPU 直接操作的是缓存中的数据。但在多处理器下，将可能导致各自的缓存数据不一致（这也是可见性问题的由来），为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，而嗅探是实现缓存一致性的常见机制。 嗅探机制工作原理：每个处理器通过监听在总线上传播的数据来检查自己的缓存值是不是过期了，如果处理器发现自己缓存行对应的内存地址修改，就会将当前处理器的缓存行设置无效状态，当处理器对这个数据进行修改操作的时候，会重新从主内存中把数据读到处理器缓存中。 注意： 基于 CPU 缓存一致性协议，JVM 实现了 volatile 的可见性，但由于总线嗅探机制，会不断的监听总线，如果大量使用 volatile 会引起总线风暴。所以，volatile 的使用要适合具体场景。 重排序什么是指令重排序? 为了提高性能，编译器和处理器常常会对既定的代码执行顺序进行指令重排序。 【源代码】 -&gt; 【编译器优化重排序】-&gt; 【指令集并行重排序】-&gt; 【内存系统重排序】-&gt; 【最终执行指令序列】 一般重排序可以分为如下三种： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序; 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序; 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行的。 as-if-serial语义不管怎么重排序，单线程下的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 java编译器会在生成指令系列时在适当的位置会插入内存屏障指令来禁止特定类型的处理器重排序。 为了实现volatile的内存语义，JMM会限制特定类型的编译器和处理器重排序，JMM会针对编译器制定volatile重排序规则表： 内存屏障 说明 StoreStore 屏障 禁止上面的普通写和下面的 volatile 写重排序。 StoreLoad 屏障 防止上面的 volatile 写与下面可能有的 volatile 读/写重排序。 LoadLoad 屏障 禁止下面所有的普通读操作和上面的 volatile 读重排序。 LoadStore 屏障 禁止下面所有的普通写操作和上面的 volatile 读重排序。 happens-before规则如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。 volatile域规则：对一个volatile域的写操作，happens-before于任意线程后续对这个volatile域的读。 volatile在DCL的应用1234567891011121314151617public class Singleton &#123; public static volatile Singleton singleton; /** * 构造函数私有，禁止外部实例化 */ private Singleton() &#123;&#125;; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 现在我们分析一下为什么要在变量singleton之间加上volatile关键字。要理解这个问题，先要了解对象的构造过程，实例化一个对象其实可以分为三个步骤： 分配内存空间。 初始化对象。 将内存空间的地址赋值给对应的引用。 但是由于操作系统可以对指令进行重排序，所以上面的过程也可能会变成如下过程： 分配内存空间。 将内存空间的地址赋值给对应的引用。 初始化对象 如果是这个流程，多线程环境下就可能将一个未初始化的对象引用暴露出来，从而导致不可预料的结果。因此，为了防止这个过程的重排序，我们需要将变量设置为volatile类型的变量。 一次性安全发布12345678910111213141516171819public class BackgroundFloobleLoader &#123; public volatile Flooble theFlooble; public void initInBackground() &#123; // do lots of stuff theFlooble = new Flooble(); // this is the only write to theFlooble &#125;&#125;public class SomeOtherClass &#123; public void doWork() &#123; while (true) &#123; // do some stuff... // use the Flooble, but only if it is ready if (floobleLoader.theFlooble != null) doSomething(floobleLoader.theFlooble); &#125; &#125;&#125; 如果 theFlooble 引用不是 volatile 类型，doWork() 中的代码在解除对 theFlooble 的引用时，将会得到一个不完全构造的 Flooble。 该模式的一个必要条件是：被发布的对象必须是线程安全的，或者是有效的不可变对象（有效不可变意味着对象的状态在发布之后永远不会被修改）。volatile 类型的引用可以确保对象的发布形式的可见性，但是如果对象的状态在发布后将发生更改，那么就需要额外的同步。 无法保证原子性所谓的原子性是指在一次操作或者多次操作中，要么所有的操作全部都得到了执行并且不会受到任何因素的干扰而中断，要么所有的操作都不执行。 在多线程环境下，volatile 关键字可以保证共享数据的可见性，但是并不能保证对数据操作的原子性。也就是说，多线程环境下，使用 volatile 修饰的变量是线程不安全的。 要解决这个问题，我们可以使用锁机制，或者使用原子类（如 AtomicInteger）。 这里特别说一下，对任意单个使用 volatile 修饰的变量的读 / 写是具有原子性，但类似于 flag = !flag 这种复合操作不具有原子性。简单地说就是，单纯的赋值操作是原子性的。 volatile 和 synchronizedvolatile只能修饰实例变量和类变量，而synchronized可以修饰方法，以及代码块。 volatile保证数据的可见性，但是不保证原子性(多线程进行写操作，不保证线程安全);而synchronized是一种排他(互斥)的机制。 volatile用于禁止指令重排序：可以解决单例双重检查对象初始化代码执行乱序问题。 volatile可以看做是轻量版的synchronized，volatile不保证原子性，但是如果是对一个共享变量进行多个线程的赋值，而没有其他的操作，那么就可以用volatile来代替synchronized，因为赋值本身是有原子性的，而volatile又保证了可见性，所以就可以保证线程安全了。 总结1、volatile修饰符适用于以下场景：某个属性被多个线程共享，其中有一个线程修改了此属性，其他线程可以立即得到修改后的值，比如booleanflag;或者作为触发器，实现轻量级同步。 2、volatile属性的读写操作都是无锁的，它不能替代synchronized，因为它没有提供原子性和互斥性。因为无锁，不需要花费时间在获取锁和释放锁_上，所以说它是低成本的。 3、volatile只能作用于属性，我们用volatile修饰属性，这样compilers就不会对这个属性做指令重排序。 4、volatile提供了可见性，任何一个线程对其的修改将立马对其他线程可见，volatile属性不会被线程缓存，始终从主 存中读取。 5、volatile提供了happens-before保证，对volatile变量v的写入happens-before所有其他线程后续对v的读操作。 6、volatile可以使得long和double的赋值是原子的。 7、volatile可以在单例双重检查中实现可见性和禁止指令重排序，从而保证安全性。 参考资料 面试官想到，一个Volatile，敖丙都能吹半小时 面试官最爱的volatile关键字 一文吃透Volatile，征服面试官 java语言的线程安全volatile用法 线程安全(上)–彻底搞懂volatile关键字","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java-synchronized关键字剖析","date":"2021-07-29T05:37:27.000Z","path":"wiki/Java-synchronized关键字剖析/","text":"之前写过一篇关于synchronized关键字的文章，是当时听马士兵老师的公开课时记录的一些关键笔记📒 链接🔗 下面我们还是要学习和总结一下synchronized synchronized 特性 有序性as-if-serial 不管编译器和CPU如何重排序，必须保证在单线程情况下程序的结果是正确的，还有就是有数据依赖的也是不能重排序的。 12int a = 1;int b = a; 这两段是怎么都不能重排序的，b的值依赖a的值，a如果不先赋值，那就为空了。 可见性主要依靠Java内存模型实现 原子性通过汇编指令控制 可重入synchronized锁对象的时候有个计数器，他会记录下线程获取锁的次数，在执行完对应的代码块之后，计数器就会-1，直到计数器清零，就释放锁了 不可中断不可中断就是指，一个线程获取锁之后，另外一个线程处于阻塞或者等待状态，前一个不释放，后一个也一直会阻塞或者等待，不可以被中断。值得一提的是，Lock的tryLock方法是可以被中断的。 了解对象头Mark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。 在64位的虚拟机中： 32位虚拟机中： 可以参见之前的文章 👉 Java对象头 synchronized实现之前的文章已经在 Java代码、字节码、JVM级别和汇编指令四个级别介绍了synchronzied的实现。JDK对synchronzied不断的优化，大家熟悉的锁升级过程，其实就是在源码里面，调用了不同的实现去获取获取锁，失败就调用更高级的实现，最后升级完成。 升级方向：【 无锁 】 -&gt; 【 偏向锁 】-&gt; 【 轻量级锁 】-&gt; 【 重量级锁 】 Tip：切记这个升级过程是不可逆的； 无锁无锁是指没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 无锁的特点是修改操作会在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。 偏向锁对象头是由 Mark Word 和 Class pointer 组成，锁争夺也就是对象头指向的Monitor对象的争夺，一旦有线程持有了这个对象，标志位修改为1，就进入偏向模式，同时会把这个线程的ID记录在对象的Mark Word中。 这个过程是采用了CAS乐观锁操作的，每次同一线程进入，虚拟机就不进行任何同步的操作了，对标志位+1就好了，不同线程过来，CAS会失败，也就意味着获取锁失败。 偏向锁在1.6之后是默认开启的，1.5中是关闭的，需要手动开启参数是xx:-UseBiasedLocking=false。 初次执行到synchronized代码块的时候，锁对象变成偏向锁（通过CAS修改对象头里的锁标志位），字面意思是“偏向于第一个获得它的线程”的锁。执行完同步代码块后，线程并不会主动释放偏向锁。 当第二次到达同步代码块时，线程会判断此时持有锁的线程是否就是自己（持有锁的线程ID也在对象头里），如果是则正常往下执行。由于之前没有释放锁，这里也就不需要重新加锁。如果自始至终使用锁的线程只有一个，很明显偏向锁几乎没有额外开销，性能极高。 偏向锁是指当一段同步代码一直被同一个线程所访问时，即不存在多个线程的竞争时，那么该线程在后续访问时便会自动获得锁，从而降低获取锁带来的消耗，即提高性能。 当一个线程访问同步代码块并获取锁时，会在 Mark Word 里存储锁偏向的线程 ID。在线程进入和退出同步块时不再通过 CAS 操作来加锁和解锁，而是检测 Mark Word 里是否存储着指向当前线程的偏向锁。轻量级锁的获取及释放依赖多次 CAS 原子指令，而偏向锁只需要在置换 ThreadID 的时候依赖一次 CAS 原子指令即可。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程是不会主动释放偏向锁的。 关于偏向锁的撤销，需要等待全局安全点，即在某个时间点上没有字节码正在执行时，它会先暂停拥有偏向锁的线程，然后判断锁对象是否处于被锁定状态。如果线程不处于活动状态，则将对象头设置成无锁状态，并撤销偏向锁，恢复到无锁（标志位为01）或轻量级锁（标志位为00）的状态。 偏向锁关闭，或者多个线程竞争偏向锁怎么办呢？ 轻量级锁还是跟Mark Work 相关，如果这个对象是无锁的，jvm就会在当前线程的栈帧中建立一个叫锁记录（Lock Record）的空间，用来存储锁对象的Mark Word 拷贝，然后把Lock Record中的owner指向当前对象。 JVM接下来会利用CAS尝试把对象原本的Mark Word 更新会Lock Record的指针，成功就说明加锁成功，改变锁标志位，执行相关同步操作。 如果失败了，就会判断当前对象的Mark Word是否指向了当前线程的栈帧，是则表示当前的线程已经持有了这个对象的锁，否则说明被其他线程持有了，继续锁升级，修改锁的状态，之后等待的线程也阻塞。 轻量级锁是指当锁是偏向锁的时候，却被另外的线程所访问，此时偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，线程不会阻塞，从而提高性能。 轻量级锁的获取主要由两种情况：① 当关闭偏向锁功能时；② 由于多个线程竞争偏向锁导致偏向锁升级为轻量级锁。 一旦有第二个线程加入锁竞争，偏向锁就升级为轻量级锁（自旋锁）。这里要明确一下什么是锁竞争： 如果多个线程轮流获取一个锁，但是每次获取锁的时候都很顺利，没有发生阻塞，那么就不存在锁竞争。只有当某线程尝试获取锁的时候，发现该锁已经被占用，只能等待其释放，这才发生了锁竞争。 在轻量级锁状态下继续锁竞争，没有抢到锁的线程将自旋，即不停地循环判断锁是否能够被成功获取。获取锁的操作，其实就是通过CAS修改对象头里的锁标志位。先比较当前锁标志位是否为“释放”，如果是则将其设置为“锁定”，比较并设置是原子性发生的。这就算抢到锁了，然后线程将当前锁的持有者信息修改为自己。 长时间的自旋操作是非常消耗资源的，一个线程持有锁，其他线程就只能在原地空耗CPU，执行不了任何有效的任务，这种现象叫做忙等（busy-waiting）。如果多个线程用一个锁，但是没有发生锁竞争，或者发生了很轻微的锁竞争，那么synchronized就用轻量级锁，允许短时间的忙等现象。这是一种折衷的想法，短时间的忙等，换取线程在用户态和内核态之间切换的开销。 自旋锁我不是在上面提到了Linux系统的用户态和内核态的切换很耗资源，其实就是线程的等待唤起过程，那怎么才能减少这种消耗呢？ 自旋，过来的现在就不断自旋，防止线程被挂起，一旦可以获取资源，就直接尝试成功，直到超出阈值，自旋锁的默认大小是10次，-XX：PreBlockSpin可以修改。 自旋都失败了，那就升级为重量级的锁，像1.5的一样，等待唤起咯。 自适应自旋锁自适应意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定： 如果在同一个锁对象上，自旋等待之前成功获得过的锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，因此允许自旋等待持续相对更长的时间。 相反的，如果对于某个锁，自旋很少成功获得过，那么以后要获取这个锁时将可能减少自旋时间甚至省略自旋过程，以避免浪费处理器资源。 自适应自旋解决的是“锁竞争时间不确定”的问题。JVM很难感知确切的锁竞争时间，而交给用户分析就违反了JVM的设计初衷。自适应自旋假定不同线程持有同一个锁对象的时间基本相当，竞争程度趋于稳定。因此，可以根据上一次自旋的时间与结果调整下一次自旋的时间。 重量级锁如果锁竞争情况严重，某个达到最大自旋次数的线程，会将轻量级锁升级为重量级锁（依然是CAS修改锁标志位，但不修改持有锁的线程ID）。当后续线程尝试获取锁时，发现被占用的锁是重量级锁，则直接将自己挂起（而不是忙等），等待将来被唤醒。 重量级锁是指当有一个线程获取锁之后，其余所有等待获取该锁的线程都会处于阻塞状态。 简言之，就是所有的控制权都交给了操作系统，由操作系统来负责线程间的调度和线程的状态变更。而这样会出现频繁地对线程运行状态的切换，线程的挂起和唤醒，从而消耗大量的系统资 大家在看ObjectMonitor源码的时候，会发现Atomic::cmpxchg_ptr，Atomic::inc_ptr等内核函数，对应的线程就是park()和upark()。这个操作涉及用户态和内核态的转换了，这种切换是很耗资源的，所以知道为啥有自旋锁这样的操作了吧，按道理类似死循环的操作更费资源才是对吧？其实不是，大家了解一下就知道了。 那用户态和内核态又是啥呢？ Linux系统的体系结构分为用户空间（应用程序的活动空间）和内核。我们所有的程序都在用户空间运行，进入用户运行状态也就是（用户态），但是很多操作可能涉及内核运行，比如涉及到I/O，我们就会进入内核运行状态（内核态）。 这个过程是很复杂的，也涉及很多值的传递，我简单概括下流程： 用户态把一些数据放到寄存器，或者创建对应的堆栈，表明需要操作系统提供的服务。 用户态执行系统调用（系统调用是操作系统的最小功能单位）。 CPU切换到内核态，跳到对应的内存指定的位置执行指令。 系统调用处理器去读取我们先前放到内存的数据参数，执行程序的请求。 调用完成，操作系统重置CPU为用户态返回结果，并执行下个指令。 所以大家一直说，1.6之前是重量级锁，没错，但是他重量的本质，是ObjectMonitor调用的过程，以及Linux内核的复杂运行机制决定的，大量的系统资源消耗，所以效率才低。还有两种情况也会发生内核态和用户态的切换：异常事件和外围设备的中断 大家也可以了解下。 普通的IO读写也会涉及到用户态和内核的切换，但是为了提升IO 的性能，操作系统可以通过 零拷贝 来实现，Redis和Kafka,还有netty的底层IO模型都存在零拷贝。 锁对比 synchronized和Lock对比我们先看看他们的区别： synchronized是关键字，是JVM层面的底层啥都帮我们做了，而Lock是一个接口，是JDK层面的有丰富的API。 synchronized会自动释放锁，而Lock必须手动释放锁。 synchronized是不可中断的，Lock可以中断也可以不中断。 通过Lock可以知道线程有没有拿到锁，而synchronized不能。 synchronized能锁住方法和代码块，而Lock只能锁住代码块。 Lock可以使用读锁提高多线程读效率。 synchronized是非公平锁，ReentrantLock可以控制是否是公平锁。 两者一个是JDK层面的一个是JVM层面的，我觉得最大的区别其实在，我们是否需要丰富的api，还有一个我们的场景。 比如我现在是滴滴，我早上有打车高峰，我代码使用了大量的synchronized，有什么问题？锁升级过程是不可逆的，过了高峰我们还是重量级的锁，那效率是不是大打折扣了？这个时候你用Lock是不是很好？ synchronized使用注意事项synchronized是通过软件(JVM)实现的，简单易用，即使在JDK5之后有了Lock，仍然被广泛地使用。 使用Synchronized有哪些要注意的？ 锁对象不能为空，因为锁的信息都保存在对象头里 作用域不宜过大，影响程序执行的速度，控制范围过大，编写代码也容易出错 避免死锁 在能选择的情况下，既不要用Lock也不要用synchronized关键字，用java.util.concurrent包中的各种各样的类，如果不用该包下的类，在满足业务的情况下，可以使用synchronized关键，因为代码量少，避免出错 synchronized是公平锁吗？ synchronized实际上是非公平的，新来的线程有可能立即获得监视器，而在等待区中等候已久的线程可能再次等待，不过这种抢占的方式可以预防饥饿。 参考文档Linux探秘之用户态与内核态 傻瓜三歪让我教他「零拷贝」 偏向锁、轻量级锁、重量级锁、自旋锁、自适应自旋锁 关于 锁的四种状态与锁升级过程 图文详解 死磕Synchronized底层实现","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"分布式-秒杀系统设计","date":"2021-07-29T03:03:58.000Z","path":"wiki/分布式-秒杀系统设计/","text":"秒杀系统的挑战高并发秒杀活动的特点就是短时间内聚集大量请求瞬时到达服务端，此时数据库已经无法支撑如此大数据量的请求了。单个的数据库QPS仅有几百，你可能会想，那我是不是可以部署一个数据库集群呢，在数据库集群前使用Nginx分发，将负载平均分摊到每一个数据库不就可以了吗。这种方式一定程度上是可以的，但是国内真正的互联网大厂肯定不是这样做的，在高并发的情况下完全可以使用内存操作来代替访问数据库，比如Redis集群。在网络带宽允许的情况下，Redis的集群的系统吞吐量肯定和数据库集群不是一个量级的，这里要查几十倍几百倍。 超卖超卖最直接的后果就是可能会对公司造成直接的经济损失。防止超卖是秒杀系统必须要保证的一点。超卖的解决办法就是加锁，保证在高并发的情况下库存可以正常的正确的扣减。 恶意请求产品层策略秒杀器一般下单和购买及其迅速，根据购买记录可以甄别出一部分。可以通过校验码达到一定的方法，这就要求校验码足够安全，不被破解，采用的方式有：秒杀专用验证码，电视公布验证码，秒杀答题。 前端控制除此之外前段可以添加点击次数限制，点击一定次数之后，将按钮置灰色，或者在js层级进行控制，用户看到的是每次都点击成功了，但是仅仅发起一次服务端请求； 后端控制 添加消息队列，消息执行一定数量时，队列后续的消息不在执行 后端架构按照模块拆分，用户不同的请求分散转发到各个模块的服务器，负载均衡 数据库分库分表（分片策略）和redis集群 升级服务器带宽，压力测试，保证系统吞吐量 过载保护，限流，请求拒绝和服务降级 链接加盐链接加盐一定程度上可以保护恶意攻击，比如下单接口，如果暴露之后，就会存在恶意攻击，一个用户下了几百个单的情况或者一个IP下了很多单子，类似与黄牛抢票之后再去售卖。 秒杀系统架构图（参考） 参考资料 敖丙带你设计【秒杀系统】《吊打面试官》系列-秒杀系统设计这是我读过写得最好的【秒杀系统架构】分析与实战！这一次，彻底弄懂“秒杀系统”","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[]},{"title":"JVM-Xms,Xmx和Xss","date":"2021-07-28T13:04:14.000Z","path":"wiki/JVM-Xms-Xmx和Xss/","text":"性能调优参数Xms，Xmx，Xss的含义-Xss规定了每个线程虚拟机栈及堆栈的大小，一般情况下，256k是足够的，此配置将会影响此进程中并发线程数的大小。 -Xms表示初始化JAVA堆的大小及该进程刚创建出来的时候，他的专属JAVA堆的大小，一旦对象容量超过了JAVA堆的初始容量，JAVA堆将会自动扩容到-Xmx大小。 -Xmx表示java堆可以扩展到的最大值，在很多情况下，通常将-Xms和-Xmx设置成一样的，因为当堆不够用而发生扩容时，会发生内存抖动影响程序运行时的稳定性。 堆内存分配：JVM初始分配的内存由-Xms指定，默认是物理内存的1/64JVM最大分配的内存由-Xmx指定，默认是物理内存的1/4默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。对象的堆内存由称为垃圾回收器的自动内存管理系统回收。非堆内存分配：JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。-Xmn2G：设置年轻代大小为2G。-XX:SurvivorRatio，设置年轻代中Eden区与Survivor区的比值。 参考资料1、类似-Xms、-Xmn这些参数的含义：2、JVM三大性能调优参数Xms，Xmx，Xss的含义，你又知道多少呢","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-自定义类加载器","date":"2021-07-28T13:02:32.000Z","path":"wiki/JVM-自定义类加载器/","text":"如何自定义类加载器为什么要自定义加载器 原因：1、存放在自定义路径上的类，需要通过自定义类加载器去加载。【注意：AppClassLoader加载classpath下的类】2、类不一定从文件中加载，也可能从网络中的流中加载，这就需要自定义加载器去实现加密解密。3、可以定义类的实现机制，实现类的热部署,如OSGi中的bundle模块就是通过实现自己的ClassLoader实现的，如tomcat实现的自定义类加载模型。 如何实现自定义加载器 实现自定义类加载有以下两步：1、继承ClassLoader2、重写findClass，在findClass里获取类的字节码，并调用ClassLoader中的defineClass方法来加载类，获取class对象。注意：如果要打破双亲委派机制，需要重写loadClass方法。如下：是一个自定义 的类加载器 1234567891011121314151617181920public static class MyClassLoader extends ClassLoader&#123; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] data=null; try &#123; data= loadByte(name); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return this.defineClass(data,0,data.length); &#125; private byte[] loadByte(String name) throws IOException &#123; File file = new File(&quot;/Users/admin/test/&quot;+name); FileInputStream fi = new FileInputStream(file); int len = fi.available(); byte[] b = new byte[len]; fi.read(b); return b; &#125; &#125; 下面是要加载的类： 12345public class Demo&#123;public void say()&#123;System.out.println(&quot;hello&quot;);&#125;&#125; 该类编译后的class 文件放置在/Users/admin/test/下,然后执行如下代码去加载： 1234567MyClassLoader classLoader = new MyClassLoader(); Class clazz = classLoader.loadClass(&quot;Demo.class&quot;); Object o=clazz.newInstance(); Method method = clazz.getMethod(&quot;say&quot;); method.invoke(o);输出:hello 能不能自己写一个java.lang.String 1、代码书写后可以编译不会报错2、在另一个类中加载java.lang.String，通过反射调用自己写的String类里的方法，得到结果NoSuchMethod，说明加载的还是原来的String，因为通过双亲委派机制，会把java.lang.String一直提交给启动类加载器去加载，通过他加载，加载到的永远是/lib下面的java.lang.String3、在这个自己写的类中写上main方法public static void main(String[] args)执行main方法报错，因为这个String并不是系统的java.lang.String，所以JVM找不到main方法的签名 参考资料JVM:如何实现一个自定义类加载器？原文链接：https://blog.csdn.net/qq_28605513/article/details/85014451","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"分布式-一致性哈希算法","date":"2021-07-28T13:01:05.000Z","path":"wiki/分布式-一致性哈希算法/","text":"一致性哈希算法一致性哈希算法(Consistent Hashing Algorithm)是一种分布式算法，常用于负载均衡。Memcached client也选择这种算法，解决将key- value均匀分配到众多Memcached server上的问题。它可以取代传统的取模操作，解决了取模操作无法应对增删Memcached Server的 问题(增删server会导致同一个key,在get操作时分配不到数据真正存储的server，命中率会急剧下降)。 哈希指标 评估一个哈希算法的优劣，有如下指标，而一致性哈希全部满足： 均衡性(Balance)：将关键字的哈希地址均匀地分布在地址空间中，使地址空间得到充分利用，这是设计哈希的一个基本特性。 单调性(Monotonicity): 单调性是指当地址空间增大时，通过哈希函数所得到的关键字的哈希地址也能映射的新的地址空间，而不是仅 限于原先的地址空间。或等地址空间减少时，也是只能映射到有效的地址空间中。简单的哈希函数往往不能满足此性质。 分散性(Spread): 哈希经常用在分布式环境中，终端用户通过哈希函数将自己的内容存到不同的缓冲区。此时，终端有可能看不到所 有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不 同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为 它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够 尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load): 负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于 一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能 够尽量降低缓冲的负荷 一致性哈希 将节点通过hash映射到hash环上，理想的情况是多个节点直接分布均匀 当我们的对象通过hash算法分配在hash环上的时候，它是固定分配到一个节点的空间上的，当我们在BC之间插入一个节点时，仅仅会影 响到BC这一段空间上的数据，而不是整个环上的数据都要跟着变化； 现实情况下，节点之间可能分配不均匀 这和传统的hash取模一样，同样会数据倾斜的问题！ 虚拟节点 这个时候虚拟节点就此诞生，下面让我们来看一下虚拟节点在一致性Hash中的作用。当我们在Hash环上新增若干个点，那么每个点之间 的距离就会接近相等。按照这个思路我们可以新增若干个片/表，但是成本有限，我们通过复制多个A、B、C的副本({A1-An},{B1-Bn},{C1- Cn})一起参与计算，按照顺时针的方向进行数据分布，按照下图示意: 此时A=[A,C1)&amp;[A1,C2)&amp;[A2,B4)&amp;[A3,A4)&amp;[A4,B1)；B=[B,A1)&amp;[B2,C)&amp;[B3,C3)&amp;[B4,C4)&amp;[B1,A)；C=[C1,B)&amp;[C2,B2)&amp;[C,B3)&amp;[B3,C3)&amp; [C4,A3)；由图可以看出分布点越密集，平衡性约好。 算法实现一致性哈希算法有多种具体的实现，包括 Chord 算法，KAD 算法等，都比较复杂。 参考资料1 、一致性哈希算法的原理与实现2、浅谈一致性Hash原理及应用3、https://juejin.cn/post/6844903598694858766","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"}]},{"title":"分布式-CAP理论","date":"2021-07-28T10:15:10.000Z","path":"wiki/分布式-CAP理论/","text":"CAP 原则CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP原则的精髓就是要么AP，要么CP，要么AC，但是不存在CAP。 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值，即写操作之后的读操作，必须返回该值。（分为弱一致性、强一致性和最终一致性） 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 取舍策略 CA without P 如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库RDBMS：Oracle、MySQL就是CA。 CP without A 如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。 AP wihtout C 要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。 解决方案——BASEBASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果。 核心思想：即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 基本可用Basically Available 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态Soft state 软状态也称弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性Eventually consistent 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 参考资料原文链接：https://blog.csdn.net/lixinkuan328/article/details/95535691","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"}]},{"title":"Mysql-事务特性与实现原理","date":"2021-07-28T10:13:07.000Z","path":"wiki/Mysql-事务特性与实现原理/","text":"MySQL事务特性与实现原理1. 事务特性 原子性(Atomicity) 事务中的所有操作作为一个整体像原子一样不可分割，要么全部成功,要么全部失败。 一致性(Consistency) 事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态。一致性状态是指:1.系统的状态满足数据的完整性约束(主码,参照完整性,check约束等) 2.系统的状态反应数据库本应描述的现实世界的真实状态,比如转账前后两个账户的金额总和应该保持不变。 隔离性(Isolation) 并发执行的事务不会相互影响,其对数据库的影响和它们串行执行时一样。比如多个用户同时往一个账户转账,最后账户的结果应该和他们按先后次序转账的结果一样。 持久性(Durability) 事务一旦提交,其对数据库的更新就是持久的。任何事务或系统故障都不会导致数据丢失。 在事务的ACID特性中,C即一致性是事务的根本追求,而对数据一致性的破坏主要来自两个方面1.事务的并发执行2.事务故障或系统故障 2. 事务实现原理 并发控制技术保证了事务的隔离性,使数据库的一致性状态不会因为并发执行的操作被破坏。 日志恢复技术保证了事务的原子性,使一致性状态不会因事务或系统故障被破坏。同时使已提交的对数据库的修改不会因系统崩溃而丢失,保证了事务的持久性。 2.1 回滚日志（undo）undo log属于 「 逻辑日志 」，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。 undo log用于存放数据被修改前的值，如果修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。另外InnoDB MVCC事务特性也是基于undo日志实现的。 因此，undo log有两个作用：提供回滚和多个行版本控制(MVCC)。 2.2 重做日志（redo）redo log重做日志记录的是新数据的备份，属于物理日志。在事务提交前，只要将redo log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是redo log已经持久化。系统可以根据redo log的内容，将所有数据恢复到最新的状态。 redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。 MySQL中redo log刷新规则采用一种称为Checkpoint的机制（利用LSN实现），为了确保安全性，又引入double write机制。 事务基本操作开启事务：start transaction回滚事务：rollback提交事务：commit 数据库隔离级别SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。另外，这篇分布式事务不理解？一次给你讲清楚！推荐大家阅读。 Read Uncommitted（读取未提交内容）在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容）这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。 Repeatable Read（可重读）这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化）这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 事务隔离级别产生的问题 脏读(Drity Read)某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 12345事务A第一次读取到price=100同时事务B更新update price=120，但是此时的事务B还未commit事务A读取的price=120事务B-&gt;rollback操作事务A读取到的是脏数据 不可重复读(Non-repeatable read)在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 1234事务A第一次读取到price=100同时事务B更新update price=120，并commit事务A读取的price=120事务A多次读取的结果不一致 幻读(Phantom Read)幻读和不可重复读的区别在于，幻读主要表现在数据的删除和插入，而不可重复读表现在数据的更新。 1234事务A第一次读取到price=100同时事务B更新delete price=100 这条记录，并commit事务A读取的price=100price这条记录已经不存在，但是事务A还是可以读取到 1、在可重复读隔离级别下，普通查询是快照读，是不会看到别的事务插入的数据的，幻读只在当前读下才会出现。2、幻读专指新插入的行，读到原本存在行的更新结果不算。因为当前读的作用就是能读到所有已经提交记录的最新值。 参考资料详细分析MySQL事务日志(redo log和undo log)数据库事务的概念及其实现原理数据库事务实现原理mysql数据库的隔离级别MYSQL数据库的四种隔离级别MySQL幻读MySQL 事务&amp;&amp;锁机制&amp;&amp;MVCCInnodb中的事务隔离级别和锁的关系 - 美团技术团队","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Mysql-MVCC多版本并发控制","date":"2021-07-28T10:12:28.000Z","path":"wiki/Mysql-MVCC多版本并发控制/","text":"MVCC多版本并发控制MVCC，全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。 MVCC在MySQL InnoDB中的实现主要是为了 「 提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读 」 当前读和快照读 当前读 像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 快照读 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本 说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现 MVCC模型在MySQL中的具体实现则是由 3个隐式字段，undo日志 ，Read View 等去完成的，具体可以看下面的MVCC实现原理 MVCC有什么好处，解决了什么问题多版本并发控制（MVCC）是一种用来「 解决读-写冲突的无锁并发控制 」，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题 MVCC的实现原理MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个point的概念 隐式字段每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段 DB_TRX_ID 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID DB_ROLL_PTR 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） DB_ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了 undo日志undo log主要分为两种： insert undo log 代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log 事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 Read View(读视图)什么是Read View，说白了Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大) 所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 Read View遵循一个可见性算法，主要是将要被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本; 总结所谓的MVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用读已提交（READ COMMITTD）、可重复读（REPEATABLE READ）这两种隔离级别的事务在执行普通的SELECT操作时访问记录的版本链的过程，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，数据的可重复读其实就是ReadView的重复使用。 这样去解释这些技术，主要是希望大家对现象背后的本质多点思考，不然你去背出这几种隔离级别，以及各种数据现象是没有任何意义的，实际开发过程中真的出现了问题，你不懂本质以及过程，你去排查也会很难受的，到头来还是要看书，看资料。 参考资料1、MVCC多版本并发控制2、MVCC浅析3、乐观锁、悲观锁和MVCC，今天让你一次搞懂4、面试官：谈谈你对Mysql的MVCC的理解？5、Mysql中MVCC的使用及原理详解6、[Innodb中的事务隔离级别和锁的关系]","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Java对象头","date":"2021-07-28T09:59:59.000Z","path":"wiki/Java对象头/","text":"Java对象头JOL查看对象头信息在项目中引入以下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 写一个main方法，创建一个Object，然后打印对象信息： 1234 public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable());&#125; 打印结果如下： 12345678java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 由此可知，new Object()在内存中占16个字节，组成部分8字节的markword+4字节的class point+4字节的对齐； Java对象在内存中的布局 markword 存储sync锁标志，分代年龄等一些关键信息 8字节 class pointer 指向当前对象所属类类型 4字节 查看java命令默认带的参数命令： java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=134217728-XX:MaxHeapSize=2147483648-XX:+PrintCommandLineFlags-XX:+UseCompressedClassPointers 压缩类指针 4字节-XX:+UseCompressedOops 普通对象指针压缩 4字节-XX:+UseParallelGC instance data 寸尺当前对象的实例数据 padding 对齐填充，当对象所占字节数不能被8整除之后，进行填充对齐。 目前的操作系统基本上都是64位的； 顺丰面试题，new Object()在内存中占多少个字节1、如果创建的是空对象，没有实例数据 默认开启了class pointer指针压缩 8字节markword + 4字节class pointer + 4字节 padding 如果关闭了类指针压缩 8字节markword + 8字节class pointer 2、如果创建的对象有实力数据，如下对象： 1Person（int age , String name） 默认开启了class pointer指针压缩 8字节markword + 4字节class pointer + 4字节int + 4字节String + 4字节padding对齐","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-CAS原理和底层实现","date":"2021-07-28T09:59:17.000Z","path":"wiki/Java-CAS原理和底层实现/","text":"CAS原理和底层实现 什么是CASCAS是（compare and swap） 的缩写，它能在不加锁的情况下，在多线程的环境下，保证多线程一致性的改动某一值； ABA问题ABA问题是一个线程在CAS比较值和原来是否相等的过程中，别的线程修改过这个值，但是又改回去了，倒置当前线程比较的时候，发现是相等的，但是，中间是被修改过的； 添加版本号，比较值的时候同时比较版本号 CAS底层原理AtomicInteger:123456789101112public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; Unsafe:1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 运用： 12345678910111213141516171819202122232425262728package com.mashibing.jol;import sun.misc.Unsafe;import java.lang.reflect.Field;public class T02_TestUnsafe &#123; int i = 0; private static T02_TestUnsafe t = new T02_TestUnsafe(); public static void main(String[] args) throws Exception &#123; //Unsafe unsafe = Unsafe.getUnsafe(); Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); Field f = T02_TestUnsafe.class.getDeclaredField(&quot;i&quot;); long offset = unsafe.objectFieldOffset(f); System.out.println(offset); boolean success = unsafe.compareAndSwapInt(t, offset, 0, 1); System.out.println(success); System.out.println(t.i); //unsafe.compareAndSwapInt() &#125;&#125; jdk8u: unsafe.cpp: cmpxchg = compare and exchange 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(&quot;Unsafe_CompareAndSwapInt&quot;); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END jdk8u: atomic_linux_x86.inline.hpp 93行is_MP = Multi Processor 12345678inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) &quot;cmpxchgl %1,(%3)&quot; : &quot;=a&quot; (exchange_value) : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest), &quot;r&quot; (mp) : &quot;cc&quot;, &quot;memory&quot;); return exchange_value;&#125; jdk8u: os.hpp is_MP() 12345678910static inline bool is_MP() &#123; // During bootstrap if _processor_count is not yet initialized // we claim to be MP as that is safest. If any platform has a // stub generator that might be triggered in this phase and for // which being declared MP when in fact not, is a problem - then // the bootstrap routine for the stub generator needs to check // the processor count directly and leave the bootstrap routine // in place until called after initialization has ocurred. return (_processor_count != 1) || AssumeMP;&#125; jdk8u: atomic_linux_x86.inline.hpp 1#define LOCK_IF_MP(mp) &quot;cmp $0, &quot; #mp &quot;; je 1f; lock; 1: &quot; 最终实现：底层对应一个汇编指令「lock comxchg」，但是comxchg这条指令不是原子性的，他不能保证在比较的时候，别的线程会不会改变值；而保证线程安全的则是lock这条指令，lock这条指令在执行后面执行的时候锁定一个「北桥信号」，而不是采用纵线锁的方式； CAS在JDK中的实现1、AtomitInteger2、ConcurrentHashMap","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-FutureTask原理","date":"2021-07-28T09:58:47.000Z","path":"wiki/Java-FutureTask原理/","text":"FutureTaskFuture方法介绍123456789101112131415161718public interface Future&lt;V&gt; &#123; // 取消任务 可中断的方式取消 boolean cancel(boolean mayInterruptIfRunning); // 判断任务是否处于取消状态 boolean isCancelled(); // 判断异步任务是否执行完成 ==这里使用轮训的方式监听== boolean isDone(); // 获取异步线程的执行结果，如果没有执行完成，则一直阻塞到有结果返回； V get() throws InterruptedException, ExecutionException; // 获取异步线程的执行结果，如果没有执行完成，则一直阻塞到设置的时间，有结果返回，没有结果则抛出异常； V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 简单示范Callable&amp;Future（1）向线程池中提交任务的submit方法不是阻塞方法，而Future.get方法是一个阻塞方法（2）submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果，所以一般需要使用future.isDone先判断任务是否全部执行完成，完成后再使用future.get得到结果。（也可以用get (long timeout, TimeUnit unit)方法可以设置超时时间，防止无限时间的等待） 1234567891011121314151617181920212223242526272829303132333435public class FutureTest implements Callable&lt;Integer&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ @Override public Integer call() throws Exception &#123; System.err.println(&quot;start call method...&quot;); Thread.sleep(3000); return 1111; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; System.err.println(&quot;main method start....&quot;); FutureTest futureTest = new FutureTest(); Future1Test future1Test = new Future1Test(); long time = System.currentTimeMillis(); ExecutorService executorService = Executors.newFixedThreadPool(2); Future&lt;String&gt; future = executorService.submit(future1Test); if (!future.isDone()) &#123; System.err.println(&quot;future not done !&quot;); &#125; Future&lt;Integer&gt; future1 = executorService.submit(futureTest); // submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果 // 这里先提交了future1Test，休眠了4s, futureTest休眠了3s，但是等我们get到结果的时候，是消耗的4s时间的； System.err.println(&quot;cost time: &quot; + (System.currentTimeMillis() - time)); System.err.println(&quot;future: &quot; + future.get()); System.err.println(&quot;future1: &quot; + future1.get()); System.err.println(&quot;main method end....&quot;); executorService.shutdown(); &#125;&#125; 执行结果12345678main method start....future not done !// 说明了第一 get()方法是阻塞，第二线程池任务都执行完成之后，按提交任务顺序get结果返回值cost time: 4start call method...future: future 2 testfuture1: 1111main method end.... 注意点 线程池执行任务有两种方式execute和submit，execute是不带返回值的，submit是有返回值的; main方法中可以不使用线程池，可以直接创建线程，调用start方法就可以，切记只有在演示代码的时候后。手动直接创建线程的方式还是不要用，因为一旦请求变多，则会创建无数的线程，线程数大于CPU核数，进而导致CPU频繁切换上下分进行调度，性能严重下降。 而且线程的数据是存放在内存中的，会占用大量的内存，增加垃圾回收的压力。严重的会发生OOM; 异常main方法中我们使用的是Future future接收异步任务执行的放回结果，但实际上Future其实是一个interface，并不能接收返回结果的，那实际我们调用future.get()是，是实例了一个FutureTask对象来接受的； FutureTask讲解下面主要针对Future的实现类FutureTask的几个重要方法展开 FutureTask继承关系1234567891011public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123;...&#125;// 下面是RunnableFuture接口的继承关系public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; /** * Sets this Future to the result of its computation * unless it has been cancelled. */ void run();&#125; FutureTask 重要的成员变量 12345678910111213141516171819202122/** The underlying callable; nulled out after running */private Callable&lt;V&gt; callable;/** The result to return or exception to throw from get() *///任务执行结果或者任务异常private Object outcome; // non-volatile, protected by state reads/writes/** The thread running the callable; CASed during run() *///执行任务的线程private volatile Thread runner;/** Treiber stack of waiting threads *///等待节点，关联等待线程private volatile WaitNode waiters;private static final sun.misc.Unsafe UNSAFE;//state字段的内存偏移量 这个在线程池执行任务的时候进行状态判断的时候会用到private static final long stateOffset;//runner字段的内存偏移量private static final long runnerOffset;//waiters字段的内存偏移量private static final long waitersOffset; 定义任务的生命周期 12345678private volatile int state;private static final int NEW = 0;private static final int COMPLETING = 1;private static final int NORMAL = 2;private static final int EXCEPTIONAL = 3;private static final int CANCELLED = 4;private static final int INTERRUPTING = 5;private static final int INTERRUPTED = 6; NORMAL:指的是任务能够正常执行状态 EXCEPTIONAL：表示任务执行异常 CANCELLED：取消状态，之后的状态都表示任务取消或终端 下面看一下FutureTask中几个重要的方法 执行结果 | report方法 Returns result or throws exception for completed task.主要是上报异步任务执行的结果或返回任务执行发生的异常 1234567891011121314/** * Returns result or throws exception for completed task. * * @param s completed state value */ @SuppressWarnings(&quot;unchecked&quot;) private V report(int s) throws ExecutionException &#123; Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); &#125; 判断逻辑就是根据参数，也是是任务状态，根据不同的状态处理相应的逻辑。比如NORNAL状态，表示任务正常执行，直接返回结果就可以。如果状态大于CANCELLED，说明任务被取消或终端，会抛出CancellationException()；如果不是异常状态，则抛出ExecutionException； 任务执行 | run() 执行异步任务 1234567891011121314151617181920212223242526272829303132333435363738394041public void run() &#123; // 如果状态 state 不是 NEW，或者设置 runner 值失败 // 表示有别的线程在此之前调用 run 方法，并成功设置了 runner 值 // 保证了只有一个线程可以运行 try 代码块中的代码。 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; //以上state值变更的由CAS操作保证原子性 try &#123; Callable&lt;V&gt; c = callable; //只有c不为null且状态state为NEW的情况 if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //调用callable的call方法，并获得返回结果 result = c.call(); //运行成功 ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; setException(ex); &#125; if (ran) //设置结果 set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125; 核心逻辑就是调用Callable的call方法，==result=c.call();== 并且对任务执行的结果或异常信息进行处理； 获取结果 | get() throws InterruptedException, ExecutionException 获取异步任务执行的结果或异常信息 123456public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125; get方法执行两个操作： 判断任务的状态,如果没有执行完成，调用awaitDone方法 任务完成，调用我们上面说的report方法，返回任务执行结果 任务阻塞 | awaitDone(boolean timed, long nanos) 等到任务执行完成 也是get方法阻塞特性的关键所在 123456789101112131415161718192021222324252627282930313233343536373839404142434445private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; // CPU轮转 for (;;) &#123; // 如果线程中断了，将线程移除等待队列，抛出中断异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; // 如果任务状态大于完成，则直接返回； if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; // 如果任务完成，但是返回值outcome还没有设置，可以先让出线程执行权，让其他线程执行 else if (s == COMPLETING) // cannot time out yet Thread.yield(); // 下面是任务还没有执行完成的状态，将线程添加到等待队列 else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // 判断get方法是否设置了超时时间 else if (timed) &#123; nanos = deadline - System.nanoTime(); // 如果超出设置的时间，线程移除等到队列 if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; // 没有设置超时时间，线程直接阻塞，直到任务完成 else LockSupport.park(this); &#125; &#125; 主要执行步骤： 判断线程是否被中断，如果被中断了，就从等待的线程栈中移除该等待节点，然后抛出中断异常 读取state,判断任务是否已经完成，如果已经完成或者任务已经取消，此时调用get方法的线程不会阻塞，会直接获取到结果或者拿到异常信息； 如果s == COMPLETING，说明任务已经结束，但是结果还没有保存到outcome中，==此时线程让出执行权，给其他线程先执行；== 如果任务没有执行完成，则需要创建等待节点，等待插入到阻塞队列 判断queued，这里是将c中创建节点q加入队列头。使用Unsafe的CAS方法，对waiters进行赋值，waiters也是一个WaitNode节点，相当于队列头，或者理解为队列的头指针。通过WaitNode可以遍历整个阻塞队列 然后判断超时时间，时间是在调用get方法的时候传输进来的，如果有超时时间，则设置超时时间，如果超出时间，则将线程移除等待队列；如果没有设置时间，则直接阻塞线程； 取消任务 | cancel(boolean mayInterruptIfRunning)123456789101112131415161718192021222324252627@Param mayInterruptIfRunning 是否中断public boolean cancel(boolean mayInterruptIfRunning) &#123; /* * 在状态还为NEW的时候，根据参数中的是否允许传递， * 将状态流转到INTERRUPTING或者CANCELLED。 */ if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // in case call to interrupt throws exception if (mayInterruptIfRunning) &#123; try &#123; Thread t = runner; if (t != null) t.interrupt(); &#125; finally &#123; // final state UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true; &#125; 123456789101112131415161718192021222324252627282930313233343536private void finishCompletion() &#123; for (WaitNode q; (q = waiters) != null;) &#123; // 必须将栈顶CAS为null，否则重读栈顶并重试。 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; // 遍历并唤醒栈中节点对应的线程。 for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; WaitNode next = q.next; if (next == null) break; // 将next域置为null，这样对GC友好。 q.next = null; q = next; &#125; break; &#125; &#125; /* * done方法是暴露给子类的一个钩子方法。 * * 这个方法在ExecutorCompletionService.QueueingFuture中的override实现是把结果加到阻塞队列里。 * CompletionService谁用谁知道，奥秘全在这。 */ done(); /* * callable置为null主要为了减少内存开销, * 更多可以了解JVM memory footprint相关资料。 */ callable = null;&#125; Callable&amp;Future使用场景 异步任务需要拿到返回值 多线程并发调用，顺序组装返回值，一些并发框架中会看到相应体现 还有一些分布式任务调度的场景，远程调用需要回填执行结果 还有很多通信框架中都有体现 参考资料 (1) future.get方法阻塞问题的解决，实现按照任务完成的先后顺序获取任务的结果(2) Java多线程引发的性能问题以及调优策略(3) 可取消的异步任务——FutureTask用法及解析(4) FutureTask源码解读","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-join方法原理解析","date":"2021-07-28T09:58:22.000Z","path":"wiki/Java-join方法原理解析/","text":"join方法12345join重载方法1 join()2 join(long millis) //参数为毫秒3 join(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 功能演示123456789101112131415public class JoinDemo implements Runnable&#123; public void run() &#123; System.err.println(&quot;join thread demo &quot;); &#125; public static void main(String[] args) throws Exception &#123; System.err.println(&quot;main thread start... &quot;); Runnable r = new JoinDemo(); Thread t = new Thread(r); t.setName(&quot;ibli joinTest ...&quot;); t.start();// t.join(); System.err.println(&quot;main thread end... &quot;); &#125;&#125; 以上将t.join();注释掉，执行的一种可能结果如下： 12345678main thread start... main thread end... join thread demo还有可能是这种结果：main thread start... join thread demomain thread end... 但是把注释去掉，结果如下： 123main thread start... join thread demo main thread end... 这是一个非常简单的demo,效果是显而易见的。当main线程去调用t.join()是，会将自己当前线程阻塞，等到t线程执行完成到达完结状态，main线程才可以继续执行。 我们看一下join()设置超时时间的方法： 123456789101112131415161718192021222324public class JoinDemo implements Runnable&#123; public void run() &#123; System.err.println(&quot;join thread demo &quot;); try &#123; // 线程睡眠4s Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; List&lt;String&gt; strings = null; System.err.println(strings.get(0)); &#125; public static void main(String[] args) throws Exception &#123; System.err.println(&quot;main thread start... &quot;); Runnable r = new JoinDemo(); Thread t = new Thread(r); t.setName(&quot;ibli joinTest ...&quot;); t.start(); // 但是主线程join的超时时间是1s t.join(1000); System.err.println(&quot;main thread end... &quot;); &#125;&#125; 执行效果： 123456main thread start... join thread demo main thread end... Exception in thread &quot;ibli joinTest ...&quot; java.lang.NullPointerException at com.ibli.threadTest.api.JoinDemo.run(JoinDemo.java:14) at java.lang.Thread.run(Thread.java:748) 上面的执行结果可以看到，子线程设置了4s的超时时间，但是主线程在1秒超时后，并没有等待子线程执行完毕，就被唤醒执行后续操作了；这样的预期是否符合你的预期呢？下面我们按照join的源码去分析吧！ join方法原理下面是join的原理图 join()源码 首先会调用join(0)方法，其实是join的重载方法； 123public final void join() throws InterruptedException &#123; join(0);&#125; 下面是join的核心实现： 1234567891011121314151617181920212223242526public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; // 首先校验参数是否合法 if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; // 如果join方法没有参数，则相当于直接调用wait方法 if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 下面是isAlive方法的源码 1public final native boolean isAlive(); 这是一个本地方法，作用是判断当前的线程是否处于活动状态。什么是活动状态呢？活动状态就是线程已经启动且尚未终止。线程处于正在运行或准备开始运行的状态，就认为线程是“存活”的。 这里有一个点要注意，join为什么阻塞的是主线程，而不是子线程呢？ 不理解的原因是阻塞主线程的方法是放在previousThread这个实例作用，让大家误以为应该阻塞previousThread线程。实际上主线程会持有previousThread这个对象的锁，然后调用wait方法去阻塞，而这个方法的调用者是在主线程中的。所以造成主线程阻塞。 其实join()方法的核心在于wait(),在主线程中调用t.join()相当于在main方法中添加 new JoinDemo().wait();是一样的效果；在这里只不过是wait方法写在了子线程的方法中。 再次重申一遍，join方法的作用是在主线程阻塞，等在子线程执行完之后，由子线程唤醒主线程，再继续执行主线程调用t.join()方法之后的逻辑。 那么主线程是在什么情况下知道要继续执行呢？就是上面说的，主线程其实是由join的子线程在执行完成之后调用的notifyAll()方法，来唤醒等待的线程。怎么证明呢？ 其实大家可以去翻看JVM的源码实现，Thread.cpp文件中，有一段代码： 123456void JavaThread::exit(bool destroy_vm, ExitType exit_type) &#123; // Notify waiters on thread object. This has to be done after exit() is called // on the thread (if the thread is the last thread in a daemon ThreadGroup the // group should have the destroyed bit set before waiters are notified). ensure_join(this);&#125; 其中调用ensure_join方法 123456789101112131415161718static void ensure_join(JavaThread* thread) &#123; // We do not need to grap the Threads_lock, since we are operating on ourself. Handle threadObj(thread, thread-&gt;threadObj()); assert(threadObj.not_null(), &quot;java thread object must exist&quot;); ObjectLocker lock(threadObj, thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception(); // Thread is exiting. So set thread_status field in java.lang.Thread class to TERMINATED. java_lang_Thread::set_thread_status(threadObj(), java_lang_Thread::TERMINATED); // Clear the native thread instance - this makes isAlive return false and allows the join() // to complete once we&#x27;ve done the notify_all below //这里是清除native线程，这个操作会导致isAlive()方法返回false java_lang_Thread::set_thread(threadObj(), NULL); // 在这里唤醒等待的线程 lock.notify_all(thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception();&#125; 在JVM的代码中，线程执行结束的最终调用了lock.notify_all(thread)方法来唤醒所有处于等到的线程 使用场景 比如我们使用Callable执行异步任务，需要在主线程处理任务的返回值时，可以调用join方法； 还有一些场景希望线程之间顺序执行的； join()方法与sleep()的比较我们先说一下sleep方法： 让当前线程休眠指定时间。 休眠时间的准确性依赖于系统时钟和CPU调度机制。 不释放已获取的锁资源，如果sleep方法在同步上下文中调用，那么其他线程是无法进- 入到当前同步块或者同步方法中的。 可通过调用interrupt()方法来唤醒休眠线程。 sleep是静态方法，可以在任何地方调用 相比与sleep方法sleep是静态方法，而且sleep的线程不是放锁资源，而join方法是对象方法，并且在等待的过程中会释放掉对象锁； 关于join方法会释放对象锁，那到底是释放的那个对象的锁呢，可以参照 关于join() 是否会释放锁的一些思考 参考资料 1、Java多线程中join方法的理解2、Thread.join的作用和原理3、Thread.join的作用和原理 d 山脚太拥挤 我们更高处见。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java内存模型","date":"2021-07-28T09:57:51.000Z","path":"wiki/Java内存模型/","text":"Java内存模型 什么是JMM?Java Memory Model简称JMM, 是一系列的Java虚拟机平台对开发者提供的多线程环境下的内存可见性、是否可以重排序等问题的无关具体平台的统一的保证。(可能在术语上与Java运行时内存分布有歧义，后者指堆、方法区、线程栈等内存区域)。 JMM规范的内容 所有变量存储在主内存 主内存是虚拟机内存的一部分 每条线程有自己的工作内存 线程的工作内存保存变量的主内存副本 线程对变量的操作必须在工作内存中进行 不同线程之间无法直接访问对方工作内存中的变量 线程间变量值的传递均需要通过主内存来完成 JMM并不是一个客观存在的东西，它实际是为了规范Java虚拟机制定到一套标准。那为什么需要这套标准呢？ 其实我们都知道JVM是运行在操作系统之上的。而目前的操作系统都是基于冯诺伊曼设置的计算机系统体系来的。CPU是计算机中用来执行控制和计算的核心组件。 所有的计算任务全部在CPU中完成，但是我们的所有变量的数据全部存储在主内存中。CPU在执行计算时，需要去主内存加载数据，CPU执行运算的速度极快，这就设计一个CPU执行速度和数据加载速度不一致的问题。 在操作系统级别解决这个问题的办法是引入了CPU缓存。每个CPU都有自己私有的L1缓存和L2缓存，当执行计算时，会优先去CPU自己的缓存中寻找数据，没有的话才会重新加载内存数据。这种方式一定程度上解决了CPU计算和数据加载不一致的问题。 但是也会引入一个新的问题，就是数据一致性问题。 缓存一致性与MESI协议 首先看一下什么是MESI协议 缓存一致性协议给缓存行（通常为64字节）定义了个状态：独占（exclusive）、共享（share）、修改（modified）、失效（invalid），用来描述该缓存行是否被多处理器共享、是否修改。所以缓存一致性协议也称MESI协议。 独占（exclusive）：仅当前处理器拥有该缓存行，并且没有修改过，是最新的值。 共享（share）：有多个处理器拥有该缓存行，每个处理器都没有修改过缓存，是最新的值。 修改（modified）：仅当前处理器拥有该缓存行，并且缓存行被修改过了，一定时间内会写回主存，会写成功状态会变为S。 失效（invalid）：缓存行被其他处理器修改过，该值不是最新的值，需要读取主存上最新的值。 如何解决缓存一致性问题呢？ 如上图所示，共享变量是存储在主内存Memory中，在CPU计算时，每一个CPU都有改变量的独立拷贝，每个CPU可以去读取甚至修改共享变量的值，但是为了保证数据的一致性，一个CPU modify了变量的值，需要通知其他的CPU这个变量的最新值是什么。那么可以怎么做呢。 1、在初始状态，每个CPU还没有加载共享变量，所有每一个CPU的缓存行的状态都是invalid； 2、当CPU0去使用这个共享变量的时候，首先去自己的缓存中查找，肯定是缓存不命中的，也就是cache miss,这个时候去主内存Memory中去加载，当共享变量的值加载到CPU0的缓存后，CPU缓存行状态变成shared, 也就是共享状态； 3、如果这个时候有其他的CPU也读取了共享变量的值，它们的cache line 的状态同样也是shared共享状态；此时一个CPU如果修改共享变量的值，而没有通知其他的CPU,就会造成缓存一致性问题； 4、当CPU0尝试去修改共享变量的值时，它会发出一个read invalidate命令，同时CPU0的缓存行状态设置为exclusive(独占),同时将其他加载了这个共享变量的cacheline的状态设置为invalid。通俗一点就是CPU0独占的这个变量的缓存行，其他的CPU缓存的共享变量都失效了； 5、CPU0接下来修改共享变量的值，它会将cacheline的状态修改为modified,其实也是独占共享变量的cacheline，只不过是此时缓存行的数据和主内存Memory的数据不一致的，而exclusive虽然也是独占状态，但是共享变量的值是一样的，modified的值需要write back到Memory中去的，而exclusive是不需要的； 6、在cacheline没有替换出CPU0的cache之前，当有其他CPU来读取共享变量，此时肯定是cache miss ,因为CPU0的modify操作已经将它的缓存失效了。如果CPU0的状态是modified状态，它必须响应其他CPU的读操作，会告知其他CPU主内存的数据是dirty data。所以其他的CPU的状态可能会变成shared。如果CPU0还没有write back操作，其他的CPU状态还是invalid状态。 Store Buffer正如上面所描述的，在CPU0进行共享变量的修改，会同步修改其他CPU的cacheline状态为invalid，这个操作是和共享变量的写操作同步进行的，因此共享变量的写操作的性能是非常差的。在修改其他的CPU cacheline状态时，CPU0其实是处于阻塞状态的。所以为了优化这个问题，提出了Store Buffer的解决方案。 这样的话，写操作不必等到cacheline被加载，而是直接写到store buffer中，然后去执行后续的操作。由于是store buffer相当于是异步处理，在这里可能会出现因为并发执行导致的执行执行交叉问题，具体解决方法是依赖于内存屏障。 具体可以参考这篇文章：Linux内核同步机制之（三）：memory barrier Invalidate Queue处理失效的缓存也不是简单的，需要读取主存。并且存储缓存也不是无限大的，那么当存储缓存满的时候，处理器还是要等待失效响应的。为了解决上面两个问题，引进了失效队列（invalidate queue）。 处理失效的工作如下： 收到失效消息时，放到失效队列中去。 为了不让处理器久等失效响应，收到失效消息需要马上回复失效响应。 为了不频繁阻塞处理器，不会马上读主存以及设置缓存为invlid，合适的时候再一块处理失效队列。 happens- before原则 虽然指令重排提高了并发的性能，但是Java虚拟机会对指令重排做出一些规则限制，并不能让所有的指令都随意的改变执行位置，主要有以下几点： 1、单线程每个操作，happen-before于该线程中任意后续操作；2、volatile写happen-before与后续对这个变量的读；3、synchronized解锁happen-before后续对这个锁的加锁；4、final变量的写happen-before于final域对象的读，happen-before后续对final变量的读；5、传递性规则，A先于B，B先于C，那么A一定先于C发生； https://www.processon.com/view/5c8b0978e4b0c996d363dcbc?fromnew=1 深入理解Java内存模型-3y","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-synchronzied底层原理","date":"2021-07-28T09:57:19.000Z","path":"wiki/Java-synchronzied底层原理/","text":"synchronzied底层原理synchronzied四个层级实现 Java代码 通过添加synchronzied给对象或者方法或者代码块 字节码层级通过一组 MONITORENTER/MONITOREXIT指令 JVM层级：锁升级过程 汇编执行通过 lock comxchg指令保证原子操作 JDK早期，synchronized 叫做重量级锁， 因为申请锁资源必须通过kernel, 系统调用 1234567891011121314151617181920;hello.asm;write(int fd, const void *buffer, size_t nbytes)section data msg db &quot;Hello&quot;, 0xA len equ $ - msgsection .textglobal _start_start: mov edx, len mov ecx, msg mov ebx, 1 ;文件描述符1 std_out mov eax, 4 ;write函数系统调用号 4 int 0x80 mov ebx, 0 mov eax, 1 ;exit函数系统调用号 int 0x80 优化后的synchronized如下👇： Java层级12345678public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable()); synchronized (object)&#123; System.out.println(ClassLayout.parseInstance(object).toPrintable()); &#125; &#125; 字节码层级12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// access flags 0x9 public static main([Ljava/lang/String;)V // parameter args TRYCATCHBLOCK L0 L1 L2 null TRYCATCHBLOCK L2 L3 L2 null L4 LINENUMBER 13 L4 NEW java/lang/Object DUP INVOKESPECIAL java/lang/Object.&lt;init&gt; ()V ASTORE 1 L5 LINENUMBER 14 L5 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 1 INVOKESTATIC org/openjdk/jol/info/ClassLayout.parseInstance (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout; INVOKEVIRTUAL org/openjdk/jol/info/ClassLayout.toPrintable ()Ljava/lang/String; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L6 LINENUMBER 16 L6 ALOAD 1 DUP ASTORE 2 MONITORENTER L0 LINENUMBER 17 L0 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 1 INVOKESTATIC org/openjdk/jol/info/ClassLayout.parseInstance (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout; INVOKEVIRTUAL org/openjdk/jol/info/ClassLayout.toPrintable ()Ljava/lang/String; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L7 LINENUMBER 18 L7 ALOAD 2 MONITOREXIT L1 GOTO L8 L2 FRAME FULL [[Ljava/lang/String; java/lang/Object java/lang/Object] [java/lang/Throwable] ASTORE 3 ALOAD 2 MONITOREXIT L3 ALOAD 3 ATHROW L8 LINENUMBER 19 L8 FRAME CHOP 1 RETURN L9 LOCALVARIABLE args [Ljava/lang/String; L4 L9 0 LOCALVARIABLE object Ljava/lang/Object; L5 L9 1 MAXSTACK = 2 MAXLOCALS = 4&#125; 主要通过MONITORENTER 和 MONITOREXIT 两个字节码指令控制加锁过程 JVM层级通过锁升级过程实现加锁；无锁 -&gt; 偏向锁 -&gt; 自旋锁（轻量级锁 自适应锁）-&gt; 重量级锁锁升级过程可以查看 锁升级过程 复制理解 汇编指令级别linux操作系统安装hsdis插件，查看java代码的汇编指令： 1234567891011121314public class T &#123; static volatile int i = 0; public static void n() &#123; i++; &#125; public static synchronized void m() &#123;&#125; publics static void main(String[] args) &#123; for(int j=0; j&lt;1000_000; j++) &#123; m(); n(); &#125; &#125;&#125; 执行以下命令： 1java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly T C1 Compile Level 1 (一级优化) C2 Compile Level 2 (二级优化) 找到m() n()方法的汇编码，会看到 lock comxchg …..指令","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-锁消除和锁膨胀","date":"2021-07-28T09:57:00.000Z","path":"wiki/Java-锁消除和锁膨胀/","text":"锁消除和锁粗化锁消除 （lock eliminate）1234public void add(String str1,String str2)&#123; StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2);&#125; 我们都知道 StringBuffer 是线程安全的，因为它的关键方法都是被 synchronized 修饰过的，但我们看上面这段代码，我们会发现，sb 这个引用只会在 add 方法中使用，不可能被其它线程引用（因为是局部变量，栈私有），因此 sb 是不可能共享的资源，JVM 会自动消除 StringBuffer 对象内部的锁。 锁粗化 （lock coarsening）123456789public String test(String str)&#123; int i = 0; StringBuffer sb = new StringBuffer(): while(i &lt; 100)&#123; sb.append(str); i++; &#125; return sb.toString():&#125; JVM 会检测到这样一连串的操作都对同一个对象加锁（while 循环内 100 次执行 append，没有锁粗化的就要进行 100 次加锁/解锁），此时 JVM 就会将加锁的范围粗化到这一连串的操作的外部（比如 while 虚幻体外），使得这一连串操作只需要加一次锁即可。 锁降级https://www.zhihu.com/question/63859501 其实，只被VMThread访问，降级也就没啥意义了。所以可以简单认为锁降级不存在！","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-锁升级过程","date":"2021-07-28T09:56:39.000Z","path":"wiki/Java-锁升级过程/","text":"锁升级使用JOL查看加锁之后的对象信息12345678public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable()); synchronized (object)&#123; System.out.println(ClassLayout.parseInstance(object).toPrintable()); &#125; &#125; 查看打印结果： 1234567891011121314151617java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totaljava.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) f0 b8 d0 0f (11110000 10111000 11010000 00001111) (265337072) 4 4 (object header) 00 70 00 00 (00000000 01110000 00000000 00000000) (28672) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 锁升级过程 锁升级过程： new - 偏向锁 - 轻量级锁 （无锁, 自旋锁，自适应自旋）- 重量级锁 自旋锁什么时候升级为重量级锁？ 超过自选次数仍没有获得锁 为什么有自旋锁还需要重量级锁？ 自旋是消耗CPU资源的，如果锁的时间长，或者自旋线程多，CPU会被大量消耗重量级锁有等待队列，所有拿不到锁的进入等待队列，不需要消耗CPU资源 偏向锁是否一定比自旋锁效率高？ 不一定，在明确知道会有多线程竞争的情况下，偏向锁肯定会涉及锁撤销，这时候直接使用自旋锁JVM启动过程，会有很多线程竞争（明确），所以默认情况启动时不打开偏向锁，过一段儿时间再打开 synchronized优化的过程和markword息息相关用markword中最低的三位代表锁状态 其中1位是偏向锁位 两位是普通锁位 Object o = new Object()锁 = 0 01 无锁态注意：如果偏向锁打开，默认是匿名偏向状态 o.hashCode()001 + hashcode 1200000001 10101101 00110100 0011011001011001 00000000 00000000 00000000 little endian big endian 00000000 00000000 00000000 01011001 00110110 00110100 10101101 00000000 默认synchronized(o)00 -&gt; 轻量级锁默认情况 偏向锁有个时延，默认是4秒why? 因为JVM虚拟机自己有一些默认启动的线程，里面有好多sync代码，这些sync代码启动时就知道肯定会有竞争，如果使用偏向锁，就会造成偏向锁不断的进行锁撤销和锁升级的操作，效率较低。 1-XX:BiasedLockingStartupDelay=0 如果设定上述参数new Object () - &gt; 101 偏向锁 -&gt;线程ID为0 -&gt; Anonymous BiasedLock打开偏向锁，new出来的对象，默认就是一个可偏向匿名对象101 如果有线程上锁上偏向锁，指的就是，把markword的线程ID改为自己线程ID的过程偏向锁不可重偏向 批量偏向 批量撤销 如果有线程竞争撤销偏向锁，升级轻量级锁线程在自己的线程栈生成LockRecord ，用CAS操作将markword设置为指向自己这个线程的LR的指针，设置成功者得到锁 如果竞争加剧竞争加剧：有线程超过10次自旋， -XX:PreBlockSpin， 或者自旋线程数超过CPU核数的一半， 1.6之后，加入自适应自旋 Adapative Self Spinning ， JVM自己控制升级重量级锁：-&gt; 向操作系统申请资源，linux mutex , CPU从3级-0级系统调用，线程挂起，进入等待队列，等待操作系统的调度，然后再映射回用户空间 (以上实验环境是JDK11，打开就是偏向锁，而JDK8默认对象头是无锁)偏向锁默认是打开的，但是有一个时延，如果要观察到偏向锁，应该设定参数 如果计算过对象的hashCode，则对象无法进入偏向状态！ 轻量级锁重量级锁的hashCode存在与什么地方？答案：线程栈中，轻量级锁的LR中，或是代表重量级锁的ObjectMonitor的成员中 关于epoch: (不重要) 批量重偏向与批量撤销渊源：从偏向锁的加锁解锁过程中可看出，当只有一个线程反复进入同步块时，偏向锁带来的性能开销基本可以忽略，但是当有其他线程尝试获得锁时，就需要等到safe point时，再将偏向锁撤销为无锁状态或升级为轻量级，会消耗一定的性能，所以在多线程竞争频繁的情况下，偏向锁不仅不能提高性能，还会导致性能下降。于是，就有了批量重偏向与批量撤销的机制。 原理以class为单位，为每个class维护解决场景批量重偏向（bulk rebias）机制是为了解决：一个线程创建了大量对象并执行了初始的同步操作，后来另一个线程也来将这些对象作为锁对象进行操作，这样会导致大量的偏向锁撤销操作。批量撤销（bulk revoke）机制是为了解决：在明显多线程竞争剧烈的场景下使用偏向锁是不合适的。 一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向。每个class对象会有一个对应的epoch字段，每个处于偏向锁状态对象的Mark Word中也有该字段，其初始值为创建该对象时class中的epoch的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其epoch字段改为新值。下次获得锁时，发现当前对象的epoch值和class的epoch不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其Mark Word的Thread Id 改成当前线程Id。当达到重偏向阈值后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑。 没错，我就是厕所所长 加锁，指的是锁定对象 锁升级的过程 JDK较早的版本 OS的资源 互斥量 用户态 -&gt; 内核态的转换 重量级 效率比较低 现代版本进行了优化 无锁 - 偏向锁 -轻量级锁（自旋锁）-重量级锁 1、偏向锁 - markword 上记录当前线程指针，下次同一个线程加锁的时候，不需要争用，只需要判断线程指针是否同一个，所以，偏向锁，偏向加锁的第一个线程 。hashCode备份在线程栈上 线程销毁，锁降级为无锁 2、有争用 - 锁升级为轻量级锁 - 每个线程有自己的LockRecord在自己的线程栈上，用CAS去争用markword的LR的指针，指针指向哪个线程的LR，哪个线程就拥有锁 3、自旋超过10次，升级为重量级锁 - 如果太多线程自旋 CPU消耗过大，不如升级为重量级锁，进入等待队列（不消耗CPU）-XX:PreBlockSpin 自旋锁在 JDK1.4.2 中引入，使用 -XX:+UseSpinning 来开启。JDK 6 中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。 4、自适应自旋锁意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 5、偏向锁由于有锁撤销的过程revoke，会消耗系统资源，所以，在锁争用特别激烈的时候，用偏向锁未必效率高。还不如直接使用轻量级锁。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"JVM-strace","date":"2021-07-28T09:52:23.000Z","path":"wiki/JVM-strace/","text":"strace 命令查看操作系统日志strace -ff -o out java ***.class -ff : 跟踪进程下所有线程用到的系统命令-o : 将跟踪的操作系统日志输出 下面查看JDK1.8下，BIO模式都有哪些系统命令的执行 123456789101112131415161718192021222324252627282930313233package com.ibli.javaBase.io.bio;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.net.ServerSocket;import java.net.Socket;/** * @Author gaolei * @Date 2021/4/3 2:55 下午 * @Version 1.0 */public class SockerIo &#123; public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(9090); // 阻塞 Socket client = serverSocket.accept(); InputStream inputStream = client.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 读阻塞 System.err.println(bufferedReader.readLine()); while (true)&#123; &#125; &#125;&#125; 服务端 1、javac SockerIo.java 得到SockerIo.class然后，使用strace启动java程序👇：2、strace -ff -0 out java SockerIo得到如下日志： 客户端使用nc连接9090端口，然后请求数据 nc 127.0.0.1 9090 发送如下数据 strace查看日志 查看主线程日志：如上图，👆文件最大的是主线程日志： 根据上面👆strace命令跟踪的日志可以看到，JDK1.8下的BIO的多路复用器是使用的「poll」","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"Java-NIO核心组件--channel","date":"2021-07-28T09:51:42.000Z","path":"wiki/Java-NIO核心组件-channel/","text":"NIO核心组件 - ChannelSocketChannel 和 ServerSocketChannel学习此部分可以对比Socket和ServerSocket 服务端代码 123456789101112131415161718192021222324252627282930313233343536373839public class NioSocketServer01 &#123; public static void main(String[] args) &#123; try &#123; // ServerSocketChannel 支持阻塞/非阻塞 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 设置成非阻塞。默认阻塞true serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(8080)); // 循环监听客户端连接 while (true) &#123; // 如果有客户端连接，则返回一个socketChannel实例，否则socketChannel=null SocketChannel socketChannel = serverSocketChannel.accept(); // 代码执行到此处，说明有客户端链接 if (socketChannel != null) &#123; // 读取客户端发送的数据，并输出 ByteBuffer buffer = ByteBuffer.allocate(1024); socketChannel.read(buffer); System.err.println(new String(buffer.array())); // 将数据在写会客户端 buffer.flip(); socketChannel.write(buffer); //验证客户端 socketChannel设置成false时，从服务端read数据的操作变成非阻塞的 //ByteBuffer buffer = ByteBuffer.allocate(1024); //buffer.put(&quot;this is server!&quot;); //buffer.flip(); //socketChannel.write(buffer); &#125; else &#123; Thread.sleep(1000L); System.err.println(&quot;no client&quot;); &#125; &#125; &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 客户端代码 12345678910111213141516171819202122232425262728293031public class NioSocketClient1 &#123; public static void main(String[] args) &#123; try &#123; SocketChannel socketChannel = SocketChannel.open(); // 默认阻塞IO true socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(&quot;localhost&quot;, 8080)); // finishConnect的主要作用就是确认通道连接已建立，方便后续IO操作（读写）不会因连接没建立而导致NotYetConnectedException异常。 if (socketChannel.isConnectionPending()) &#123; // finishConnect一直阻塞到connect建立完成 socketChannel.finishConnect(); &#125; ByteBuffer byteBuffer = ByteBuffer.allocate(1024); byteBuffer.put(&quot;hello world&quot;.getBytes()); byteBuffer.flip(); socketChannel.write(byteBuffer); byteBuffer.clear(); int r = socketChannel.read(byteBuffer); // 非阻塞方法 byteBuffer的数据还是上面put的 if (r &gt; 0) &#123; System.out.println(&quot;get msg:&#123;&#125;&quot; + new String(byteBuffer.array())); &#125; else &#123; System.out.println(&quot;server no back&quot;); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO核心组件--selector","date":"2021-07-28T09:51:30.000Z","path":"wiki/Java-NIO核心组件-selector/","text":"多路复用器select1、select选择器会告诉客户端哪些连接有数据要读取，但是读取的操作还是用户自己触发的，这种叫做「同步」 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.ibli.javaBase.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.*;import java.util.Iterator;import java.util.Set;/** * @Author gaolei * @Date 2021/4/3 4:09 下午 * @Version 1.0 */public class SelectMultiple &#123; private ServerSocketChannel server = null; private Selector selector = null; int port = 9090; public void initServer() throws IOException &#123; server = ServerSocketChannel.open(); server.configureBlocking(false); server.bind(new InetSocketAddress(port)); server.register(selector, SelectionKey.OP_ACCEPT); &#125; public void start() throws IOException &#123; initServer(); System.err.println(&quot;server started ....&quot;); while (true) &#123; // selector.select() 调用系统内核的select while (selector.select() &gt; 0) &#123; // 从多路复用器中选择有效的key Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectionKeys.iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); if (key.isAcceptable()) &#123; acceptHandle(key); &#125; else if (key.isReadable()) &#123; readHandle(key); &#125; &#125; &#125; &#125; &#125; public void acceptHandle(SelectionKey key) throws IOException &#123; ServerSocketChannel ssc = (ServerSocketChannel) key.channel(); SocketChannel client = ssc.accept(); client.configureBlocking(false); ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1024); client.register(selector, SelectionKey.OP_READ, byteBuffer); System.err.println(&quot;client arrived &quot; + client.getRemoteAddress()); &#125; public void readHandle(SelectionKey key) throws IOException &#123; SocketChannel client = (SocketChannel) key.channel(); ByteBuffer buffer = (ByteBuffer) key.attachment(); buffer.clear(); int read = 0; while (true) &#123; read = client.read(buffer); if (read &gt; 0) &#123; // 服务端读到的数据，再写一遍给到客户端 buffer.flip(); while (buffer.hasRemaining()) &#123; client.write(buffer); &#125; buffer.clear(); &#125; else if (read == 0) &#123; break; &#125; else &#123; // client 发生错误 或者断开 read == -1 // 导致空转 最终CPU达到100% client.close(); break; &#125; &#125; &#125;&#125; 上面的写法是一个selector既担任boss又担任worker","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO核心组件--buffer","date":"2021-07-28T09:51:10.000Z","path":"wiki/Java-NIO核心组件-buffer/","text":"Buffer 读写NIO之BufferBuffer作为NIO三大核心组件之一，本质上是一块可以写入数据，以及从中读取数据的内存，实际上也是一个byte[]数据,只是在NIO中被封装成了NIO Buffer对象并提供了一组方法来访问这个内存块。 下面是一个简单的Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// 读取一个text.txt文件，生成一个新的text1.txt文件public class FirstNioDemo &#123; public static void main(String[] args) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(&quot;/Users/gaolei/Desktop/text.txt&quot;); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/text1.txt&quot;); FileChannel inChannel = fileInputStream.getChannel(); FileChannel outChannel = fileOutputStream.getChannel(); // 声明缓冲区大小为1024字节 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // 从通道中读取数据 inChannel.read(byteBuffer); // 读模式切换为写模式 byteBuffer.flip(); //把缓冲区的数据写到通道 outChannel.write(byteBuffer); // 数据写完之后清空全部缓冲区 byteBuffer.clear(); //关闭文件流 fileInputStream.close(); fileOutputStream.close(); &#125;&#125;``` &gt; 执行结果：生成/Users/gaolei/Desktop/text1.txt文件 **Buffer进行数据读写操作的一般步骤** 1、写入数据到Buffer 2、调用flip()方法 3、从Buffer中读取数据 4、调用clear()方法或者compact()方法 &gt; clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 ### Buffer三个核心的属性 - capacity 容量 与buffer处在什么模式无关- position 游标位置 指向下一个存放/读取数据的位置 范围（0 ～ capacity–1）- limit ### 读写操作中Buffer三大属性的变化初始状态 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71f90dfd671f80eb9f6142f135b7c2dfc92.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 第一次读取数据 position处于起始位置，limit和capacity都处于结尾 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-41b47d9e54d58c7b39caf9e514fc9b5261f.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 第二次读取数据 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-07f3d1aa1f886b592b386cd4d846810911d.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 当写数据的时候，需要调用flip方法： 当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position） &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b9323701bbb34a6c12f61d5ac2652ab7eeb.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; Clear方法 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71f90dfd671f80eb9f6142f135b7c2dfc92.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; ### JAVA NIO下的Buffer分类- ByteBuffer- MappedByteBuffer- CharBuffer- DoubleBuffer- FloatBuffer- IntBuffer- LongBuffer- ShortBuffer&gt; Java基本类型除了布尔类型，都有其对应的Buffer ### ByteBuffer使用&gt; 下面以ByteBuffer为例子看一下Buffer如何使用```java// 创建一个byteBuffer，设置容量为1024字节ByteBuffer byteBuffer = ByteBuffer.allocate(1024); 1、如下代码，其实调用了new HeapByteBuffer(capacity, capacity)来创建一个buffer 12345public static ByteBuffer allocate(int capacity) &#123; if (capacity &lt; 0) throw new IllegalArgumentException(); return new HeapByteBuffer(capacity, capacity); &#125; 2、创建了buffer之后要往里面写数据，除了上面从channel中读取数据之外，还可以调用put方法,如下 12ByteBuffer byteBuffer = ByteBuffer.allocate(1024);byteBuffer.put(&quot;hello world&quot;.getBytes()); 3、如果写将buffer中的数据写出去，必须先调用flap方法 flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。 4、将数据写到通道中 inChannel.write(buf); 5、数据写出到通道之后，要将缓存清空，一般调用clear方法clear方法 12345678public final Buffer clear() &#123; //position将被设回0 position = 0; //limit被设置成 capacity的值 limit = capacity; mark = -1; return this; &#125; Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。compact方法如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。 1234567891011public ByteBuffer compact() &#123; //compact()方法将所有未读的数据拷贝到Buffer起始处。 System.arraycopy(hb, ix(position()), hb, ix(0), remaining()); //position设到最后一个未读元素正后面 position(remaining()); //limit属性设置成capacity limit(capacity()); discardMark(); return this; &#125;现在Buffer准备好写数据了，但是不会覆盖未读的数据 零拷贝原理– 零拷贝，第一次接触零拷贝是在kafka的数据存储部分–IO流程：内存映射缓冲区比普通IO操作文件快很多，甚至比channel还要快很多。因为避免了很多系统调用（System.read System.write）。减少了内核缓冲区的数据拷贝到用户缓冲区。 举个栗子： 123456789101112public static void main(String[] args) throws IOException &#123; FileChannel in = FileChannel.open(Paths.get(&quot;/Users/gaolei/Desktop/text.txt&quot;), StandardOpenOption.READ); FileChannel out = FileChannel.open(Paths.get(&quot;/Users/gaolei/Desktop/text1.txt&quot;), StandardOpenOption.READ, StandardOpenOption.CREATE, StandardOpenOption.WRITE); MappedByteBuffer inBuffer = in.map(FileChannel.MapMode.READ_ONLY, 0, in.size()); MappedByteBuffer outBuffer = out.map(FileChannel.MapMode.READ_WRITE, 0, in.size()); byte[] bytes = new byte[inBuffer.limit()]; inBuffer.get(bytes); outBuffer.put(bytes); in.close(); out.close(); &#125; 普通的网络IO拷贝流程1、首先系统从磁盘上拷贝文件到内核空间缓冲区2、然后在内核空间拷贝数据到用户空间3、第三次，用户缓冲区再将数据拷贝到内核部分的socket缓冲4、内核在将存储在socket缓冲区的数据拷贝并发送到网卡缓冲区以上一个常规的网络IO经历了4次数据拷贝； 设置缓冲区的意义在于提升性能，当用户空间仅仅需要一小部分数据的时候，操作系统会在磁盘上读取一块数据方法内核缓冲区，这个叫做局部性原理。 零拷贝减去了内核空间数据到用户空间数据的拷贝，从而提升IO性能。假设读取的文件很大，操作系统需要读取磁盘大量数据到内核空间，这时候内核缓冲区的作用是很难体现的。因为如果用户空间需要少量数据的时候是可以直接在内核空间获取的（局部性原理）。正式因为有了零拷贝，操作系统在磁盘读取数据之后，可以直接发送到网卡缓冲区，从而大大提升IO性能。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO","date":"2021-07-28T09:50:38.000Z","path":"wiki/Java-NIO/","text":"Java NIO Java NIO 对于Java BIO的优化 Java 非阻塞IO 及时不使用线程池，也可以处理多个客户端请求 12345678910111213141516171819202122232425262728293031323334353637383940public static void main(String[] args) throws IOException, InterruptedException &#123; LinkedList&lt;SocketChannel&gt; clients = new LinkedList&lt;&gt;(); ServerSocketChannel ss = ServerSocketChannel.open(); ss.bind(new InetSocketAddress(9090)); ss.configureBlocking(false); while (true) &#123; Thread.sleep(1000L); // 非阻塞 SocketChannel client = ss.accept(); if (client == null) &#123; System.err.println(&quot;client is null&quot;); &#125; else &#123; client.configureBlocking(false); int port = client.socket().getPort(); System.err.println(&quot;client port &quot; + port); clients.add(client); &#125; ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4096); // 串型话 // 真实场景下 每一个client一个独自的buffer for (SocketChannel c : clients) &#123; // -1 出现空轮训 int num = c.read(byteBuffer); if (num &gt; 0) &#123; byteBuffer.flip(); byte[] aaa = new byte[byteBuffer.limit()]; byteBuffer.get(aaa); String b = new String(aaa); System.err.println(c.socket().getPort() + &quot; : &quot; + b); // 清空 循环下一次client在使用 byteBuffer.clear(); &#125; &#125; &#125;&#125; 以上可以实现，一个线程可以处理多个客户端链接，服务端非阻塞接收，接收之后，读取数据也是非阻塞的； NIO的非阻塞是操作系统内部实现的，底层调用了linux内核的accept函数 d Java的NIO有什么弊端 服务端还是会进行空转 不管有没有客户端连接建立，服务端都要不断执行accept方法 不管客户端连接有没有传输数据，都会执行一遍read操作 资源浪费问题 还是会存在C10k的问题","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-传统的BIO","date":"2021-07-28T09:50:21.000Z","path":"wiki/Java-传统的BIO/","text":"传统的BIOSocket 和 ServerSocket1234567891011121314151617public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(9090); // 阻塞 Socket client = serverSocket.accept(); InputStream inputStream = client.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 读阻塞 System.err.println(bufferedReader.readLine()); while (true)&#123; &#125; &#125; new ServerSocket(9090); 这个java程序创建new ServerSocket(9090);会调用操作系统内核，也就是系统调用，比如linux操作系统，应用进程也就是我们的java进程，会调用linux的内核方法，创建一个socket，在linux系统中就是一个文件描述符fd，最终对得到： 123socket() = XXfdbind(XXfd,9090)listen(XXfd) socket 的read方法 ，读取客户端发送的数据，如果没有，则一直阻塞 serverSocket的accept方法，等待客户端的链接，如果没有链接，则一直阻塞等待 serverSocket 一次只能处理一个客户端请求 BIO程序有哪些弊端？ 服务端一次处理一个请求，并发非常低 没有客户端请求，服务端一直阻塞，占用资源 如果在bio的基础上，利用多线程处理客户端请求？ d C10K问题 来一个链接，服务端创建一个线程 ，去处理请求，服务端继续监听客户端，是不是可以增加并发？有什么问题？ 线程消耗内存资源 如果一下子过来10万个请求呢？服务器要创建10万个线程，内存就崩了。 如果搞一个线程池呢？ 并发度最大为最大线程数？ 并发度已经定死了？","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"JVM-jstat","date":"2021-07-28T09:46:25.000Z","path":"wiki/JVM-jstat/","text":"jstatjstat是一个简单的实用工具，在JDK中存在，用于提供与JVM性能相关的统计信息，例如垃圾收集，编译活动。 jstat的主要优势在于，它可以在运行JVM且无需任何先决条件的情况下动态捕获这些指标。 这是什么意思？ 例如，如果要捕获与垃圾回收相关的统计信息，则需要在启动JVM之前传递以下参数： -Xlog:gc*:file={file-path} 此参数将启用GC日志并将其打印在指定的文件路径中。 假设您尚未传递此参数，那么将不会生成与GC相关的统计信息。 这是jstat可以派上用场的地方。 您可以动态地连接到JVM并捕获GC，编译相关的统计信息如下所示。 jstat操作执行命令： 1jstat -gc -t 11656 10000 30 -gc ：将显示与垃圾收集相关的统计信息 自JVM启动以来的-t时间戳将被打印 11656：目标JVM进程ID 10000：每10,000毫秒（即10秒）将打印一次统计信息。 30 ：将打印30次迭代的统计信息。 因此，以上选项将导致JVM打印指标300秒（即10秒x 30次迭代）。 （请注意，除了-gc之外，您还可以传递其他各种选项来生成不同的数据集。有关不同选项的更多详细信息，请参见此处 。）打印结果： 1234Timestamp S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 34486.1 1536.0 1536.0 0.0 878.8 226816.0 132809.2 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 34496.3 1536.0 1536.0 0.0 878.8 226816.0 138030.9 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 34506.3 1536.0 1536.0 0.0 878.8 226816.0 195648.1 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 字段解读S0C –幸存者0区域的容量，以KB为单位 S1C –幸存者1区域的容量，以KB为单位 S0U –幸存者0区域使用的空间以KB为单位 S1U –幸存者1区域以KB为单位使用空间 EC –伊甸园地区容量（KB） 欧盟–伊甸园地区的已利用空间（以KB为单位） OC –旧区域容量（KB） OU –旧区域的已利用空间，以KB为单位 MC –元空间区域容量，以KB为单位 MU –元空间区域使用的空间以KB为单位 CCSC –压缩类空间区域的容量，以KB为单位 CCSU –压缩类空间区域以KB为单位使用空间 YGC –迄今为止发生的年轻GC事件的数量 YGCT –到目前为止，年轻GC花费的时间 FGC –迄今为止已发生的完全GC事件的数量 FGCT –到目前为止已花费的完整GC时间 GCT –到目前为止所花费的GC时间总量（基本上是YGCT + FGCT） 参考资料jstat分析_jstat –分析","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-jstack","date":"2021-07-28T09:46:12.000Z","path":"wiki/JVM-jstack/","text":"jstackjstack 功能主要分为两个功能： a． 针对活着的进程做本地的或远程的线程dump； b． 针对core文件做线程dump。 jstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack 操作方式 jps -l | grep keyword -&gt; pidjstack pid jstack结果如下； 123456789101112131415161718&quot;lettuce-nioEventLoop-4-1&quot; #639 daemon prio=5 os_prio=0 tid=0x00007ff27025d800 nid=0x258f runnable [0x00007ff262af7000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked &lt;0x000000008988d6a8&gt; (a io.netty.channel.nio.SelectedSelectionKeySet) - locked &lt;0x000000008988d770&gt; (a java.util.Collections$UnmodifiableSet) - locked &lt;0x000000008988d600&gt; (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101) at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68) at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:803) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:748) 参考资料原文链接：https://blog.csdn.net/weixin_30013175/article/details/113901522","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-类加载机制","date":"2021-07-28T09:45:58.000Z","path":"wiki/JVM-类加载机制/","text":"类加载机制1. 类加载机制具体流程 Java 的类加载过程可以分为 5 个阶段：载入、验证、准备、解析和初始化。这 5 个阶段一般是顺序发生的，但在动态绑定的情况下，解析阶段发生在初始化阶段之后。 1.1 Loading（载入） JVM 在该阶段的主要目的是将字节码从不同的数据源（可能是 class 文件、也可能是 jar 包，甚至网络）转化为二进制字节流加载到内存中，并生成一个代表该类的 java.lang.Class 对象。 通过一个类的全限定名来获取定义次类的二进制流(ZIP 包、网络、运算生成、JSP 生成、数据库读取)。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法去这个类的各种数据的访问入口。 数组类的特殊性：数组类本身不通过类加载器创建，它是由 Java 虚拟机直接创建的。但数组类与类加载器仍然有很密切的关系，因为数组类的元素类型最终是要靠类加载器去创建的，数组创建过程如下： 如果数组的组件类型是引用类型，那就递归采用类加载加载。 如果数组的组件类型不是引用类型，Java 虚拟机会把数组标记为引导类加载器关联。 数组类的可见性与他的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为 public。 内存中实例的 java.lang.Class 对象存在方法区中。作为程序访问方法区中这些类型数据的外部接口。加载阶段与连接阶段的部分内容是交叉进行的，但是开始时间保持先后顺序。 java agent发生在加载环节，用来增强字节码。 1.2 Verification（验证）JVM 会在该阶段对二进制字节流进行校验，只有符合 JVM 字节码规范的才能被 JVM 正确执行。该阶段是保证 JVM 安全的重要屏障，下面是一些主要的检查。 1.2.1 文件格式验证 是否以魔数 0xCAFEBABE 开头 主、次版本号是否在当前虚拟机处理范围之内 常量池的常量是否有不被支持常量的类型（检查常量 tag 标志） 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量 CONSTANT_Utf8_info 型的常量中是否有不符合 UTF8 编码的数据 Class 文件中各个部分集文件本身是否有被删除的附加的其他信息 …… 只有通过这个阶段的验证后，字节流才会进入内存的方法区进行存储，所以后面 3 个验证阶段全部是基于方法区的存储结构进行的，不再直接操作字节流。 1.2.2 元数据验证 这个类是否有父类（除 java.lang.Object 之外） 这个类的父类是否继承了不允许被继承的类（final 修饰的类） 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法 类中的字段、方法是否与父类产生矛盾（覆盖父类 final 字段、出现不符合规范的重载） 这一阶段主要是对类的元数据信息进行语义校验，保证不存在不符合 Java 语言规范的元数据信息。 1.2.3 字节码验证 保证任意时刻操作数栈的数据类型与指令代码序列都鞥配合工作（不会出现按照 long 类型读一个 int 型数据） 保证跳转指令不会跳转到方法体以外的字节码指令上 保证方法体中的类型转换是有效的（子类对象赋值给父类数据类型是安全的，反过来不合法的） …… 这是整个验证过程中最复杂的一个阶段，主要目的是通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。这个阶段对类的方法体进行校验分析，保证校验类的方法在运行时不会做出危害虚拟机安全的事件。 1.2.4 符号引用验证 符号引用中通过字符创描述的全限定名是否能找到对应的类 在指定类中是否存在符方法的字段描述符以及简单名称所描述的方法和字段 符号引用中的类、字段、方法的访问性（private、protected、public、default）是否可被当前类访问 …… 最后一个阶段的校验发生在迅疾将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生。符号引用验证可以看做是对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验，还有以上提及的内容。符号引用的目的是确保解析动作能正常执行，如果无法通过符号引用验证将抛出一个 java.lang.IncompatibleClass.ChangeError 异常的子类。如 java.lang.IllegalAccessError、java.lang.NoSuchFieldError、java.lang.NoSuchMethodError 等。 1.3 Preparation（准备）1.3.1 设置默认值JVM 会在该阶段对类变量（也称为静态变量，static 关键字修饰的）分配内存并初始化（对应数据类型的默认初始值，如 0、0L、null、false 等）。 也就是说，假如有这样一段代码： 123public String chenmo = &quot;沉默&quot;;public static String wanger = &quot;王二&quot;;public static final String cmower = &quot;沉默王二&quot;; chenmo 不会被分配内存，而 wanger 会；但 wanger 的初始值不是“王二”而是 null。 需要注意的是，static final 修饰的变量被称作为常量，和类变量不同。常量一旦赋值就不会改变了，所以 cmower 在准备阶段的值为“沉默王二”而不是 null。 1.3.2 各种类型默认值 1.4 Resolution（解析）该阶段将常量池中的符号引用转化为直接引用。 **符号引用** : 以一组符号（任何形式的字面量，只要在使用时能够无歧义的定位到目标即可）来描述所引用的目标。 **直接引用** : 直接引用可以使直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用和迅疾的内存布局实现有关 在编译时，Java 类并不知道所引用的类的实际地址，因此只能使用符号引用来代替。比如 com.Wanger 类引用了 com.Chenmo 类，编译时 Wanger 类并不知道 Chenmo 类的实际内存地址，因此只能使用符号 com.Chenmo。 直接引用通过对符号引用进行解析，找到引用的实际内存地址。 1.5 Initialization（初始化）该阶段是类加载过程的最后一步。在准备阶段，类变量已经被赋过默认初始值，而在初始化阶段，类变量将被赋值为代码期望赋的值。换句话说，初始化阶段是执行类构造器方法的过程。 oh，no，上面这段话说得很抽象，不好理解，对不对，我来举个例子。 String cmower = new String(&quot;沉默王二&quot;);上面这段代码使用了 new 关键字来实例化一个字符串对象，那么这时候，就会调用 String 类的构造方法对 cmower 进行实例化。 面试题，什么时候后触发初始化 1、创建类的实例的时候，new一个对象 2、访问某个类或接口的静态变量，或者对该静态变量赋值 3、调用类的静态方法 4、反射 5、初始化某个类的子类，则其父类也会被初始化 6、Java虚拟机启动时被标记为启动类的类 2. 双亲委派机制聊完类加载过程，就不得不聊聊类加载器。 2.1 什么是类加载 一般来说，Java 程序员并不需要直接同类加载器进行交互。JVM 默认的行为就已经足够满足大多数情况的需求了。不过，如果遇到了需要和类加载器进行交互的情况，而对类加载器的机制又不是很了解的话，就不得不花大量的时间去调试ClassNotFoundException 和 NoClassDefFoundError 等异常。 对于任意一个类，都需要由它的类加载器和这个类本身一同确定其在 JVM 中的唯一性。也就是说，如果两个类的加载器不同，即使两个类来源于同一个字节码文件，那这两个类就必定不相等（比如两个类的 Class 对象不 equals）。 站在程序员的角度来看，Java 类加载器可以分为三种。 1）启动类加载器（Bootstrap Class-Loader），加载 jre/lib 包下面的 jar 文件，比如说常见的 rt.jar。 2）扩展类加载器（Extension or Ext Class-Loader），加载 jre/lib/ext 包下面的 jar 文件。 3）应用类加载器（Application or App Clas-Loader），根据程序的类路径（classpath）来加载 Java 类。 来来来，通过一段简单的代码了解下。 1234567891011public class Test &#123; public static void main(String[] args) &#123; ClassLoader loader = Test.class.getClassLoader(); while (loader != null) &#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; &#125;&#125; 每个 Java 类都维护着一个指向定义它的类加载器的引用，通过 类名.class.getClassLoader() 可以获取到此引用；然后通过 loader.getParent() 可以获取类加载器的上层类加载器。 这段代码的输出结果如下： sun.misc.Launcher$AppClassLoader@73d16e93sun.misc.Launcher$ExtClassLoader@15db9742第一行输出为 Test 的类加载器，即应用类加载器，它是 sun.misc.Launcher$AppClassLoader 类的实例；第二行输出为扩展类加载器，是 sun.misc.Launcher$ExtClassLoader 类的实例。那启动类加载器呢？ 按理说，扩展类加载器的上层类加载器是启动类加载器，但在我这个版本的 JDK 中， 扩展类加载器的 getParent() 返回 null。所以没有输出。 2.2 双亲委派的意义使用双亲委派模型，Java类随着它的加载器一起具备了一种带有优先级的层次关系，通过这种层次模型，可以避免类的重复加载，也可以避免核心类被不同的类加载器加载到内存中造成冲突和混乱，从而保证了Java核心库的安全。 3. 类加载的应用https://juejin.cn/post/6931972267609948167 类加载 热部署 加密保护 依赖冲突 4. 双亲委派如何破坏线程上下文加载器 https://enfangzhong.github.io/2019/12/17/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E7%A0%B4%E5%9D%8F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/ https://www.cnblogs.com/joemsu/p/9310226.html#_caption_2 5、类卸载发生在什么时候同时满足一下三种情况 👇 1、该类的所有实例都已经被回收，也就是说堆中不存在该类的任何实例 2、加载该类的ClassLoader已经被回收 3、该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问类的方法","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-jmap","date":"2021-07-28T09:45:41.000Z","path":"wiki/JVM-jmap/","text":"jmap命令jmap是一个多功能的命令。它可以生成 java 程序的 dump 文件， 也可以查看堆内对象示例的统计信息、查看 ClassLoader 的信息以及 finalizer 队列。 jmap -heap pid1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Attaching to process ID 7183, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.242-b08using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2051014656 (1956.0MB) NewSize = 42991616 (41.0MB) MaxNewSize = 683671552 (652.0MB) OldSize = 87031808 (83.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 233308160 (222.5MB) used = 161611280 (154.12452697753906MB) free = 71696880 (68.37547302246094MB) 69.26945032698384% usedFrom Space: capacity = 1572864 (1.5MB) used = 899896 (0.8582077026367188MB) free = 672968 (0.6417922973632812MB) 57.213846842447914% usedTo Space: capacity = 1572864 (1.5MB) used = 0 (0.0MB) free = 1572864 (1.5MB) 0.0% usedPS Old Generation capacity = 223346688 (213.0MB) used = 115841432 (110.4749984741211MB) free = 107505256 (102.5250015258789MB) 51.866196466723515% used41772 interned Strings occupying 4324472 bytes. 参考资料jvm 性能调优工具之 jmapJVM调试工具-jmap通过jstack与jmap分析一次线上故障","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"SpringBean循环依赖","date":"2021-07-28T09:34:20.000Z","path":"wiki/SpringBean循环依赖/","text":"Spring Bean 循环依赖为什么会存在循环依赖 如上图👆所示，A对象的一个属性是B,B对象的一个属性是A,而Spring中的bean默认情况下都是单例的\b，所以这两个Bean就产生了循环依赖的问题！ 那么循环依赖的问题出现在什么情况呢 想一下属性赋值的方式有几种呢？ 构造器赋值 这种形式循环依赖问题无法解决 GET/SET方法 调用SET方法进行赋值的时候，可以通过三级缓存的策略来解决循环依赖的问题 所以，三级缓存的策略是针对于使用SET方法对属性赋值的场景下的！ 循环依赖如何解决 在实例化的过程中，将处于半成品的对象全部放到缓存中，方便后续来进行调用；只要有了当前对象的引用地址，那么后续来进行赋值即可； d 能不能将创建好的对象也放到缓存中呢？ 不能，如果放在一起将无法区分对象是成品对象还是半成品对象了所以再次引出多级缓存的概念，可以创建两个缓存对象，一个用来存放已经实例化的半成品对象，另一个存放完成实例化并且完成初始化的成品对象，这个应该比较好理解吧！ 思考一下以上的设计有没有问题呢？ 为什么需要三级缓存？Spring在解决对象Bean循环依赖的问题的解决方案是使用了「三级缓存」；为什么需要三级缓存，也就是三个Map对象； org.springframework.beans.factory.support.DefaultSingletonBeanRegistry 123456// 一级缓存private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap(256);// 二级缓存private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap(16);// 三级缓存private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap(16); 三级缓存中分别保存的是什么内容 一级缓存： 成品对象 二级缓存： 半成品对象 三级缓存； lambda表达式 如果只有二级缓存可不可行 在Spring源码中，只有addSingleton方法和doCreateBean方法中向三级缓存中添加东西的； org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#addSingletonFactory 123456789protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized(this.singletonObjects) &#123; this.singletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125; org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#getSingleton(java.lang.String, boolean) 1234567891011121314151617181920@Nullable protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; this.isSingletonCurrentlyInCreation(beanName)) &#123; synchronized(this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, &quot;Bean name must not be null&quot;); synchronized(this.singletonObjects) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; if (this.singletonsCurrentlyInDestruction) &#123; throw new BeanCreationNotAllowedException(beanName, &quot;Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)&quot;); &#125; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(&quot;Creating shared instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; this.beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = this.suppressedExceptions == null; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = new LinkedHashSet(); &#125; try &#123; singletonObject = singletonFactory.getObject(); newSingleton = true; &#125; catch (IllegalStateException var16) &#123; singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; throw var16; &#125; &#125; catch (BeanCreationException var17) &#123; BeanCreationException ex = var17; if (recordSuppressedExceptions) &#123; Iterator var8 = this.suppressedExceptions.iterator(); while(var8.hasNext()) &#123; Exception suppressedException = (Exception)var8.next(); ex.addRelatedCause(suppressedException); &#125; &#125; throw ex; &#125; finally &#123; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = null; &#125; this.afterSingletonCreation(beanName); &#125; if (newSingleton) &#123; this.addSingleton(beanName, singletonObject); &#125; &#125; return singletonObject; &#125; &#125;","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"Spring加载配置文件原理","date":"2021-07-28T09:33:48.000Z","path":"wiki/Spring加载配置文件原理/","text":"Spring如何加载配置文件到应用程序加载Xml文件配置，获取对象 xml文件 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;user&quot; class=&quot;com.ibli.javaBase.reflection.User&quot;&gt; &lt;property name=&quot;age&quot; value=&quot;12&quot;/&gt; &lt;property name=&quot;name&quot; value=&quot;gaolei&quot;/&gt; &lt;property name=&quot;sex&quot; value=&quot;male&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 测试类 1234567public class IocDemo &#123; public static void main(String[] args) &#123; ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;spring-ioc.xml&quot;); User user = (User) ac.getBean(&quot;user&quot;); System.out.println(user); &#125;&#125; Spring 加载Xml文件流程 首先猜想一下宏观的流程 我们可以大体猜想流程是什么样的，如下👇 接下来debug源码看一下具体流程： ClassPathXmlApplicationContext调用refresh方法 12345678public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); this.setConfigLocations(configLocations); if (refresh) &#123; // Spring 启动入口 this.refresh(); &#125; &#125; Spring 启动入口 this.refresh(); 👆 调用AbstractRefreshableApplicationContext下的refreshBeanFactory org.springframework.context.support.AbstractRefreshableApplicationContext#refreshBeanFactory 1234567891011121314151617181920protected final void refreshBeanFactory() throws BeansException &#123; if (this.hasBeanFactory()) &#123; this.destroyBeans(); this.closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = this.createBeanFactory(); beanFactory.setSerializationId(this.getId()); this.customizeBeanFactory(beanFactory); // 从这里进入下一步 👇 this.loadBeanDefinitions(beanFactory); synchronized(this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException var5) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + this.getDisplayName(), var5); &#125; &#125; 关键方法是this.loadBeanDefinitions(beanFactory); 找到XmlBeanDefinitionReader 这是读取配置的关键所在 关键对象 XmlBeanDefinitionReader 这个在 「梳理Spring启动脉络」中提到了，Spring提供的抽象接口！ 12345678910protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化beanDefinitionReader对象 this.initBeanDefinitionReader(beanDefinitionReader); // 加载配置文件 获得BeanDefinitions this.loadBeanDefinitions(beanDefinitionReader); &#125; 继续调用 loadBeanDefinitions 这个有很多重载方法，一直点下去就行！ 12345678910111213protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = this.getConfigResources(); if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = this.getConfigLocations(); //spring-ioc.xml if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125; &#125; configLocations 就是我们Xml配置文件的路径 接下来一直调用loadBeanDefinitions方法 直到这一步 👇 org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.support.EncodedResource) 1234567891011121314151617181920212223242526272829try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); Throwable var4 = null; try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; var6 = this.doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; catch (Throwable var24) &#123; var4 = var24; throw var24; &#125; finally &#123; if (inputStream != null) &#123; if (var4 != null) &#123; try &#123; inputStream.close(); &#125; catch (Throwable var23) &#123; var4.addSuppressed(var23); &#125; &#125; else &#123; inputStream.close(); &#125; &#125; &#125; &#125; 这里看到 nputStream 很明显，这里是通过IO流读取制定位置的文件的 ! 获取到文件输入流之后，将输入流转换成Document文件去解析 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException 12// 转换成Document的关键方法Document doc = this.doLoadDocument(inputSource, resource); 调用doRegisterBeanDefinitions方法 org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#doRegisterBeanDefinitions调用parseBeanDefinitions方法去解析数据 调用DefaultBeanDefinitionDocumentReader的parseBeanDefinitions方法 来解析Element org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#parseBeanDefinitions 1234567891011121314151617181920protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for(int i = 0; i &lt; nl.getLength(); ++i) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element)node; if (delegate.isDefaultNamespace(ele)) &#123; this.parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125; &#125; 调用parseDefaultElement方法 org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#parseDefaultElement 123456789101112private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, &quot;import&quot;)) &#123; this.importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, &quot;alias&quot;)) &#123; this.processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, &quot;bean&quot;)) &#123; this.processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, &quot;beans&quot;)) &#123; this.doRegisterBeanDefinitions(ele); &#125; &#125; 这里看到if (delegate.nodeNameEquals(ele, &quot;bean&quot;)) 会不会很兴奋呢，接下来就是解析的方法了👇 跳转到 processBeanDefinition(ele, delegate); 12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 是的 就是这个方法了 👉 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, this.getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException var5) &#123; this.getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, var5); &#125; this.getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; parseBeanDefinitionElement 将元素数据解析到beanDefinition org.springframework.beans.factory.xml.BeanDefinitionParserDelegate#parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Nullable public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(&quot;id&quot;); String nameAttr = ele.getAttribute(&quot;name&quot;); List&lt;String&gt; aliases = new ArrayList(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, &quot;,; &quot;); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(id) &amp;&amp; !aliases.isEmpty()) &#123; beanName = (String)aliases.remove(0); if (this.logger.isTraceEnabled()) &#123; this.logger.trace(&quot;No XML &#x27;id&#x27; specified - using &#x27;&quot; + beanName + &quot;&#x27; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; this.checkNameUniqueness(beanName, aliases, ele); &#125; // 将element数据最终转换成一个beanDefinition对象 是不是很惊奇 哈哈哈 👉 AbstractBeanDefinition beanDefinition = this.parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName(beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (this.logger.isTraceEnabled()) &#123; this.logger.trace(&quot;Neither XML &#x27;id&#x27; nor &#x27;name&#x27; specified - using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception var9) &#123; this.error(var9.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; else &#123; return null; &#125; &#125;","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"Spring梳理启动脉络","date":"2021-07-28T09:33:20.000Z","path":"wiki/Spring梳理启动脉络/","text":"Spring是如何启动的Spring最大的核心就是Bean容器；容器： 从对象创建，使用和销毁全部由容器帮我们控制，用户仅仅使用就可以。 两大核心 IOC 控制反转 AOP 面向切面编程 思考：我们是如何使用Spring的呢？ 加入从配置文件中加载bean 我们猜想一下大致流程是怎样的 123&lt;bean id=getPerson class=com.ibli.Person&gt;&lt;property name=id value=1&gt;&lt;property name=age value=20&gt; 配置文件如上👆，这里是伪代码！ 先猜想大致流程： 通过上面猜想创建的对象流程，创建出对象，对象已经好了，就是使用了，那么如何使用呢？ 一般情况下我们会可以这样使用，写一下伪代码吧👇： 12创建一个ApplicationContext对象Object obj = applicationContext.getBean(&quot;bean name); 思考，创建的对象如何存储？ 或者容器到底是什么呢？ 应该可以猜到是Map结构，具体是什么Map,先不管； 1、首先容器是创建好的，容器创建好之后，才可以加载配置文件 也就是我们猜想的Map 2、加载配置文件 配置文件可能会有多种方式，比如XML格式，property格式，yaml格式，注解格式，这个格式各不相同，又是如何加载的呢？Spring提供了一个接口，BeanDefinitionReader,它有一个抽象实现类AbstractBeanDefinitionReader，不同配置文件的Reader来继承这个抽象类，实现它们自己的逻辑； 123public class PropertiesBeanDefinitionReader extends AbstractBeanDefinitionReaderpublic class GroovyBeanDefinitionReader extends AbstractBeanDefinitionReader implements GroovyObjectpublic class XmlBeanDefinitionReader extends AbstractBeanDefinitionReader 3、读取的配置文件会转换成Spring定义的格式，也就是BeanDefinition； BeanDefinition定义了类的所有相关的数据； 此时得到的BeanDefinition的属性值只是「符号类型」,并不是真正的属性值； 我们可能会见过这中加载数据源的方式👇 12345&lt;bean id=dataSource class=com.alibab.durid.pool.DruidDataSource&gt;&lt;property name=url value=$&#123;jdbc.url&#125;&gt;&lt;property name=username value=$&#123;jdbc.username&#125;&gt;&lt;property name=password value=$&#123;jdbc.password&#125;&gt;&lt;/bean&gt; 数据源的具体配置是放在配置文件中的，当通过XmlBeanDefinitionReader读取并解析到的BeanDefinition，仅仅是将Xml中的文件数据存放到BeanDefinition中，属性的值是${jdbc.url}而不是真正的我们数据源的地址； 4、得到所有的BeanDefinition之后，通过BeanFactoryPostProcessor来处理上一步骤中，属性value不是真实数据的问题 比如PlaceHolderConfigurerSupport(占位符处理) 经过工厂后置处理器处理之后，BeanDefinition的属性值就是真实需要的数据了； 5、BeanDefinition数据准备完成之后，由BeanFactory来完成Bean的创建 实例化 对象中分配堆内存等操作 反射调用无参构造函数 创建对象 但是属性是空的 初始化 6、初始化之前需要准备的工作 1、准备BeanPostProcessors2、观察者模式，准备监听器 事件 广播器 7、初始化环节有很多步骤 对象的填充 其实就是调用get/set方法对属性赋值 调用aware方法 如果我们的对象中的属性是BeanFactory 我们不用自己去完成setBeanFactory方法，只需要当前类实现BeanFactoryAware方法即可 123public interface BeanFactoryAware extends Aware &#123; void setBeanFactory(BeanFactory var1) throws BeansException;&#125; 处理before操作 调用init方法 执行after方法 before和after此处是调用的BeanPostProcessor的方法 1234// 前置方法postProcessBeforeInitialization// 后置方法postProcessAfterInitialization 8、执行到此，完成对象的创建，得到一个可以使用的对象","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"","date":"2021-07-28T09:32:41.602Z","path":"wiki/Spring-Overview/","text":"title: Spring-Overviewtoc: truedate: 2021-07-28 17:32:41tags: Spring categories: [Spring Family , Spring Framework]SpringThe Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. A key element of Spring is infrastructural support at the application level: Spring focuses on the “plumbing” of enterprise applications so that teams can focus on application-level business logic, without unnecessary ties to specific deployment environments. 学习清单 什么是Spring，为什么需要Spring(优点) Spring有哪些模块 什么是IOC 和 AOP 什么是依赖注入，有几种实现方式？ Spring中的动态代理是如何实现的 学习方法 先梳理脉络，先宏观，再细节 Spring源码注释很重要 见名知意 学习spring的命名规范 猜测和验证 坚持看 不要三分钟热度 学习资料 Spring框架官方网站 【官网】可以下载Spirng源码在本地查看更舒服！ 源码请戳 👉👉 【源码】 👈👈 Spring5最新完整教程IDEA版通俗易懂 视频教程 狂神说 （这个大佬在B站很火的）原链接请点击👉 【传送】 24集彻底搞懂aop ioc mvc底层原理 视频目录比较好 👉 【2020年史上最新Spring源码合集，24集彻底搞懂aop ioc mvc底层原理。】 手撕SpringIOC源码 马士兵教育 【400分钟学完Spring源码设计及原理，手撕SpringIOC源码，从我做起】 图灵学院公开课 课程目录还不错 5个小时 21年录制比较新 【2021年新版Java-Spring底层原理，阿里P8大佬全套讲解】 Mybatis + Spring 源码解读 VIP公开课【终于有字节跳动技术大牛把【mybatis底层原理：spring整理mybatis】讲明白了】","tags":[],"categories":[]},{"title":"mysql乐观锁实现分布式锁","date":"2021-07-28T09:21:46.000Z","path":"wiki/mysql乐观锁实现分布式锁/","text":"基于数据表乐观锁实现分布式锁整体的实际思路要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。当我们要锁住某个方法或资源的时候，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。 基于数据表实现分布式锁的几个要点1、这把锁依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。2、这把锁没有失效时间，一旦解决操作失败，就会导致记录一直在数据库中，其他线程无法在获得锁。3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁的操作。4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据库表中数据已经存在了。 当然，我们也可以有其它方式解决上面的问题： 1、数据库是单点？那就搞两个数据库，数据库之前双向同步，一旦挂掉快速切换到备库上。2、没有失效时间？可以做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。3、非阻塞？可以写一个while循环，直到insert成功再返回成功。4、非重入？可以在数据库表中加一个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库中可以查到的话，就直接把锁分配给它即可。 乐观锁&amp;悲观锁乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。 乐观锁的实现实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。 1、使用版本号实现乐观锁使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。 12345671.查询出商品信息select (status,status,version) from t_goods where id=#&#123;id&#125;2.根据商品信息生成订单3.修改商品status为2update t_goodsset status=2,version=version+1where id=#&#123;id&#125; and version=#&#123;version&#125;; 需要注意的是，乐观锁机制往往基于系统中数据存储逻辑，因此也具备一定的局限性。由于乐观锁机制是在我们的系统中实现的，对于来自外部系统的用户数据更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。在系统设计阶段，我们应该充分考虑到这些情况，并进行相应的调整（如将乐观锁策略在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。 这一点其实在微服务架构中只要做好数据隔离就可以避免，比如user这张数据表，按照边界划分应该属于用户中心服务的，其他服务比如仓储，物流等需要用户的信息，应该有用户中心暴露出接口，而不是仓储去数据库查询user这张表的数据，甚至update user的数据。 2、使用时间戳一般都是使用update_time字段，并且这个字段肯定是跟随数据库时间配置的，即 update on current_timestamp ； 乐观锁的优点与不足乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。能够提升数据库的吞吐量；但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。 参考资料基于数据库的分布式锁实现分布式锁方式（一、基于数据库的分布式锁）分布式锁看这篇就够了乐观锁与悲观锁深入学习","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"","date":"2021-07-28T09:05:46.241Z","path":"wiki/mybatis配置文件解析/","text":"title: mybatis配置文件解析toc: truedate: 2021-07-28 17:05:46tags: mybatiscategories: [Spring Family] Mybatis配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- 引入数据库属性文件 --&gt; &lt;properties resource=&quot;database.properties&quot;&gt; &lt;!-- &lt;property name=&quot;username&quot; value=&quot;sa&quot;&gt;&lt;/property&gt; --&gt; &lt;/properties&gt; &lt;!-- mybatis配置文件 --&gt; &lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;false&quot;/&gt; &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt; &lt;/settings&gt; &lt;!-- 别名的配置 --&gt; &lt;typeAliases&gt; &lt;!-- &lt;typeAlias type=&quot;com.xit.pojo.User&quot; alias=&quot;user&quot;/&gt; --&gt; &lt;package name=&quot;com.xit.pojo&quot;/&gt; &lt;/typeAliases&gt; &lt;!-- 配置运行环境 --&gt; &lt;environments default=&quot;default&quot;&gt; &lt;environment id=&quot;default&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;!-- 由JDBC管理事务 --&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!-- 配置数据源：连接池 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 引入Mapper映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;com/xit/pojo/UserMapper.xml&quot;/&gt; &lt;!-- URL方式 --&gt; &lt;!-- &lt;mapper url=&quot;file:///C:/eclipse-workspace/mybatis-01/src/com/xit/pojo/UserMapper.xml&quot;/&gt; --&gt; &lt;/mappers&gt;&lt;/configuration&gt; Mybatis有几部分全局配置properties=&gt;ettings=&gt;typeAliases=&gt;typeHandlers=&gt;objectFactory=&gt;plugins=&gt;environment=&gt;databaseIdProvider=&gt;mappers Mybatis 加载Mapper文件有几种方式？以上是Mybatis官方文档介绍的样例👆，原文链接请点击这 有4种方式；按照优先级从高到底依次是： package resource url class 下面是Mybatis加载mybatis-config.xml文件配置的源码，从代码中也可以看到加载的4中方式和优先级！👇org.apache.ibatis.builder.xml.XMLConfigBuilder#typeHandlerElement 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private void mapperElement(XNode parent) throws Exception &#123; if (parent != null) &#123; Iterator var2 = parent.getChildren().iterator(); while(true) &#123; while(var2.hasNext()) &#123; XNode child = (XNode)var2.next(); String resource; if (&quot;package&quot;.equals(child.getName())) &#123; resource = child.getStringAttribute(&quot;name&quot;); this.configuration.addMappers(resource); &#125; else &#123; resource = child.getStringAttribute(&quot;resource&quot;); String url = child.getStringAttribute(&quot;url&quot;); String mapperClass = child.getStringAttribute(&quot;class&quot;); XMLMapperBuilder mapperParser; InputStream inputStream; if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(resource); inputStream = Resources.getResourceAsStream(resource); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, resource, this.configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(url); inputStream = Resources.getUrlAsStream(url); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, url, this.configuration.getSqlFragments()); mapperParser.parse(); &#125; else &#123; if (resource != null || url != null || mapperClass == null) &#123; throw new BuilderException(&quot;A mapper element may only specify a url, resource or class, but not more than one.&quot;); &#125; Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass); this.configuration.addMapper(mapperInterface); &#125; &#125; &#125; return; &#125; &#125; &#125;``` ## Mybatis有几种执行器mybatis有3中执行器； ```textpackage org.apache.ibatis.session;public enum ExecutorType &#123; SIMPLE, // 默认 REUSE, BATCH; private ExecutorType() &#123; &#125;&#125;","tags":[],"categories":[]},{"title":"mybatis-工作原理","date":"2021-07-28T08:52:38.000Z","path":"wiki/mybatis-工作原理/","text":"Mybatis工作原理Mybatis整体框架 工作原理解析 1）读取MyBatis配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。 读取配置文件将mybatis-config.xml转换为org.apache.ibatis.session.Configuration类，这里mybatis包含9个全局配置； 2）加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。 扫描Mapping目录下的***Mapper.xml文件； 3）构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。 1234567@Bean(name = &quot;sqlSessionFactory&quot;)public SqlSessionFactory sqlSessionFactory(HikariDataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:com/****/mapping/**/*.xml&quot;)); return bean.getObject();&#125; 生成工厂实例： 123456789101112131415161718192021222324252627public class SqlSessionFactoryBean implements FactoryBean&lt;SqlSessionFactory&gt;, InitializingBean, ApplicationListener&lt;ApplicationEvent&gt; &#123; private static final Log LOGGER = LogFactory.getLog(SqlSessionFactoryBean.class); private Resource configLocation; private Configuration configuration; private Resource[] mapperLocations; private DataSource dataSource; private TransactionFactory transactionFactory; private Properties configurationProperties; private SqlSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); private SqlSessionFactory sqlSessionFactory; private String environment = SqlSessionFactoryBean.class.getSimpleName(); private boolean failFast; private Interceptor[] plugins; private TypeHandler&lt;?&gt;[] typeHandlers; private String typeHandlersPackage; private Class&lt;?&gt;[] typeAliases; private String typeAliasesPackage; private Class&lt;?&gt; typeAliasesSuperType; private DatabaseIdProvider databaseIdProvider; private Class&lt;? extends VFS&gt; vfs; private Cache cache; private ObjectFactory objectFactory; private ObjectWrapperFactory objectWrapperFactory; public SqlSessionFactoryBean() &#123; &#125;&#125; 4）创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。 SqlSession对象完成和数据库的交互： 5）Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。 3种执行期类型（Simple Pre Batch） Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。 6）MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。 借助MappedStatement中的结果映射关系，将返回结果转化成HashMap、JavaBean等存储结构并返回。 7）输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。8）输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。 参考资料【1】MyBatis的工作原理 C语言网","tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://example.com/tags/mybatis/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"mybatis","slug":"Spring-Family/mybatis","permalink":"http://example.com/categories/Spring-Family/mybatis/"}]},{"title":"Redis-缓存穿透、击穿和雪崩","date":"2021-07-28T08:51:22.000Z","path":"wiki/Redis-缓存穿透、击穿和雪崩/","text":"缓存击穿，穿透和雪崩背景首先说一下为什么会写这片文章，因为这个对我来说是印象非常深刻的，那是还在实习的时候，当时接了一个任务（其实就是练手的），大致需求是写一个白名单，然后有一个功能对白名单开放。因为是新功能，需要在部分地区试点，如果没有问题才会放开到全国城市运行。就是这么一个小的功能，让当时的我，呵呵。我记得那是第一次使用Redis,根本不知道什么是缓存雪崩啊，缓存击穿啊，穿透呀这些，还有更可怕的缓存一致性问题（TODO 下期再说）。 当时团队十几位大佬review我的代码，哼。这就是为什么印象会深刻一些吧，关键那是我第一个review代码，还是第一次使用redis，整个review就像是十几个大佬在面试我。真是怀疑人生了。。我是废物，别笑。 12345678910// 大致伪代码public boolean isPermit(String cityCode)&#123; // 先查询缓存中是否存在 String tmpCity = getCache(cityCode); if(StringUtil.isNotBlank(tmpCity))&#123; reture Boolean.TRUE; &#125; // 缓存中没有在去查数据库 reture judgeForDb(cityCode);&#125; 我上面的例子其实就是典型的缓存穿透的问题。因为仅仅是开放了十几个城市来试点功能，所以大部分的查询都是缓存不命中的。 下面引出今天我们的三个关键词： 缓存穿透 要查询的数据，缓存中基本上没有，所以大概率情况下缓存是不命中的，而是去数据库中去查询数据，导致缓存相当于是一个摆设。如果Key不是热点访问还可以，如果是热点Key，而且并发量也会很大的情况下，绝大多数的请求都会打到数据库上，很容易造成数据库宕机。 解决方案1、布隆过滤器进行校验，bloom filter典型应用场景（用户名是否存在，黑名单机制，单词错误检测）2、缓存空值方法，这个网上也是说的比较多的，这种方式也是可以的 在此我说一下我的解决方案：1、校验一个城市是否是城市白名单的城市时，直接查询缓存，存储的数据结构是使用的hash；2、如果没有查到这个城市，则认为这个城市不在白名单中，因为我是先查的hash，然后在查询hash中的具体城市。如果hash的值都没有查到，那说明缓存失效了；3、针对缓存失效的情况，可以再查DB来更新缓存，这个时候有人要较真了，会不会出线缓存击穿的情况呀，这个看具体场景，因为我的业务是不需要的，所以这里就直接查库更新缓存了，如果需要的话，那可以上缓存击穿的解决方案，或者看看上面两种解决方案有没有合适的，哈哈哈；4、因为城市是在后台进行配置的，所以我是在增删改的操作时，保证了缓存数据的一致性的前提下，才选择相信缓存的。这是我当时的解决方案。 缓存击穿 某个热点Key在一段时间内失效了，此时有大量请求瞬时抵达，会严重增加数据库的压力。因为我们的缓存数据一般都是要设置过期时间的，当缓存失效时，会去查询数据库同时更新缓存数据。 解决方案1、可以加锁，来保证只有第一个请求进来时达到数据库上，然后更新缓存，第二个请求进来是就会命中缓存，当然如果是分布式服务，那就需要使用分布式锁了。2、合理设置缓存时间，可以将热点Key时间设置长一些，或者根据业务将失效时间设置在业务量比较小的波段，都是可以的。有的解决方案会不设置Key的过期时间，这个，看情况吧，不建议这样。 如果大量的key不设置过期时间，则长期占用内存也是不好的。 缓存雪崩 缓存雪崩顾名思义，就是大量Key在一段时间或者瞬时失效，或者Redis服务重启（所有Key失效，因为是存在内存中），从而导致大量请求打到数据库上，增加数据库压力 解决方案1、将热点key设置不同波段的过期时间，把过期时间散列开。2、也可以使用分布式锁来限制高并发的请求，和缓存击穿的解决方案同理。3、对于Redis重启或宕机的问题，可以考虑集群部署，并保证数据的同步和一致性； 生活远不止眼前的苟且","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-字符串底层原理","date":"2021-07-28T08:50:26.000Z","path":"wiki/Redis-字符串底层原理/","text":"Redis底层实现及原理 关键词 SDS embstr 二进制安全 空间预分配 String类型不同的编码方式 使用整数存储： 只对长度小于或等于 21 字节，并且可以被解释为整数的字符串进行编码 使用EMBSTR 编码： 尝试将 RAW 编码的字符串编码为 EMBSTR 编码， 使用SDS编码: 这个对象没办法进行编码，尝试从 SDS 中移除所有空余空间 下面举个例子看一下👇 embstr与动态字符串 embstr的创建只需分配一次内存，而raw为两次（一次为sds分配对象，另一次为redisObject分配对象，embstr省去了第一次）。 相对地，释放内存的次数也由两次变为一次。 embstr的redisObject和sds放在一起，更好地利用缓存带来的优势 但是redis并未提供任何修改embstr的方式，即embstr是只读的形式。对embstr的修改实际上是先转换为raw再进行修改。 SDS(simple dynamic string)SDS定义123456789struct sdshdr&#123; //记录buf数组中已使用字节的数量 //等于 SDS 保存字符串的长度 int len; //记录 buf 数组中未使用字节的数量 int free; //字节数组，用于保存字符串 char buf[];&#125; SDS有什么优点1、常数复杂度获取字符串长度sdshdr 中由于 len 属性的存在，获取 SDS 字符串的长度只需要读取 len 属性，时间复杂度为 O(1)，而对于 C 语言来说， 获取字符串的长度通常是遍历字符串计数来实现的，时间复杂度为 O(n)。 2、杜绝缓冲区溢出我们知道在 C 语言中使用 strcat 函数来进行两个字符串的拼接，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。而对于 SDS 数据类型，在进行字符修改的时候， 会首先根据记录的 len属性检查内存空间是否满足需求，如果不满足，会进行相应的空间扩展，然后在进行修改操作，所以不会出现缓冲区溢出。 3、减少修改字符串时带来的内存重分配次数C语言由于不记录字符串的长度，所以如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。而对于SDS，由于len属性和free属性的存在，对于修改字符串SDS实现了空间预分配和惰性空间释放两种策略： 3.1 字符串长度增加操作时，进行空间预分配 对字符串进行空间扩展的时候，扩展的内存比实际需要的多，这样可以减少连续执行字符串增长操作所需的内存重分配次数。 3.2 字符串长度减少操作时，惰性空间释放 对字符串进行缩短操作时，程序不立即使用内存重新分配来回收缩短后多余的字节，而是使用 free 属性将这些字节的数量记录下来，等待后续使用。（当然SDS也提供了相应的API，当我们有需要时，也可以手动释放这些未使用的空间。 4、二进制安全因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取； 而所有 SDS 的API 都是以处理二进制的方式来处理 buf 里面的元素，并且 SDS不是以空字符串来判断是否结束，而是以 len 属性表示的长度来判断字符串是否结束。 5、兼容部分C字符串函数虽然 SDS 是二进制安全的，但是一样遵从每个字符串都是以空字符串结尾的惯例，这样可以重用 C 语言库&lt;string.h&gt; 中的一部分函数。 为什么字符串长度大于44就是用raw方式编码这个是因为C语言函数库分配内存的长度只能是2/4/8/16/32/64；最大分配64位的长度；但是redisObj的长度加上字符串对象头的长度，占用20位，所以字符串长度最多是44位，超过这个长度，就是用raw方式进行编码； – 《Redis深度历险-String数据结构》 参考资料1、《闲扯Redis二》String数据类型之底层解析2、每个程序员都应该知道的Redis知识 - String底层原理3、Redis详解（四）—— redis的底层数据结构4、redis string底层数据结构","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-overview","date":"2021-07-28T08:49:42.000Z","path":"wiki/Redis-overview/","text":"Redis （Remote Dictionary Server）Redis简介Redis 本质上是一个 Key-Value 类型的内存数据库，很像 memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据 flush 到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value DB。Redis 的出色之处不仅仅是性能，Redis 最大的魅力是支持保存多种数据结构，此外单个 value 的最大限制是 1GB，不像 memcached 只能保存 1MB 的数据，因此 Redis 可以用来实现很多有用的功能。比方说用他的 List 来做 FIFO 双向链表，实现一个轻量级的高性 能消息队列服务，用他的 Set 可以做高性能的 tag 系统等等。另外 Redis 也可以对存入的 Key-Value 设置 expire 时间，因此也可以被当作一 个功能加强版的 memcached 来用。Redis 的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此 Redis 适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 如何设置密码及验证密码？设置密码：config set requirepass 123456授权密码：auth 123456 Redis 有哪几种数据淘汰策略？noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但 DEL 和几个例外）allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。allkeys-random: 回收随机的键使得新添加的数据有空间存放。volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 Redis 有哪些适合的场景？（1）会话缓存（Session Cache）最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会话比其他存储（如 Memcached）的优势在于：Redis 提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 （2）全页缓存（FPC）除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似 PHP 本地 FPC。再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）队列Redis 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本地程序语言（如 Python）对 list 的 push/pop操作。如果你快速的在 Google 中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用 Redis 作为 broker，你可以从这里去查看。 （4）排行榜/计数器Redis 在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis 只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可：当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行：ZRANGE user_scores 0 10 WITHSCORESAgora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数据的，你可以在这里看到。###（5）发布/订阅最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！ Redis 常见的性能问题和解决方案1、master 最好不要做持久化工作，如 RDB 内存快照和 AOF 日志文件2、如果数据比较重要，某个 slave 开启 AOF 备份，策略设置成每秒同步一次3、为了主从复制的速度和连接的稳定性，master 和 Slave 最好在一个局域网内4、尽量避免在压力大得主库上增加从库5、主从复制不要采用网状结构，尽量是线性结构，Master&lt;–Slave1&lt;—-Slave2 …. Redis为什么这么快https://mp.weixin.qq.com/s/EjDeypra_d9Tfsn-WkJZdw 比较好的学习资源 📒📒📒https://redisbook.readthedocs.io/en/latest/index.html redis的数据结构quick listhttps://www.cnblogs.com/hunternet/p/12624691.html ziplisthttps://segmentfault.com/a/1190000017328042","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-哈希表实现","date":"2021-07-28T08:46:45.000Z","path":"wiki/Redis-哈希表实现/","text":"Redis之Hash表底层实现 关键词 字段dict | 渐进式哈希 | ziplist | 哈希表 字典底层结构 dict 字典结构体12345678typedf struct dict&#123; dictType *type;//类型特定函数，包括一些自定义函数，这些函数使得key和 //value能够存储 void *private;//私有数据 dictht ht[2];//两张hash表 int rehashidx;//rehash索引，字典没有进行rehash时，此值为-1 unsigned long iterators; //正在迭代的迭代器数量&#125;dict; type和private这两个属性是为了实现字典多态而设置的，当字典中存放着不同类型的值，对应的一些复制，比较函数也不一样，这两个属性配合起来可以实现多态的方法调用； ht[2]，两个hash表 rehashidx，这是一个辅助变量，用于记录rehash过程的进度，以及是否正在进行rehash等信息，当此值为-1时，表示该dict此时没有rehash过程 iterators，记录此时dict有几个迭代器正在进行遍历过程 dictht 哈希表结构体1234567typedf struct dictht&#123; dictEntry **table;//存储数据的数组 二维 unsigned long size;//数组的大小 unsigned long sizemask;//哈希表的大小的掩码，用于计算索引值，总是等于 //size-1 unsigned long used;//// 哈希表中中元素个数&#125;dictht; dictEntry 哈希数组结构12345678910typedf struct dictEntry&#123; void *key;//键 union&#123; void val; unit64_t u64; int64_t s64; double d; &#125;v;//值 struct dictEntry *next；//指向下一个节点的指针&#125;dictEntry; 注意这里还有一个指向下一个哈希表节点的指针，我们知道哈希表最大的问题是存在哈希冲突，如何解决哈希冲突，有开放地址法和链地址法。这里采用的便是链地址法，通过next这个指针可以将多个哈希值相同的键值对连接在一起，用来解决哈希冲突。 扩容与缩容当哈希表保存的键值对太多或者太少时，就要通过 rerehash(重新散列）来对哈希表进行相应的扩展或者收缩。具体步骤： 1、如果执行扩展操作，会基于原哈希表创建一个大小等于 ht[0].used*2n 的哈希表（也就是每次扩展都是根据原哈希表已使用的空间扩大一倍创建另一个哈希表）。相反如果执行的是收缩操作，每次收缩是根据已使用空间缩小一倍创建一个新的哈希表。2、重新利用上面的哈希算法，计算索引值，然后将键值对放到新的哈希表位置上。3、所有键值对都迁徙完毕后，释放原哈希表的内存空间。 触发扩容的条件： 1、服务器目前没有执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。 2、服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。ps：负载因子 = 哈希表已保存节点数量 / 哈希表大小。 为什么扩容的时候要考虑BIGSAVE的影响，而缩容时不需要？ BGSAVE时，dict要是进行扩容，则此时就需要为dictht[1]分配内存，若是dictht[0]的数据量很大时，就会占用更多系统内存，造成内存页过多分离，所以为了避免系统耗费更多的开销去回收内存，此时最好不要进行扩容； 缩容时，结合缩容的条件，此时负载因子&lt;0.1，说明此时dict中数据很少，就算为dictht[1]分配内存，也消耗不了多少资源； 渐进式哈希什么叫渐进式 rehash？也就是说扩容和收缩操作不是一次性、集中式完成的，而是分多次、渐进式完成的。如果保存在Redis中的键值对只有几个几十个，那么 rehash 操作可以瞬间完成，但是如果键值对有几百万，几千万甚至几亿，那么要一次性的进行 rehash，势必会造成Redis一段时间内不能进行别的操作。所以Redis采用渐进式 rehash,这样在进行渐进式rehash期间，字典的删除查找更新等操作可能会在两个哈希表上进行，第一个哈希表没有找到，就会去第二个哈希表上进行查找。但是进行增加操作，一定是在新的哈希表上进行的。 渐进式哈希其实就是慢慢的，一步一步的将hash表的数据迁移到另一个hash表中 redis会有一个定时任务去检测是否需要进行rehash rehash的过程中会在字典dict中维护一个rehashidx的标志 在rehash的过程中，两个hash表中都会有数据，此时如果有数据新增，将会存在ht[1]也就是第二个哈希表上； 在rehash的过程中，如果有删改查，则优先选择第一张表，如果第一张表没有查到数据，则查找第二章哈希表； 参考资料1、Redis详解（四）—— redis的底层数据结构2、Redis底层数据结构之hash3、Redis Hash数据结构的底层实现4、图解redis五种数据结构底层实现(动图哦)5、redis hash底层数据结构6、Redis底层数据结构之hash","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-list底层实现","date":"2021-07-28T08:45:33.000Z","path":"wiki/Redis-list底层实现/","text":"Redis List 底层实现 关键字 连锁更新问题 | quicklist | ziplist | linkedlist List底层数据结构在 3.0 版本的 Redis 中，List 类型有两种实现方式：数据结构底层采用压缩列表ziplist或linkedlist两种数据结构进行存储，首先以ziplist进行存储，在不满足ziplist的存储要求后转换为linkedlist列表。当列表对象同时满足以下两个条件时，列表对象使用ziplist进行存储，否则用linkedlist存储。 ziplist转换成linkedlist的条件1、触发一下任意一条即进行转换： 列表对象保存的所有字符串元素的长度小于64字节 列表对象保存的元素数量小于512个。 2、redis.conf配置文件 12list-max-ziplist-value 64 list-max-ziplist-entries 512 3、ziplist和linkedlist底层实现1、使用压缩列表（ziplist）实现的列表对象。2、使用双端链表（linkedlist）实现的列表对象。 在 3.2 版本后新增了 quicklist 数据结构实现了 list，现在就来分析下 quicklist 的结构。 quicklistziplist会引入频繁的内存申请和释放，而linkedlist由于指针也会造成内存的浪费，而且每个节点是单独存在的，会造成很多内存碎片，所以结合两个结构的特点，设计了quickList。quickList 是一个 ziplist 组成的双向链表。每个节点使用 ziplist 来保存数据。本质上来说， quicklist 里面保存着一个一个小的 ziplist。 quicklist表头结构12345678910111213141516171819typedef struct quicklist &#123; //指向头部(最左边)quicklist节点的指针 quicklistNode *head; //指向尾部(最右边)quicklist节点的指针 quicklistNode *tail; //ziplist中的entry节点计数器 unsigned long count; /* total count of all entries in all ziplists */ //quicklist的quicklistNode节点计数器 unsigned int len; /* number of quicklistNodes */ //保存ziplist的大小，配置文件设定，占16bits int fill : 16; /* fill factor for individual nodes */ //保存压缩程度值，配置文件设定，占16bits，0表示不压缩 unsigned int compress : 16; /* depth of end nodes not to compress;0=off */&#125; quicklist; head 和 tail 分别指向这个双端链表的表头和表尾, quicklist 存储的节点是一个叫做 quicklistNode 的结构, 如果这个 quicklist 是空的,那么 head 和 tail 会同时成为空指针, 如果这个双端链表的大小为 1, 那么 head 和 tail 会同时指向一个相同的节点 count 是一个计数器, 表示当前这个 list 结构一共存储了多少个元素, 它的类型是 unsigned long, 所以一个 list 能存储的最多的元素在 字长为 64 bit 的机器上是 (1 &lt;&lt; 64) - 1, 字长为 32 bit 的机器上是 (1 &lt;&lt; 32) - 1 len 表示了这个双端链表的长度(quicklistNodes 的数量) fill 表示了单个节点(quicklistNode)的负载比例(fill factor), 这是什么意思呢 Lists 结构使用了一种特殊的编码方式来节省空间, Lists 中每一个节点所能存储的东西可以通过最大长度或者一个最大存储的空间大小来限制,对于想限制每个节点最大存储空间的用户, 用 -5 到 -1 来表示这个限制值 5: 最大存储空间: 64 Kb &lt;– 通常情况下不要设置这个值 4: 最大存储空间: 32 Kb &lt;– 非常不推荐 3: 最大存储空间: 16 Kb &lt;– 不推荐 2: 最大存储空间: 8 Kb &lt;– 推荐 1: 最大存储空间: 4 Kb &lt;– 推荐 对于正整数则表示最多能存储到你设置的那个值, 当前的节点就装满了通常在 -2 (8 Kb size) 或 -1 (4 Kb size) 时, 性能表现最好但是如果你的使用场景非常独特的话, 调整到适合你的场景的值！！！！ redis.conf, 其中有一个可配置的参数叫做 list-max-ziplist-size, 默认值为 -2, 它控制了 quicklist 中的 fill 字段的值, 负数限制 quicklistNode 中的 ziplist 的字节长度, 正数限制 quicklistNode 中的 ziplist 的最大长度 compress 则表示 quicklist 中的节点 quicklistNode, 除开最两端的 compress 个节点之后, 中间的节点都会被压缩 Lists 在某些情况下是会被压缩的, 压缩深度是表示除开 list 两侧的这么多个节点不会被压缩, 剩下的节点都会被尝试进行压缩, 头尾两个节点一定不会被进行压缩,因为要保证 push/pop 操作的性能, 有以下的值可以设置:0: 关闭压缩功能 1: 深度 1 表示至少在 1 个节点以后才会开始尝试压缩, 方向为从头到尾或者从尾到头 12[head]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[tail][head], [tail] 永远都是不会被压缩的状态; 中间的节点则会被压缩 2 不会尝试压缩 head 或者 head-&gt;next 或者 tail-&gt;prev 或者 tail 但是会压缩这中间的所有节点 1[head]-&gt;[next]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[prev]-&gt;[tail] 3: 以此类推，最大为2的16次方。 quicklistNode 节点123456789101112131415161718192021222324252627282930typedef struct quicklistNode &#123; struct quicklistNode *prev; //前驱节点指针 struct quicklistNode *next; //后继节点指针 //不设置压缩数据参数recompress时指向一个ziplist结构 //设置压缩数据参数recompress指向quicklistLZF结构 unsigned char *zl; //压缩列表ziplist的总长度 unsigned int sz; /* ziplist size in bytes */ //ziplist中包的节点数，占16 bits长度 unsigned int count : 16; /* count of items in ziplist */ //表示是否采用了LZF压缩算法压缩quicklist节点，1表示压缩过，2表示没压缩，占2 bits长度 unsigned int encoding : 2; /* RAW==1 or LZF==2 */ //表示一个quicklistNode节点是否采用ziplist结构保存数据，2表示压缩了，1表示没压缩，默认是2，占2bits长度 unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ //标记quicklist节点的ziplist之前是否被解压缩过，占1bit长度 //如果recompress为1，则等待被再次压缩 unsigned int recompress : 1; /* was this node previous compressed? */ //测试时使用 unsigned int attempted_compress : 1; /* node can&#x27;t compress; too small */ //额外扩展位，占10bits长度 unsigned int extra : 10; /* more bits to steal for future usage */&#125; quicklistNode; prev 和 next 分别指向当前 quicklistNode 的前一个和后一个节点 zl 指向实际的 ziplist sz 存储了当前这个 ziplist 的占用空间的大小, 单位是字节 count 表示当前有多少个元素存储在这个节点的 ziplist 中, 它是一个 16 bit 大小的字段, 所以一个 quicklistNode 最多也只能存储 65536 个元素 encoding 表示当前节点中的 ziplist 的编码方式, 1(RAW) 表示默认的方式存储, 2(LZF) 表示用 LZF 算法压缩后进行的存储 container 表示 quicklistNode 当前使用哪种数据结构进行存储的, 目前支持的也是默认的值为 2(ZIPLIST), 未来也许会引入更多其他的结构 recompress 是一个 1 bit 大小的布尔值, 它表示当前的 quicklistNode 是不是已经被解压出来作临时使用 attempted_compress 只在测试的时候使用 extra 是剩下多出来的 bit, 可以留作未来使用 quicklistLZF 结构定义1234Copytypedef struct quicklistLZF &#123; unsigned int sz; //压缩后的ziplist大小 char compressed[];//柔性数组，存放压缩后的ziplist字节数组&#125; quicklistLZF; 当指定使用lzf压缩算法压缩ziplist的entry节点时，quicklistNode结构的zl成员指向quicklistLZF结构; 参考资料1、Redis列表list 底层原理2、Redis中string、list的底层数据结构原理3、《闲扯Redis五》List数据类型底层之quicklist4、Redis源码剖析和注释（七）— 快速列表(quicklist)5、redis 列表结构 底层实现(quicklist)","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"kafka安装与初体验","date":"2021-07-28T08:43:24.000Z","path":"wiki/kafka安装与初体验/","text":"Kafka的安装安装zookeeper1brew install zookeeper 默认端口：2181默认安装位置：/usr/local/Cellar/zookeeper配置文件位置：/usr/local/etc/zookeeper日志文件位置：/usr/local/var/log/zookeeper/zookeeper.log 启动zookeeper1nohup zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; 安装Kafka1brew install kafka 默认端口：9092默认安装位置：/usr/local/Cellar/kafka配置文件位置：/usr/local/etc/kafka日志文件位置：/usr/local/var/lib/kafka-logs 启动kafkanohup kafka-server-start /usr/local/etc/kafka/server.properties &amp; 订阅发布Demo创建一个Topic1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看创建的Topic1kafka-topics --list --zookeeper localhost:2181 生产者生产消息1kafka-console-producer --broker-list localhost:9092 --topic test 消费者消费消息1kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic test1 --from-beginning –from-beginning: 将从第一个消息开始接收 SpringBoot集成Kafka源码地址：https://gitee.com/IBLiplus/kafka-demo.git项目启动前按照上述安装启动步骤，在本地启动kafka.创建Maven项目，引入一下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;pring-kafka&lt;/artifactId&gt; &lt;version&gt;2.5.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 添加如下配置，端口号可以自己定配置文件： 123456789101112131415161718192021222324252627282930313233server.port=9010spring.kafka.bootstrap-servers= 127.0.0.1:9092# 发生错误后，消息重发的次数。spring.kafka.producer.retries= 0#当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。spring.kafka.producer.batch-size= 16384# 设置生产者内存缓冲区的大小。spring.kafka.producer.buffer-memory= 33554432# 键的序列化方式spring.kafka.producer.key-serializer= org.apache.kafka.common.serialization.StringSerializer# 值的序列化方式spring.kafka.producer.value-serializer = org.apache.kafka.common.serialization.StringSerializer# acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。# acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。# acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。spring.kafka.producer.acks= 1# 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5Dspring.kafka.consumer.auto-commit-interval= 1S# 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：# latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）# earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录spring.kafka.consumer.auto-offset-reset= earliest# 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量spring.kafka.consumer.enable-auto-commit= false# 键的反序列化方式spring.kafka.consumer.key-deserializer= org.apache.kafka.common.serialization.StringDeserializer# 值的反序列化方式spring.kafka.consumer.value-deserializer= org.apache.kafka.common.serialization.StringDeserializer# 在侦听器容器中运行的线程数。spring.kafka.listener.concurrency= 5#listner负责ack，每调用一次，就立即commitspring.kafka.listener.ack-mode= manual_immediatespring.kafka.listener.missing-topics-fatal= false 生产者生产消息： 123456789101112131415161718192021222324252627282930@Componentpublic class ProductDemo &#123; Logger log = LoggerFactory.getLogger(ProductDemo.class); @Resource private KafkaTemplate&lt;String, Object&gt; kafkaTemplate; //自定义topic public static final String TOPIC_TEST = &quot;topic.test&quot;; public static final String TOPIC_GROUP1 = &quot;topic.group1&quot;; public static final String TOPIC_GROUP2 = &quot;topic.group2&quot;; public void send(String obj) &#123; log.info(&quot;准备发送消息为：&#123;&#125;&quot;, obj); //发送消息 ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(TOPIC_TEST, obj); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() &#123; @Override public void onFailure(Throwable throwable) &#123; //发送失败的处理 log.info(TOPIC_TEST + &quot; - 生产者 发送消息失败：&quot; + throwable.getMessage()); &#125; @Override public void onSuccess(SendResult&lt;String, Object&gt; stringObjectSendResult) &#123; //成功的处理 log.info(TOPIC_TEST + &quot; - 生产者 发送消息成功：&quot; + stringObjectSendResult.toString()); &#125; &#125;); &#125;&#125; 消费消息 1234567891011121314151617181920212223242526@Componentpublic class ConsumerDemo &#123; Logger log = LoggerFactory.getLogger(ConsumerDemo.class); @KafkaListener(topics = ProductDemo.TOPIC_TEST, groupId = ProductDemo.TOPIC_GROUP1) public void topic_test(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) &#123; Optional message = Optional.ofNullable(record.value()); if (message.isPresent()) &#123; Object msg = message.get(); log.info(&quot;topic_test 消费了： Topic:&quot; + topic + &quot;,Message:&quot; + msg); ack.acknowledge(); &#125; &#125; @KafkaListener(topics = ProductDemo.TOPIC_TEST, groupId = ProductDemo.TOPIC_GROUP2) public void topic_test1(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) &#123; Optional message = Optional.ofNullable(record.value()); if (message.isPresent()) &#123; Object msg = message.get(); log.info(&quot;topic_test1 消费了： Topic:&quot; + topic + &quot;,Message:&quot; + msg); ack.acknowledge(); &#125; &#125;&#125; 测试接口： 123456789@Resourceprivate ProductDemo productDemo;@GetMapping(&quot;/kafka/test&quot;)public void testKafka()&#123; logger.info(&quot;start test&quot;); productDemo.send(&quot;hello kafka&quot;); logger.info(&quot;end test&quot;);&#125; 山脚太拥挤 我们更高处见。","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"}]},{"title":"docker整体架构图","date":"2021-07-28T08:20:03.000Z","path":"wiki/docker整体架构图/","text":"Docker的总架构图 docker是一个C/S模式的架构，后端是一个松耦合架构，模块各司其职。 1、用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。2、Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；3、Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。4、Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；5、当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；6、当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。7、libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"}]},{"title":"docker常用手册","date":"2021-07-28T07:54:20.000Z","path":"wiki/docker常用手册/","text":"docker中文文档 http://www.dockerinfo.net/documentdocker doc https://docs.docker.com/engine/reference/commandline/docker/docker 中文社区 https://www.docker.org.cn/ 搜索可用镜像docker search tutorial 检查运行的镜像docker inspect efe 发布镜像docker push 下载镜像docker pull 在容器中安装新的程序apt-get updateapt-get install vim 保存对容器的修改docker commit pid","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"}]},{"title":"docker-compost安装mongodb","date":"2021-07-28T07:24:01.000Z","path":"wiki/docker-compost安装mongodb/","text":"mongo 配置文件 -&gt; https://www.cnblogs.com/xibuhaohao/p/12580331.html docker-compose 配置文件123456789mongo: image: mongo:4.4.7 #根据需要选择自己的镜像 ports: - 27017:27017 #对外暴露停供服务的端口，正式生产的时候理论不用暴露。 volumes: - ./mongodb/data/db:/data/db # 挂载数据目录 - ./mongodb/data/log:/var/log/mongodb # 挂载日志目录 - ./mongodb/data/config:/etc/mongo # 挂载配置目录 # command: --config /docker/mongodb/mongod.conf # 配置文件 按照上面👆配置文件设置目录/data/db/mongodb/datals -lconfig db log mongo 配置文件1234567891011121314151617181920212223242526272829303132333435363738# Where and how to store data.storage: dbPath: /data/db/mongodb/data/db journal: enabled: true# engine:# mmapv1:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /data/db/mongodb/data/log# network interfacesnet: port: 27017 bindIp: 0.0.0.0# how the process runsprocessManagement: timeZoneInfo: /usr/share/zoneinfo#security:#operationProfiling:#replication:#sharding:## Enterprise-Only Options:#auditLog:#snmp: bindIp: 0.0.0.0 允许远程访问 docker-compose启动mongodocker-compose up -ddocker ps 进入dockerdocker psdocker exec -it xxxxxxxxxx bash mongo创建数据库登录mongo 查看数据库show dbs 创建数据库use wechat_spider 然后 db 查看 创建用户1234567db.createUser( &#123; user:&quot;wechat&quot;, pwd:&quot;123456&quot;, roles:[&#123;role:&quot;readWrite&quot;,db:&quot;wechat_spider&quot;&#125;] &#125; ) Java客户端链接配置mvn12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;pring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt;&lt;/dependency&gt; 配置文件12345678910111213spring: data: mongodb: username: &#x27;wechat&#x27; password: &#x27;123456&#x27;# port: 3333 port: 27017 database: wechat_spider# host: 123.56.77.177 host: 39.107.117.232 repositories: type: auto domain1234567891011121314151617181920212223242526272829import org.springframework.data.mongodb.core.mapping.Document;/** * @Author gaolei * @Date 2021/7/28 上午10:02 * @Version 1.0 */@Document(collection = &quot;passenger&quot;)public class Passenger &#123; private String name; private String password; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; controller12345678910111213141516171819@RestControllerpublic class TestContoller &#123; @Autowired private MongoTemplate mongoTemplate; @RequestMapping(&quot;/insert&quot;) public String insert() &#123; Passenger passenger = new Passenger(); passenger.setName(&quot;hello&quot;); passenger.setPassword(&quot;world1&quot;); passenger = mongoTemplate.insert(passenger); if (passenger != null) &#123; return &quot;success&quot;; &#125; else &#123; return &quot;false&quot;; &#125; &#125;&#125;","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://example.com/tags/mongodb/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MongoDB","slug":"DataBase/MongoDB","permalink":"http://example.com/categories/DataBase/MongoDB/"}]},{"title":"基于BIO实现简易tomcat","date":"2021-07-28T07:11:55.000Z","path":"wiki/基于BIO实现简易tomcat/","text":"基于传统的BIO手写一个简易Tomcat本文主要基于传统的BIO来实现一个简单的Http请求处理过程；1、Servlet请求无非就是doGet/doPost，所以我们定义抽象Servlet记忆GET/POST方法；2、基于Socket和ServerSocket实现CS通信；3、模拟Spring加载配置文件，注册请求以及控制器； GlRequest 封装一个请求 当然是一个很简单的请求，这里只处理请求的URL和请求方法；获取请求，也就是输入流，解析数据Url和Method，并做相应的处理； 12345678910111213141516171819202122232425262728293031public class GlRequest &#123; private String url; private String method; public GlRequest(InputStream is) &#123; try &#123; // 解析http请求的具体内容； String content = &quot;&quot;; byte[] buff = new byte[1024]; int len = 0; if ((len = is.read(buff)) &gt; 0) &#123; content = new String(buff, 0, len); &#125; String line = content.split(&quot;\\\\n&quot;)[0]; String [] arr = line.split(&quot;\\\\s&quot;); this.method = arr[0]; this.url = arr[1].split(&quot;\\\\?&quot;)[0]; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public String getUrl() &#123; return this.url; &#125; public String getMethod() &#123; return this.method; &#125;&#125; GlResponse 定义返回值response 处理请求返回值，将业务处理的结果通过输出流输出；输出大致分为两部分，第一是返回的数据，第二是返回数据的Header; 1234567891011121314151617public class GlResponse &#123; private OutputStream outputStream; public GlResponse(OutputStream os) &#123; this.outputStream = os; &#125; public void write(String string) throws Exception &#123; StringBuffer sb = new StringBuffer(); sb.append(&quot;HTTP/1.1 200 OK\\n&quot;) .append(&quot;Content-Type: text/html;\\n&quot;) .append(&quot;\\r\\n&quot;) .append(string); outputStream.write(sb.toString().getBytes()); &#125;&#125; GlServlet 定义抽象servlet，定义GET方法和POST方法 定义抽象的Servlet和doGet方法和doPost方法，具体的业务去实现自己的方法和逻辑； 12345678910111213141516public abstract class GlServlet &#123; private final static String GET = &quot;GET&quot;; public void service(GlRequest request, GlResponse response) throws Exception &#123; if (GET.equals(request.getMethod())) &#123; doGet(request, response); &#125; else &#123; doPost(request, response); &#125; &#125; public abstract void doGet(GlRequest request, GlResponse response) throws Exception; public abstract void doPost(GlRequest request, GlResponse response) throws Exception; &#125; FirstServlet 具体的业务Servlet实现抽象Servlet的方法12345678910111213public class FirstServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; // 具体的逻辑 this.doPost(request, response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This is first servlet from BIO&quot;); &#125;&#125; SecondServlet 具体的业务Servlet实现抽象Servlet方法123456789101112public class SecondServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; doPost(request,response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This second request form BIO&quot;); &#125;&#125; web-bio.properties 配置文件 配置请求和处理器，Spring中是通过Controller下的@XXXMapping注解去扫描并加载到工厂的； 12345servlet.one.className=com.ibli.netty.tomcat.bio.servlet.FirstServletservlet.one.url=/firstServlet.doservlet.two.className=com.ibli.netty.tomcat.bio.servlet.SecondServletservlet.two.url=/secondServlet.do GlTomcat测试类 启动服务端，在网页中访问本地8080端口，输入配置文件中定义的url进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class GlTomcat &#123; private ServerSocket server; private final Integer PORT = 8080; private Properties webXml = new Properties(); private Map&lt;String, GlServlet&gt; servletMapping = new HashMap&lt;String, GlServlet&gt;(); /** * 模拟项目main方法，启动加载配置 * * @param args 启动参数 */ public static void main(String[] args) &#123; new GlTomcat().start(); &#125; /** * Tomcat的启动入口 */ private void start() &#123; //1、加载web配置文件，解析配置 init(); //2、启动服务器socket，等待用户请求 try &#123; server = new ServerSocket(this.PORT); System.err.println(&quot;Gl tomcat started in port &quot; + this.PORT); while (true) &#123; Socket client = server.accept(); // 3、获得请求信息，解析HTTP协议的内容 process(client); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 加载配置文件 */ private void init() &#123; try &#123; String WEB_INF = this.getClass().getResource(&quot;/&quot;).getPath(); FileInputStream fis = new FileInputStream(WEB_INF + &quot;web-bio.properties&quot;); webXml.load(fis); for (Object k : webXml.keySet()) &#123; String key = k.toString(); if (key.endsWith(&quot;.url&quot;)) &#123; //servlet.two.url String servletName = key.replaceAll(&quot;\\\\.url&quot;, &quot;&quot;); String url = webXml.getProperty(key); //servlet.two.className String className = webXml.getProperty(servletName + &quot;.className&quot;); //反射创建servlet实例 // load-on-startup &gt;=1 :web启动的时候初始化 0：用户请求的时候才启动 GlServlet obj = (GlServlet) Class.forName(className).newInstance(); // 将url和servlet建立映射关系 servletMapping.put(url, obj); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 解析客户端请求 * * @param client 客户端 */ private void process(Socket client) throws Exception &#123; InputStream is = null; OutputStream os = null; try &#123; //请求 is = client.getInputStream(); //封装返回值 os = client.getOutputStream(); GlRequest request = new GlRequest(is); GlResponse response = new GlResponse(os); String url = request.getUrl(); if (servletMapping.containsKey(url)) &#123; servletMapping.get(url).service(request, response); &#125; else &#123; response.write(&quot;404 Not found!&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (os != null) &#123; os.flush(); os.close(); &#125; if (is != null) &#123; is.close(); client.close(); &#125; &#125; &#125;&#125; 打印请求信息123456789101112Request content : GET /fitstServlet.do HTTP/1.1Host: localhost:8080Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 OPR/74.0.3911.160Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9Sec-Fetch-Site: noneSec-Fetch-Mode: navigateSec-Fetch-User: ?1Sec-Fetch-Dest: documentAccept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9 客户端发送请求及结果展示 请求： http://localhost:8080/firstServlet.do","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"netty实现简易RPC调用","date":"2021-07-27T12:01:08.000Z","path":"wiki/netty实现简易RPC调用/","text":"基于Netty手写一个RPC简易远程调用 抽象协议12345678910111213141516171819202122@Datapublic class InvokerProtocol implements Serializable &#123; // 基于二进制流调用协议 /** * 类名 */ private String className; /** * 方法名 */ private String methodName; /** * 形参 */ private Class&lt;?&gt;[] params; /** * 实参 */ private Object[] values;&#125; 注册中心RpcRegistry 基于Netty实现的RPC注册中心 1、 ServerBootstrap 启动8080端口，等待客户端链接；2、 RegisterHandler用来处理RPC接口的发现和注册； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class RpcRegistry &#123; private Integer post; public RpcRegistry(Integer post) &#123; this.post = post; &#125; private void start() &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap(); server.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; // 接受客户端请求的处理 ChannelPipeline pipeline = ch.pipeline(); //配置通用解码器 int fieldLength = 4; pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, fieldLength, 0, fieldLength)); pipeline.addLast(new LengthFieldPrepender(fieldLength)); //对象编码器 pipeline.addLast(&quot;encoder&quot;, new ObjectEncoder()); pipeline.addLast(&quot;decoder&quot;, new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(new RegisterHandler()); &#125; &#125;) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); try &#123; ChannelFuture future = server.bind(this.post).sync(); System.out.println(&quot;Rpc registry started in port &quot; + this.post); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; new RpcRegistry(8080).start(); &#125; &#125; RegisterHandler 执行RPC的发现和注册 1、扫描固定包下或者路径下的类;2、接口为key，具体实例作为value； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class RegisterHandler extends ChannelInboundHandlerAdapter &#123; /** * 注册中心容器 */ private static final ConcurrentHashMap&lt;String, Object&gt; REGISTRY_MAP = new ConcurrentHashMap&lt;String, Object&gt;(); private List&lt;String&gt; classNameList = new ArrayList&lt;String&gt;(); public RegisterHandler() &#123; // 1、扫描所有需要注册的类 scannerClass(&quot;com.ibli.netty.rpc.provider&quot;); // 执行注册 doRegistry(); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Object result; InvokerProtocol request = (InvokerProtocol) msg; if (REGISTRY_MAP.containsKey(request.getClassName())) &#123; Object provider = REGISTRY_MAP.get(request.getClassName()); Method method = provider.getClass().getMethod(request.getMethodName(), request.getParams()); result = method.invoke(provider, request.getValues()); ctx.write(result); ctx.flush(); ctx.close(); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; private void doRegistry() &#123; if (classNameList.isEmpty()) &#123; return; &#125; for (String className : classNameList) &#123; try &#123; Class&lt;?&gt; clazz = Class.forName(className); Class&lt;?&gt; i = clazz.getInterfaces()[0]; REGISTRY_MAP.put(i.getName(), clazz.newInstance()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void scannerClass(String packageName) &#123; URL url = this.getClass().getClassLoader().getResource(packageName.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File dir = new File(url.getFile()); for (File file : dir.listFiles()) &#123; if (file.isDirectory()) &#123; scannerClass(packageName + &quot;.&quot; + file.getName()); &#125; else &#123; classNameList.add(packageName + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;).trim()); &#125; &#125; &#125;&#125; API以及实现RPC接口 定义一个简单的服务接口 作为一个微服务对外暴露的API; 123456789public interface IRpcService &#123; int add(int a, int b); int mul(int a, int b); int sub(int a, int b); int div(int a, int b);&#125; RPC接口实现 provider实现具体的接口，提供具体的服务； 1234567891011121314151617public class RpcServiceImpl implements IRpcService &#123; public int add(int a, int b) &#123; return a + b; &#125; public int mul(int a, int b) &#123; return a * b; &#125; public int sub(int a, int b) &#123; return a - b; &#125; public int div(int a, int b) &#123; return a / b; &#125;&#125; RPC调用方调用RPC12345678public class RpcConsumer &#123; public static void main(String[] args) &#123; IRpcService rpc = RpcProxy.create(IRpcService.class); System.err.println(rpc.add(1,3)); System.err.println(rpc.mul(3,3)); System.err.println(rpc.sub(14,3)); &#125;&#125; RpcProxy 动态代理对象请求RPC 通过Netty Bootstrap访问8080端口； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class RpcProxy &#123; public static &lt;T&gt; T create(Class&lt;?&gt; clazz) &#123; MethodProxy proxy = new MethodProxy(clazz); Class&lt;?&gt;[] interfaces = clazz.isInterface() ? new Class[]&#123;clazz&#125; : clazz.getInterfaces(); T result = (T) Proxy.newProxyInstance(clazz.getClassLoader(), interfaces, proxy); return result; &#125; public static class MethodProxy implements InvocationHandler &#123; private Class&lt;?&gt; clazz; public MethodProxy(Class&lt;?&gt; clazz) &#123; this.clazz = clazz; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); &#125; else &#123; return rpcInvoke(proxy, method, args); &#125; &#125; private Object rpcInvoke(Object proxy, Method method, Object[] args) &#123; //封装请求的内容 InvokerProtocol msg = new InvokerProtocol(); msg.setClassName(this.clazz.getName()); msg.setMethodName(method.getName()); msg.setParams(method.getParameterTypes()); msg.setValues(args); final RpcProxyHandler consumerHandler = new RpcProxyHandler(); EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap client = new Bootstrap(); client.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; //接收课客户端请求的处理流程 ChannelPipeline pipeline = ch.pipeline(); int fieldLength = 4; //通用解码器设置 pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, fieldLength, 0, fieldLength)); //通用编码器 pipeline.addLast(new LengthFieldPrepender(fieldLength)); //对象编码器 pipeline.addLast(&quot;encoder&quot;, new ObjectEncoder()); //对象解码器 pipeline.addLast(&quot;decoder&quot;, new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(&quot;handler&quot;, consumerHandler); &#125; &#125;) .option(ChannelOption.TCP_NODELAY, true); ChannelFuture future = client.connect(&quot;localhost&quot;, 8080).sync(); future.channel().writeAndFlush(msg).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; group.shutdownGracefully(); &#125; return consumerHandler.getResponse(); &#125; &#125;&#125; RPC调用方接受并处理调用结果123456789101112131415161718public class RpcProxyHandler extends ChannelInboundHandlerAdapter &#123; private Object response; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; this.response = msg; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); &#125; public Object getResponse() &#123; return this.response; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"netty实现简易tomcat","date":"2021-07-27T11:54:15.000Z","path":"wiki/netty实现简易tomcat/","text":"基于Netty手写一个简易的Tomcat容器本文主要基于传统的BIO来实现一个简单的Http请求处理过程；1、Servlet请求无非就是doGet/doPost，所以我们定义抽象Servlet记忆GET/POST方法；2、基于Netty API实现CS通信；3、模拟Spring加载配置文件，注册请求以及控制器； Netty版本12345&lt;dependency&gt; &lt;groupId&gt;o.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.6.Final&lt;/version&gt;&lt;/dependency&gt; GlRequest 基于Netty&amp;HttpRequest的API操作，非常简单1234567891011121314151617181920212223242526272829303132public class GlRequest &#123; private ChannelHandlerContext ctx; private HttpRequest req; public GlRequest(ChannelHandlerContext ctx, HttpRequest req) &#123; this.ctx = ctx; this.req = req; &#125; public String getUrl() &#123; return this.req.uri(); &#125; public String getMethod() &#123; return this.req.method().name(); &#125; public Map&lt;String, List&lt;String&gt;&gt; getParams() &#123; QueryStringDecoder decoder = new QueryStringDecoder(req.uri()); return decoder.parameters(); &#125; public String getParam(String name) &#123; Map&lt;String, List&lt;String&gt;&gt; params = getParams(); List&lt;String&gt; strings = params.get(name); if (strings == null) &#123; return null; &#125; return strings.get(0); &#125;&#125; GlResponse 基于Netty&amp;FullHttpResponse的API操作 FullHttpResponse作为返回请求的主体； 123456789101112131415161718192021222324252627282930313233public class GlResponse &#123; private ChannelHandlerContext ctx; private HttpRequest req; public GlResponse(ChannelHandlerContext ctx, HttpRequest req) &#123; this.req = req; this.ctx = ctx; &#125; public void write(String string) throws Exception &#123; if (string == null || string.length() == 0) &#123; return; &#125; try &#123; FullHttpResponse response = new DefaultFullHttpResponse( HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(string.getBytes(&quot;UTF-8&quot;)) ); response.headers().set(&quot;Content-Type&quot;, &quot;text/html&quot;); ctx.write(response); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; ctx.flush(); ctx.close(); &#125; &#125;&#125; GlServlet 定义抽象servlet，定义GET方法和POST方法 定义抽象的Servlet和doGet方法和doPost方法，具体的业务去实现自己的方法和逻辑； 12345678910111213141516public abstract class GlServlet &#123; private final static String GET = &quot;GET&quot;; public void service(GlRequest request, GlResponse response) throws Exception &#123; if (GET.equals(request.getMethod())) &#123; doGet(request, response); &#125; else &#123; doPost(request, response); &#125; &#125; public abstract void doGet(GlRequest request, GlResponse response) throws Exception; public abstract void doPost(GlRequest request, GlResponse response) throws Exception; &#125; FirstServlet 具体的业务Servlet实现抽象Servlet的方法12345678910111213public class FirstServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; // 具体的逻辑 this.doPost(request, response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This is first servlet from NIO&quot;); &#125;&#125; SecondServlet 具体的业务Servlet实现抽象Servlet方法123456789101112public class SecondServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; doPost(request,response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This second request form NIO&quot;); &#125;&#125; web-nio.properties 配置文件 配置请求和处理器，Spring中是通过Controller下的@XXXMapping注解去扫描并加载到工厂的； 12345servlet.one.className=com.ibli.netty.tomcat.nio.servlet.FirstServletservlet.one.url=/firstServlet.doservlet.two.className=com.ibli.netty.tomcat.nio.servlet.SecondServletservlet.two.url=/secondServlet.do GlTomcat 启动服务端，在网页中访问本地8080端口，输入配置文件中定义的url进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class GlTomcat &#123; private final Integer PORT = 8080; private Properties webXml = new Properties(); private Map&lt;String, GlServlet&gt; servletMapping = new HashMap&lt;String, GlServlet&gt;(); public static void main(String[] args) &#123; new GlTomcat().start(); &#125; /** * Tomcat的启动入口 */ private void start() &#123; //1、加载web配置文件，解析配置 init(); // Boss线程 EventLoopGroup bossGroup = new NioEventLoopGroup(); // Worker线程 EventLoopGroup workGroup = new NioEventLoopGroup(); //2、创建Netty服务端对象 ServerBootstrap server = new ServerBootstrap(); //3、 配置服务端参数 server.group(bossGroup, workGroup) // 配置主线程的处理逻辑 .channel(NioServerSocketChannel.class) // 子线程的回调逻辑 .childHandler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel client) &#123; // 处理具体的回调逻辑 // 责任链模式 //返回-编码 client.pipeline().addLast(new HttpResponseEncoder()); //请求-解码 client.pipeline().addLast(new HttpRequestDecoder()); //用户自己的逻辑处理 client.pipeline().addLast(new GlTomcatHandler()); &#125; &#125;) // 配置主线程可分配的最大线程数 .option(ChannelOption.SO_BACKLOG, 128) //保持长链接 .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = null; try &#123; future = server.bind(this.PORT).sync(); System.err.println(&quot;Gl tomcat started in pory &quot; + this.PORT); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; /** * 加载配置文件 * 这其实使用了策略模式 */ private void init() &#123; try &#123; String WEB_INF = this.getClass().getResource(&quot;/&quot;).getPath(); FileInputStream fis = new FileInputStream(WEB_INF + &quot;web-nio.properties&quot;); webXml.load(fis); for (Object k : webXml.keySet()) &#123; String key = k.toString(); if (key.endsWith(&quot;.url&quot;)) &#123; //servlet.two.url String servletName = key.replaceAll(&quot;\\\\.url&quot;, &quot;&quot;); String url = webXml.getProperty(key); //servlet.two.className String className = webXml.getProperty(servletName + &quot;.className&quot;); //反射创建servlet实例 // load-on-startup &gt;=1 :web启动的时候初始化 0：用户请求的时候才启动 GlServlet obj = (GlServlet) Class.forName(className).newInstance(); // 将url和servlet建立映射关系 servletMapping.put(url, obj); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 处理用户请求 */ public class GlTomcatHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof HttpRequest) &#123; HttpRequest req = (HttpRequest) msg; GlRequest request = new GlRequest(ctx,req); GlResponse response = new GlResponse(ctx,req); String url = request.getUrl(); if (servletMapping.containsKey(url))&#123; servletMapping.get(url).service(request,response); &#125; else &#123; response.write(&quot;404 Not Fount&quot;); &#125; &#125; &#125; &#125;&#125; 测试结果 请求 : http://localhost:8080/secoundServlet.do 这的地址写错误 ⚠️ 请求 : http://localhost:8080/secondServlet.do","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java位运算","date":"2021-07-27T11:47:25.000Z","path":"wiki/Java位运算/","text":"在计算机中所有数据都是以二进制的形式储存的。位运算其实就是直接对在内存中的二进制数据进行操作，因此处理数据的速度非常快。在实际编程中，如果能巧妙运用位操作，完全可以达到四两拨千斤的效果，正因为位操作的这些优点，所以位操作在各大IT公司的笔试面试中一直是个热点问题。 位操作基础基本的位操作符有与、或、异或、取反、左移、右移这6种，它们的运算规则如下所示： 在这6种操作符，只有~取反是单目操作符，其它5种都是双目操作符。 位操作只能用于整形数据，对float和double类型进行位操作会被编译器报错。 位操作符的运算优先级比较低，因为尽量使用括号来确保运算顺序，否则很可能会得到莫明其妙的结果。比如要得到像1，3， 5，9这些2^i+1的数字。写成int a = 1 « i + 1;是不对的，程序会先执行i + 1，再执行左移操作。应该写成int a = (1 « i) + 1; 另外位操作还有一些复合操作符，如&amp;=、|=、 ^=、«=、»=。 12345678package com.king.bit;public class BitMain &#123; public static void main(String [] args) &#123; int a = -15, b = 15; System.out.println(a &gt;&gt; 2); // -4：-15 = 1111 0001(二进制)，右移二位，最高位由符号位填充将得到1111 1100即-4 System.out.println(b &gt;&gt; 2); // 3：15=0000 1111(二进制)，右移二位，最高位由符号位填充将得到0000 0011即3 &#125;&#125; 常用位操作小技巧下面对位操作的一些常见应用作个总结，有判断奇偶、交换两数、变换符号及求绝对值。这些小技巧应用易记，应当熟练掌握。 判断奇偶只要根据最未位是0还是1来决定，为0就是偶数，为1就是奇数。因此可以用if ((a &amp; 1) == 0)代替if (a % 2 == 0)来判断a是不是偶数。下面程序将输出0到100之间的所有偶数： 12345for (int i = 0; i &lt; 100; i ++) &#123; if ((i &amp; 1) == 0) &#123; // 偶数 System.out.println(i); &#125;&#125; 交换两数123456int c = 1, d = 2;c ^= d;d ^= c;c ^= d;System.out.println(&quot;c=&quot; + c);System.out.println(&quot;d=&quot; + d); 可以这样理解： 第一步 a=b 即a=(ab)；第二步 b=a 即b=b(ab)，由于运算满足交换律，b(ab)=bba。由于一个数和自己异或的结果为0并且任何数与0异或都会不变的，所以此时b被赋上了a的值；第三步 a=b 就是a=ab，由于前面二步可知a=(ab)，b=a，所以a=ab即a=(ab)a。故a会被赋上b的值； 变换符号变换符号就是正数变成负数，负数变成正数。如对于-11和11，可以通过下面的变换方法将-11变成11 11111 0101(二进制) –取反-&gt; 0000 1010(二进制) –加1-&gt; 0000 1011(二进制) 同样可以这样的将11变成-11 10000 1011(二进制) –取反-&gt; 0000 0100(二进制) –加1-&gt; 1111 0101(二进制) 因此变换符号只需要取反后加1即可。完整代码如下： 123int a = -15, b = 15;System.out.println(~a + 1);System.out.println(~b + 1); 求绝对值位操作也可以用来求绝对值，对于负数可以通过对其取反后加1来得到正数。对-6可以这样： 11111 1010(二进制) –取反-&gt;0000 0101(二进制) -加1-&gt; 0000 0110(二进制) 来得到6。 因此先移位来取符号位，int i = a » 31;要注意如果a为正数，i等于0，为负数，i等于-1。然后对i进行判断——如果i等于0，直接返回。否之，返回~a+1。完整代码如下： 12int i = a &gt;&gt; 31;System.out.println(i == 0 ? a : (~a + 1)); 现在再分析下。对于任何数，与0异或都会保持不变，与-1即0xFFFFFFFF异或就相当于取反。因此，a与i异或后再减i（因为i为0或-1，所以减i即是要么加0要么加1）也可以得到绝对值。所以可以对上面代码优化下： 12int j = a &gt;&gt; 31;System.out.println((a ^ j) - j); 注意这种方法没用任何判断表达式，而且有些笔面试题就要求这样做，因此建议读者记住该方法（_讲解过后应该是比较好记了）。 位操作与空间压缩筛素数法在这里不就详细介绍了，本文着重对筛素数法所使用的素数表进行优化来减小其空间占用。要压缩素数表的空间占用，可以使用位操作。下面是用筛素数法计算100以内的素数示例代码（注2）： 1234567891011121314151617// 打印100以内素数：// （1）对每个素数，它的倍数必定不是素数；// （2）有很多重复访问如flag[10]会在访问flag[2]和flag[5]时各访问一次；int max = 100;boolean[] flags = new boolean[max];int [] primes = new int[max / 3 + 1];int pi = 0;for (int m = 2; m &lt; max ; m ++) &#123; if (!flags[m]) &#123; primes[pi++] = m; for(int n = m; n &lt; max; n += m) &#123; flags[n] = true; &#125; &#125;&#125;System.out.println(Arrays.toString(primes)); 运行结果如下： 1[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0] 在上面程序是用bool数组来作标记的，bool型数据占1个字节（8位），因此用位操作来压缩下空间占用将会使空间的占用减少八分之七。 下面考虑下如何在数组中对指定位置置1，先考虑如何对一个整数在指定位置上置1。对于一个整数可以通过将1向左移位后与其相或来达到在指定位上置1的效果，代码如下所示： 1234// 在一个数指定位上置1int e = 0;e |= 1 &lt;&lt; 10;System.out.println(e); 同样，可以1向左移位后与原数相与来判断指定位上是0还是1（也可以将原数右移若干位再与1相与）。 12345//判断指定位上是0还是1if ((e &amp; (1 &lt;&lt; 10)) != 0) System.out.println(&quot;指定位上为1&quot;);else System.out.println(&quot;指定位上为0&quot;); 扩展到数组上，我们可以采用这种方法，因为数组在内存上也是连续分配的一段空间，完全可以“认为”是一个很长的整数。先写一份测试代码，看看如何在数组中使用位操作： 1234567891011int[] bits = new int[40];for (int m = 0; m &lt; 40; m += 3) &#123; bits[m / 32] |= (1 &lt;&lt; (m % 32));&#125;// 输出整个bitsfor (int m = 0; m &lt; 40; m++) &#123; if (((bits[m / 32] &gt;&gt; (m % 32)) &amp; 1) != 0) System.out.print(&#x27;1&#x27;); else System.out.print(&#x27;0&#x27;);&#125; 运行结果如下： 11001001001001001001001001001001001001001 可以看出该数组每3个就置成了1，证明我们上面对数组进行位操作的方法是正确的。因此可以将上面筛素数方法改成使用位操作压缩后的筛素数方法： 12345678910111213int[] flags2 = new int[max / 32 + 1];pi = 0;for (int m = 2; m &lt; max ; m ++) &#123; if ((((flags2[m / 32] &gt;&gt; (m % 32)) &amp; 1) == 0)) &#123; primes[pi++] = m; for(int n = m; n &lt; max; n += m) &#123; flags2[n / 32] |= (1 &lt;&lt; (n % 32)); &#125; &#125;&#125; System.out.println();System.out.println(Arrays.toString(primes)); 运行结果如下： 1[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0] 位操作工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.king.bit; /** * Java 位运算的常用方法封装 */public class BitUtils &#123; /** * 获取运算数指定位置的值 * 例如： 0000 1011 获取其第 0 位的值为 1, 第 2 位 的值为 0 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @return 指定位置的值(0 or 1) */ public static byte getBitValue(byte source, int pos) &#123; return (byte) ((source &gt;&gt; pos) &amp; 1); &#125; /** * 将运算数指定位置的值置为指定值 * 例: 0000 1011 需要更新为 0000 1111, 即第 2 位的值需要置为 1 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @param value * 只能取值为 0, 或 1, 所有大于0的值作为1处理, 所有小于0的值作为0处理 * * @return 运算后的结果数 */ public static byte setBitValue(byte source, int pos, byte value) &#123; byte mask = (byte) (1 &lt;&lt; pos); if (value &gt; 0) &#123; source |= mask; &#125; else &#123; source &amp;= (~mask); &#125; return source; &#125; /** * 将运算数指定位置取反值 * 例： 0000 1011 指定第 3 位取反, 结果为 0000 0011; 指定第2位取反, 结果为 0000 1111 * * @param source * * @param pos * 指定位置 (0&lt;=pos&lt;=7) * * @return 运算后的结果数 */ public static byte reverseBitValue(byte source, int pos) &#123; byte mask = (byte) (1 &lt;&lt; pos); return (byte) (source ^ mask); &#125; /** * 检查运算数的指定位置是否为1 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @return true 表示指定位置值为1, false 表示指定位置值为 0 */ public static boolean checkBitValue(byte source, int pos) &#123; source = (byte) (source &gt;&gt;&gt; pos); return (source &amp; 1) == 1; &#125; /** * 入口函数做测试 * * @param args */ public static void main(String[] args) &#123; // 取十进制 11 (二级制 0000 1011) 为例子 byte source = 11; // 取第2位值并输出, 结果应为 0000 1011 for (byte i = 7; i &gt;= 0; i--) &#123; System.out.printf(&quot;%d &quot;, getBitValue(source, i)); &#125; // 将第6位置为1并输出 , 结果为 75 (0100 1011) System.out.println(&quot;\\n&quot; + setBitValue(source, 6, (byte) 1)); // 将第6位取反并输出, 结果应为75(0100 1011) System.out.println(reverseBitValue(source, 6)); // 检查第6位是否为1，结果应为false System.out.println(checkBitValue(source, 6)); // 输出为1的位, 结果应为 0 1 3 for (byte i = 0; i &lt; 8; i++) &#123; if (checkBitValue(source, i)) &#123; System.out.printf(&quot;%d &quot;, i); &#125; &#125; &#125;&#125; BitSet类BitSet类：大小可动态改变, 取值为true或false的位集合。用于表示一组布尔标志。 此类实现了一个按需增长的位向量。位 set 的每个组件都有一个 boolean 值。用非负的整数将 BitSet 的位编入索引。可以对每个编入索引的位进行测试、设置或者清除。通过逻辑与、逻辑或和逻辑异或操作，可以使用一个 BitSet 修改另一个 BitSet 的内容。默认情况下，set 中所有位的初始值都是 false。 每个位 set 都有一个当前大小，也就是该位 set 当前所用空间的位数。注意，这个大小与位 set 的实现有关，所以它可能随实现的不同而更改。位 set 的长度与位 set 的逻辑长度有关，并且是与实现无关而定义的。 除非另行说明，否则将 null 参数传递给 BitSet 中的任何方法都将导致 NullPointerException。 在没有外部同步的情况下，多个线程操作一个 BitSet 是不安全的。 构造函数: BitSet() or BitSet(int nbits)，默认初始大小为64。 123456789101112131415161718192021222324252627282930public void set(int pos): 位置pos的字位设置为true。public void set(int bitIndex, boolean value): 将指定索引处的位设置为指定的值。public void clear(int pos): 位置pos的字位设置为false。public void clear(): 将此 BitSet 中的所有位设置为 false。public int cardinality(): 返回此 BitSet 中设置为 true 的位数。public boolean get(int pos): 返回位置是pos的字位值。public void and(BitSet other): other同该字位集进行与操作，结果作为该字位集的新值。public void or(BitSet other): other同该字位集进行或操作，结果作为该字位集的新值。public void xor(BitSet other): other同该字位集进行异或操作，结果作为该字位集的新值。public void andNot(BitSet set): 清除此 BitSet 中所有的位,set – 用来屏蔽此 BitSet 的 BitSetpublic int size(): 返回此 BitSet 表示位值时实际使用空间的位数。public int length(): 返回此 BitSet 的“逻辑大小”：BitSet 中最高设置位的索引加 1。public int hashCode(): 返回该集合Hash 码， 这个码同集合中的字位值有关。public boolean equals(Object other): 如果other中的字位同集合中的字位相同，返回true。public Object clone(): 克隆此 BitSet，生成一个与之相等的新 BitSet。public String toString(): 返回此位 set 的字符串表示形式。 例1：标明一个字符串中用了哪些字符 1234567891011121314151617181920212223242526package com.king.bit;import java.util.BitSet;public class WhichChars &#123; private BitSet used = new BitSet(); public WhichChars(String str) &#123; for (int i = 0; i &lt; str.length(); i++) used.set(str.charAt(i)); // set bit for char &#125; public String toString() &#123; String desc = &quot;[&quot;; int size = used.size(); for (int i = 0; i &lt; size; i++) &#123; if (used.get(i)) desc += (char) i; &#125; return desc + &quot;]&quot;; &#125; public static void main(String args[]) &#123; WhichChars w = new WhichChars(&quot;How do you do&quot;); System.out.println(w); &#125;&#125; 例2： 1234567891011121314151617181920package com.king.bit;import java.util.BitSet;public class MainTestThree &#123; /** * @param args */ public static void main(String[] args) &#123; BitSet bm = new BitSet(); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(0); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(1); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); System.out.println(bm.get(65)); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(65); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); &#125;&#125; 例3： 12345678910111213141516171819202122package com.king.bit;import java.util.BitSet;public class MainTestFour &#123; /** * @param args */ public static void main(String[] args) &#123; BitSet bm1 = new BitSet(7); System.out.println(bm1.isEmpty() + &quot;--&quot; + bm1.size()); BitSet bm2 = new BitSet(63); System.out.println(bm2.isEmpty() + &quot;--&quot; + bm2.size()); BitSet bm3 = new BitSet(65); System.out.println(bm3.isEmpty() + &quot;--&quot; + bm3.size()); BitSet bm4 = new BitSet(111); System.out.println(bm4.isEmpty() + &quot;--&quot; + bm4.size()); &#125; &#125; 位操作技巧123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 1. 获得int型最大值System.out.println((1 &lt;&lt; 31) - 1);// 2147483647， 由于优先级关系，括号不可省略System.out.println(~(1 &lt;&lt; 31));// 2147483647 // 2. 获得int型最小值System.out.println(1 &lt;&lt; 31);System.out.println(1 &lt;&lt; -1); // 3. 获得long类型的最大值System.out.println(((long)1 &lt;&lt; 127) - 1); // 4. 乘以2运算System.out.println(10&lt;&lt;1); // 5. 除以2运算(负奇数的运算不可用)System.out.println(10&gt;&gt;1); // 6. 乘以2的m次方System.out.println(10&lt;&lt;2); // 7. 除以2的m次方System.out.println(16&gt;&gt;2); // 8. 判断一个数的奇偶性System.out.println((10 &amp; 1) == 1);System.out.println((9 &amp; 1) == 1); // 9. 不用临时变量交换两个数（面试常考）a ^= b;b ^= a;a ^= b; // 10. 取绝对值（某些机器上，效率比n&gt;0 ? n:-n 高）int n = -1;System.out.println((n ^ (n &gt;&gt; 31)) - (n &gt;&gt; 31));/* n&gt;&gt;31 取得n的符号，若n为正数，n&gt;&gt;31等于0，若n为负数，n&gt;&gt;31等于-1若n为正数 n^0-0数不变，若n为负数n^-1 需要计算n和-1的补码，异或后再取补码，结果n变号并且绝对值减1，再减去-1就是绝对值 */ // 11. 取两个数的最大值（某些机器上，效率比a&gt;b ? a:b高）System.out.println(b&amp;((a-b)&gt;&gt;31) | a&amp;(~(a-b)&gt;&gt;31)); // 12. 取两个数的最小值（某些机器上，效率比a&gt;b ? b:a高）System.out.println(a&amp;((a-b)&gt;&gt;31) | b&amp;(~(a-b)&gt;&gt;31)); // 13. 判断符号是否相同(true 表示 x和y有相同的符号， false表示x，y有相反的符号。)System.out.println((a ^ b) &gt; 0); // 14. 计算2的n次方 n &gt; 0System.out.println(2&lt;&lt;(n-1)); // 15. 判断一个数n是不是2的幂System.out.println((n &amp; (n - 1)) == 0);/*如果是2的幂，n一定是100... n-1就是1111....所以做与运算结果为0*/ // 16. 求两个整数的平均值System.out.println((a+b) &gt;&gt; 1); // 17. 从低位到高位,取n的第m位int m = 2;System.out.println((n &gt;&gt; (m-1)) &amp; 1); // 18. 从低位到高位.将n的第m位置为1System.out.println(n | (1&lt;&lt;(m-1)));/*将1左移m-1位找到第m位，得到000...1...000n在和这个数做或运算*/ // 19. 从低位到高位,将n的第m位置为0System.out.println(n &amp; ~(0&lt;&lt;(m-1)));/* 将1左移m-1位找到第m位，取反后变成111...0...1111n再和这个数做与运算*/","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java注解","date":"2021-07-27T11:45:12.000Z","path":"wiki/Java注解/","text":"Java基础之注解机制详解 注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它是框架学习和设计者必须掌握的基础。 注解基础注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它主要的作用有以下四方面：— 生成文档，通过代码里标识的元数据生成javadoc文档。— 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。— 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理， 例如使用反射注入实例。这么来说是比较抽象的，我们具体看下注解的常见分类： Java自带的标准注解， 包括@Override、@Deprecated和@SuppressWarnings，分别用于标明重写某个方法、标明某个类或方法过时、标明要忽略的警告，用这些注解标明后编译器就会进行检查。 元注解，元注解是用于定义注解的注解，包括@Retention、@Target、@Inherited、@Documented，@Retention用于标明注解被保留的阶段，@Target用于标明注解使用的范围，@Inherited用于标明注解可继承，@Documented用于标明是否生成javadoc文档。 自定义注解，可以根据自己的需求定义注解，并可用元注解对自定义注解进行注解。接下来我们通过这个分类角度来理解注解。 Java内置注解我们从最为常见的Java内置的注解开始说起，先看下下面的代码： 123456789101112131415161718192021222324252627282930313233class A&#123; public void test() &#123; &#125;&#125;class B extends A&#123; /** * 重载父类的test方法 */ @Override public void test() &#123; &#125; /** * 被弃用的方法 */ @Deprecated public void oldMethod() &#123; &#125; /** * 忽略告警 * * @return */ @SuppressWarnings(&quot;rawtypes&quot;) public List processList() &#123; List list = new ArrayList(); return list; &#125;&#125; Java 1.5开始自带的标准注解，包括@Override、@Deprecated和@SuppressWarnings： @Override：表示当前的方法定义将覆盖父类中的方法 @Deprecated：表示代码被弃用，如果使用了被@Deprecated注解的代码则编译器将发出警告 @SuppressWarnings：表示关闭编译器警告信息 我们再具体看下这几个内置注解，同时通过这几个内置注解中的元注解的定义来引出元注解。 内置注解 - @Override我们先来看一下这个注解类型的定义： 1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 从它的定义我们可以看到，这个注解可以被用来修饰方法，并且它只在编译时有效，在编译后的class文件中便不再存在。这个注解的作用我们大家都不陌生，那就是告诉编译器被修饰的方法是重写的父类的中的相同签名的方法，编译器会对此做出检查，若发现父类中不存在这个方法或是存在的方法签名不同，则会报错。 内置注解 - @Deprecated这个注解的定义如下： 12345 @Documented@Retention(RetentionPolicy.RUNTIME)@Target(value=&#123;CONSTRUCTOR, FIELD, LOCAL_VARIABLE, METHOD, PACKAGE, PARAMETER, TYPE&#125;)public @interface Deprecated &#123;&#125; 从它的定义我们可以知道，它会被文档化，能够保留到运行时，能够修饰构造方法、属性、局部变量、方法、包、参数、类型。这个注解的作用是告诉编译器被修饰的程序元素已被“废弃”，不再建议用户使用。 内置注解 - @SuppressWarnings这个注解我们也比较常用到，先来看下它的定义： 12345@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.SOURCE)public @interface SuppressWarnings &#123;String[] value();&#125; 它能够修饰的程序元素包括类型、属性、方法、参数、构造器、局部变量，只能存活在源码时，取值为String[]。它的作用是告诉编译器忽略指定的警告信息，它可以取的值如下所示： // TODO 参考资料https://www.pdai.tech/md/java/basic/java-basic-x-annotation.html","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java泛型","date":"2021-07-27T11:39:32.000Z","path":"wiki/Java泛型/","text":"Java泛型1、泛型定义 使用泛型机制编写的程序代码要比那些杂乱地使用Object变量，然后在进行强制类型转换的代码具有更好的安全性和可读性。 –《Java核心技术》 泛型是在编译时期作用的； 泛型变量使用大写形式，在Java库中，一般使用变量E表示集合的元素类型，K和V表示表的关键字与值的类型。 2、通配符2.1 无边界通配符无边界通配符又成为非限定通配符 1234567891011121314public static void main(String[] args) &#123; List&lt;String&gt; list1 = new ArrayList&lt;&gt;(); list1.add(&quot;1&quot;); list1.add(&quot;2&quot;); list1.add(&quot;3&quot;); list1.add(&quot;4&quot;); loop(list1); &#125; public static void loop(List&lt;?&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; 2.2 上边界通配符上边界通配符和下边界通配符都属于限定通配符 12345678910111213141516public static void main(String[] args) &#123; //List中的类型必须是Number的子类，不然会报编译错误 List&lt;Integer&gt; list1 = new ArrayList&lt;&gt;(); list1.add(1); list1.add(2); list1.add(3); list1.add(4); loop(list1); &#125; // 传进来的list的类型必须是Number或Number的子类才可以 public static void loop(List&lt;? extends Number&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; ? extends Number如果限定的类型有多个，之间使用 &amp; 进行分割 2.3 下边界通配符1234567891011121314151617181920public static void main(String[] args) &#123; //List的泛型是Number 添加的元素只要是Number下的类型就可以 List&lt;Number&gt; list1 = new ArrayList&lt;&gt;(); list1.add(1); list1.add(2L); list1.add(new BigDecimal(22)); list1.add(4); loop(list1); &#125; /** * 通用类型必须是Number到Object之间的类型 * * @param list */ public static void loop(List&lt;? super Number&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; 3、泛型的使用 泛型必须先声明，再使用，不然会有编译错误；泛型的声明是用过一对&lt;&gt;来完成，约定使用一个大写的字母来表示;通配符不能用作返回值; 123456public &lt;T&gt; T testA(T t, Test1&lt;T&gt; test1) &#123; System.out.println(&quot;这是传入的T:&quot; + t); t = test1.t; System.out.println(&quot;这是赋值后的T:&quot; + t); return t;&#125; 要从泛型类取数据时，用extends； 要往泛型类写数据时，用super； 既要取又要写，就不用通配符（即extends与super都不用）。 3.1 泛型类12345public class Demo&lt;K, V&gt; &#123; public &lt;K&gt; K test(V v) &#123; return null; &#125;&#125; 3.2 泛型方法1234567891011121314151617181920212223242526272829303132333435public class DemoTest4&lt;K, V&gt; &#123; /** * &lt;T&gt; 代表泛型的声明 * * @param t 本方法声明的泛型类型 * @param &lt;T&gt; 本方法声明的泛型类型 * @return */ public &lt;T&gt; T test(T t) &#123; return null; &#125; /** * 普通的泛型方法 * * @param k 类中定义的泛型类型 * @param &lt;X&gt; 本方法中声明的泛型类型 * @return */ public &lt;X&gt; X aa(K k) &#123; return (X) null; &#125; /** * 静态方法中是无法使用类中声明的泛型类型的 * 可以使用在本方法中声明的泛型类型 * * @return */ public static &lt;X&gt; X bb() &#123; return null; &#125;&#125; 3.3 泛型接口首先看一下不使用泛型接口的Demo 12345678910111213141516171819202122先定义接口，声明两个方法public interface IGeneric &#123; Integer aa(Integer a); Integer bb(Integer b);&#125;//然后创建一个类来实现方法：public class IntegerDemo implements IGeneric&#123; @Override public Integer aa(Integer a) &#123; return null; &#125; @Override public Integer bb(Integer b) &#123; return null; &#125;&#125; 上面是没有使用泛型的接口设计，但是aa方法的操作类型相当于在接口中写死了，如果此时我们需要一个String类型的aa方法，那是不是还要在声明一个String类型的接口，然后再去实现呢，这样是不是显得代码很臃肿，代码重复；所以我们可以看一下使用泛型之后是怎么样的。 1234567891011121314151617181920212223242526272829303132定义泛型接口public interface IGenericInte&lt;T&gt; &#123; T aa(T a); T bb(T b);&#125;下面是根据不同类型的实现类泛型传如Integer类型public class IGenericInteger implements IGenericInte&lt;Integer&gt; &#123; @Override public Integer aa(Integer a) &#123; return null; &#125; @Override public Integer bb(Integer b) &#123; return null; &#125;&#125;泛型传入String类型public class IGenericString implements IGenericInte&lt;String&gt; &#123; @Override public String aa(String a) &#123; return null; &#125; @Override public String bb(String b) &#123; return null; &#125;&#125; 4、泛型擦除在虚拟机上没有泛型类型对象，所有的对象都属于普通类。Java在处理泛型类型的时候，会处理成一个相应的原始类型。 擦除类型变量，并替换为限定类型，如果没有限定类型，默认使用Object替代。如果有限定类型，并且是多个，会使用第一个限定的类型来替换。 1234public interface IGenericInte&lt;T&gt; &#123; T aa(T a); T bb(T b);&#125; 像上面这个T是一个无限定的变量，泛型擦除之后会直接使用Object替换。当然调用泛型方法时，如果擦除返回类型，编译器插入强制类型转换 12Pair&lt;Employee&gt; buddies = ....Employee buddy = buddies.getFirst(); 擦除getFirst的返回类型后将返回Object类型。编译器自动插入Employee的强制类型转换，也就是说，编译器调用方法是其实是执行了一下两个虚拟机指令： 对原始方法Pair.getFirst()方法的调用 将返回的Object类型强制转换为Employee类型 1public static &lt;T extends Comparable&gt; T foo(T [] args) 在擦除类型之后变成： 1public static Comparable T foo(Comparable [] args) 参数类型T已经被擦除，只留下限定类型Comparable; 总之有关Java泛型转换的事实： 虚拟机没有泛型，只有普通的类和方法 所有的类型参数都用它们的限定类型替换 ==桥方法被合成来保证多态== 为了保持类型安全型，必要时插入强制类型转换 第一条应该很好理解，这也是为什么会有泛型擦除这个概念，是因为JVM不能操作泛型；第二条就是解释泛型如何进行类型的擦除；第三条是泛型方法可能与多态的理念矛盾，所以使用桥方法来过渡或兼容；第四条上面也有提到，会出现强制类型转换的情况； 5、泛型的约束与局限性当然泛型的设计在java中并没有那么完美，它确实可以解决代码结构重用等问题，但是也是有一些局限性，下面是我根据《Java核心技术》进行的总结： 5.1 不能使用基础数据类型实例化类型参数原因是类型擦除之后，如果使用Object原始类型，Object是无法存储基本数据类型的值。所以只能通过其包装类型声明； 5.2 运行时查询类型只适用与原始类型1234567public class DemoTest5&lt;T&gt; &#123; public static void main(String[] args) &#123; DemoTest5&lt;String&gt; demoTest5 = new DemoTest5&lt;&gt;(); DemoTest5&lt;Integer&gt; demoTest4 = new DemoTest5&lt;&gt;(); System.err.println(demoTest4.getClass().equals(demoTest5.getClass())); &#125;&#125; demoTest4.getClass().equals(demoTest5.getClass())其实比较的是DemoTest5这个类类型，我们输出一下demoTest4.getClass()的结果看一下： 1class com.ibli.javaBase.generics.DemoTest5 所以这里有一道非常经典的面试题，如何判断一个泛型他的具体类型是什么，这里我们可以使用反射去拿到泛型的具体类型； 5.3 不能创造参数化类型的数组对于参数化类型的数组，在类型擦除之后，会变成Object[]类型，如果此时试图存储一个String类型的元素，就会抛出一个Array-StoreException异常；主要目的还是处于到数组安全的保护，可以参考几篇文章: 1、如果Java不支持参数化类型数组，那么Arrays.asList()如何处理它们？2、java不能创建参数化类型的泛型数组3、java.lang.ArrayStoreException 5.4 Varargs警告向参数个数可变的方法传递一个泛型类型的实例的场景，编译器会发出警告！抑制这种警告的方式有两种： 在调用方法上增加注解@SuppressWarnings(“unchecked”) 还可以使用@SafeVarargs注解直接标注方法 参考 java不能创建参数化类型的泛型数组 5.5 不能实例化类型变量不能使用new T(..) 或则new T[…]和T.class这样的表达式的类型变量；因为类型擦除后，T变成Object，显然我们在这里并不是想要创建一个Object实例。解决办法是在调用者提供一个构造器表达式，下面是用Supplier函数实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Pair&lt;T&gt; &#123; private T first; private T second; public T getFirst() &#123; return first; &#125; public void setFirst(T first) &#123; this.first = first; &#125; public T getSecond() &#123; return second; &#125; public void setSecond(T second) &#123; this.second = second; &#125; public Pair(T first, T second) &#123; this.first = first; this.second = second; &#125; public static &lt;T&gt; Pair&lt;T&gt; build(Supplier&lt;T&gt; constr) &#123; return new Pair&lt;&gt;(constr.get(), constr.get()); &#125; /** * Cannot infer type arguments for Pair2&lt;&gt; * 当函数头返回值为Pair时,无法推断,改为Pair2后可以推断. * @param c1 * @return */ public static &lt;T&gt; Pair&lt;T&gt; build(Class&lt;T&gt; c1)&#123; try &#123; return new Pair&lt;&gt;(c1.newInstance(),c1.newInstance()); &#125; catch (InstantiationException | IllegalAccessException e) &#123; return null; &#125; &#125;&#125; Supplier是一个函数接口，返回一个无参数并且返回类型为T的函数： 12345678910@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; /** * Gets a result. * * @return a result */ T get();&#125; 12345678910111213141516171819202122232425262728293031323334public class TestMakePair &#123; public static void main(String[] args) &#123; /** * 1.接受Supplier&lt;T&gt;--它是一个函数式接口。表示无参数且返回类型为T的函数。 * 因为不能实例化类型变量，如： * public Pair() &#123;first = new T();second = new T();&#125; * 所以最好的方式是让调用者提供一个构造器表达式.形式如下: * @param constr * @return */ Pair&lt;String&gt; pair = Pair.build(String::new); System.out.println(pair.getFirst().length()); /** * public void buildT()&#123; 2.传统的方式是通过Class.newInstance方法来构造泛型对象. 但由于细节过于复杂,T.class是不合法的.它会被擦除为Object.class.如下: Illegal class literal for the type parameter T T.class.newInstance(); &#125; * 3. * T.class是不合法的,但若API涉及如下 * reason:因为String.class是Class&lt;String&gt;的一个实例. */ Pair&lt;String&gt; pair1 = Pair.build(String.class); System.out.println(pair1.getFirst().length()); &#125;&#125;执行结果：00 5.6 不能构造泛型数组就像不能实例化一个泛型实例一样，也不能实例化数组。数组本身也有类型，用来监控存储在JVM中的数组，这个类型会被擦除，例如： 1234public static &lt;T extends Comparable&gt; T[] foo(T[] a)&#123; T[] mm = new T[2]; ...&#125; 类型擦除，会让这个方法永远构造Comparabel[2]数组； 5.7 泛型类的静态上下文中类型变量无效这个应该是比较好理解的，上文也提到过了，泛型类型是作用在泛型类上的，一些静态的方法或这静态的属性不能够使用泛型类的变量类型，编译器会直接报错； 5.8 不能抛出或者捕获泛型类的实例Java既不能抛出也不能捕获泛型类对象，实际上，甚至泛型类扩展Throwable都是不合法的。 12345678public static &lt;T extends Throwable&gt; void doWork(Class&lt;T&gt; t)&#123; try&#123; ... &#125;catch (T ex)&#123; 此处无法捕获 catch必须捕获具体的异常 .... &#125;&#125; 在异常规范中使用类型变量是允许的，如下： 123456789public static &lt;T extends Throwable&gt; void doWork(Class&lt;T&gt; t) throws T &#123; try&#123; ... &#125;catch (Throwable ex)&#123; t.initCause(ex); throw t; &#125;&#125; 5.9 可以消除对受查异常的检查Java异常处理要求必须为所有的受查异常提供一个处理器，但是使用泛型，可以规避这一点； 1234@SuppressWarnings(&quot;unchecked&quot;)public static &lt;T extends Throwable&gt; void throwAs(Throwable e) throws T&#123; throw (T)e;&#125; 调用上面的方法，编译器会认为t是一个非受查异常; 5.10 注意擦除后的冲突比如一个泛型类的equals方法，擦除之后，和Object的equals冲突；解决办法是重新命名引发错误的方法； 6、泛型的继承关系如果Manage extends Employee,那么Pair&lt; Manage &gt;是Pair&lt; Employee &gt;的子类吗？ 不是的！但是泛型类可以扩展或实现其他的泛型类，很典型的一个例子ArrayList: 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;&#125; ArrayList[E]继承了AbstractList[E]; 对于Java泛型的一些思考 编译器如何推断出具体的类型？ 参考资料：深入理解 Java 泛型 ------------------- 他日若遂凌云志 敢笑黄巢不丈夫 -------------------","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java反射","date":"2021-07-27T11:39:08.000Z","path":"wiki/Java反射/","text":"Java反射 反向探知，在程序运行是动态的获取类的相关属性这种动态获取类的内容以及动态调用对象的方法和获取属性的机制，叫做java反射机制； 反射的优缺点 优点增加了程序的灵活性，避免的固有逻辑写死到程序中代码简介，提高程序的复用性 缺点相比于直接调用，反射有比较大的性能消耗内部暴露和安全隐患 （因为反射可以操作private成员变量和调用private成员方法） 反射的基本操作获取类对象的4种方式123456789101112// 调用forName方法得到一个对象，这也是最容易想到的方式Class&lt;?&gt; object = Class.forName(&quot;com.ibli.javaBase.reflection.User&quot;);// 通过实例对象调用getClass方法Teacher teacher = new Teacher();Class&lt;?&gt; objectT = teacher.getClass();// 通过类加载器的方式Class&lt;?&gt; loader = ClassLoader.getSystemClassLoader().loadClass(&quot;com.ibli.javaBase.reflection.User&quot;);//通过一个类.classClass&lt;?&gt; tt = Teacher.class; 基本信息操作 类修饰符 PUBLIC PRIVATE PROTECTED STATIC FINAL SYNCHRONIZED VOLATILE TRANSIENT NATIVE INTERFACE ABSTRACT modifiers 1 2 4 8 16 32 64 128 256 512 1024 12345678910111213141516// 类的修饰符 具体的值可以参考JDK API文档中的定义 返回值是int类型 public：1System.err.println(tt.getModifiers());// 包名System.err.println(tt.getPackage());// 类的名称System.err.println(tt.getName());// 父类System.err.println(tt.getSuperclass());// 类加载器System.err.println(tt.getClassLoader());// 简称System.err.println(tt.getSimpleName());// 类实现的所有的接口System.err.println(tt.getInterfaces().length);// 所有的注解类型System.err.println(tt.getAnnotations().length); 执行结果： 123456781package com.ibli.javaBase.reflectioncom.ibli.javaBase.reflection.Teacherclass java.lang.Objectsun.misc.Launcher$AppClassLoader@18b4aac2Teacher00 查看类的变量12345678910111213141516// User extend Person(aa,bb)Class&lt;User&gt; obj = User.class;User user = obj.newInstance();// 能够拿到类的所有的变量Field[] fields = obj.getDeclaredFields();for (Field field : fields)&#123; System.out.println(field.getModifiers() + &quot; &quot; + field.getName());&#125;System.out.println(&quot; &quot;);// 只能够拿到类的public的变量Field[] fields1 = obj.getFields();for (Field field : fields1)&#123; System.out.println(field.getModifiers() + &quot; &quot; + field.getName());&#125;System.out.println(&quot; &quot;); 执行结果： 123456782 age2 name1 sex10 height 1 sex1 aa1 bb 结论： getDeclaredFields（1）getDeclaredFields能够获取本类的所有成员变量，无论是public还是private;（2）但是不能获取父类的任何属性；（3）可以获取static类型的属性； getFields（1）只能够获取本类的public属性；（2）能够获取父类的public属性；（3）可以获取static类型的属性； 修改属性1234567891011// 设置Person中的变量aaField aaField = obj.getField(&quot;aa&quot;);aaField.setInt(user,111);System.err.println(user.getAa());// 设置User私有成员变量Field ageField = obj.getDeclaredField(&quot;age&quot;);// 设置访问权限ageField.setAccessible(true);ageField.set(user,333);System.err.println(user.getAge()); 执行结果： 12111333 查看方法1234567891011121314151617Class&lt;User&gt; obj = User.class;User user = obj.newInstance();// 可以获取父类的方法Method[] methods = obj.getMethods();for (Method method : methods) &#123; System.out.println(method.getModifiers() + &quot; &quot; + method.getName());&#125;System.err.println(&quot; ----- &quot;);// 获取本类中的所有方法Method[] methods1 = obj.getDeclaredMethods();for (Method method : methods1) &#123; System.out.println(method.getModifiers() + &quot; &quot; + method.getName());&#125;System.err.println(&quot; 。。。。。。 &quot;);// 执行结果就不展示了 结论： getDeclaredMethods（1）可以获取本类中的所有方法；（2）可以获取本类的静态方法 getMethods（1）可以获取本类中的所有==公有==方法；（2）可以获取父类中的所有==公有==方法；（3）可以获取本类和父类的公有静态方法； 调用方法123456789// 访问私有方法Method sleep = obj.getDeclaredMethod(&quot;sleep&quot;);sleep.setAccessible(true);sleep.invoke(user);// 如果是静态方法，invoke第一个参数传null即可Method say = obj.getDeclaredMethod(&quot;say&quot;,String.class);say.setAccessible(true);say.invoke(null,&quot;hello java&quot;); 执行结果： 12Im sleeping!say hello java 构造器的使用123456789101112Class&lt;User&gt; obj = User.class;// 查询共有的构造器Constructor&lt;?&gt;[] constructors = obj.getConstructors();for (Constructor&lt;?&gt; constructor : constructors)&#123; System.out.println(constructor.getModifiers() + &quot; &quot; + constructor.getName());&#125;// 可以获取私有的构造器Constructor&lt;?&gt;[] constructors1 = obj.getDeclaredConstructors();for (Constructor&lt;?&gt; constructor : constructors1)&#123; System.err.println(constructor.getModifiers() + &quot; &quot; + constructor.getName());&#125; 执行结果： 1234561 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User2 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User 结论： getConstructors（1）获得本类所有的公有构造器 getDeclaredConstructors（1）获得本类所有的构造器（public&amp;private） 实例化对象1234567// 使用newInstance创建对象 调用无参构造器User user = obj.newInstance();// 获取构造器来实例化对象Constructor&lt;User&gt; constructor = obj.getDeclaredConstructor(Integer.class, String.class);constructor.setAccessible(true);User temp = constructor.newInstance(22, &quot;java&quot;);System.err.println(temp.getAge() + &quot; &quot; + temp.getName()); 执行结果： 22 java 反射性能为什么差 可以从两方面考虑，第一个是反射生成Class对象时性能差，第二是通过反射调用对象方式是的性能差； （1） 调用forName 本地方法（2）每次newInstance 都会进行一次安全检查（3）在默认情况下，方法的反射调用为委派实现，委派给本地实现来进行方法调用。在调用超过 15次之后，委派实现便会将委派对象切换至动态实现。这个动态实现的字节码是自动生成的，它将直接使用 invoke 指令来调用目标方法。 方法的反射调用会带来不少性能开销，原因主要有三个： 变长参数方法导致的Object数组 基本类型的自动装箱、拆箱 (参考资料2) 还有最重要的方法内联。 参考资料(1)反射为什么慢(2)关于装箱拆箱为什么会影响效率(3)jvm之方法内联优化 反射使用的场景 JDBC封装 Spring IOC jdbcTemplate Mybatis使用大量反射 使用反射注意点 在获取Field,method,construtor的时候，应尽量避免是用getDelcaredXXX(),应该传进参数获取指定的字段，方法和构造器； 使用缓存机制缓存反射操作相关元数据的原因是因为反射操作相关元数据的实时获取是比较耗时的 --------------------- 前途浩浩荡荡 万事尽可期待。----------------------- 反射在IOC中的应用","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java并发编程之同步锁","date":"2021-07-26T14:27:05.000Z","path":"wiki/Java并发编程之同步锁/","text":"","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java多线程之ThreadLocal","date":"2021-07-26T08:28:01.000Z","path":"wiki/Java多线程之ThreadLocal/","text":"ThreadLocalMap结构 ThreadLocal底层实际上是依赖ThreadLocalMap来实现数据存储的，而ThreadLocalMap并不是真正的Map结构，它是基于ThreadLocalMap类中的内部类Entry类型的数组来实现。 123456789 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 它的key其实就是当前threadlocal变量，继承了WeakReference。然后Object value;实际存储的值。 Thread类中有ThreadLocal类型的变量，如下👇 12345678910 /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */// 子线程可以获取到inheritableThreadLocals中的值 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; 为什么ThreadLocalMap使用ThreadLocal当作key而不是Thread呢？ 1、因为一个线程可能会出现多个ThreadLocal变量，所以一个线程一个ThreadLocalMap（实质上是Entry数组）来存放多个ThreadLocal变量。 2、倘若是Thread作为key，就会变成多个线程共同访问一个ThreadLocalMap，就会变成线程公用的变量，那个每个线程中可能存储多个ThreadLocal变量的情况下，Entry可能真的用到map结果才可以实现呀 3、多个线程共同访问ThreadLocalMap，那么可能会出现ThreadLocalMap提及很大从而降低性能，而且何时销毁这个变量是无法确定的 ThreadLocal set流程 下面是set方法的源码 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; set的操作就是向Entry数组中添加当前变量和值👇 用数组是因为，我们开发过程中可以一个线程可以有多个TreadLocal来存放不同类型的对象的，但是他们都将放到你当前线程的ThreadLocalMap里，所以肯定要数组来存。 至于Hash冲突，我们先看一下源码： 1234567891011121314151617181920212223private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); &#125; 我从源码里面看到ThreadLocalMap在存储的时候会给每一个ThreadLocal对象一个threadLocalHashCode，在插入过程中，根据ThreadLocal对象的hash值，定位到table中的位置i，**int i = key.threadLocalHashCode &amp; (len-1)**。 然后会判断一下：如果当前位置是空的，就初始化一个Entry对象放在位置i上； 1234if (k == null) &#123; replaceStaleEntry(key, value, i); return;&#125; 如果位置i不为空，如果这个Entry对象的key正好是即将设置的key，那么就刷新Entry中的value； 1234if (k == key) &#123; e.value = value; return;&#125; 如果位置i的不为空，而且key不等于entry，那就找下一个空位置，直到为空为止。 ThreadLocal是如何引起内存泄漏的？如上面Entry的源码大家也看到了，static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; 一个ThreadLocal变量存在两条引用： 1、ThreadLocalRef（栈）-》ThreadLocal（key）和 ThreadLocalMap -&gt; ThreadLocal（key） 2、ThreadRef -&gt; Thread -&gt; ThreadLocalMap -&gt; Entry -&gt; Value 内存泄漏值的是ThreadLocal被回收了，ThreadLocalMap -&gt; Entry -&gt; key没有了指向，但是Entry的value的指向还在，长期占用内存，就可能会导致内存泄漏。 如何避免内存泄漏ThreadLocal使用完之后及时remove 为什么建议ThreadLocal变量为static类型的ThreadLocal能够实现线程隔离的关键在于Thread持有自己的一个ThreadLocalMap变量，需要每一个ThreadLocal变量占用一个Entry就可以了，没有必要作为成员变量频繁创建，浪费内存空间。 参考资料 Java面试必问：ThreadLocal终极篇 【对线面试官】ThreadLocal","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"基础面试题目","date":"2021-07-26T02:05:07.000Z","path":"wiki/基础面试题目/","text":"1.String不可变String 对象的不可变性 了解了 String 对象的实现后，你有没有发现在实现代码中 String 类被 final 关键字修饰了，而且变量 char 数组也被 final 修饰了。 我们知道类被 final 修饰代表该类不可继承，而 char[] 被 final+private 修饰，代表了 String 对象不可被更改。Java 实现的这个特性叫作 String 对象的不可变性，即 String 对象一旦创建成功，就不能再对它进行改变。 Java 这样做的好处在哪里呢？ 第一，保证 String 对象的安全性。假设 String 对象是可变的，那么 String 对象将可能被恶意修改。 第二，保证 hash 属性值不会频繁变更，确保了唯一性，使得类似 HashMap 容器才能实现相应的 key-value 缓存功能。 第三，可以实现字符串常量池。在 Java 中，通常有两种创建字符串对象的方式，一种是通过字符串常量的方式创建，如 String str=“abc”；另一种是字符串变量通过 new 形式的创建，如 String str = new String(“abc”)。 当代码中使用第一种方式创建字符串对象时，JVM 首先会检查该对象是否在字符串常量池中，如果在，就返回该对象引用，否则新的字符串将在常量池中被创建。这种方式可以减少同一个值的字符串对象的重复创建，节约内存。 2.String 和 StringBuilder、StringBuffer 的区别？https://www.cnblogs.com/weibanggang/p/9455926.html 3.描述一下 JVM 加载 class 文件的原理机制？https://www.cnblogs.com/williamjie/p/11167920.html 4.char 型变量中能不能存贮一个中文汉字，为什么？正确答案： char型变量是用来存储Unicode编码的字符的，unicode编码字符集中包含了汉字， 所以，char型变量中当然可以存储汉字啦。不过，如果某个特殊的汉字没有被包含在unicode编码字符集中， 那么，这个char型变量中就不能存储这个特殊汉字。 补充说明：unicode编码占用两个字节，所以，char类型的变量也是占用两个字节 5.抽象类（abstract class）和接口（interface）有什么异同？https://blog.csdn.net/aptentity/article/details/68942916 6.静态嵌套类(Static Nested Class)和内部类（Inner Class）的不同？https://blog.csdn.net/machinecat0898/article/details/80071242 7.抽象的（abstract）方法是否可同时是静态的（static）,（native）， synchronized 修饰？ 答：都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。本地方法是由本地代码（如C代码）实现的方法，而抽象方法是没有实现的，也是矛盾的。synchronized和方法的实现细节有关，抽象方法不涉及实现细节，因此也是相互矛盾的。 8.如何实现对象克隆https://www.cnblogs.com/fnlingnzb-learner/p/10649509.html 9.内部类可以引用它的包含类（外部类）的成员吗？有没有什么限制https://www.cnblogs.com/aademeng/articles/11084885.htmlhttps://www.cnblogs.com/dolphin0520/p/3811445.html 静态内部类：它是用static修饰的，在访问限制上它只能访问外部类中的static所修饰的成员变量或者是方法成员内部类：成员内部类是最普通的内部类，它可以无条件访问外部类的所有成员属性和成员方法（包括private成员和静态成员）。【注意】当成员内部类拥有和外部类同名的成员变量或者方法时，会发生隐藏现象，即默认情况下访问的是成员内部类的成员。如果要访问外部类的同名成员，需要以下面的形式进行访问：局部内部类：局部内部类是定义在外围类的方法中的，在访问的时候它可以直接访问外围类的所有成员！但是不能随便访问局部变量，除非这个局部变量被final修饰。匿名内部类： 10.Java 中的 final 关键字有哪些用法？https://www.cnblogs.com/dotgua/p/6357951.html多线程下的final语义 👇https://www.codercc.com/backend/basic/juc/concurrent-keywords/final.html#_1-final%E7%9A%84%E7%AE%80%E4%BB%8B 修饰变量基本类型的变量，值是不可以变化的引用类型的变量，引用是不可以变化的，但是可以修改引用的值方法参数： 保证这个变量在这个方法中的值不会发生变化 修饰方法它表示该方法不能被覆盖。这种使用方式主要是从设计的角度考虑，即明确告诉其他可能会继承该类的程序员，不希望他们去覆盖这个方法。这种方式我们很容易理解，然而，关于private和final关键字还有一点联系，这就是类中所有的private方法都隐式地指定为是final的，由于无法在类外使用private方法，所以也就无法覆盖它 修饰类用final修饰的类是无法被继承的 11.Thread 类的 sleep()方法和对象的 wait()方法都可以让线程暂停执行，它们有什么区别?sleep()方法是Thread类 sleep是Thread的静态native方法,可随时调用,会使当前线程休眠,并释放CPU资源,但不会释放对象锁; wait()方法是Object类 wait()方法是Object的native方法,只能在同步方法或同步代码块中使用,调用会进入休眠状态,并释放CPU资源与对象锁,需要我们调用notify/notifyAll方法唤醒指定或全部的休眠线程,再次竞争CPU资源. 注意:sleep(long millis)存在睡眠时间,不算特点因为wait()方法存在重载wait(long timeout),即设置了等待超时时间它们两个都需要再次抢夺CPU资源 12.线程的 sleep()方法和 yield()方法有什么区别？sleep()方法在给其他线程运行机会时不考虑线程的优先级。因此会给低优先级的线程运行的机会，而yield()方法只会给相同优先级或更高优先级的线程运行的机会。线程执行sleep()方法后会转入阻塞状态，所以执行sleep()方法的线程在指定的时间内肯定不会被执行，而yield()方法只是使当前线程重新回到就绪状态，所以执行yield()方法的线程有可能在进入到就绪状态后又立马被执行。 13.线程的基本状态以及状态之间的关系 https://blog.csdn.net/zhangdongnihao/article/details/104029972 https://juejin.cn/post/6885159254764814349 14.访问修饰符 public,private,protected,以及不写（默认）时的区别？ 15.请说出与线程同步以及线程调度相关的方法。（1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；（2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常；（3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关；（4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； 补充：Java 5 通过 Lock 接口提供了显式的锁机制（explicit lock），增强了灵活性以及对线程的协调。Lock 接口中定义了加锁（lock()）和解锁（unlock()）的方法，同时还提供了 newCondition()方法来产生用于线程之间通信的 Condition 对象；此外，Java 5 还提供了信号量机制（semaphore），信号量可以用来限制对某个共享资源进行访问的线程的数量。在对资源进行访问之前，线程必须得到信号量的许可（调用 Semaphore 对象的 acquire()方法）；在完成对资源的访问后，线程必须向信号量归还许可（调用 Semaphore 对象的 release()方法）。 16.synchronized 关键字的用法？12345678910111213141516171819202122public class SyncDemo &#123; final Object lock = new Object(); public synchronized void m1()&#123;&#125; public void m2()&#123; synchronized (SyncDemo.class)&#123; // TODO &#125; synchronized (lock)&#123; &#125; synchronized (this)&#123; &#125; &#125; public synchronized static void m3()&#123; &#125;&#125; 17.Java 中如何实现序列化，有什么意义？序列化就是一种用来处理对象流的机制，所谓对象流也就是将对象的内容进行流化。可以对流化后的对象进行读写操作，也可将流化后的对象传输于网络之间。序列化是为了解决对象流读写操作时可能引发的问题（如果不进行序列化可能会存在数据乱序的问题）。要实现序列化，需要让一个类实现 Serializable 接口，该接口是一个标识性接口，标注该类对象是可被序列化的，然后使用一个输出流来构造一个对象输出流并通过 writeObject(Object)方法就可以将实现对象写出（即保存其状态）；如果需要反序列化则可以用一个输入流建立对象输入流，然后通过 readObject 方法从流中读取对象。序列化除了能够实现对象的持久化之外，还能够用于对象的深度克隆 18.阐述 JDBC 操作数据库的步骤下面的代码以连接本机的 Oracle 数据库为例，演示 JDBC 操作数据库的步骤。（1） 加载驱动。Class.forName(&quot;oracle.jdbc.driver.OracleDriver&quot;);（2） 创建连接。Connection con = DriverManager.getConnection(&quot;jdbc:oracle:thin:@localhost:1521:orcl&quot;,&quot;scott&quot;, &quot;tiger&quot;);（3） 创建语句。 123PreparedStatement ps = con.prepareStatement(&quot;select * from emp where sal between ? and ?&quot;);ps.setint(1, 1000);ps.setint(2, 3000); （4）执行语句。ResultSet rs = ps.executeQuery();（5）处理结果。 1234while(rs.next()) &#123; System.out.println(rs.getint(&quot;empno&quot;) + &quot; - &quot; + rs.getString(&quot;ename&quot;));&#125; （6） 关闭资源。 12345678910finally &#123; if(con != null) &#123; try &#123; con.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 提示：关闭外部资源的顺序应该和打开的顺序相反，也就是说先关闭 ResultSet、再关闭 Statement、在关闭 Connection。上面的代码只关闭了 Connection（连接），虽然通常情况下在关闭连接时，连接上创建的语句和打开的游标也会关闭，但不能保证总是如此，因此应该按照刚才说的顺序分别关闭。此外，第一步加载驱动在 JDBC 4.0 中是可以省略的（自动从类路径中加载驱动），但是我们建议保留。 19.Statement 和 PreparedStatement 有什么区别？哪个性能更好？与 Statement 相比，①PreparedStatement 接口代表预编译的语句，它主要的优势在于可以减少 SQL 的编译错误并增加 SQL 的安全性（减少 SQL 注射攻击的可能性）；②PreparedStatement 中的 SQL 语句是可以带参数的，避免了用字符串连接拼接 SQL 语句的麻烦和不安全；③当批量处理 SQL 或频繁执行相同的查询时，PreparedStatement 有明显的性能上的优势，由于数据库可以将编译优化后的SQL语句缓存起来，下次执行相同结构的语句时就会很快（不用再次编译和生成执行计划）。 补充：为了提供对存储过程的调用，JDBC API 中还提供了 CallableStatement 接口。存储过程（Stored Procedure）是数据库中一组为了完成特定功能的 SQL 语句的集合，经编译后存储在数据库中，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。虽然调用存储过程会在网络开销、安全性、性能上获得很多好处，但是存在如果底层数据库发生迁移时就会有很多麻烦，因为每种数据库的存储过程在书写上存在不少的差别。 20.在进行数据库编程时，连接池有什么作用？由于创建连接和释放连接都有很大的开销（尤其是数据库服务器不在本地时，每次建立连接都需要进行 TCP 的三次握手，释放连接需要进行 TCP 四次握手，造成的开销是不可忽视的），为了提升系统访问数据库的性能，可以事先创建若干连接置于连接池中，需要时直接从连接池获取，使用结束时归还连接池而不必关闭连接，从而避免频繁创建和释放连接所造成的开销，这是典型的用空间换取时间的策略（浪费了空间存储连接，但节省了创建和释放连接的时间）。池化技术在Java 开发中是很常见的，在使用线程时创建线程池的道理与此相同。基于 Java 的开源数据库连接池主要有：C3P0、Proxool、DBCP、BoneCP、Druid 等。 补充：在计算机系统中时间和空间是不可调和的矛盾，理解这一点对设计满足性能要求的算法是至关重要的。大型网站性能优化的一个关键就是使用缓存，而缓存跟上面讲的连接池道理非常类似，也是使用空间换时间的策略。可以将热点数据置于缓存中，当用户查询这些数据时可以直接从缓存中得到，这无论如何也快过去数据库中查询。当然，缓存的置换策略等也会对系统性能产生重要影响，对于这个问题的讨论已经超出了这里要阐述的范围。 21.什么是 DAO 模式?DAO（Data Access Object）顾名思义是一个为数据库或其他持久化机制提供了抽象接口的对象，在不暴露底层持久化方案实现细节的前提下提供了各种数据访问操作。在实际的开发中，应该将所有对数据源的访问操作进行抽象化后封装在一个公共API中。用程序设计语言来说，就是建立一个接口，接口中定义了此应用程序中将会用到的所有事务方法。在这个应用程序中，当需要和数据源进行交互的时候则使用这个接口，并且编写一个单独的类来实现这个接口，在逻辑上该类对应一个特定的数据存储。DAO 模式实际上包含了两个模式，一是 DataAccessor（数据访问器），二是 Data Object（数据对象），前者要解决如何访问数据的问题，而后者要解决的是如何用对象封装数据。 22.Java 中是如何支持正则表达式操作的？Java 中的 String 类提供了支持正则表达式操作的方法，包括：matches()、replaceAll()、replaceFirst()、split()。此外，Java 中可以用 Pattern 类表示正则表达式对象，它提供了丰富的 API 进行各种正则表达式操作。面试题： - 如果要从字符串中截取第一个英文左括号之前的字符串，例如：北京市(朝阳区)(西城区)(海淀区)，截取结果为：北京市，那么正则表达式怎么写？ 123456789101112import java.util.regex.Matcher;import java.util.regex.Pattern;class RegExpTest &#123; public static void main(String[] args) &#123; String str = &quot;北京市(朝阳区)(西城区)(海淀区)&quot;; Pattern p = Pattern.compile(&quot;.*?(?=\\()&quot;); Matcher m = p.matcher(str); if(m.find()) &#123; System.out.println(m.group()); &#125; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"面向对象特征","date":"2021-07-26T01:59:21.000Z","path":"wiki/面向对象特征/","text":"封装封装是保证软件部件具有优良的模块性的基础，封装的目标就是实现软件内部的“高内聚、低耦合”。防止程序相互依赖而带来的变动影响。在面向对象的编程语言中，对象是封装的最基本单位，面向对象的封装比传统语言的封装更为清晰、也更为有力，面向对象的封装就是把描述一个对象的属性和行为的代码封装在一个“模块”中，或者说是一个类中，属性用变量定义，行为用方法定义，方法可以直接访问同一个对象中的属性。 将一个类中的成员变量全部定义为私有的，只有这个类自己的方法才可以访问到这些成员变量，这就基本上实现对象的封装，把握一个原则：把对同一事物进行操作的方法和相关的方法放在同一个类中，把方法和它操作的数据放在同一个类中。 抽象抽象就是找出一些事物的相似和共性之处，然后将这些事物归为一类，这个类只考虑这些事物的相似和共性之处，并且会忽略与当前主题和目标无关的那些方面，将注意力集中在与当前目标有关的方面。 继承在定义和实现一个类的时候，可以在一个已经存在的类的基础上来进行，把这个已经存在的类所定义的内容作为自己的内容，并可以加入若干新的内容，或修改原来的方法使之更适合特殊的需要，这就是继承。 继承是子类自动共享父类资源（数据或者方法）的机制，这是类之间的一种关系，提高了软件的可重用性和可扩展性。 多态多态是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在该编程时不确定，而是在程序运行期间才确定，即一个引用变量倒底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。因为在程序运行时才确定具体的类，这样，不用修改源程序代码，就可以让引用变量绑定到各种不同的类实现上，从而导致该引用调用的具体方法随之改变，即不修改程序代码就可以改变程序运行时所绑定的具体代码，让程序可以选择多个运行状态，这就是多态性。 多态性增强了软件的灵活性和扩展性，简单一句话理解多态的话就是，编译看左边，运行看右边 编译看左边 – 是指 想要成功的保存,就要使用左边也就是 只能使用父类提供的功能!!如果父类中没有，那么会编译报错运行看右边 – 是指 想要得到结果,就要看右边也就是 使用子类的方法体!!!","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"kafka-overview","date":"2021-07-22T03:00:12.000Z","path":"wiki/kafka-overview/","text":"参考资料 kafka详细教程 Kafka 集群管理 OrcHome kafka中文教程 面试官：说说Kafka处理请求的全流程 蘑菇街千亿级消息Kafka上云实践 kafka 集群搭建 kafka-2-11集群部署","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"}]},{"title":"Java-为什么禁止把SimpleDateFormat定义成static变量?","date":"2021-07-21T12:06:46.000Z","path":"wiki/Java-为什么禁止把SimpleDateFormat定义成static变量/","text":"本文参照 《Java技术灵魂15问》 简介在日常开发中，我们经常会用到时间，我们有很多办法在 Java 代码中获取时 间。但是不同的方法获取到的时间的格式都不尽相同，这时候就需要一种格式化工 具，把时间显示成我们需要的格式。 最常用的方法就是使用 SimpleDateFormat 类。这是一个看上去功能比较简单 的类，但是，一旦使用不当也有可能导致很大的问题。在 Java 开发手册中，有如下明确规定: 那么，本文就围绕 SimpleDateFormat 的用法、原理等来深入分析下如何以正 确的姿势使用它。 SimpleDateFormat 是 Java 提供的一个格式化和解析日期的工具类。它允许进 行格式化(日期 -&gt; 文本)、解析(文本 -&gt; 日期)和规范化。SimpleDateFormat 使 得可以选择任何用户定义的日期 - 时间格式的模式。 在 Java 中，可以使用 SimpleDateFormat 的 format 方法，将一个 Date 类型 转化成 String 类型，并且可以指定输出格式。 SimpleDateFormat 用法12345 // Date转StringDate data = new Date();SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);String dataStr = sdf.format(data);System.out.println(dataStr); 以上代码，转换的结果是:2018-11-25 13:00:00，日期和时间格式由”日期 和时间模式”字符串指定。如果你想要转换成其他格式，只要指定不同的时间模式就 行了。 在 Java 中，可以使用 SimpleDateFormat 的 parse 方法，将一个 String 类型 转化成 Date 类型。 12// String转Data System.out.println(sdf.parse(dataStr)); 日期和时间模式表达方法在使用 SimpleDateFormat 的时候，需要通过字母来描述时间元素，并组装成 想要的日期和时间模式。常用的时间元素和字母的对应表如下: 模式字母通常是重复的，其数量确定其精确表示。如下表是常用的输出格式的表 示方法。 输出不同时区的时间时区是地球上的区域使用同一个时间定义。以前，人们通过观察太阳的位置(时 角)决定时间，这就使得不同经度的地方的时间有所不同(地方时)。1863 年，首次 使用时区的概念。时区通过设立一个区域的标准时间部分地解决了这个问题。 世界各个国家位于地球不同位置上，因此不同国家，特别是东西跨度大的国家日 出、日落时间必定有所偏差。这些偏差就是所谓的时差。 现今全球共分为 24 个时区。由于实用上常常 1 个国家，或 1 个省份同时跨着 2 个或更多时区，为了照顾到行政上的方便，常将 1 个国家或 1 个省份划在一起。所以 时区并不严格按南北直线来划分，而是按自然条件来划分。例如，中国幅员宽广，差 不多跨 5 个时区，但为了使用方便简单，实际上在只用东八时区的标准时即北京时间 为准。 由于不同的时区的时间是不一样的，甚至同一个国家的不同城市时间都可能不一 样，所以，在 Java 中想要获取时间的时候，要重点关注一下时区问题。默认情况下，如果不指明，在创建日期的时候，会使用当前计算机所在的时区作为默认时区，这也是为什么我们通过只要使用new Date()就可以获取中国的当前 时间的原因。 那么，如何在 Java 代码中获取不同时区的时间呢? SimpleDateFormat 可以 实现这个功能。 123SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); sdf.setTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;)); System.out.println(sdf.format(Calendar.getInstance().getTime())); 以上代码，转换的结果是:2018-11-24 21:00:00 。既中国的时间是 11 月 25 日的 13 点，而美国洛杉矶时间比中国北京时间慢了 16 个小时(这还和冬夏令时有关 系，就不详细展开了)。 如果你感兴趣，你还可以尝试打印一下美国纽约时间(America/New_York)。 纽约时间是 2018-11-25 00:00:00。纽约时间比中国北京时间早了 13 个小时。 当然，这不是显示其他时区的唯一方法，不过本文主要为了介绍 SimpleDate-Format，其他方法暂不介绍了。 SimpleDateFormat 线程安全性由于 SimpleDateFormat 比较常用，而且在一般情况下，一个应用中的时间显 示模式都是一样的，所以很多人愿意使用如下方式定义 SimpleDateFormat: 123456789public class Main &#123; private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static void main(String[] args) &#123; simpleDateFormat.setTimeZone(TimeZone.getTimeZone(&quot;America/New_York&quot;)); System.out.println(simpleDateFormat.format(Calendar.getInstance(). getTime())); &#125; &#125; ⚠️ 这种定义方式，存在很大的安全隐患。 我们来看一段代码，以下代码使用线程池来执行时间输出。 123456789101112131415161718192021222324252627282930313233343536373839public class Main &#123; /** * 定义一个全局的SimpleDateFormat */ private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); /** * 使用ThreadFactoryBuilder定义一个线程池 */ private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); /** * 定义一个CountDownLatch，保证所有子线程执行完之后主线程再执行 */ private static CountDownLatch countDownLatch = new CountDownLatch(100); public static void main(String[] args) &#123; // 定义一个线程安全的 HashSet Set&lt;String&gt; dates = Collections.synchronizedSet(new HashSet&lt;String&gt;()); for (int i = 0; i &lt; 100; i++) &#123; // 获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // 时间增加 calendar.add(Calendar.DATE, finalI); // 通过 simpleDateFormat 把时间转换成字符串 String dateString = simpleDateFormat.format(calendar. getTime()); // 把字符串放入 Set 中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;); &#125; // 阻塞，直到 countDown 数量为 0 countDownLatch.await(); // 输出去重后的时间个数 System.out.println(dates.size()); &#125; &#125; 以上代码，其实比较简单，很容易理解。就是循环一百次，每次循环的时候都在 当前时间基础上增加一个天数(这个天数随着循环次数而变化)，然后把所有日期放入 一个线程安全的、带有去重功能的 Set 中，然后输出 Set 中元素个数。 正常情况下，以上代码输出结果应该是 100。但是实际执行结果是一个小于 100 的数字。 原因就是因为 SimpleDateFormat 作为一个非线程安全的类，被当做了共享变 量在多个线程中进行使用，这就出现了线程安全问题。 线程不安全原因通过以上代码，我们发现了在并发场景中使用 SimpleDateFormat 会有线程安 全问题。其实，JDK 文档中已经明确表明了 SimpleDateFormat 不应该用在多线程 场景中: Date formats are not synchronized. It is recommended to create separate format instances for each thread. If multiple threads access a format concurrently, it must be synchronized externally. 那么接下来分析下为什么会出现这种问题，SimpleDateFormat 底层到底是怎 么实现的?我们跟一下 SimpleDateFormat 类中 format 方法的实现其实就能发现端倪。 123456789101112131415161718192021222324252627282930313233// Called from Format after creating a FieldDelegate private StringBuffer format(Date date, StringBuffer toAppendTo, FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo; &#125; SimpleDateFormat 中的 format 方法在执行过程中，会使用一个成员变量 calendar 来保存时间。这其实就是问题的关键。 由于我们在声明 SimpleDateFormat 的时候，使用的是 static 定义的。那么 这 个 SimpleDateFormat就是一个共享变量， 随 之，SimpleDateFormat 中 的 calendar 也就可以被多个线程访问到。 假设线程 1 刚刚执行完 calendar.setTime 把时间设置成 2018-11-11，还 没等执行完，线程 2 又执行了 calendar.setTime 把时间改成了 2018-12-12。 这时候线程 1 继续往下执行，拿到的 calendar.getTime 得到的时间就是线程 2 改 过之后的。 除了 format 方法以外，SimpleDateFormat 的 parse 方法也有同样的问题。 所以，不要把 SimpleDateFormat 作为一个共享变量使用。 如何解决线程安全问题 使用局部变量 不要使用static 加同步锁12345678910111213141516for (int i = 0; i &lt; 100; i++) &#123; // 获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // 加锁 synchronized (simpleDateFormat) &#123; // 时间增加 calendar.add(Calendar.DATE, finalI); // 通过 simpleDateFormat 把时间转换成字符串 String dateString = simpleDateFormat.format(calendar.getTime()); // 把字符串放入 Set 中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125; &#125;); &#125; 其实以上代码还有可以改进的地方，就是可以把锁的粒度再设置的小一点，可以 只对 simpleDateFormat.format 这一行加锁，这样效率更高一些。 使用 ThreadLocal 第三种方式，就是使用 ThreadLocal。 ThreadLocal 可以确保每个线程都可以 得到单独的一个 SimpleDateFormat 的对象，那么自然也就不存在竞争问题了。 12345678910/** * 使用ThreadLocal定义一个全局的SimpleDateFormat */ private static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;() &#123; @Override protected SimpleDateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; // 用法 String dateString = simpleDateFormatThreadLocal.get().format(calendar.getTime()); 用 ThreadLocal 来实现其实是有点类似于缓存的思路，每个线程都有一个独享 的对象，避免了频繁创建对象，也避免了多线程的竞争。 当然，以上代码也有改进空间，就是，其实 SimpleDateFormat 的创建过程可 以改为延迟加载。这里就不详细介绍了。 使用 DateTimeFormatter如果是 Java8 应用，可以使用 DateTimeFormatter 代替 SimpleDateFormat， 这是一个线程安全的格式化工具类。就像官方文档中说的，这个类 simple beautiful strong immutable thread-safe。 123456789// 解析日期String dateStr = &quot;2016年10月25日&quot;;DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日&quot;);LocalDate date = LocalDate.parse(dateStr, formatter);// 日期转换为字符串LocalDateTime now = LocalDateTime.now();DateTimeFormatter format = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日 hh:mm a&quot;);String nowStr = now.format(format);System.out.println(nowStr); 总结本 文 介 绍 了 SimpleDateFormat 的 用 法，SimpleDateFormat 主 要 可 以 在 String 和 Date 之间做转换，还可以将时间转换成不同时区输出。同时提到在并发场 景中 SimpleDateFormat 是不能保证线程安全的，需要开发者自己来保证其安全性。 主要的几个手段有改为局部变量、使用 synchronized 加锁、使用 Threadlocal 为每一个线程单独创建一个等。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java并发编程-多线程的发展和意义","date":"2021-07-21T11:45:01.000Z","path":"wiki/Java并发编程-多线程的发展和意义/","text":"线程基础概念什么是线程线程是CPU执行任务的基本单位，一个进程中包含一个或者多个线程，一个进程内的多个线程共享进程的资源，每一个线程有自己的独立内存，是线程不共享的。 并行与并发 并行 同一时刻，横向有多少个线程可以运行 并发 系统和服务器同一时刻能够承受的并发线程 线程的特征 异步（不需要等待） 比如说注册之后发送验证码，验证码的过程可以异步去做不需要客户去在注册接口等待这个时间； 并行（CPU核数） Java中线程的使用 继承Thread 实现Runnalbe 实现Callable/Future 线程原理1234567public class ThreadDemo extend Thread&#123; int a = 0; public void run()&#123; int b = 0; b = a + 1; &#125;&#125; 执行start方法，其实是调用JVM相关的指令， thread.cpp java thread.start() -&gt; cpp thread.start() -&gt; os指令:create.thread start.thread操作系统层面会创建线程，线程创建之后，线程可以启动，（线程启动之后并不一定马上执行）这些线程统一有CPU调度算法来处理；决定那个线程分配给那个执行CPU；CPU执行线程任务的时候，会调用run方法 -&gt; cpp run方法 -&gt; java thread.run() ⚠️ CompletableFuture 异步回调通知，基于Future的优化 线程的生命周期线程创建，当线程中的指令执行完成之后，run（）结束 线程销毁其他线程状态 等待状态 （sleep join wait） 锁阻塞状态 （blocked 竞争锁失败 park） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ThreadStatusDemo &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;time waitting&quot;).start(); new Thread(()-&gt;&#123; while (true)&#123; synchronized (ThreadStatusDemo.class)&#123; try &#123; ThreadStatusDemo.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;,&quot;waitting&quot;).start(); new Thread(new BlockDemo(),&quot;block demo 1&quot;).start(); new Thread(new BlockDemo(),&quot;block demo 2&quot;).start(); &#125; static class BlockDemo extends Thread&#123; @Override public void run() &#123; synchronized (BlockDemo.class)&#123; while (true)&#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 查看线程状态 12jps -ljstack pid 线程如何停止interrupt() 停止线程主动停止方式 -&gt; run方法执行结束被动停止方式 一般中断线程是在无法控制线程的情况下，比如线程wait ， 线程sleep ， 线程while(true)Thread.currnetThread().isInterrupted() stop方法停止线程 禁止使用 相当于kill线程 不友好 interrupt 功能 唤醒阻塞状态的线程 修改中断标志，false -&gt; true 问题排查","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"为什么禁止开发人员修改 serialVersionUID 字段的值?","date":"2021-07-21T09:47:46.000Z","path":"wiki/为什么禁止开发人员修改-serialVersionUID-字段的值/","text":"序列化是一种对象持久化的手段。普遍应用在网络传输、RMI 等场景中。类通 过实现 java.io.Serializable 接口以启用其序列化功能。Java 对象的序列化与反序列化、深入分析 Java 的序列化与反序列化、单例与 序列化的那些事儿 在这几篇文章中，分别介绍过了序列化涉及到的类和接口、如何自定义序列化 策略、transient 关键字和序列化的关系等，还通过学习 ArrayList 对序列化的实现源 码深入学习了序列化。并且还拓展分析了一下序列化对单例的影响等。但是，还有一个知识点并未展开介绍，那就是关于 serialVersionUID 。这个 字段到底有什么用?如果不设置会怎么样?为什么《Java 开发手册》中有以下规定: 背景知识Serializable 和 Externalizable类通过实现 java.io.Serializable 接口以启用其序列化功能。未实现此接 口的类将无法进行序列化或反序列化。可序列化类的所有子类型本身都是可序列 化的。如果读者看过 Serializable 的源码，就会发现，他只是一个空的接口，里 面什么东西都没有。Serializable 接口没有方法或字段，仅用于标识可序列化的 语义。但是，如果一个类没有实现这个接口，想要被序列化的话，就会抛出 java. io.NotSerializableException 异常。 它是怎么保证只有实现了该接口的方法才能进行序列化与反序列化的呢?原因是在执行序列化的过程中，会执行到以下代码: 12345678910111213141516if (obj instanceof String) &#123; writeString((String) obj, unshared); &#125; else if (cl.isArray()) &#123; writeArray(obj, desc, unshared); &#125; else if (obj instanceof Enum) &#123; writeEnum((Enum&lt;?&gt;) obj, desc, unshared); &#125; else if (obj instanceof Serializable) &#123; writeOrdinaryObject(obj, desc, unshared); &#125; else &#123; if (extendedDebugInfo) &#123; throw new NotSerializableException( cl.getName() + &quot;\\n&quot; + debugInfoStack.toString()); &#125; else &#123; throw new NotSerializableException(cl.getName()); &#125; &#125; 在进行序列化操作时，会判断要被序列化的类是否是 Enum、Array 和 Serializable 类型，如果都不是则直接抛出 NotSerializableException。Java 中还提供了 Externalizable 接口，也可以实现它来提供序列化能力。 Externalizable 继承自 Serializable，该接口中定义了两个抽象方法: writeExternal() 与 readExternal()。当使用 Externalizable 接口来进行序列化与反序列化的时候需要开发人员重 写 writeExternal() 与 readExternal() 方法。否则所有变量的值都会变成默认值。 transienttransient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可 以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。 自定义序列化策略在序列化过程中，如果被序列化的类中定义了 writeObject 和 readObject 方法， 虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的 序列化和反序列化。 如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteOb- ject 方法以及 ObjectInputStream 的 defaultReadObject 方法。用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程， 比如可以在序列化的过程中动态改变序列化的数值。 所以，对于一些特殊字段需要定义序列化的策略的时候，可以考虑使用 tran- sient 修 饰， 并 自 己 重 写 writeObject 和 readObject 方 法， 如 java.util. ArrayList 中就有这样的实现。 我们随便找几个 Java 中实现了序列化接口的类，如 String、Integer 等，我们 可以发现一个细节，那就是这些类除了实现了 Serializable 外，还定义了一个 serialVersionUID 那么，到底什么是 serialVersionUID 呢?为什么要设置这样一个字段呢? 什么是 serialVersionUID序列化是将对象的状态信息转换为可存储或传输的形式的过程。我们都知道， Java 对象是保存在 JVM 的堆内存中的，也就是说，如果 JVM 堆不存在了，那么对 象也就跟着消失了。 而序列化提供了一种方案，可以让你在即使 JVM 停机的情况下也能把对象保存 下来的方案。就像我们平时用的 U 盘一样。把 Java 对象序列化成可存储或传输的形 式(如二进制流)，比如保存在文件中。这样，当再次需要这个对象的时候，从文件中 读取出二进制流，再从二进制流中反序列化出对象。 虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重 要的一点是两个类的序列化 ID 是否一致，这个所谓的序列化 ID，就是我们在代码中 定义的 serialVersionUID。 如果 serialVersionUID 变了会怎样我们举个例子吧，看看如果 serialVersionUID 被修改了会发生什么? 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SerializableDemo1 &#123; public static void main(String[] args) &#123;//Initializes The Object User1 user = new User1(); user.setName(&quot;hollis&quot;); //Write Obj to File ObjectOutputStream oos = null; try &#123; oos = new ObjectOutputStream(new FileOutputStream(&quot;tempFile&quot;)); oos.writeObject(user); &#125; catch( IOException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(oos); &#125; &#125; &#125; class User1 implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125;``` 我们先执行以上代码，把一个 User1 对象写入到文件中。然后我们修改一下 User1 类，把 serialVersionUID 的值改为 2L。```javaclass User1 implements Serializable &#123; private static final long serialVersionUID = 2L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 然后执行以下代码，把文件中的对象反序列化出来: 1234567891011121314151617181920212223242526272829 public class SerializableDemo2 &#123; public static void main(String[] args) &#123;//Read Obj from File File file = new File(&quot;tempFile&quot;); ObjectInputStream ois = null; try &#123; ois = new ObjectInputStream(new FileInputStream(file)); User1 newUser = (User1) ois.readObject(); System.out.println(newUser); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(ois); try &#123; FileUtils.forceDelete(file); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;``` 执行结果如下```javajava.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdescserialVersionUID = 1, local class serialVersionUID = 2 可以发现，以上代码抛出了一个 java.io.InvalidClassException，并且 指出 serialVersionUID 不一致。 这是因为，在进行反序列化时，JVM 会把传来的字节流中的 serialVersio- nUID 与本地相应实体类的 serialVersionUID 进行比较，如果相同就认为是一致 的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即是 Invalid- CastException。 这也是《Java 开发手册》中规定，在兼容性升级中，在修改类的时候，不要 修改 serialVersionUID 的原因。除非是完全不兼容的两个版本。所以，serialVersionUID 其实是验证版本一致性的。 如果读者感兴趣，可以把各个版本的 JDK 代码都拿出来看一下，那些向下兼容 的类的 serialVersionUID 是没有变化过的。比如 String 类的 serialVersionUID一直都是 -6849794470754667710L。 但是，作者认为，这个规范其实还可以再严格一些，那就是规定:如果一个类实现了 Serializable 接口，就必须手动添加一个 private static final long serialVersionUID变量，并且设置初始值。 为什么要明确定一个 serialVersionUID如果我们没有在类中明确的定义一个 serialVersionUID 的话，看看会发生什么。 尝试修改上面的 demo 代码，先使用以下类定义一个对象，该类中不定义 serialVersionUID，将其写入文件。 1234567891011class User1 implements Serializable &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 然后我们修改 User1 类，向其中增加一个属性。在尝试将其从文件中读取出来， 并进行反序列化。 12345678910111213141516class User1 implements Serializable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125;&#125; 执 行 结 果:java.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdesc serialVersionUID = -2986778152837257883, local class serialVersionUID = 7961728318907695402 同样，抛出了 InvalidClassException，并且指出两个 serialVersio- nUID 不同，分别是 -2986778152837257883 和 7961728318907695402。从这里可以看出，系统自己添加了一个 serialVersionUID。 所以，一旦类实现了 Serializable，就建议明确的定义一个 serialVersionUID。不然在修改类的时候，就会发生异常。 serialVersionUID 有两种显示的生成方式: 一是默认的1L，比如:private static final long serialVersionUID = 1L;二是根据类名、接口名、成员方法及属性等来生成一个 64 位的哈希字段，比如:private static final long serialVersionUID = xxxxL; 小结serialVersionUID 是用来验证版本一致性的。所以在做兼容性升级的时候， 不要改变类中 serialVersionUID 的值。 如果一个类实现了 Serializable 接口，一定要记得定义 serialVersionUID，否则会发生异常。可以在 IDE 中通过设置，让他帮忙提示，并且可以一键快速生成一 个 serialVersionUID。 之所以会发生异常，是因为反序列化过程中做了校验，并且如果没有明确定义的 话，会根据类的属性自动生成一个。 参考资料 Java技术灵魂15问","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"极尽HashMap底层原理","date":"2021-07-21T06:26:25.000Z","path":"wiki/极尽HashMap底层原理/","text":"HashMap 中的容量与扩容实现，细致入微，值的一品 Java 8系列之重新认识HashMap 美团技术团队 HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(&quot;美团&quot;,&quot;小美&quot;); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下: int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考[http://blog.csdn.net/liuqiyao_01/article/details/14475159]，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考[http://blog.csdn.net/v_july_v/article/details/6105630]。 实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125; &#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;，&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， &quot;C&quot;); new Thread(&quot;Thread1&quot;) &#123; public void run() &#123; map.put(7, &quot;B&quot;); System.out.println(map); &#125;; &#125;.start(); new Thread(&quot;Thread2&quot;) &#123; public void run() &#123; map.put(3, &quot;A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 JDK8下hashmap的线程安全问题1、往数组添加元素时，如果发生hash碰撞，后面的数据会覆盖掉前面的数据，这也就是为什么ConcurrentHashMap 使用cas去设置数组头节点了。 2、删除的时候，每个线程操作链表时，是先读出来，然后操作完，在重新放到数组上面，可能会出现覆盖的情况。 3、当多个线程同时检测到总数量超过门限值的时候就会同时调用resize操作，各自生成新的数组并rehash后赋给该map底层的数组 table，结果最终只有最后一个线程生成的新数组被赋给table变量，其他线程的均会丢失。而且当某些线程已经完成赋值而其他线程刚开 始的时候，就会用已经被赋值的table作为原始数组，这样也会有问题。 https://blog.csdn.net/mydreamongo/article/details/8960667 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。(5) 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"elasticsearch字符串查询汇总","date":"2021-07-20T14:08:29.000Z","path":"wiki/elasticsearch字符串查询汇总/","text":"filterexistsfuzzyidsprefixregexptermtermsterms_setwildcardtext搜索 intervalmatchmatch_bool_prefixmatch_phrasematch_phrase_prefixmulti_matchcommonquery_stringsimple_query_string 参考资料 查询是否包含字符串_十九种Elasticsearch字符串搜索方式终极介绍","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch 查询值前缀不包含某个字符串","date":"2021-07-20T13:48:31.000Z","path":"wiki/elasticsearch-查询值前缀不包含某个字符串/","text":"需求 查询IP不是以11.开头的所有文档，然后获取文档访问量前100条 curl -X GET &quot;localhost:9200/yj_visit_data2,yj_visit_data3/_search?pretty&quot; -u elastic:elastic -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;11.&quot; &#125; &#125; &#125;, &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;1.&quot; &#125; &#125; &#125; ] &#125; &#125; ], &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;visitTime&quot;: &#123; &quot;gte&quot;: 1577808000000, &quot;lte&quot;: 1609430399000 &#125; &#125; &#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;term_article&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;ip&quot;, &quot;min_doc_count&quot;: 20, &quot;size&quot;: 10000 &#125; &#125; &#125;&#125;&#x27;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"https协议","date":"2021-07-20T08:30:45.000Z","path":"wiki/https协议/","text":"参考资料 《 HTTPS 升级指南 》 深入理解 HTTPS 原理、过程与实践 HTTPS实现原理 深入理解HTTPS原理、过程与实践","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"elasticsearch-reindex","date":"2021-07-20T03:59:18.000Z","path":"wiki/elasticsearch-reindex/","text":"reindex 常规使用 Reindex要求为源索引中的所有文档启用_source。Reindex不尝试设置目标索引，它不复制源索引的设置，你应该在运行_reindex操作之前设置目标索引，包括设置映射、碎片计数、副本等。 如下示例将把文档从twitter索引复制到new_twitter索引： 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 下面是返回值： 12345678910111213141516171819&#123; &quot;took&quot; : 299, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 2, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 就像_update_by_query一样，_reindex获取源索引的快照，但它的目标必须是不同的索引，因此不太可能发生版本冲突。可以像index API那样配置dest元素来控制乐观并发控制。仅仅省略version_type(如上所述)或将其设置为internal，都会导致Elasticsearch盲目地将文档转储到目标中，覆盖任何碰巧具有相同类型和id的文档 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot;, &quot;version_type&quot;: &quot;internal&quot; &#125;&#125;&#x27; 将version_type设置为external将导致Elasticsearch保存源文件的版本，创建任何缺失的文档，并更新目标索引中比源索引中版本更旧的文档： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;&#125;&#x27; 设置op_type=create将导致_reindex只在目标索引中创建缺失的文档。所有现有文件将导致版本冲突： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 默认情况下，版本冲突将中止_reindex进程，“conflicts”请求体参数可用于指示_reindex处理关于版本冲突的下一个文档，需要注意的是，其他错误类型的处理不受“conflicts”参数的影响，当在请求体中设置“conflicts”:“proceed”时，_reindex进程将继续处理版本冲突，并返回所遇到的版本冲突计数： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 返回值如下： 12345678910111213141516171819&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 2, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 可以通过向源添加查询来限制文档。这将只复制由kimchy发出的tweet到new_twitter： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; source中的index可以是一个列表，允许你在一个请求中从多个源复制。这将从twitter和blog索引复制文档： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: [&quot;twitter&quot;,&quot;blog&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; 注意：Reindex API不处理ID冲突，因此最后编写的文档将“胜出”，但顺序通常是不可预测的，因此依赖这种行为不是一个好主意，相反，可以使用脚本确保id是惟一的。还可以通过设置大小来限制处理文档的数量，示例将只复制一个单一的文件从twitter到new_twitter： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 1, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 如果你想从twitter索引中获得一组特定的文档，你需要使用sort。排序会降低滚动的效率，但在某些上下文中，这样做是值得的。如果可能的话，选择一个比大小和排序更具选择性的查询。这将把10000个文档从twitter复制到new_twitter： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10000, &quot;source&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;sort&quot;: &#123; &quot;age&quot;: &quot;desc&quot; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; source部分支持搜索请求中支持的所有元素。例如，只有原始文档中的一部分字段可以使用源过滤重新索引，如下所示： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;_source&quot;: [&quot;user&quot;, &quot;_doc&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 与_update_by_query一样，_reindex支持修改文档的脚本。与_update_by_query不同，脚本允许修改文档的元数据。这个例子改变了源文档的版本： 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;if (ctx._source.foo == \\&quot;bar\\&quot;) &#123;ctx._version++; ctx._source.remove(\\&quot;foo\\&quot;)&#125;&quot;, &quot;lang&quot;: &quot;painless&quot; &#125;&#125;&#x27; 就像在_update_by_query中一样，你可以设置ctx.op更改在目标索引上执行的操作，值为noop，delete。设置ctx.op到任何其他字段都会返回一个错误，在ctx中设置任何其他字段也是如此。可以修改以下值：_id、_index、_version、_routing。将_version设置为null或将它从ctx映射中清除，就像没有在索引请求中发送版本一样;它将导致在目标索引中覆盖文档，而不管目标上的版本或在_reindex请求中使用的版本类型。默认情况下，如果_reindex看到一个带有路由的文档，那么该路由将被保留，除非脚本更改了它，你可以设置路由对dest的请求，以改变这一点：keep：将为每个匹配发送的批量请求上的路由设置为匹配上的路由。这是默认值。discard：将为每个匹配发送的批量请求上的路由设置为null。=：将为每个匹配发送的批量请求上的路由设置为=之后的所有文本。例如，你可以使用以下请求将所有文档从具有公司名称cat的源索引复制到路由设置为cat的dest索引中： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;company&quot;: &quot;cat&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; 默认情况下，_reindex使用滚动批次为1000，可以使用源元素中的size字段更改批大小： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 100 &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; _reindex还可以通过像这样指定管道来使用Ingest节点特性： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;pipeline&quot;: &quot;some_ingest_pipeline&quot; &#125;&#125;&#x27; 远程reindex12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;username&quot;: &quot;user&quot;, &quot;password&quot;: &quot;pass&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; host参数必须包含scheme, host, port（如：http://otherhost:9200）,也可以加路径（如：http://otherhost:9200/proxy），username和password是可选的，如果远程集群开启了安全认证，那么是必选的，如果需要使用username和password，需要使用https。远程主机需要设置白名单，可以通过elasticsearch.yml文件里的reindex.remote.whitelist属性进行设置，如果设置多个值可以使用逗号来进行分隔（如：otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*），这里的配置可以忽略scheme，如： reindex.remote.whitelist: &quot;otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*&quot; 必须让每个处理reindex的节点上添加白名单的配置。这个特性应该适用于可能找到的任何版本的Elasticsearch的远程集群，这应该允许通过从旧版本的集群reindex，将Elasticsearch的任何版本升级到当前版本。要启用发送到旧版本Elasticsearch的查询，无需验证或修改即可将查询参数直接发送到远程主机，注意：远程reindex不支持手动或者自动slicing。从远程服务器reindex使用堆上缓冲区，默认最大大小为100mb，如果远程索引包含非常大的文档，则需要使用更小的批处理大小，下面的示例将批处理大小设置为10。 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 还可以使用socket_timeout字段设置远程连接上的套接字读取超时，使用connect_timeout字段设置连接超时，他们的默认值为30秒，下面示例例将套接字读取超时设置为1分钟，连接超时设置为10秒： 12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;socket_timeout&quot;: &quot;1m&quot;, &quot;connect_timeout&quot;: &quot;10s&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 配置SSL参数远程reindex支持配置SSL参数，除了在Elasticsearch秘钥库中添加安全设置之外，还需要在elasticsearch.yml文件中进行配置，不可能在_reindex请求体中配置。支持以下设置：reindex.ssl.certificate_authorities应受信任的PEM编码证书文件的路径列表，不同同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.path包含要信任的证书的Java密钥存储文件的路径，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.secure_password一起使用。reindex.ssl.truststore.secure_passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.password一起使用。reindex.ssl.truststore.typereindex.ssl.truststore.path信任存储库的类型，必须是jks或PKCS12，如果reindex.ssl.truststore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.verification_mode指示用于防止中间人攻击和伪造证书的验证类型。可以设置为full（验证主机名和证书路径）、certificate（验证证书路径，但不验证主机名）、none（不执行验证——这在生产环境中是强烈不鼓励的），默认是full。reindex.ssl.certificate指定PEM编码证书的路径或者证书链用于HTTP客户端身份认证，这个配置还需要设置reindex.ssl.key值，不能同时设置reindex.ssl.certificate和reindex.ssl.keystore.path。reindex.ssl.key指定与用于客户端身份验证的证书相关联的PEM编码私钥的路径，不能同时设置reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.secure_key_passphrase一起使用。reindex.ssl.secure_key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.key_passphrase一起使用。reindex.ssl.keystore.path指定密钥存储库的路径，其中包含用于HTTP客户机身份验证的私钥和证书(如果远程集群需要)，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.keystore.type密钥存储库的类型(reindex.ssl.keystore.path)，必须是jks或者PKCS12，如果reindex.ssl.keystore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.keystore.password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.secure_password一起使用。reindex.ssl.keystore.secure_password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.password一起使用。reindex.ssl.keystore.key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.secure_key_password一起使用。reindex.ssl.keystore.secure_key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.key_password一起使用。 URL参数除了标准的pretty参数外， reindex还支持refresh, wait_for_completion, wait_for_active_shards, timeout, scroll和requests_per_second。发送refresh url参数将导致对所写请求的所有索引进行刷新，这与Index API的refresh参数不同，后者只会刷新接收新数据的碎片，与index API不同的是，它不支持wait_for。如果请求包含wait_for_completion=false，则Elasticsearch将执行一些执行前检查，启动请求，然后返回一个任务，该任务可与Tasks api一起用于取消或获取任务状态，Elasticsearch还将创建此任务的记录，作为.tasks/task/${taskId}的文档，你可以自己决定是保留或者删除他，当你已经完成了，删除他，这样es会回收他使用的空间。wait_for_active_shards控制在进行重新索引之前必须激活多少个shard副本，超时控制每个写请求等待不可用碎片变为可用的时间，两者在批量API中的工作方式完全相同，由于_reindex使用滚动搜索，你还可以指定滚动参数来控制“搜索上下文”存活的时间(例如?scroll=10m)，默认值是5分钟。requests_per_second可以设置为任何正数(1.4、6、1000等)，并通过在每个批中填充等待时间来控制_reindex发出批索引操作的速率，可以通过将requests_per_second设置为-1来禁用。节流是通过在批之间等待来完成的，这样就可以给_reindex内部使用的滚动设置一个考虑填充的超时，填充时间是批大小除以requests_per_second和写入时间之间的差额，默认情况下批处理大小为1000，所以如果requests_per_second被设置为500： target_time = 1000 / 500 per second = 2 seconds padding time = target_time - write_time = 2 seconds - 0.5 seconds = 1.5 seconds 由于批处理是作为单个_bulk请求发出的，因此较大的批处理大小将导致Elasticsearch创建许多请求，然后等待一段时间再启动下一个请求集，这是“bursty”而不是“smooth”，默认值是-1。 响应体 12345678910111213141516171819&#123; &quot;took&quot;: 639, &quot;timed_out&quot;: false, &quot;total&quot;: 5, &quot;updated&quot;: 0, &quot;created&quot;: 5, &quot;deleted&quot;: 0, &quot;batches&quot;: 1, &quot;noops&quot;: 0, &quot;version_conflicts&quot;: 2, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: 1, &quot;throttled_until_millis&quot;: 0, &quot;failures&quot;: []&#125; took整个操作花费的总毫秒数。timed_out如果在reindex期间执行的任何请求超时，则将此标志设置为true。total成功处理的文档数量。updated成功更新的文档数量。created成功创建的文档的数量。deleted成功删除的文档数量。batches由reindex回拉的滚动响应的数量。noops由于用于reindex的脚本返回了ctx.op的noop值而被忽略的文档数量。version_conflictsreindex命中的版本冲突数。retriesreindex尝试重试的次数,bulk是重试的批量操作的数量，search是重试的搜索操作的数量。throttled_millis请求休眠以符合requests_per_second的毫秒数。requests_per_second在reindex期间每秒有效执行的请求数。throttled_until_millis在_reindex响应中，该字段应该始终等于零，它只有在使用任务API时才有意义，在任务API中，它指示下一次(毫秒)再次执行节流请求，以符合requests_per_second。failures出现多个错误以数组返回。 使用task api获取task 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&amp;wait_for_completion=false&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 返回值为 123&#123; &quot;task&quot; : &quot;8uQK-B00RiWq03awtJok1Q:18&quot;&#125; 你可以用任务API获取所有正在运行的reindex请求的状态：curl -XGET &quot;http://127.0.0.1:9200/_tasks?detailed=true&amp;actions=*reindex&amp;pretty&quot;或者根据task id获取curl -XGET &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48?pretty&quot;返回值为： 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;completed&quot; : true, &quot;task&quot; : &#123; &quot;node&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;id&quot; : 48, &quot;type&quot; : &quot;transport&quot;, &quot;action&quot; : &quot;indices:data/write/reindex&quot;, &quot;status&quot; : &#123; &quot;total&quot; : 0, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 0, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : 0.0, &quot;throttled_until_millis&quot; : 0 &#125;, &quot;description&quot; : &quot;reindex from [twitter] to [new_twitter][_doc]&quot;, &quot;start_time_in_millis&quot; : 1566216815832, &quot;running_time_in_nanos&quot; : 86829, &quot;cancellable&quot; : true, &quot;headers&quot; : &#123; &#125; &#125;, &quot;error&quot; : &#123; &quot;type&quot; : &quot;index_not_found_exception&quot;, &quot;reason&quot; : &quot;no such index [new_twitter] and [action.auto_create_index] ([twitter,index10,-index1*,+ind*,-myIndex]) doesn&#x27;t match&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;new_twitter&quot; &#125;&#125; 取消task任何reindex接口都可以使用task cancel api取消： 123456789101112131415 curl -XPOST &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48/_cancel?pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:48] doesn&#x27;t support cancellation&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; 取消应该很快发生，但可能需要几秒钟，Tasks API将继续列出任务，直到它醒来取消自己。 rethrottle可以在url中使用_rethrottle，并使用requests_per_second参数来设置节流： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex/8uQK-B00RiWq03awtJok1Q:250/_rethrottle?requests_per_second=-1&amp;pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:250] is missing&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; reindex改变属性名称_reindex可以重命名属性名，假设你创建了一个包含如下文档的索引： 12345curl -XPOST &quot;http://127.0.0.1:9200/test/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;text&quot;: &quot;words words&quot;, &quot;flag&quot;: &quot;foo&quot;&#125;&#x27; 在reindex的时候想把flag修改为tag，示例如： 12345678910111213curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.tag = ctx._source.remove(\\&quot;flag\\&quot;)&quot; &#125;&#125;&#x27; 查看order2的数据： 123456789101112131415curl -XGET &quot;http://127.0.0.1:9200/order2/_doc/1?pretty&quot;&#123; &quot;_index&quot; : &quot;order2&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;words words&quot;, &quot;tag&quot; : &quot;foo&quot; &#125;&#125; 切片Reindex支持切片滚动，以并行化重新索引过程。这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分 手动切片通过为每个请求提供一个片id和片的总数，手工切片一个重索引请求： 123456789101112131415161718192021222324252627curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 0, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27;curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 1, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 你可以通过以下方法来验证： curl -XGET &quot;http://127.0.0.1:9200/_refresh?pretty&quot; curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot; 返回值为： 123456789&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; 自动切面你还可以让_reindex使用切片滚动自动并行化_uid上的切片，使用slices指定要使用的片数: 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?slices=5&amp;refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 通过下面请求进行验证： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot;&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; reindex多个索引如果有许多索引需要reindex，通常最好一次reindex一个索引，而不是使用一个glob模式来获取许多索引。这样，如果有任何错误，可以删除部分完成的索引并从该索引重新开始，从而恢复该过程。它还使并行化过程变得相当简单：将索引列表拆分为reindex并并行运行每个列表。可以使用一次性脚本： 1234567891011for index in i1 i2 i3 i4 i5; do curl -HContent-Type:application/json -XPOST localhost:9200/_reindex?pretty -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;-reindexed&quot; &#125; &#125;&#x27;done reindex每日索引尽管有上述建议，你仍然可以结合使用_reindex和Painless来reindex每日索引，从而将新模板应用于现有文档。假设有以下文件组成的索引: curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.30/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d{“system.cpu.idle.pct”: 0.908}’ curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.31/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#39;{“system.cpu.idle.pct”: 0.105} metricbeat-*索引的新模板已经加载到Elasticsearch中，但它只适用于新创建的索引。下面的脚本从索引名称中提取日期，并创建一个附加-1的新索引。所有来自metricbeat-2016.05.31的数据将reindex到metricbeat-2016.05.31-1。 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;metricbeat-*&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;metricbeat&quot; &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._index = &#x27;metricbeat-&#x27; + (ctx._index.substring(&#x27;metricbeat-&#x27;.length(),ctx._index.length())) + &#x27;-1&#x27;&quot; &#125;&#125;&#x27; 以前metricbeat索引中的所有文档现在都可以在*-1索引中找到。 curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.30-1/_doc/1?pretty&quot;curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.31-1/_doc/1?pretty&quot; 前一种方法还可以与更改字段名称结合使用，以便仅将现有数据加载到新索引中，并在需要时重命名任何字段。 提取索引中的子集合_reindex可用于提取索引的随机子集进行测试： 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;function_score&quot; : &#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;random_score&quot; : &#123;&#125; &#125; &#125;, &quot;sort&quot;: &quot;_score&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;random_twitter&quot; &#125;&#125;&#x27; _reindex默认按_doc排序，因此random_score不会有任何效果，除非你覆盖sort属性为_score。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"Http状态码及含义","date":"2021-07-20T03:31:58.000Z","path":"wiki/Http状态码及含义/","text":"http状态码由3个十进制数字组成。第一个数字表示状态码的分类，后面的两位表示该分类下不同的状态。分为5个大类。 分类 1** 信息。服务器收到请求，请继续执行请求 2** 成功。请求被成功接收并处理 3** 重定向。需要进一步操作来完成请求 4** 客户端错误。无法完成请求，或请求包含语法错误 5** 服务器错误。服务器在处理请求的过程中发成错误 各个状态说明 100： 继续请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分。 101： 切换协议请求者已要求服务器切换协议，服务器已确认并准备切换。 200： 成功服务器已成功处理了请求。 201： 已创建请求成功并且服务器创建了新的资源。 202： 已接受服务器已接受请求，但尚未处理。 203： 非授权信息服务器已成功处理了请求，但返回的信息可能来自另一来源。 204： 无内容服务器成功处理了请求，但没有返回任何内容。 205： 重置内容服务器成功处理了请求，内容被重置。 206： 部分内容服务器成功处理了部分请求。 300： 多种选择针对请求，服务器可执行多种操作。 301： 永久移动请求的网页已永久移动到新位置，即永久重定向。 302： 临时移动请求的网页暂时跳转到其他页面，即暂时重定向。 303： 查看其他位置如果原来的请求是 POST，重定向目标文档应该通过 GET 提取。 304： 未修改此次请求返回的网页未修改，继续使用上次的资源。 305： 使用代理请求者应该使用代理访问该网页。 307： 临时重定向请求的资源临时从其他位置响应。 400： 错误请求服务器无法解析该请求。 401： 未授权请求没有进行身份验证或验证未通过。 403： 禁止访问服务器拒绝此请求。 404： 未找到服务器找不到请求的网页。 405： 方法禁用服务器禁用了请求中指定的方法。 406： 不接受无法使用请求的内容响应请求的网页。 407： 需要代理授权请求者需要使用代理授权。 408： 请求超时服务器请求超时。 409： 冲突服务器在完成请求时发生冲突。 410： 已删除请求的资源已永久删除。 411： 需要有效长度服务器不接受不含有效内容长度标头字段的请求。 412： 未满足前提条件服务器未满足请求者在请求中设置的其中一个前提条件。 413： 请求实体过大请求实体过大，超出服务器的处理能力。 414： 请求 URI 过长请求网址过长，服务器无法处理。 415： 不支持类型请求的格式不受请求页面的支持。 416： 请求范围不符页面无法提供请求的范围。 417： 未满足期望值服务器未满足期望请求标头字段的要求。 500： 服务器内部错误服务器遇到错误，无法完成请求。 501： 未实现服务器不具备完成请求的功能。 502： 错误网关服务器作为网关或代理，从上游服务器收到无效响应。 503： 服务不可用服务器目前无法使用。 504： 网关超时服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505： HTTP 版本不支持服务器不支持请求中所用的 HTTP 协议版本。","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"操作系统-死锁","date":"2021-07-20T03:29:18.000Z","path":"wiki/操作系统-死锁/","text":"造成死锁的原因当前线程拥有其他线程需要的资源，当前线程等待其他线程释放资源，线程持有资源不可剥夺，线程一直循环等待。 避免死锁的方法1、固定加锁的顺序 2、尽可能缩小锁范围，减少锁粒度 3、使用可释放的定时锁（申请一段时间，超时之后，放弃） 参考资料 死锁是什么？如何避免死锁？ 哲学家就餐问题","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"操作系统-进程调度策略","date":"2021-07-20T03:16:04.000Z","path":"wiki/操作系统进程调度策略/","text":"操作系统-进程调度策略1. 什么是进程进程是操作系统进行资源分配的基本单位，每个进程都有它自己的内存空间和系统资源。进程实现了多处理机环境下进程调度，分派，切 换时，都需要花费较大的时间和空间开销； 为了提升系统的执行效率，减少CPU的【空转时间】和【调度切换】的时间，以便于系统的管理，所以有了线程的概念，线程是系统进行 资源调度的最小单位。一个进程包含一个或者多个线程，一个进程下的线程共享这个进程的资源。进程与进程之间的资源是不共享的，线 程和线程之间的资源也是不共享的。 2.什么是进程调度算法当CPU有很多任务需要处理时，由于资源有限，不可能同时执行所有的作业和任务，这就需要确定一个规则来确定任务的执行顺序，这种 方式或者规则被称为调度算法。 进程有三种状态 运行态（running）： 正在占用CPU资源，正在运行中 就绪态（ready）：进程具备运行条件，等待系统分配CPU来运行 态阻塞 （wait）: 进程不具备运行条件，正在等待某个时间的完成 所谓进程调度，就是从进程的就绪队列（阻塞）中按照一定的算法选择一个进程并将 CPU 分配给它运行，以实现进程的并发执行。 这是操作系统中最基本（最低级）的一种调度，在一般的操作系统中都必须配置进程调度。 进程调度的频率很高，一般几十毫秒一次。 3、进程调度算法分类进程调度算法分为两种： 抢占式调度：系统会按照进程的优先级高低来进行调度，进程之间可以插队，适合分时和实时OS 非抢占式调度：系统按照进程先到先服务的方式调度，进程之间不能插队，简单，系统开销较小。 4、调度算法介绍4.1 先来先服务算法FCFS按照作业提交或进程变为就绪状态的先后次序,分派CPU； 当前作业或进程占用CPU，直到执行完或阻塞，才出让CPU(非抢占方式)。 在作业或进程唤醒后(如I/O完成)，并不立即恢复执行，通常等到当前作业或进程出让CPU。是最简单的算法。 在进程调度中采用FCFS算法时，则每次调度是从就绪队列中选择一个最先进入该队列的进程，为之分配处理机，使之投入运行。该进程一 直运行到完成或发生某事件而阻塞后才放弃处理机 FCFS算法属于非抢占（剥夺）调度算法，这种算法比较简单，能够保证不会出现饥饿现象，但是，如果很多长时间作业先到达，后面大部分是短时间作业，这种方式对于后面的短时间任务不太友好。 4.2 短进程/作业优先算法SJF短作业(进程)优先调度算法SJ(P)F，是指对短作业或短进程优先调度的算法。它们可以分别用于作业调度和进程调度。短作业优先(SJF)的调 度算法是从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。而短进程优先(SPF)调度算法则是从就绪队列 中选出一个估计运行时间最短的进程，将处理机分配给它，使它立即执行并一直执行到完成，或发生某事件而被阻塞放弃处理机时再重新 调度。 1)优点 1)比FCFS改善平均周转时间和平均带权周转时间，缩短作业的等待时间；假定所有任务同时到达，平均等待时间最短。 2)提高系统的吞吐量。 2)缺点 1)对长作业非常不利，可能长时间得不到执行；长作业可能被饿死。 2)未能依据作业的紧迫程度来划分执行的优先级。 3)难以准确估计作业(进程)的执行时间，从而影响调度性能。 4.3 最高响应比优先算法HRNFCFS方式只考虑每个作业的等待时间而未考虑执行时间的长短，而SJF方式只考虑执行时间而未考虑等待时间的长短。 因此，这两种调度算法在某些极端情况下会带来某些不便。 HRN调度策略同时考虑每个作业的等待时间长短和估计需要的执行时间长短，从中选出响应比最高的作业投入执行。 (1)优点 1)同时到达任务，短者优先。等待时间相等时，服务时间越短，优先级越高，符合SJF思想。 2)长作业随等待时间增加响应比增加。服务时间相等时，等待时间越长，优先级越高。对于长作业，只要其等待时间足够长，也能获 得处理机。 (2)缺点 1)吞吐量降低。这种算法是介于FCFS和SJF之间的一种折中算法。由于长作业也有机会投入运行，在同一时间内处理的作业数显然要 少于SJF法，从而采用HRN方式时其吞吐量将小于采用SJF法时的吞吐量。 2)系统开销增加。原因在于每次调度前要计算响应比。 4.4 最高优先数算法在进程调度中，每次调度时，系统把处理机分配给就绪队列中优先数最高的进程。它又分为两种：非抢占式优先数算法和抢占式优先数算法。 在非抢占式优先数算法下，系统一旦把处理机分配给就绪队列中优先数最高的进程后，这个进程就会一直运行，直到完成或发生某事 件使它放弃处理机，这时系统才能重新将处理机分配给就绪队列中的另一个优先数最高的进程。 在抢占式优先数算法下，系统先将处理机分配给就绪队列中优先数最高的进程度让它运行，但在运行的过程中，如果出现另一个优先 数比它高的进程，它就要立即停止，并将处理机分配给新的高优先数进程。 在抢占式优先数算法下会麻烦一些。 4.5 时间片轮转调度算法轮转(Round Robin，RR)调度算法是让每个进程在就绪队列中的等待时间与享受服务的时间成正比例。该算法适用于分时系统 每个进程所享受的CPU处理时间都是一致的。 过程： 将系统中所有的就绪进程按照FCFS原则，排成一个队列。 每次调度时将CPU分派给队首进程，让其执行一个时间片。时间片的长度从几个ms到几百ms。 在一个时间片结束时，发生时钟中断。 调度程序据此暂停当前进程的执行，将其送到就绪队列的末尾，并通过上下文切换执行当前的队首进程。 进程可以未使用完一个时间片，就出让CPU，如进程阻塞时。 4.6 最短剩余时间优先算法最短剩余时间优先(Shortest Remaining Time Next，SRTN)调度算法多用于剥夺式的调度中。 在进程调度中，每次调度时，系统把处理机分配给就绪队列中运行完所需时间最短的进程。 最短剩余时间优先算法也可用于不剥夺式调度方式中，此时退化为短作业优先算法。 也就是短作业优先算法的升级版，只不过它是抢占式的。 4.7 多级反馈排队算法 设置多个就绪队列，分别赋予不同的优先级，如逐级降低，队列1的优先级最高。每个队列执行时间片的长度也不同，规定优先 级越低则时间片越长，如逐级加倍。 新进程进入内存后，先投入队列1的末尾，按FCFS算法调度；若按队列1一个时间片未能执行完，则降低投入到队列2的末尾，同样按 FCFS算法调度；如此下去，降低到最后的队列，则按“时间片轮转”算法调度直到完成。 仅当较高优先级的队列为空，才调度较低优先级的队列中的进程执行。如果进程执行时有新进程进入较高优先级的队列，则抢先执行 新进程，并把被抢先的进程投入原队列的末尾。 可以发现，对于短作业可能在第一队列中很快被处理完成，对于长作业，如果在第一队列中没有被执行完成，则会移入等待队列被执行，虽然等待时间变长了，但是运行时间也变长了，所以这种算法很好的兼容了长短作业，同时有比较好的响应时间。 4.8 系统算法使用场景批处理系统、分时系统和实时系统中，各采用哪几种进程（作业）调度算法？批处理系统常用调度算法：①、先来先服务：FCFS②、最短作业优先③、最短剩余时间优先④、响应比最高者优先 分时系统调度算法：①、轮转调度②、优先级调度③、多级队列调度④、彩票调度 实时系统调度算法：①、单比率调度②、限期调度③、最少裕度法 5、操作算法性能指标5.1 CPU利用率CPU忙碌时间占总时间的比例 5.2 系统吞吐量系统吞吐量 = 作业完成量 / 完成时间 利用尽可能少的时间处理尽可能多的作业； 5.3 周转时间周转时间 = 完成时间 - 到达时间点 平均周转时间 = 周转时间和 / 作业数 带权周转时间 = 周转时间 / 作业实际运行时间， 这个时间肯定大于等于1，并且数值越小，说明算法越好。 平均带权周转时间 = 带权周转时间和 / 作业数 从作业被提交到系统到作业完成所经历的时间都有哪些部分？ 5.4 等待时间等待处理机时间之和： 等待时间 = 周转时间 - 运行时间（- IO时间） 这个等待时间越大，说明算法越差，作业需要考虑等待高级调度的时间，而进程是不需要的。 5.5 响应时间从提交请求到产生首次响应的时间 5.6 饥饿进程或作业长时间得不到服务，这种现象称之为“饥饿” 参考资料操作系统中的进程调度策略有哪几种 https://blog.csdn.net/qq_35642036/article/details/82809812 https://mp.weixin.qq.com/s/FaHKGRI69TqDj0AJtNiVoA","tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"进程间通信IPC","date":"2021-07-20T01:59:21.000Z","path":"wiki/进程间通信IPC/","text":"前言1、首先是管道，管道的缺点是通信效率差，只能单向，而且不能随机读取数据 2、解决管道的问题，可以使用消息队列，消息队列可以随机读取，比较灵活，但是消息队列其实是存放在内核中的，数据拷贝涉及到用 户态和内核态的切换，且大数据消息效率比较差。 3、为了解决消息队列频繁切换上下文的情况，可以使用共享内存的方式，减少数据的拷贝 4、共享内存会带来多进程的安全问题，可以使用信号量的方式来屏蔽这个问题 5、以上的方式都是单机多进程之间的通信，如果的跨机器进程间通信如何实现呢，socket！ 1. 管道|匿名管道1.1 有名管道创建管道：mkfifo name 一个线程向管道输入数据 （如果没有进程取出数据，则一直阻塞）： echo &#39;ibli&#39; &gt; name 另一个线程取出管道的数据： cat &lt; name 对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。 1.2 匿名管道存储在内存中的特殊文件ps -ef | grep zookeeper 对于匿名管道，它的通信范围是存在父子关系的进程。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。 1.3 管道的缺点1、通信方式效率很差，不适合进程间频繁的交换数据。 2、管道只能一端写入，另一段读出 1.4 管道的优点1、实现简单，操作简单 2、很容易知道管道的数据已经被另一个进程读取（阻塞特性） 2. 消息队列消息队列存放在内核中，只有在内核重启也就是操作系统重启或者显示的删除一个消息队列时，该消息队列才会被真正的删除。 和管道不同的是，消息队列在某个进程往一个队列写入消息之前，并不需要另外某个进程在该队列上等待消息的到达。 消息队列可以实现消息的随机查询，消息不一定按照先进先出的次序读取，也可以按照消息的类型读取，比FIFO更加灵活。 2.1 消息队列缺点1、进程间通信可能不及时。 2、**消息队列不适合比较大数据的传输**，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度 也是有上限。在 Linux 内核中，会有两个宏定义 MSGMAX 和 MSGMNB，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的 最大长度。 3、**消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数 据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。 3. 共享内存上面👆消息队列存在一个问题就是数据拷贝会有用户态到内核态的互相切换，这个会有性能开销，解决这个问题，可以使用** 共享内存 ** 现代操作系统，对于内存管理，采用的是** 虚拟内存技术 **，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不 同的 **物理内存 **中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影 响。这就完成了内存共享机制了。 4. 信号量为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，信 号量就实现了这一保护机制。 4.1 什么是信号量信号量的本质就是一个计数器，用来实现进程之间的互斥与同步。例如信号量的初始值是 1，然后 a 进程来访问内存1的时候，我们就把 信号量的值设为 0，然后进程b 也要来访问内存1的时候，看到信号量的值为 0 就知道已经有进程在访问内存1了，这个时候进程 b 就会访 问不了内存1。所以说，信号量也是进程之间的一种通信方式。 4.2 信号量分类Linux环境中，有三种类型： 1、Posix（可移植性操作系统接口）有名信号量（使用Posix IPC名字标识） 2、Posix基于内存的信号量（存放在共享内存区中） 3、System V信号量（在内核中维护） 这三种信号量都可用于进程间或线程间的同步。 4.3 信号量原理信号量表示资源的数量，控制信号量的方式有两种原子操作： 一个是 P 操作，这个操作会把信号量减去 -1，相减后如果信号量 &lt; 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 &gt;= 0，则表明还有资源可使用，进程可正常继续执行。 另一个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 &lt;= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 &gt; 0，则表明当前没有阻塞中的进程； P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。 5. Socket上面我们说的共享内存、管道、信号量、消息队列，他们都是多个进程在一台主机之间的通信，那两个相隔几千里的进程能够进行通信吗？ 答是必须的，这个时候 Socket 这家伙就派上用场了，例如我们平时通过浏览器发起一个 http 请求，然后服务器给你返回对应的数据，这 种就是采用 Socket 的通信方式了。 套接字特性 套接字的特性由3个属性确定，它们分别是：域、端口号、协议类型。 （1）套接字的域 它指定套接字通信中使用的网络介质，最常见的套接字域有两种： 一是AF_INET，它指的是Internet网络。 当客户使用套接字进行跨网络的连接时，它就需要用到服务器计算机的IP地址和端口来指定一台联网机器上的某个特定服务，所以在使用 socket作为通信的终点，服务器应用程序必须在开始通信之前绑定一个端口，服务器在指定的端口等待客户的连接。 另一个域AF_UNIX，表示UNIX文件系统。 它就是文件输入/输出，而它的地址就是文件名。 （2）套接字的端口号 每一个基于TCP/IP网络通讯的程序(进程)都被赋予了唯一的端口和端口号，端口是一个信息缓冲区，用于保留Socket中的输入/输出信 息，端口号是一个16位无符号整数，范围是0-65535，以区别主机上的每一个程序（端口号就像房屋中的房间号），低于256的端口号保 留给标准应用程序，比如pop3的端口号就是110，每一个套接字都组合进了IP地址、端口，这样形成的整体就可以区别每一个套接字。 （3）套接字协议类型 因特网提供三种通信机制， 一是流套接字， 流套接字在域中通过TCP/IP连接实现，同时也是AF_UNIX中常用的套接字类型。流套接字提供的是一个有序、可靠、双向字节流的连接， 因此发送的数据可以确保不会丢失、重复或乱序到达，而且它还有一定的出错后重新发送的机制。 二个是数据报套接字， 它不需要建立连接和维持一个连接，它们在域中通常是通过UDP/IP协议实现的。它对可以发送的数据的长度有限制，数据报作为一个单 独的网络消息被传输,它可能会丢失、复制或错乱到达，UDP不是一个可靠的协议，但是它的速度比较高，因为它并一需要总是要建立和 维持一个连接。 三是原始套接字， 原始套接字允许对较低层次的协议直接访问，比如IP、 ICMP协议，它常用于检验新的协议实现，或者访问现有服务中配置的新设备，因 为RAW SOCKET可以自如地控制Windows下的多种协议，能够对网络底层的传输机制进行控制，所以可以应用原始套接字来操纵网络层 和传输层应用。比如，我们可以通过RAW SOCKET来接收发向本机的ICMP、IGMP协议包，或者接收TCP/IP栈不能够处理的IP包，也可以 用来发送一些自定包头或自定协议的IP包。网络监听技术很大程度上依赖于SOCKET_RAW。 参考资料 进程间通信IPC (InterProcess Communication) 记一次面试：进程之间究竟有哪些通信方式？ Linux进程间通信(四) - 共享内存 凉了！某丙没答好「进程间通信」，被面试官挂了….","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"elasticsearch统计每年每小时访问量","date":"2021-07-19T14:53:39.000Z","path":"wiki/elasticsearch统计每年每小时访问量/","text":"需求背景，要统计文章在一年的时间内，每个小时的访问情况，按照0点举例子，每个文章，一年内每一天0点的访问次数累加起来； Elasticsearch索引如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;c&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ip&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;p&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ua&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;visitTime&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125; &#125;&#125; Java RestHighLevelClient写法12345678910111213141516171819202122232425262728public void getDateDist() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(&quot;yj_visit_data2&quot;); TermsAggregationBuilder termsAggregation = AggregationBuilders.terms(&quot;article&quot;) .field(&quot;article.keyword&quot;).size(2200) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) .calendarInterval(DateHistogramInterval.HOUR)); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(termsAggregation); sourceBuilder.query(QueryBuilders.rangeQuery(&quot;visitTime&quot;).gt(&quot;1609430400000&quot;).lte(&quot;1625068799000&quot;)); sourceBuilder.timeout(new TimeValue(900000)); SearchRequest request = new SearchRequest(); request.source(sourceBuilder); SearchResponse search = restHighLevelClient.search(request, RequestOptions.DEFAULT); Aggregations aggregations = search.getAggregations(); log.info(&quot;agg -&gt; &#123;&#125;&quot;, aggregations.asList().size()); List&lt;? extends Terms.Bucket&gt; buckets = ((ParsedStringTerms) aggregations.asList().get(0)).getBuckets(); List&lt;ArticleHourData&gt; hourDataList = new ArrayList&lt;&gt;(); for (Terms.Bucket bucket : buckets) &#123; List&lt;? extends Histogram.Bucket&gt; innerBuckets = ((ParsedDateHistogram) bucket.getAggregations().asList().get(0)).getBuckets(); hourDataList.add(calcBucket(innerBuckets, bucket.getKeyAsString())); &#125; log.info(&quot;result ----&gt; &#123;&#125;&quot;, JSONObject.toJSONString(hourDataList)); &#125; 聚合分析123456789101112131415161718192021public ArticleHourData calcBucket(List&lt;? extends Histogram.Bucket&gt; innerBuckets, String article) &#123; log.info(&quot;innerBuckets get(0) ---&gt; &#123;&#125;&quot;, JSON.toJSONString(innerBuckets.get(0))); Map&lt;String, ? extends List&lt;? extends Histogram.Bucket&gt;&gt; hourMap = innerBuckets.stream() .collect(Collectors.groupingBy(bucket -&gt; getHour(bucket.getKeyAsString()))); log.info(&quot;collect ======&gt; &#123;&#125; &quot;, JSONObject.toJSONString(hourMap.keySet())); ArticleHourData hourData = ArticleHourData.builder().article(article).build(); if (hourMap.isEmpty()) &#123; return hourData; &#125; HashMap&lt;String, Long&gt; hashMap = new HashMap&lt;&gt;(); for (String hour : hourMap.keySet()) &#123; List&lt;? extends Histogram.Bucket&gt; list = hourMap.get(hour); if (CollectionUtils.isEmpty(list)) &#123; continue; &#125; hashMap.put(hour, list.stream().mapToLong(Histogram.Bucket::getDocCount).sum()); &#125; hourData.setCountMap(hashMap); return hourData; &#125; 获取时间的小时123456789101112private String getHour(String date) &#123; date = date.replace(&quot;Z&quot;, &quot; UTC&quot;); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSS Z&quot;); Date d = null; try &#123; d = format.parse(date); &#125; catch (ParseException e) &#123; e.printStackTrace(); return null; &#125; return String.valueOf(DateUtil.asLocalDateTime(d).getHour()); &#125; Python写法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from json.decoder import JSONDecoderfrom elasticsearch import Elasticsearchimport logging,jsonfrom datetime import datetimees = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;39.107.117.232&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;elastic&#x27;), timeout = 90000)sqs = &#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;art&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot;, &quot;size&quot;: 5 &#125;, &quot;aggs&quot;: &#123; &quot;art_total&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot; &#125; &#125;, &quot;_time&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;visitTime&quot;, &quot;calendar_interval&quot;: &quot;hour&quot; &#125; &#125; &#125; &#125; &#125;&#125;_search_result = es.search(index=&quot;yj_visit_data2&quot; , body=sqs)_result_json = json.dumps(_search_result,sort_keys=True, indent=4, separators=(&#x27;, &#x27;, &#x27;: &#x27;), ensure_ascii=False)aggregations = _search_result[&#x27;aggregations&#x27;]art = aggregations[&#x27;art&#x27;]buckets = art[&#x27;buckets&#x27;]#print(type(buckets)) ; print(buckets)def getHour(time): return (int)(time[11:13])# 计算每个小时的点击数def countByMonth(dataList , hourTar): _count = 0 for data in dataList: timestamp = data[&#x27;key_as_string&#x27;] hour = getHour(timestamp) if hour == hourTar: _count = (int)(data[&#x27;doc_count&#x27;]) + _count return _count final_list = []# 循环计算每一个文章for outBucket in buckets: simple_result = &#123;&#125; _time = outBucket[&#x27;_time&#x27;] innerBuckets = _time[&#x27;buckets&#x27;] print(&quot;time inner bucker size&quot; , len(innerBuckets)) simple_list = [] for num in range(0,24): simple_list.append(countByMonth(innerBuckets,num)) simple_result[0] = outBucket[&#x27;key&#x27;] simple_result[1] = simple_list final_list.append(simple_result)print(&quot;final result ----&gt; &quot;,final_list)","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch踩坑","date":"2021-07-19T09:46:59.000Z","path":"wiki/elasticsearch踩坑/","text":"search.max_bucketsThis limit can be set by changing the [search.max_buckets] cluster level setting.]]; 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;search.max_buckets&quot;: 100000 &#125;&#125; Required one of fields [field, script][Elasticsearch exception [type=illegal_argument_exception, reason=Required one of fields [field, script], but none were specified. ] 123456789101112@Test public void test1() &#123; NativeSearchQuery nativeSearchQuery = new NativeSearchQuery(QueryBuilders.matchAllQuery(), null); Script script = new Script(&quot;doc[&#x27;article.keyword&#x27;]&quot;); nativeSearchQuery.addAggregation(AggregationBuilders.terms(&quot;art&quot;) .field(&quot;article.keyword&quot;).size(10) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) // 这一行没有写 .calendarInterval(DateHistogramInterval.HOUR))); SearchHits&lt;YjVisitData&gt; search = elasticsearchRestTemplate.search(nativeSearchQuery, YjVisitData.class); System.err.println(&quot;search.getAggregations().asList() &#123;&#125;&quot; + search.getAggregations().asList().size()); &#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"python3基本数据类型","date":"2021-07-18T13:05:52.000Z","path":"wiki/python基本数据类型/","text":"Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。 等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值 标准数据类型Python3 中有六个标准的数据类型： Number（数字） String（字符串） List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 Number（数字）Python3 支持 int、float、bool、complex（复数）。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。 123&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class &#x27;int&#x27;&gt; &lt;class &#x27;float&#x27;&gt; &lt;class &#x27;bool&#x27;&gt; &lt;class &#x27;complex&#x27;&gt; 此外还可以用 isinstance 来判断： 1234&gt;&gt;&gt; a = 111&gt;&gt;&gt; isinstance(a, int)True&gt;&gt;&gt; ⚠️ isinstance 和 type 的区别在于：type()不会认为子类是一种父类类型。isinstance()会认为子类是一种父类类型。 ⚠️ 注意：Python3 中，bool 是 int 的子类，True 和 False 可以和数字相加 True==1，False==0 是会返回 Ture，但可以通过 is 来判断类型。1、Python可以同时为多个变量赋值，如a, b = 1, 2。2、一个变量可以通过赋值指向不同类型的对象。3、数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。4、在混合计算时，Python会把整型转换成为浮点数。 String（字符串）Python中的字符串用单引号 ‘ 或双引号 “ 括起来，同时使用反斜杠 \\ 转义特殊字符。字符串的截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 List（列表）List（列表） 是 Python 中使用最频繁的数据类型。 列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号 [] 之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 Tuple（元组）元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： 1234567891011#!/usr/bin/python3tuple = ( &#x27;abcd&#x27;, 786 , 2.23, &#x27;runoob&#x27;, 70.2 )tinytuple = (123, &#x27;runoob&#x27;)print (tuple) # 输出完整元组print (tuple[0]) # 输出元组的第一个元素print (tuple[1:3]) # 输出从第二个元素开始到第三个元素print (tuple[2:]) # 输出从第三个元素开始的所有元素print (tinytuple * 2) # 输出两次元组print (tuple + tinytuple) # 连接元组 Set（集合）集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。 基本功能是进行成员关系测试和删除重复元素。 可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 创建格式：parame = &#123;value01,value02,...&#125; 或者 set(value) 1234567891011121314151617181920212223242526#!/usr/bin/python3sites = &#123;&#x27;Google&#x27;, &#x27;Taobao&#x27;, &#x27;Runoob&#x27;, &#x27;Facebook&#x27;, &#x27;Zhihu&#x27;, &#x27;Baidu&#x27;&#125;print(sites) # 输出集合，重复的元素被自动去掉# 成员测试if &#x27;Runoob&#x27; in sites : print(&#x27;Runoob 在集合中&#x27;)else : print(&#x27;Runoob 不在集合中&#x27;)# set可以进行集合运算a = set(&#x27;abracadabra&#x27;)b = set(&#x27;alacazam&#x27;)print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素 Dictionary（字典）字典（dictionary）是Python中另一个非常有用的内置数据类型。 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。 键(key)必须使用不可变类型。 在同一个字典中，键(key)必须是唯一的。 #!/usr/bin/python3 dict = &#123;&#125; dict[&#39;one&#39;] = &quot;1 - 菜鸟教程&quot; dict[2] = &quot;2 - 菜鸟工具&quot; tinydict = &#123;&#39;name&#39;: &#39;runoob&#39;,&#39;code&#39;:1, &#39;site&#39;: &#39;www.runoob.com&#39;&#125; print (dict[&#39;one&#39;]) # 输出键为 &#39;one&#39; 的值 print (dict[2]) # 输出键为 2 的值 print (tinydict) # 输出完整的字典 print (tinydict.keys()) # 输出所有键 print (tinydict.values()) # 输出所有值 注意：1、字典是一种映射类型，它的元素是键值对。2、字典的关键字必须为不可变类型，且不能重复。3、创建空字典使用 { }。 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python3基础语法","date":"2021-07-18T12:31:55.000Z","path":"wiki/python3基础语法/","text":"编码默认情况下，Python 3 源码文件以 UTF-8 编码，所有字符串都是 unicode 字符串。 当然你也可以为源码文件指定不同的编码：# -*- coding: cp-1252 -*- 标识符 第一个字符必须是字母表中字母或下划线 _ 。 标识符的其他的部分由字母、数字和下划线组成。 标识符对大小写敏感。在 Python 3 中，可以用中文作为变量名，非 ASCII 标识符也是允许的了。 python保留字import keywordkeyword.kwlist[‘False’, ‘None’, ‘True’, ‘and’, ‘as’, ‘assert’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] 注释Python中单行注释以 # 开头,多行注释可以用多个 # 号，还有 ‘’’ 和 “””： 行与缩进python最具特色的就是使用缩进来表示代码块，不需要使用大括号 {} 。缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数。 多行语句Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠 \\ 来实现多行语句，例如： 123total = item_one + \\ item_two + \\ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠 \\，例如： total = [&#39;item_one&#39;, &#39;item_two&#39;, &#39;item_three&#39;, &#39;item_four&#39;, &#39;item_five&#39;] 数字(Number)类型python中数字有四种类型：整数、布尔型、浮点数和复数。 int (整数) , 如 1, 只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 bool (布尔), 如 True。 float (浮点数), 如 1.23、3E-2 complex (复数), 如 1 + 2j、 1.1 + 2.2j 字符串(String) python中单引号和双引号使用完全相同。 使用三引号(‘’’ 或 “””)可以指定一个多行字符串。 转义符 \\ 反斜杠可以用来转义，使用r可以让反斜杠不发生转义。。 如 r”this is a line with \\n” 则\\n会显示，并不是换行。 按字面意义级联字符串，如”this “ “is “ “string”会被自动转换为this is string。 字符串可以用 + 运算符连接在一起，用 * 运算符重复。 Python 中的字符串有两种索引方式，从左往右以 0 开始，从右往左以 -1 开始。 Python中的字符串不能改变。 Python 没有单独的字符类型，一个字符就是长度为 1 的字符串。 字符串的截取的语法格式如下：变量[头下标:尾下标:步长] 空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。 记住：空行也是程序代码的一部分。 同一行显示多条语句Python 可以在同一行中使用多条语句，语句之间使用分号 ; 分割，以下是一个简单的实例： 12#!/usr/bin/python3import sys; x = &#x27;runoob&#x27;; sys.stdout.write(x + &#x27;\\n&#x27;) 多个语句构成代码组缩进相同的一组语句构成一个代码块，我们称之代码组。 像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。 我们将首行及后面的代码组称为一个子句(clause)。 如下实例： 12345678910111213141516171819202122232425if expression : suiteelif expression : suite else : suite``` ## print 输出print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=&quot;&quot;：## import 与 from...import在 python 用 import 或者 from...import 来导入相应的模块。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数,格式为： from somemodule import somefunction从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import *## 命令行参数很多程序可以执行一些操作来查看一些基本信息，Python可以使用-h参数查看各参数帮助信息： $ python -husage: python [option] … [-c cmd | -m mod | file | -] [arg] …Options and arguments (and corresponding environment variables):-c cmd : program passed in as string (terminates option list)-d : debug output from parser (also PYTHONDEBUG=x)-E : ignore environment variables (such as PYTHONPATH)-h : print this help message and exit [ etc. ] ```我们在使用脚本形式执行 Python 时，可以接收命令行输入的参数;","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python操作elasticsearch","date":"2021-07-18T12:20:25.000Z","path":"wiki/python操作elasticsearch/","text":"下载python对应的elasticsearch依赖包pip3 install elasticsearch==7.10.0 python操作elasticsearch代码1234567from elasticsearch import Elasticsearchprint(&quot;init ...&quot;)es = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;XXXXXX&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;XXXXXX&#x27;))# print(es.get(index=&#x27;yj_ip_pool&#x27;, doc_type=&#x27;_doc&#x27;, id=&#x27;9256058&#x27;))countRes = es.count(index=&#x27;yj_ip_pool&#x27;)print(countRes) 查询效果12345gaolei:awesome-python3-webapp gaolei$ /usr/local/opt/python/bin/python3.7 /Users/gaolei/Documents/DemoProjects/awesome-python3-webapp/www/es_test.pyinit ...&#123;&#x27;count&#x27;: 20095400, &#x27;_shards&#x27;: &#123;&#x27;total&#x27;: 1, &#x27;successful&#x27;: 1, &#x27;skipped&#x27;: 0, &#x27;failed&#x27;: 0&#125;&#125;gaolei:awesome-python3-webapp gaolei$","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"kibana添加用户及控制权限","date":"2021-07-15T15:53:44.000Z","path":"wiki/kibana添加用户及控制权限/","text":"操作步骤： 【修改elasticsearch配置文件】 -&gt; 【重启elasticsearch】 -&gt; 【初始化账号&amp;密码】 -&gt; 【修改kibana配置文件】 -&gt; 【重启kibana】 -&gt; 【初始账号登录kibana】 -&gt; 【创建、配置角色】 -&gt; 【创建新用户】 配置elasticsearch开启自带的xpack的验证功能xpack.security.enabled: true 配置单节点模式discovery.type: single-node 坑1: 集群节点配置报错cluster.initial_master_nodes 坑2: 本次存储节点配置报错maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?node.max_local_storage_nodes: 2 为内置账号设置密码在elasticsearch的bin下执行以下命令：./elasticsearch-setup-passwords interactive 配置kibana123#使用初始用户kibanaelasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;密码&quot; 重启kibana之后使用初始账号 elastic 登录 创建、配置角色 创建新用户 查看新用户访问界面角色里面配置了kibana的访问权限，只开通了discover和dashboard两个入口 索引的话，有好几个索引，但是只配置了一个索引的权限 参考资料 kibana7.2添加登录及权限 Elasticsearch 安装","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch稳定性调优","date":"2021-07-15T07:22:49.000Z","path":"wiki/elasticsearch稳定性调优/","text":"Elasticsearch性能优化总结Elasticsearch调优实践 稳定性调优一 Linux参数调优 修改系统资源限制 👇单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些 echo &quot;* - nofile 655360&quot; &gt;&gt;/etc/security/limits.conf单用户内存地址空间 echo &quot;* - as unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户线程数 echo &quot;* - nproc 2056474&quot; &gt;&gt;/etc/security/limits.conf单用户文件大小 echo &quot;* - fsize unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户锁定内存 echo &quot;* - memlock unlimited&quot; &gt;&gt;/etc/security/limits.conf单进程可以使用的最大map内存区域数量 echo &quot;vm.max_map_count = 655300&quot; &gt;&gt;/etc/sysctl.confTCP全连接队列参数设置， 这样设置的目的是防止节点数较多（比如超过100）的ES集群中，节点异常重启时全连接队列在启动瞬间打满，造成节点hang住，整个集群响应迟滞的情况echo &quot;net.ipv4.tcp_abort_on_overflow = 1&quot; &gt;&gt;/etc/sysctl.conf echo &quot;net.core.somaxconn = 2048&quot; &gt;&gt;/etc/sysctl.conf降低tcp alive time，防止无效链接占用链接数 echo 300 &gt;/proc/sys/net/ipv4/tcp_keepalive_time ES节点配置jvm.options-Xms和-Xmx设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给系统cache使用。 jvm内存建议不要低于2G，否则有可能因为内存不足导致ES无法正常启动或OOMjvm建议不要超过32G，否则jvm会禁用内存对象指针压缩技术，造成内存浪费 elasticsearch.yml设置内存熔断参数，防止写入或查询压力过高导致OOM，具体数值可根据使用场景调整。indices.breaker.total.limit: 30% indices.breaker.request.limit: 6% indices.breaker.fielddata.limit: 3% 调小查询使用的cache，避免cache占用过多的jvm内存，具体数值可根据使用场景调整。indices.queries.cache.count: 500 indices.queries.cache.size: 5% 单机多节点时，主从shard分配以ip为依据，分配到不同的机器上，避免单机挂掉导致数据丢失。cluster.routing.allocation.awareness.attributes: ip node.attr.ip: 1.1.1.1 ES使用方式节点数较多的集群，增加专有master，提升集群稳定性ES集群的元信息管理、index的增删操作、节点的加入剔除等集群管理的任务都是由master节点来负责的，master节点定期将最新的集群状态广播至各个节点。所以，master的稳定性对于集群整体的稳定性是至关重要的。当集群的节点数量较大时（比如超过30个节点），集群的管理工作会变得复杂很多。此时应该创建专有master节点，这些节点只负责集群管理，不存储数据，不承担数据读写压力；其他节点则仅负责数据读写，不负责集群管理的工作。 这样把集群管理和数据的写入/查询分离，互不影响，防止因读写压力过大造成集群整体不稳定。 将专有master节点和数据节点的分离，需要修改ES的配置文件，然后滚动重启各个节点。 专有master节点的配置文件（conf/elasticsearch.yml）增加如下属性：node.master: truenode.data: falsenode.ingest: false数据节点的配置文件增加如下属性（与上面的属性相反）：node.master: falsenode.data: truenode.ingest: true 控制index、shard总数量上面提到，ES的元信息由master节点管理，定期同步给各个节点，也就是每个节点都会存储一份。这个元信息主要存储在clusterstate中，如所有node元信息（indices、节点各种统计参数）、所有index/shard的元信息（mapping, location, size）、元数据ingest等。 ES在创建新分片时，要根据现有的分片分布情况指定分片分配策略，从而使各个节点上的分片数基本一致，此过程中就需要深入遍历clusterstate。当集群中的index/shard过多时，clusterstate结构会变得过于复杂，导致遍历clusterstate效率低下，集群响应迟滞。基础架构部数据库团队曾经在一个20个节点的集群里，创建了4w+个shard，导致新建一个index需要60s+才能完成。 当index/shard数量过多时，可以考虑从以下几方面改进： 降低数据量较小的index的shard数量 把一些有关联的index合并成一个index 数据按某个维度做拆分，写入多个集群 Segment Memory优化前面提到，ES底层采用Lucene做存储，而Lucene的一个index又由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。Lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在Lucene4.0以后采用的数据结构是FST (Finite State Transducer)。Lucene加载segment的时候将其全量装载到内存中，加快查询速度。这部分内存被称为SegmentMemory， 常驻内存，占用heap，无法被GC。 前面提到，为利用JVM的对象指针压缩技术来节约内存，通常建议JVM内存分配不要超过32G。当集群的数据量过大时，SegmentMemory会吃掉大量的堆内存，而JVM内存空间又有限，此时就需要想办法降低SegmentMemory的使用量了，常用方法有下面几个： 定期删除不使用的index 对于不常访问的index，可以通过close接口将其关闭，用到时再打开 通过force_merge接口强制合并segment，降低segment数量 基础架构部数据库团队在此基础上，对FST部分进行了优化，释放高达40%的Segment Memory内存空间。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"启动ELK脚本命令","date":"2021-07-14T15:37:17.000Z","path":"wiki/启动ELK脚本命令/","text":"esuser 授权chown -R esuser /usr/local/elasticsearch/* elastcisearch 启动脚本nohup ./elasticsearch-7.10.0/bin/elasticsearch &gt;&gt; ./elasticsearch-7.10.0/nohup.out 2&gt;&amp;1 &amp; kibana 启动脚本nohup ./bin/kibana &gt;&gt; ./nohup.out 2&gt;&amp;1 &amp; logstash 启动脚本nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf &gt;&gt; /usr/local/logstash/logstash-7.10.0/nohup.out 2&gt;&amp;1 &amp;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch调优实践","date":"2021-07-14T15:12:16.000Z","path":"wiki/elasticsearch调优实践-0/","text":"从性能和稳定性两方面，从linux参数调优、ES节点配置和ES使用方式三个角度入手，介绍ES调优的基本方案。当然，ES的调优绝不能一概而论，需要根据实际业务场景做适当的取舍和调整 Elasticsearch性能优化总结Elasticsearch调优实践 Linux优化关闭交换分区，防止内存置换降低性能。将 /etc/fstab 文件中包含swap的行注释掉sed -i &#39;/swap/s/^/#/&#39; /etc/fstabswapoff -a 磁盘挂载选项noatime：禁止记录访问时间戳，提高文件系统读写性能data=writeback： 不记录data journal，提高文件系统写入性能barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了nobh：关闭buffer_head，防止内核打断大块数据的IO操作mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data 对于SSD磁盘，采用电梯调度算法因为SSD提供了更智能的请求调度算法，不需要内核去做多余的调整 (仅供参考)echo noop &gt; /sys/block/sda/queue/scheduler ES节点配置conf/elasticsearch.yml文件： 适当增大写入buffer和bulk队列长度，提高写入性能和稳定性indices.memory.index_buffer_size: 15%thread_pool.bulk.queue_size: 1024 计算disk使用量时，不考虑正在搬迁的shard在规模比较大的集群中，可以防止新建shard时扫描所有shard的元数据，提升shard分配速度。cluster.routing.allocation.disk.include_relocations: false 三 ES使用方式控制字段的存储选项ES底层使用Lucene存储数据，主要包括行存（StoreFiled）、列存（DocValues）和倒排索引（InvertIndex）三部分。 大多数使用场景中，没有必要同时存储这三个部分，可以通过下面的参数来做适当调整： StoreFiled行存，其中占比最大的是source字段，它控制doc原始数据的存储。在写入数据时，ES把doc原始数据的整个json结构体当做一个string，存储为source字段。查询时，可以通过source字段拿到当初写入时的整个json结构体。 所以，如果没有取出整个原始json结构体的需求，可以通过下面的命令，在mapping中关闭source字段或者只在source中存储部分字段，数据查询时仍可通过ES的docvaluefields获取所有字段的值。注意：关闭source后， update, updatebyquery, reindex等接口将无法正常使用，所以有update等需求的index不能关闭source。 关闭 _source12345678910PUT my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; _source只存储部分字段通过includes指定要存储的字段或者通过excludes滤除不需要的字段 123456789101112131415161718PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;*.count&quot;, &quot;meta.*&quot; ], &quot;excludes&quot;: [ &quot;meta.description&quot;, &quot;meta.other.*&quot; ] &#125; &#125; &#125;&#125; docvalues 控制列存。ES主要使用列存来支持sorting, aggregations和scripts功能，对于没有上述需求的字段，可以通过下面的命令关闭docvalues，降低存储成本。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false &#125; &#125; &#125; &#125;&#125; ndex：控制倒排索引。ES默认对于所有字段都开启了倒排索引，用于查询。对于没有查询需求的字段，可以通过下面的命令关闭倒排索引。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; allES的一个特殊的字段 ES把用户写入json的所有字段值拼接成一个字符串后，做分词，然后保存倒排索引，用于支持整个json的全文检索。这种需求适用的场景较少，可以通过下面的命令将all字段关闭，节约存储成本和cpu开销。（ES 6.0+以上的版本不再支持_all字段，不需要设置）12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; fieldnames该字段用于exists查询，来确认某个doc里面有无一个字段存在。若没有这种需求，可以将其关闭。12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_field_names&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; 开启最佳压缩对于打开了上述_source字段的index，可以通过下面的命令来把lucene适用的压缩算法替换成 DEFLATE，提高数据压缩率。PUT /my_index/_settings&#123; &quot;index.codec&quot;: &quot;best_compression&quot;&#125; bulk批量写入写入数据时尽量使用下面的bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w，具体可根据业务场景选取一个适当的数量。 调整translog同步策略默认情况下，translog的持久化策略是，对于每个写入请求都做一次flush，刷新translog数据到磁盘上。这种频繁的磁盘IO操作是严重影响写入性能的，如果可以接受一定概率的数据丢失（这种硬件故障的概率很小），可以通过下面的命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。 1234567891011PUT my_index&#123;&quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;translog&quot;: &#123; &quot;sync_interval&quot;: &quot;5s&quot;, &quot;durability&quot;: &quot;async&quot; &#125; &#125;&#125;&#125; 调整refresh_interval写入Lucene的数据，并不是实时可搜索的，ES必须通过refresh的过程把内存中的数据转换成Lucene的完整segment后，才可以被搜索。默认情况下，ES每一秒会refresh一次，产生一个新的segment，这样会导致产生的segment较多，从而segment merge较为频繁，系统开销较大。如果对数据的实时可见性要求较低，可以通过下面的命令提高refresh的时间间隔，降低系统开销。 PUT my_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;refresh_interval&quot; : &quot;30s&quot; &#125; &#125;&#125; merge并发控制ES的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。默认值是Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))，当节点配置的cpu核数较高时，merge占用的资源可能会偏高，影响集群的性能，可以通过下面的命令调整某个index的merge过程的并发度： PUT /my_index/_settings&#123; &quot;index.merge.scheduler.max_thread_count&quot;: 2&#125; 写入数据不指定_id，让ES自动产生当用户显示指定id写入数据时，ES会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。这样每次写入时，ES都会耗费一定的资源做查询。如果用户写入数据时不指定doc，ES则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。 所以，在不需要通过id字段去重、update的使用场景中，写入不指定id可以提升写入速率。基础架构部数据库团队的测试结果显示，无id的数据写入性能可能比有_id的高出近一倍，实际损耗和具体测试场景相关。 routing对于数据量较大的index，一般会配置多个shard来分摊压力。这种场景下，一个查询会同时搜索所有的shard，然后再将各个shard的结果合并后，返回给用户。对于高并发的小查询场景，每个分片通常仅抓取极少量数据，此时查询过程中的调度开销远大于实际读取数据的开销，且查询速度取决于最慢的一个分片。开启routing功能后，ES会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。如果查询时指定routing，那么ES只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。 routing的使用方式如下： 12# 写入PUT my_index/my_type/1?routing=user1&#123; &quot;title&quot;: &quot;This is a document&quot;&#125;# 查询GET my_index/_search?routing=user1,user2 &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;document&quot; &#125; &#125;&#125; 为string类型的字段选取合适的存储方式存为text类型的字段（string字段默认类型为text）：做分词后存储倒排索引，支持全文检索，可以通过下面几个参数优化其存储方式： - norms：用于在搜索时计算该doc的_score（代表这条数据与搜索条件的相关度），如果不需要评分，可以将其关闭。 - indexoptions：控制倒排索引中包括哪些信息（docs、freqs、positions、offsets）。对于不太注重score/highlighting的使用场景，可以设为 docs来降低内存/磁盘资源消耗。 - fields: 用于添加子字段。对于有sort和聚合查询需求的场景，可以添加一个keyword子字段以支持这两种功能。 123456789101112131415161718192021222324252627282930313233343536 &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;norms&quot;: false, &quot;index_options&quot;: &quot;docs&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; ``` #### 存为keyword类型的字段不做分词，不支持全文检索。text分词消耗CPU资源，冗余存储keyword子字段占用存储空间。如果没有全文索引需求，只是要通过整个字段做搜索，可以设置该字段的类型为keyword，提升写入速率，降低存储成本。 设置字段类型的方法有两种：一是创建一个具体的index时，指定字段的类型；二是通过创建template，控制某一类index的字段类型。- 通过mapping指定 tags 字段为keyword类型```jsonPUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;tags&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; 通过template，指定my_index*类的index，其所有string字段默认为keyword类型PUT _template/my_template12345678910111213141516171819202122&#123; &quot;order&quot;: 0, &quot;template&quot;: &quot;my_index*&quot;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; ] &#125; &#125;, &quot;aliases&quot;: &#123; &#125; &#125; 查询时，使用query-bool-filter组合取代普通query默认情况下，ES通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。但对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。此时，可以通过query-bool-filter组合来让ES不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。 普通查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;Kimchy&quot; &#125; &#125;&#125; query-bool-filter 加速查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;Kimchy&quot; &#125; &#125; &#125; &#125;&#125; index按日期滚动，便于管理写入ES的数据最好通过某种方式做分割，存入不同的index。常见的做法是将数据按模块/功能分类，写入不同的index，然后按照时间去滚动生成index。这样做的好处是各种数据分开管理不会混淆，也易于提高查询效率。同时index按时间滚动，数据过期时删除整个index，要比一条条删除数据或deletebyquery效率高很多，因为删除整个index是直接删除底层文件，而deletebyquery是查询-标记-删除。 举例说明，假如有[modulea,moduleb]两个模块产生的数据，那么index规划可以是这样的：一类index名称是modulea + {日期}，另一类index名称是module_b+ {日期}。对于名字中的日期，可以在写入数据时自己指定精确的日期，也可以通过ES的ingest pipeline中的index-name-processor实现（会有写入性能损耗）。 按需控制index的分片数和副本数分片（shard）：一个ES的index由多个shard组成，每个shard承载index的一部分数据。 副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。 shard数量（numberofshards）设置过多或过低都会引发一些问题：shard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。 对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理。基础架构部数据库团队的推荐方案是： 对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置35个shard，numberofreplicas设置为1即可（也就是一主一从，共两副本） 。对于数据量较大（100GB以上）的index：一般把单个shard的数据量控制在（20GB50GB）让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"修改mysql表创建时间","date":"2021-07-14T13:42:53.000Z","path":"wiki/修改mysql表创建时间/","text":"修改服务器时间date -s &quot;2021-07-14 21:22:10&quot; 执行DDLalter table mirror_user comment &#39;用户表&#39;; 服务器时间修正ntpdate ntp1.aliyun.com","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"hutool导出excel","date":"2021-07-14T13:01:13.000Z","path":"wiki/hutool导出excel/","text":"如果你仅需一个Java导出excel的工具，👇就可以满足你的临时需求，当然代码下面这么写肯定是不规范的，可以稍后完善！ 添加依赖123456789101112131415161718&lt;!-- https://mvnrepository.com/artifact/cn.hutool/hutool-all --&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; 数据类1234567@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class IPData &#123; private String ip;&#125; Export方法示例12345678910public void export(List&lt;IPData&gt; rows) throws FileNotFoundException &#123; ExcelWriter writer = ExcelUtil.getWriter(true); writer.renameSheet(&quot;所有数据&quot;); //甚至sheet的名称 writer.addHeaderAlias(&quot;ip&quot;, &quot;IP&quot;); writer.write(rows, true); writer.setOnlyAlias(true); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/IP1.xlsx&quot;); writer.flush(fileOutputStream); writer.close(); &#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"git常用命令","date":"2021-07-14T01:55:55.000Z","path":"wiki/git常用命令/","text":"初始化本地仓库git initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin git@github.com:XXXXXgit push -u origin master 批量删除分支远程：git branch -r| grep &#39;ss-1&#39; | sed &#39;s/origin\\///g&#39; | xargs -I &#123;&#125; git push origin :&#123;&#125;本地：git branch -a | grep &#39;feature-re-1&#39; | xargs git branch -D","tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"}]},{"title":"Linux常用命令","date":"2021-07-13T03:44:22.000Z","path":"wiki/Linux常用命令/","text":"用户｜权限chmod：修改权限命令。一般用+号添加权限，-号删除权限，x代表执行权限，r代表读取权限，w代表写入权限，常见写法比如chmod +x 文件名 添加执行权限。 还有另外一种写法，使用数字来授权，因为r=4，w=2，x=1，平时执行命令chmod 777 文件名这就是最高权限了。 第一个数字7=4+2+1代表着所有者的权限，第二个数字7代表所属组的权限，第三个数字代表其他人的权限。 常见的权限数字还有644，所有者有读写权限，其他人只有只读权限，755代表其他人有只读和执行权限。 chown：用于修改文件和目录的所有者和所属组。一般用法chown user 文件用于修改文件所有者，chown user:user 文件修改文件所有者和组，冒号前面是所有者，后面是组。 文件相关touch：用于创建文件。如果文件不存在，则创建一个新的文件，如果文件已存在，则会修改文件的时间戳。 cat：cat是英文concatenate的缩写，用于查看文件内容。使用cat查看文件的话，不管文件的内容有多少，都会一次性显示，所以他不适合查看太大的文件。 more：more和cat有点区别，more用于分屏显示文件内容。可以用空格键向下翻页，b键向上翻页 less：和more类似，less用于分行显示 tail：可能是平时用的最多的命令了，查看日志文件基本靠他了。一般用户tail -fn 100 xx.log 查看最后的100行内容 gzip：用于压缩.gz后缀文件，gzip命令不能打包目录。需要注意的是直接使用gzip 文件名源文件会消失，如果要保留源文件，可以使用 gzip -c 文件名 &gt; xx.gz，解压缩直接使用gzip -d xx.gz tar：tar常用几个选项，-x解打包，-c打包，-f指定压缩包文件名，-v显示打包文件过程，一般常用tar -cvf xx.tar 文件来打包，解压则使用tar -xvf xx.tar。 Linux的打包和压缩是分开的操作，如果要打包并且压缩的话，按照前面的做法必须先用tar打包，然后再用gzip压缩。当然，还有更好的做法就是-z命令，打包并且压缩。 使用命令tar -zcvf xx.tar.gz 文件来打包压缩，使用命令tar -zxvf xx.tar.gz来解压缩 日志相关 linux 在文档中查找关键字个数grep -o “关键字” 文档名 | wc -l grep -o “关键字” 文档名 | sort | uniq -c 清除history记录vim .bash_history命令模式下（Esc之后输入:） 输入 set nu 每行数据前面显示行号11,20d 回车 11～20行的记录就被删除了然后命令模式下 wq 保存退出就可以了如果在此查看还是有记录，可以退出当前回话之后，再进去查看，就会不再显示删除的记录了 工具相关查看yum下载的软件位置rpm -qa | grep dockerrpm -ql podman-docker-3.2.3-0.10.module_el8.4.0+886+c9a8d9ad.noarch 问题排查那如果CPU使用率达到100%呢？怎么排查？1、通过top找到占用率高的进程。 2、通过top -Hp pid找到占用CPU高的线程ID。这里找到958的线程ID 3、再把线程ID转化为16进制，printf &quot;0x%x\\n&quot; 958，得到线程ID0x3be 4、通过命令jstack 163 | grep &#39;0x3be&#39; -C5 --color 或者 jstack 163|vim +/0x3be - 找到有问题的代码 https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/1.html https://www.linuxidc.com/index.htm","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"}]},{"title":"段合并","date":"2021-07-08T12:57:54.000Z","path":"wiki/段合并/","text":"参考资料 learnku.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"统计去重数据 (近似度量)","date":"2021-07-08T08:09:05.000Z","path":"wiki/统计去重数据/","text":"cardinality用法常用写法如下👇curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;months&quot; : &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125; &#125; &#125;&#125; 精度问题cardinality 度量是一个 近似算法。 它是基于 HyperLogLog++ （HLL）算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。 我们不需要理解技术细节， 但我们最好应该关注一下这个算法的 特性 ： 可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）。 小的数据集精度是非常高的。 我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 要配置精度，我们必须指定 precision_threshold 参数的值。 这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果。参考以下示例： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; ⚠️ ⚠️precision_threshold 接受 0–40000 之间的数字，更大的值还是会被当作 40000 来处理 示例会确保当字段唯一值在 100 以内时会得到非常准确的结果。尽管算法是无法保证这点的，但如果基数在阈值以下，几乎总是 100% 正确的。高于阈值的基数会开始节省内存而牺牲准确度，同时也会对度量结果带入误差。 对于指定的阈值，HLL 的数据结构会大概使用 precision_threshold * 8 字节的内存，所以就必须在牺牲内存和获得额外的准确度间做平衡。 在实际应用中， 100 的阈值可以在唯一值为百万的情况下仍然将误差维持 5% 以内 速度问题如果想要获得唯一值的数目， 通常 需要查询整个数据集合（或几乎所有数据）。 所有基于所有数据的操作都必须迅速，原因是显然的。 HyperLogLog 的速度已经很快了，它只是简单的对数据做哈希以及一些位操作。 但如果速度对我们至关重要，可以做进一步的优化。 因为 HLL 只需要字段内容的哈希值，我们可以在索引时就预先计算好。 就能在查询时跳过哈希计算然后将哈希值从 fielddata 直接加载出来。 预先计算哈希值只对内容很长或者基数很高的字段有用，计算这些字段的哈希值的消耗在查询时是无法忽略的。 尽管数值字段的哈希计算是非常快速的，存储它们的原始值通常需要同样（或更少）的内存空间。这对低基数的字符串字段同样适用，Elasticsearch 的内部优化能够保证每个唯一值只计算一次哈希。 基本上说，预先计算并不能保证所有的字段都更快，它只对那些具有高基数和/或者内容很长的字符串字段有作用。需要记住的是，预计算只是简单的将查询消耗的时间提前转移到索引时，并非没有任何代价，区别在于你可以选择在 什么时候 做这件事，要么在索引时，要么在查询时。 创建索引时添加如下配置： 1234567891011121314151617PUT /cars/&#123; &quot;mappings&quot;: &#123; &quot;transactions&quot;: &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 多值字段的类型是 murmur3 ，这是一个哈希函数。 现在当我们执行聚合时，我们使用 color.hash 字段而不是 color 字段：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 12345678910&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color.hash&quot; &#125; &#125; &#125;&#125; 现在 cardinality 度量会读取 “color.hash“ 里的值（预先计算的哈希值），取代动态计算原始值的哈希。 单个文档节省的时间是非常少的，但是如果你聚合一亿数据，每个字段多花费 10 纳秒的时间，那么在每次查询时都会额外增加 1 秒，如果我们要在非常大量的数据里面使用 cardinality ，我们可以权衡使用预计算的意义，是否需要提前计算 hash，从而在查询时获得更好的性能，做一些性能测试来检验预计算哈希是否适用于你的应用场景。。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"多桶排序","date":"2021-07-08T07:50:24.000Z","path":"wiki/多桶排序/","text":"多值桶（ terms 、 histogram 和 date_histogram ）动态生成很多桶。 Elasticsearch 是如何决定这些桶展示给用户的顺序呢？ 默认的，桶会根据 doc_count 降序排列。这是一个好的默认行为，因为通常我们想要找到文档中与查询条件相关的最大值：售价、人口数量、频率。但有些时候我们希望能修改这个顺序，不同的桶有着不同的处理方式。 内置排序这些排序模式是桶 固有的 能力：它们操作桶生成的数据 ，比如 doc_count 。 它们共享相同的语法，但是根据使用桶的不同会有些细微差别。 让我们做一个 terms 聚合但是按 doc_count 值的升序排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;_count&quot; : &quot;asc&quot; &#125; &#125; &#125; &#125;&#125;&#x27; 用关键字 _count ，我们可以按 doc_count 值的升序排序。 我们为聚合引入了一个 order 对象， 它允许我们可以根据以下几个值中的一个值进行排序： _count按文档数排序。对 terms 、 histogram 、 date_histogram 有效。 _term按词项的字符串值的字母顺序排序。只在 terms 内使用。 _key按每个桶的键值数值排序（理论上与 _term 类似）。 只在 histogram 和 date_histogram 内使用。 按度量排序有时，我们会想基于度量计算的结果值进行排序。 在我们的汽车销售分析仪表盘中，我们可能想按照汽车颜色创建一个销售条状图表，但按照汽车平均售价的升序进行排序。 我们可以增加一个度量，再指定 order 参数引用这个度量即可： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314151617181920&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125;&#x27; 计算每个桶的平均售价。 桶按照计算平均值的升序排序。 我们可以采用这种方式用任何度量排序，只需简单的引用度量的名字。不过有些度量会输出多个值。 extended_stats 度量是一个很好的例子：它输出好几个度量值。 如果我们想使用多值度量进行排序， 我们只需以关心的度量为关键词使用点式路径：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 使用 . 符号，根据感兴趣的度量进行排序。 深度度量排序在前面的示例中，度量是桶的直接子节点。平均售价是根据每个 term 来计算的。 在一定条件下，我们也有可能对 更深 的度量进行排序，比如孙子桶或从孙桶。 我们可以定义更深的路径，将度量用尖括号（ &gt; ）嵌套起来，像这样： my_bucket&gt;another_bucket&gt;metric 。 需要提醒的是嵌套路径上的每个桶都必须是 单值 的。 filter 桶生成 一个单值桶：所有与过滤条件匹配的文档都在桶中。 多值桶（如：terms ）动态生成许多桶，无法通过指定一个确定路径来识别。 目前，只有三个单值桶： filter 、 global 和 reverse_nested 。让我们快速用示例说明，创建一个汽车售价的直方图，但是按照红色和绿色（不包括蓝色）车各自的方差来排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;histogram&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;interval&quot;: 20000, &quot;order&quot;: &#123; &quot;red_green_cars&gt;tats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;red_green_cars&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123;&quot;color&quot;: [&quot;red&quot;, &quot;green&quot;]&#125;&#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123;&quot;extended_stats&quot;: &#123;&quot;field&quot; : &quot;price&quot;&#125;&#125; &#125; &#125; &#125; &#125; &#125;&#125;&#x27; 按照嵌套度量的方差对桶的直方图进行排序。 因为我们使用单值过滤器 filter ，我们可以使用嵌套排序。 按照生成的度量对统计结果进行排序。 本例中，可以看到我们如何访问一个嵌套的度量。 stats 度量是 red_green_cars 聚合的子节点，而 red_green_cars 又是 colors 聚合的子节点。 为了根据这个度量排序，我们定义了路径 red_green_cars&gt;tats.variance 。我们可以这么做，因为 filter 桶是个单值桶。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"过滤和聚合","date":"2021-07-08T07:33:26.000Z","path":"wiki/过滤和聚合/","text":"过滤和聚合聚合范围限定还有一个自然的扩展就是过滤。因为聚合是在查询结果范围内操作的，任何可以适用于查询的过滤器也可以应用在聚合上。 过滤如果我们想找到售价在 $10,000 美元之上的所有汽车同时也为这些车计算平均售价， 可以简单地使用一个 constant_score 查询和 filter 约束： GET /cars/transactions/_search 12345678910111213141516171819&#123; &quot;size&quot; : 0, &quot;query&quot; : &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10000 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;single_avg_price&quot;: &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 从根本上讲，使用 non-scoring 查询和使用 match 查询没有任何区别。查询（包括了一个过滤器）返回一组文档的子集，聚合正是操作这些文档。使用 filtering query 会忽略评分，并有可能会缓存结果数据等等。 过滤桶但是如果我们只想对聚合结果过滤怎么办？ 假设我们正在为汽车经销商创建一个搜索页面， 我们希望显示用户搜索的结果，但是我们同时也想在页面上提供更丰富的信息，包括（与搜索匹配的）上个月度汽车的平均售价。 这里我们无法简单的做范围限定，因为有两个不同的条件。搜索结果必须是 ford ，但是聚合结果必须满足 ford AND sold &gt; now - 1M 。 为了解决这个问题，我们可以用一种特殊的桶，叫做 filter （注：过滤桶） 。 我们可以指定一个过滤桶，当文档满足过滤桶的条件时，我们将其加入到桶内。 查询结果如下：GET /cars/transactions/_search 1234567891011121314151617181920212223242526&#123; &quot;size&quot; : 0, &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;recent_sales&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold&quot;: &#123; &quot;from&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_price&quot;:&#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 使用 过滤 桶在 查询 范围基础上应用过滤器。 avg 度量只会对 ford 和上个月售出的文档计算平均售价。 因为 filter 桶和其他桶的操作方式一样，所以可以随意将其他桶和度量嵌入其中。所有嵌套的组件都会 “继承” 这个过滤，这使我们可以按需针对聚合过滤出选择部分。 后过滤器目前为止，我们可以同时对搜索结果和聚合结果进行过滤（不计算得分的 filter 查询），以及针对聚合结果的一部分进行过滤（ filter 桶）。 我们可能会想，”只过滤搜索结果，不过滤聚合结果呢？” 答案是使用 post_filter 。 它是接收一个过滤器的顶层搜索请求元素。这个过滤器在查询 之后 执行（这正是该过滤器的名字的由来：它在查询之后 post 执行）。正因为它在查询之后执行，它对查询范围没有任何影响，所以对聚合也不会有任何影响。 我们可以利用这个行为对查询条件应用更多的过滤器，而不会影响其他的操作，就如 UI 上的各个分类面。让我们为汽车经销商设计另外一个搜索页面，这个页面允许用户搜索汽车同时可以根据颜色来过滤。颜色的选项是通过聚合获得的： GET /cars/transactions/_search 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;post_filter&quot;: &#123; &quot;term&quot; : &#123; &quot;color&quot; : &quot;green&quot; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;all_colors&quot;: &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; post_filter 元素是 top-level 而且仅对命中结果进行过滤。 查询 部分找到所有的 ford 汽车，然后用 terms 聚合创建一个颜色列表。因为聚合对查询范围进行操作，颜色列表与福特汽车有的颜色相对应。 最后， post_filter 会过滤搜索结果，只展示绿色 ford 汽车。这在查询执行过 后 发生，所以聚合不受影响。 这通常对 UI 的连贯一致性很重要，可以想象用户在界面商选择了一类颜色（比如：绿色），期望的是搜索结果已经被过滤了，而 不是 过滤界面上的选项。如果我们应用 filter 查询，界面会马上变成 只 显示 绿色 作为选项，这不是用户想要的！ ⚠️ ⚠️ ⚠️ 性能考虑（Performance consideration）当你需要对搜索结果和聚合结果做不同的过滤时，你才应该使用 post_filter ， 有时用户会在普通搜索使用 post_filter 。 不要这么做！ post_filter 的特性是在查询 之后 执行，任何过滤对性能带来的好处（比如缓存）都会完全失去。 在我们需要不同过滤时， post_filter 只与聚合一起使用。 总结选择合适类型的过滤（如：搜索命中、聚合或两者兼有）通常和我们期望如何表现用户交互有关。选择合适的过滤器（或组合）取决于我们期望如何将结果呈现给用户。 在 filter 过滤中的 non-scoring 查询，同时影响搜索结果和聚合结果。 filter 桶影响聚合。 post_filter 只影响搜索结果。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 条形图","date":"2021-07-08T07:11:57.000Z","path":"wiki/聚合-条形图/","text":"参考资料","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"嵌套桶","date":"2021-07-08T07:01:16.000Z","path":"wiki/嵌套桶/","text":"两层嵌套在我们使用不同的嵌套方案时，聚合的力量才能真正得以显现。 在前例中，我们已经看到如何将一个度量嵌入桶中，它的功能已经十分强大了。 但真正令人激动的分析来自于将桶嵌套进 另外一个桶 所能得到的结果。 现在，我们想知道每个颜色的汽车制造商的分布： GET /cars/transactions/_search 12345678910111213141516171819202122&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;make&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注意前例中的 avg_price 度量仍然保持原位。 另一个聚合 make 被加入到了 color 颜色桶中。 这个聚合是 terms 桶，它会为每个汽车制造商生成唯一的桶。 这里发生了一些有趣的事。 首先，我们可能会观察到之前例子中的 avg_price 度量完全没有变化，还在原来的位置。 一个聚合的每个 层级 都可以有多个度量或桶， avg_price 度量告诉我们每种颜色汽车的平均价格。它与其他的桶和度量相互独立。 这对我们的应用非常重要，因为这里面有很多相互关联，但又完全不同的度量需要收集。聚合使我们能够用一次数据请求获得所有的这些信息。 另外一件值得注意的重要事情是我们新增的这个 make 聚合，它是一个 terms 桶（嵌套在 colors 、 terms 桶内）。这意味着它会为数据集中的每个唯一组合生成（ color 、 make ）元组。 让我们看看返回的响应（为了简单我们只显示部分结果）： 1234567891011121314151617181920212223242526&#123; &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125; &#125; &#125;&#125; 正如期望的那样，新的聚合嵌入在每个颜色桶中。 现在我们看见按不同制造商分解的每种颜色下车辆信息。 最终，我们看到前例中的 avg_price 度量仍然维持不变。 三层嵌套让我们回到话题的原点，在进入新话题之前，对我们的示例做最后一个修改， 为每个汽车生成商计算最低和最高的价格：GET /cars/transactions/_search 1234567891011121314151617181920212223&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;make&quot; &#125;, &quot;aggs&quot; : &#123; &quot;min_price&quot; : &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125;, &quot;max_price&quot; : &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 我们需要增加另外一个嵌套的 aggs 层级。 然后包括 min 最小度量。 以及 max 最大度量。 得到以下输出（只显示部分结果）： 12345678910111213141516171819202122232425262728293031323334353637&#123;... &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3, &quot;min_price&quot;: &#123; &quot;value&quot;: 10000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 20000 &#125; &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1, &quot;min_price&quot;: &#123; &quot;value&quot;: 80000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 80000 &#125; &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125;,... 有了这两个桶，我们可以对查询的结果进行扩展并得到以下信息： 有四辆红色车。红色车的平均售价是 $32，500 美元。其中三辆红色车是 Honda 本田制造，一辆是 BMW 宝马制造。最便宜的红色本田售价为 $10，000 美元。最贵的红色本田售价为 $20，000 美元。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 高级概念","date":"2021-07-08T06:52:35.000Z","path":"wiki/聚合-高级概念/","text":"桶桶 简单来说就是满足特定条件的文档的集合： 一个雇员属于 男性 桶或者 女性 桶 奥尔巴尼属于 纽约 桶 日期2014-10-28属于 十月 桶当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件。如果匹配到，文档将放入相应的桶并接着进行聚合操作。 桶也可以被嵌套在其他桶里面，提供层次化的或者有条件的划分方案。例如，辛辛那提会被放入俄亥俄州这个桶，而 整个 俄亥俄州桶会被放入美国这个桶。 Elasticsearch 有很多种类型的桶，能让你通过很多种方式来划分文档（时间、最受欢迎的词、年龄区间、地理位置 等等）。其实根本上都是通过同样的原理进行操作：基于条件来划分文档。 指标桶能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些指标的计算。分桶是一种达到目的的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。 大多数 指标 是简单的数学运算（例如最小值、平均值、最大值，还有汇总），这些是通过文档的值来计算。在实践中，指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据。 桶和指标的组合聚合 是由桶和指标组成的。 聚合可能只有一个桶，可能只有一个指标，或者可能两个都有。也有可能有一些桶嵌套在其他桶里面。例如，我们可以通过所属国家来划分文档（桶），然后计算每个国家的平均薪酬（指标）。 由于桶可以被嵌套，我们可以实现非常多并且非常复杂的聚合： 1.通过国家划分文档（桶） 2.然后通过性别划分每个国家（桶） 3.然后通过年龄区间划分每种性别（桶） 4.最后，为每个年龄区间计算平均薪酬（指标） 最后将告诉你每个 &lt;国家, 性别, 年龄&gt; 组合的平均薪酬。所有的这些都在一个请求内完成并且只遍历一次数据！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"elasticsearch-overview","date":"2021-07-08T03:17:58.000Z","path":"wiki/elasticsearch-overview/","text":"学习资料 https://www.codingdict.com/ https://elasticsearch.cn/ https://www.elastic.co/guide/en/ 铭毅天下","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"关于 Elasticsearch 内存占用及分配","date":"2021-07-08T02:39:16.000Z","path":"wiki/关于-Elasticsearch-内存占用及分配/","text":"Elasticsearch 和 Lucene 对内存使用情况： Elasticsearch 限制的内存大小是 JAVA 堆空间的大小，不包括Lucene 缓存倒排索引数据空间。 Lucene 中的 倒排索引 segments 存储在文件中，为提高访问速度，都会把它加载到内存中，从而提高 Lucene 性能。所以建议至少留系统一半内存给Lucene。Node Query Cache (负责缓存f ilter 查询结果)，每个节点有一个，被所有 shard 共享，filter query查询结果要么是 yes 要么是no，不涉及 scores 的计算。集群中每个节点都要配置，默认为：indices.queries.cache.size:10% Indexing Buffer 索引缓冲区，用于存储新索引的文档，当其被填满时，缓冲区中的文档被写入磁盘中的 segments 中。节点上所有 shard 共享。缓冲区默认大小： indices.memory.index_buffer_size: 10%如果缓冲区大小设置了百分百则 indices.memory.min_index_buffer_size 用于这是最小值，默认为 48mb。indices.memory.max_index_buffer_size 用于最大大小，无默认值。 segmentssegments会长期占用内存，其初衷就是利用OS的cache提升性能。只有在Merge之后，才会释放掉标记为Delete的segments，释放部分内存。 Shard Request Cache 用于缓存请求结果，但之缓存request size为0的。比如说 hits.total, aggregations 和 suggestions.默认最大为indices.requests.cache.size:1% Field Data Cache 字段缓存重要用于对字段进行排序、聚合是使用。因为构建字段数据缓存代价昂贵，所以建议有足够的内训来存储。Fielddata 是 「 延迟 」 加载。如果你从来没有聚合一个分析字符串，就不会加载 fielddata 到内存中，也就不会使用大量的内存，所以可以考虑分配较小的heap给Elasticsearch。因为heap越小意味着Elasticsearch的GC会比较快，并且预留给Lucene的内存也会比较大。。如果没有足够的内存保存fielddata时，Elastisearch会不断地从磁盘加载数据到内存，并剔除掉旧的内存数据。剔除操作会造成严重的磁盘I/O，并且引发大量的GC，会严重影响Elastisearch的性能。 默认情况下Fielddata会不断占用内存，直到它触发了fielddata circuit breaker。fielddata circuit breaker会根据查询条件评估这次查询会使用多少内存，从而计算加载这部分内存之后，Field Data Cache所占用的内存是否会超过indices.breaker.fielddata.limit。如果超过这个值，就会触发fielddata circuit breaker，abort这次查询并且抛出异常，防止OOM。 1indices.breaker.fielddata.limit:60% (默认heap的60%) (es7之后改成70%) 如果设置了indices.fielddata.cache.size，当达到size时，cache会剔除旧的fielddata。 indices.breaker.fielddata.limit 必须大于 indices.fielddata.cache.size，否则只会触发fielddata circuit breaker，而不会剔除旧的fielddata。 配置Elasticsearch堆内存Elasticsearch默认安装后设置的内存是 1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10g 注意点给es分配内存时要注意，至少要分配一半儿内存留给 Lucene。分配给 es 的内存最好不要超过 32G ，因为如果堆大小小于 32 GB，JVM 可以利用指针压缩，这可以大大降低内存的使用：每个指针 4 字节而不是 8 字节。如果大于32G 每个指针占用 8字节，并且会占用更多的内存带宽，降低了cpu性能。 还有一点， 要关闭 swap 内存交换空间，禁用swapping。频繁的swapping 对服务器来说是致命的。总结：给es JVM栈的内存最好不要超过32G，留给Lucene的内存越大越好，Lucene把所有的segment都缓存起来，会加快全文检索。 关闭交换区这应该显而易见了，但仍然需要明确的写出来：把内存换成硬盘将毁掉服务器的性能，想象一下：涉及内存的操作是需要快速执行的。如果介质从内存变为了硬盘，一个10微秒的操作变成需要10毫秒。而且这种延迟发生在所有本该只花费10微秒的操作上，就不难理解为什么交换区对于性能来说是噩梦。 最好的选择是禁用掉操作系统的交换区。可以用以下命令： 1sudo swapoff -a 来禁用，你可能还需要编辑 /etc/fstab 文件。细节可以参考你的操作系统文档。 如果实际环境不允许禁用掉 swap，你可以尝试降低 swappiness。此值控制操作系统使用交换区的积极性。这可以防止在正常情况下使用交换区，但仍允许操作系统在紧急情况下将内存里的东西放到交换区。 对于大多数Linux系统来说，这可以用 sysctl 值来配置： 1vm.swappiness = 1 # 将此值配置为1会比0好，在kernal内核的某些版本中，0可能会引起OOM异常。 最后，如果两种方法都不可用，你应该在ElasticSearch的配置中启用 mlockall.file。这允许JVM锁定其使用的内存，而避免被放入操作系统交换区。 在elasticsearch.yml中，做如下设置： 1bootstrap.mlockall: true 查看node节点数据GET /_cat/nodes?v&amp;h=id,ip,port,v,master,name,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count GET /_cat/nodes?v&amp;h=id,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size GET /_cat/nodes?v&amp;h=id,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count 参考文章","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"cross-fields跨字段查询","date":"2021-07-07T06:41:42.000Z","path":"wiki/cross-fields跨字段查询/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » cross-fields 跨字段查询","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"copy_to参数","date":"2021-07-07T06:34:42.000Z","path":"wiki/copy-to参数/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 自定义 _all 字段 Docs » Mapping parameters（映射参数） » Mapping(映射) » copy_to（合并参数）","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"字符串排序与多字段","date":"2021-07-07T03:05:31.000Z","path":"wiki/字符串排序与多字段/","text":"被解析的字符串字段也是多值字段， 但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如 fine old art ， 这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Elasticsearch 在排序过程中没有这样的信息。 你可以使用 min 和 max 排序模式（默认是 min ），但是这会导致排序以 art 或是 old ，任何一个都不是所希望的。 为了以字符串字段进行排序，这个字段应仅包含一项： 整个 not_analyzed 字符串。 但是我们仍需要 analyzed 字段，这样才能以全文进行查询 一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段： analyzed 用于搜索， not_analyzed 用于排序 但是保存相同的字符串两次在 _source 字段是浪费空间的。 我们真正想要做的是传递一个 单字段 但是却用两种方式索引它。所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个 fields 参数 该参数允许你转化一个简单的映射如： 1234&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;&#125; 为一个多字段映射如： 12345678910&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; tweet 主字段与之前的一样: 是一个 analyzed 全文字段。 新的 tweet.raw 子字段是 not_analyzed. 现在，至少只要我们重新索引了我们的数据，使用 tweet 字段用于搜索，tweet.raw 字段用于排序：curl -X GET &quot;localhost:9200/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;, &quot;sort&quot;: &quot;tweet.raw&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"推荐系统-Overview","date":"2021-07-07T02:08:05.000Z","path":"wiki/推荐系统-Overview/","text":"博客资料 深度解析京东个性化推荐系统演进史 用 Mahout 和 Elasticsearch 实现推荐系统 美团推荐算法实践 58同城推荐系统设计与实现 微博推荐系统的架构演进之路 Flink 在小红书推荐系统中的应用 小红书大数据在推荐系统中的应用 快看漫画个性化推荐探索与实践 数据仓库系列篇——唯品会大数据架构 推荐系统基本概念和架构 PAI平台搭建企业级个性化推荐系统 - Aliyun 蘑菇街推荐工程实践 参考资料","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"categories":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"}]},{"title":"flink 提交任务","date":"2021-07-06T15:57:04.000Z","path":"wiki/flink-提交任务/","text":"下面演示如何通过admin页面提交任务 👇 准备task jar1234567891011121314151617181920public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 执行mvn install -DskipTest 可以得到相应的jar admin提交jar 提交完jar包之后，需要设置相关参数，这个根据自己的实际情况来设置，下面是参考样例： Enter Class : com.ibli.flink.StreamWordCount也就是程序入口，我们这是写了一个main方法，如果是程序的话，可以写对应bootstrap的启动类 Program Arguments : –host localhost –port 7777 点击 submit 之后查看提交的任务状态 查看任务 可以看到是有两个任务，并且都是在执行状态；点击一个任务，还可以查看任务详情信息，和一些其他的信息，非常全面； 查看运行时任务列表 查看任务管理列表 点击任务可以跳转到详情页面 👇 下面是执行日志 我们还可以看到任务执行的标准输出结果✅ 任务源数据通过nc 输入数据，由程序读取7777端口输入流并解析数据 123gaolei:geekibli gaolei$ nc -lk 7777hello javahello flink 取消任务如下 再次查看已完成任务列表 如下：","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"apache-flink-overview","date":"2021-07-06T15:39:54.000Z","path":"wiki/apache-flink-overview/","text":"学习初衷推荐系统数据需要实时处理，使用Apache Flink实时计算用户数据，分析用户行为，达到实时业务数据分析和实现业务相关推荐； 学习资料 ashiamd.github.io 尚硅谷2021最新Java版Flink 武老师清华硕士，原IBM-CDL负责人 Apache Flink® — Stateful Computations over Data Streams Apache Flink® - 数据流上的有状态计算 https://space.bilibili.com/33807709?spm_id_from=333.788.b_765f7570696e666f.2 https://github.com/flink-china/flink-training-course https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/ [官方文档 推荐 ‼️]","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"most_fields类型","date":"2021-07-06T12:54:23.000Z","path":"wiki/most-fields类型/","text":"多字段映射首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍： DELETE /my_index 1234567891011121314151617181920PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; title 字段使用 english 英语分析器来提取词干。 title.std 字段使用 standard 标准分析器，所以没有词干提取。 接着索引一些文档： 12345PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; 这里用一个简单 match 查询 title 标题字段是否包含 jumping rabbits （跳跃的兔子）： 12345678GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;jumping rabbits&quot; &#125; &#125;&#125; 因为有了 english 分析器，这个查询是在查找以 jump 和 rabbit 这两个被提取词的文档。两个文档的 title 字段都同时包括这两个词，所以两个文档得到的评分也相同： 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125; ]&#125; 如果只是查询 title.std 字段，那么只有文档 2 是匹配的。尽管如此，如果同时查询两个字段，然后使用 bool 查询将评分结果 合并 ，那么两个文档都是匹配的（ title 字段的作用），而且文档 2 的相关度评分更高（ title.std 字段的作用）： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; 我们希望将所有匹配字段的评分合并起来，所以使用 most_fields 类型。这让 multi_match 查询用 bool 查询将两个字段语句包在里面，而不是使用 dis_max (最佳字段) 查询。 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8226396, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.10741998, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125; ]&#125; 文档 2 现在的评分要比文档 1 高。 用广度匹配字段 title 包括尽可能多的文档——以提升召回率——同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。 每个字段对于最终评分的贡献可以通过自定义值 boost 来控制。比如，使 title 字段更为重要，这样同时也降低了其他信号字段的作用： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title^10&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; title 字段的 boost 的值为 10 使它比 title.std 更重要。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 多数字段","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"multi_match 查询","date":"2021-07-06T12:37:26.000Z","path":"wiki/multi-match-查询/","text":"multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。 📒 📒 📒 multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即：best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。 默认情况下，查询的类型是 best_fields ，这表示它会为每个字段生成一个 match 查询，然后将它们组合到 dis_max 查询的内部，如下： 1234567891011121314151617181920212223&#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, ], &quot;tie_breaker&quot;: 0.3 &#125;&#125; 上面这个查询用 multi_match 重写成更简洁的形式： 123456789&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;body&quot; ], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125;&#125; ⚠️ ⚠️ ⚠️ best_fields 类型是默认值，可以不指定。 如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。 查询字段名称的模糊匹配字段名称可以用 模糊匹配 的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中，例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: &quot;*_title&quot; &#125;&#125; 提升单个字段的权重可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: [ &quot;*_title&quot;, &quot;chapter_title^2&quot; ] &#125;&#125; chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » multi_match 查询 Elasticsearch Guide [7.x] » Query DSL » Full text queries » Multi-match query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"dis_max查询","date":"2021-07-06T12:09:23.000Z","path":"wiki/dis-max查询/","text":"假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例： 1234567891011PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;&#125; 用户输入词组 Brown fox 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词： 现在运行以下 bool 查询： 12345678910&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 但是我们发现查询的结果是文档 1 的评分更高： 为了理解导致这样的原因，需要回想一下 bool 是如何计算评分的： 它会执行 should 语句中的两个查询。加和两个查询的评分。乘以匹配语句的总数。除以所有语句总数（这里为：2）。 文档 1 的两个字段都包含 brown 这个词，所以两个 match 语句都能成功匹配并且有一个评分。文档 2 的 body 字段同时包含 brown 和 fox 这两个词，但 title 字段没有包含任何词。这样， body 查询结果中的高分，加上 title 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。 在本例中， title 和 body 字段是相互竞争的关系，所以就需要找到单个 最佳匹配 的字段。 如果不是简单将每个字段的评分结果加在一起，而是将 最佳匹配 字段的评分作为查询的整体评分，结果会怎样？这样返回的结果可能是： 同时 包含 brown 和 fox 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。 dis_max 查询不使用 bool 查询，可以使用 dis_max 即分离 最大化查询 （Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ： 12345678910&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 得到我们想要的结果为： Top-level parameters for dis_maxedit queries(Required, array of query objects) Contains one or more query clauses. Returned documents must match one or more of these queries. If a document matches multiple queries, Elasticsearch uses the highest relevance score. tie_breaker(Optional, float) Floating point number between 0 and 1.0 used to increase the relevance scores of documents matching multiple query clauses. Defaults to 0.0. You can use the tie_breaker value to assign higher relevance scores to documents that contain the same term in multiple fields than documents that contain this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields. If a document matches multiple clauses, the dis_max query calculates the relevance score for the document as follows: Take the relevance score from a matching clause with the highest score.Multiply the score from any other matching clauses by the tie_breaker value.Add the highest score to the multiplied scores.If the tie_breaker value is greater than 0.0, all matching clauses count, but the clause with the highest score counts most. dis_max，只是取分数最高的那个query的分数而已，完全不考虑其他query的分数，这种一刀切的做法，可能导致在有其他query的影响下，score不准确的情况，这时为了使用结果更准确，最好还是要考虑到其他query的影响;使用 tie_breaker 将其他query的分数也考虑进去, tie_breaker 参数的意义，将其他query的分数乘以tie_breaker，然后综合考虑后与最高分数的那个query的分数综合在一起进行计算，这样做除了取最高分以外，还会考虑其他的query的分数。tie_breaker的值，设置在在0~1之间，是个小数就行，没有固定的值 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 最佳字段 Elasticsearch中文文档 Elasticsearch Guide [7.x] » Query DSL » Compound queries » Disjunction max query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch被破坏的相似度","date":"2021-07-06T11:50:11.000Z","path":"wiki/elasticsearch被破坏的相似度/","text":"在讨论更复杂的 多字段搜索 之前，让我们先快速解释一下为什么只在主分片上 创建测试索引 。 用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤：用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。 为了理解为什么会这样，可以设想，我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 foo 。可能是分片 1 有其中 3 个 foo 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。 在 什么是相关度？中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 词频/逆向文档频率 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 逆向文档频率 将 某个词在索引内所有文档出现的百分数 考虑在内，出现的频率越高，它的权重就越低。 但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。 因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 foo 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， foo 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。 在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。 为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 match 查询 里介绍的那样，如果只有一个分片，那么本地的 IDF 就是 全局的 IDF。 第二个方式就是在搜索请求后添加 ?search_type=dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。 不要在生产环境上使用 dfs_query_then_fetch 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。 参考资料 Elasticsearch: 权威指南 » 基础入门 » 排序与相关性 » 什么是相关性? Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 被破坏的相关度！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"canal同步es后部分字段为null","date":"2021-07-06T08:11:15.000Z","path":"wiki/canal同步es后部分字段为null/","text":"现象 配置文件如下： 123456789101112131415dataSourceKey: defaultDS # 源数据源的key, 对应上面配置的srcDataSources中的值destination: example # cannal的instance或者MQ的topicgroupId: g1 # 对应MQ模式下的groupId, 只会同步对应groupId的数据esMapping: _index: rd_member_fans_info # es 的索引名称 _type: _doc # es 的doc名称 _id: _id # es 的_id, 如果不配置该项必须配置下面的pk项_id则会由es自动分配# pk: id # 如果不需要_id, 则需要指定一个属性为主键属性# # sql映射 sql: &#x27;SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.create_time,t.update_time,t.`status` FROM rd_member_fans_info t&#x27;# objFields:# _labels: array:; # 数组或者对象属性, array:; 代表以;字段里面是以;分隔的# _obj: object # json对象 etlCondition: &quot;where t.update_time&gt;=&#123;&#125;&quot; # etl 的条件参数 commitBatch: 3000 # 提交批大小 ⚠️ ⚠️sql执行是没有问题的！ canal-adapter 获取binlog数据也没有问题，显示日志如下： 12021-07-06 15:39:24.588 [pool-1-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:3,&quot;redtom_id&quot;:1,&quot;fans_redtom_id&quot;:1,&quot;fans_username&quot;:&quot;1&quot;,&quot;fans_introduce&quot;:&quot;1&quot;,&quot;fans_avatar&quot;:&quot;1&quot;,&quot;is_each_following&quot;:1,&quot;follow_channel&quot;:1,&quot;create_time&quot;:1625556851000,&quot;update_time&quot;:1625556851000,&quot;status&quot;:2&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625557164000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;rd_member_fans_info&quot;,&quot;ts&quot;:1625557164587,&quot;type&quot;:&quot;INSERT&quot;&#125; 然后看一下我创建索引的mapping 解决方法调整sql如下： SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.status as is_deleted , t.create_time,t.update_time FROM rd_member_fans_info t 调整了那些东西呢？ status 的顺序提前而已！ 测试执行一下命令：curl http://127.0.0.1:8081/etl/es7/rd_member_fans_info.yml -X POST canal-adapter 日志如下： 122021-07-06 16:21:33.519 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member_fans_info2021-07-06 16:21:33.527 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - 数据全量导入完成, 一共导入 3 条数据, 耗时: 7 查看es数据：","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch-analyzer","date":"2021-07-06T07:02:01.000Z","path":"wiki/elasticsearch-analyzer/","text":"测试常见分析器GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 12345&#123; &quot;analyzer&quot;: &quot;english&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;stop&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; 默认分析器虽然我们可以在字段层级指定分析器，但是如果该层级没有指定任何的分析器，那么我们如何能确定这个字段使用的是哪个分析器呢？ 分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下： 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 在搜索时，顺序有些许不同： 查询自己定义的 analyzer ，否则 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 有时，在索引时和搜索时使用不同的分析器是合理的。我们可能要想为同义词建索引（例如，所有 quick 出现的地方，同时也为 fast 、 rapid 和 speedy 创建索引）。但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 quick 、 fast 、 rapid 或 speedy 。 为了区分，Elasticsearch 也支持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。 如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样： 查询自己定义的 analyzer ，否则字段映射里定义的 search_analyzer ，否则字段映射里定义的 analyzer ，否则索引设置中名为 default_search 的分析器，默认为索引设置中名为 default 的分析器，默认为standard 标准分析器 保持简单多数情况下，会提前知道文档会包括哪些字段。最简单的途径就是在创建索引或者增加类型映射时，为每个全文字段设置分析器。这种方式尽管有点麻烦，但是它让我们可以清楚的看到每个字段每个分析器是如何设置的。 通常，多数字符串字段都是 not_analyzed 精确值字段，比如标签（tag）或枚举（enum），而且更多的全文字段会使用默认的 standard 分析器或 english 或其他某种语言的分析器。这样只需要为少数一两个字段指定自定义分析：或许标题 title 字段需要以支持 输入即查找（find-as-you-type） 的方式进行索引。 可以在索引级别设置中，为绝大部分的字段设置你想指定的 default 默认分析器。然后在字段级别设置中，对某一两个字段配置需要指定的分析器。 📒 📒 📒对于和时间相关的日志数据，通常的做法是每天自行创建索引，由于这种方式不是从头创建的索引，仍然可以用 索引模板（Index Template） 为新建的索引指定配置和映射。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 控制分析","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"Parameter index out of range (1 > number of parameters, which is 0).","date":"2021-07-06T05:03:16.000Z","path":"wiki/Parameter-index-out-of-range-1-number-of-parameters-which-is-0/","text":"问题记录123456789101112132021-07-06 12:39:31.179 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:39:31.186 [http-nio-8081-exec-2] ERROR com.alibaba.otter.canal.client.adapter.support.Util - sqlRs has error, sql: SELECT COUNT(1) FROM ( select t.redtom_id as id, t.username, t.nickname, t.avatar, t.status, t.mobile, t.mobile_region_no, t.email, t.gender, t.password,t.salt,t.birthday,t.introduce,t.country,t.region,t.level,t.is_vip,t.follows ,t.fans,t.likes_num, t.collects_num, t.instagram_account, t.youtube_account, t.facebook_account, t.twitter_account,t.create_ip, t.create_time,t.update_time from rd_member r where t.create_time&gt;=&#x27;&#123;0&#125;&#x27;) _CNT2021-07-06 12:39:31.188 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0).java.lang.RuntimeException: java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0). at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:65) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:62) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] 如何解决我执行的操作如下：👇curl http://127.0.0.1:8081/etl/es7/customer.yml -X POST -d &quot;params=2019-08-31 00:00:00&quot; 但是我的 es7/rd_member.yml的配置文件如下： etlCondition:&quot;where a.c_time&gt;=&#39;&#123;0&#125;&#39;&quot; # etl 的条件参数 应该改成：etlCondition:&quot;where a.c_time&gt;=&#123;&#125;&quot; # etl 的条件参数","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"field name is null or empty","date":"2021-07-06T04:53:44.000Z","path":"wiki/field-name-is-null-or-empty/","text":"canal adapter 报错信息123456789101112131415161718192021222021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO o.a.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring FrameworkServlet &#x27;dispatcherServlet&#x27;2021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization started2021-07-06 12:46:31.968 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization completed in 9 ms2021-07-06 12:46:31.995 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:46:32.027 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - field name is null or emptyjava.lang.IllegalArgumentException: field name is null or empty at org.elasticsearch.index.query.BaseTermQueryBuilder.&lt;init&gt;(BaseTermQueryBuilder.java:113) ~[na:na] at org.elasticsearch.index.query.TermQueryBuilder.&lt;init&gt;(TermQueryBuilder.java:75) ~[na:na] at org.elasticsearch.index.query.QueryBuilders.termQuery(QueryBuilders.java:202) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.lambda$executeSqlImport$1(ESEtlService.java:141) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:60) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.executeSqlImport(ESEtlService.java:64) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:105) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) ~[na:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke 问题排查操作是向数据库中插入一条数据，通过canal-adapter同步到elasticsearch中，接口发生以上错误！现象是canal-adapter检测到和mysql的数据变化，但是同步到es的时候发生了错误；猜想大概是某个为空导致存到es的时候发生异常； 然后查看es7下的mapping配置： 发现我的sql查id的时候写错了，别名应该写成_id,对应elasticsearch的_id","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"elasticsearch映射","date":"2021-07-05T14:54:05.000Z","path":"wiki/elasticsearch映射/","text":"Elasticsearch 支持如下简单域类型： 字符串: string （es7之后编程text） 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 查看索引的mappingGET /gb/_mapping/tweet 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 自定义域映射尽管在很多情况下基本域数据类型已经够用，但你经常需要为单独域自定义映射，特别是字符串域。自定义映射允许你执行下面的操作： 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： 12345&#123; &quot;number_of_clicks&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;&#125; 默认， string (text) 类型域会被认为包含全文。就是说，它们的值在索引前，会通过一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。 string 域映射的两个最重要属性是 index 和 analyzer 。 indexindex 属性控制怎样索引字符串。它可以是下面三个值： analyzed首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。 no不索引这个域。这个域不会被搜索到。 (比如一些隐私信息) string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ： ⚠️ ⚠️其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。 analyzer对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english; 123456&#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 更新映射当你首次创建一个索引的时候，可以指定类型的映射。你也可以使用 /_mapping 为新类型（或者为存在的类型更新映射）增加映射。⚠️ ⚠️尽管你可以 增加 一个存在的映射，你不能 修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。 我们可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed 。 为了描述指定映射的两种方式，我们先删除 gd 索引：DELETE /gb然后创建一个新索引，指定 tweet 域使用 english 分析器： 12345678910111213141516171819202122PUT /gb &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 稍后，我们决定在 tweet 映射增加一个新的名为 tag 的 not_analyzed 的文本域，使用 _mapping ： 123456789PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中 测试映射你可以使用 analyze API 测试字符串域的映射。比较下面两个请求的输出： 1234567891011GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125; tweet 域产生两个词条 black 和 cat ， tag 域产生单独的词条 Black-cats 。换句话说，我们的映射正常工作。 参考资料 Elasticsearch权威指南","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"分析与分析器","date":"2021-07-05T14:43:44.000Z","path":"wiki/分析与分析器/","text":"分析包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 ，之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。 内置分析器但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：&quot;Set the shape to semi-transparent by calling set_trans(5)&quot; 标准分析器标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。 什么时候使用分析器当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。 全文查询，理解每个域是如何定义的，因此它们可以做正确的事： 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。 测试分析器有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本： 12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条： 12345678910111213141516171819202122232425&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。 每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于​keep_types token 过滤器​。 analyze API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。 你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域—​不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。 要做到这一点，我们必须手动指定这些域的映射。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"倒排索引","date":"2021-07-05T14:17:00.000Z","path":"wiki/倒排索引/","text":"Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容： The quick brown fox jumped over the lazy dogQuick brown foxes leap over lazy dogs in summer为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。 如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如： Quick 可以小写化为 quick 。foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的， dogs 可以为提取为 dog 。jumped 和 leap 是同义词，可以索引为相同的单词 jump 。现在索引看上去像这样：这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！ 这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch分页查询","date":"2021-07-05T14:08:23.000Z","path":"wiki/elasticsearch分页查询/","text":"和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size显示应该返回的结果数量，默认是 10from显示应该跳过的初始结果数量，默认是 0如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果： GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 ⚠️ ⚠️ ⚠️考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。 在分布式系统中深度分页 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页—​结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因 参考资料 elasticsearch权威指南 干货 | 全方位深度解读 Elasticsearch 分页查询 Paginate search results","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"多索引多类型搜索","date":"2021-07-05T14:02:34.000Z","path":"wiki/多索引多类型搜索/","text":"如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。 然而，经常的情况下，你想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示： /_search在所有的索引中搜索所有的类型/gb/_search在 gb 索引中搜索所有的类型/gb,us/_search在 gb 和 us 索引中搜索所有的文档/g*,u*/_search在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search在所有的索引中搜索 user 和 tweet 类型当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的—​只是会涉及到更多的分片。 注意 ⚠️搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch重要配置","date":"2021-07-05T13:22:32.000Z","path":"wiki/elasticsearch重要配置/","text":"虽然Elasticsearch仅需要很少的配置，但有许多设置需要手动配置，并且在进入生产之前绝对必须进行配置。 path.data 和 path.logscluster.namenode.namebootstrap.memory_locknetwork.hostdiscovery.zen.ping.unicast.hostsdiscovery.zen.minimum_master_nodespath.data 和 path.logs如果使用.zip或.tar.gz归档，则数据和日志目录是$ES_HOME的子文件夹。 如果这些重要的文件夹保留在其默认位置，则存在将Elasticsearch升级到新版本时被删除的高风险。 在生产使用中，肯定得更改数据和日志文件夹的位置： 123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch RPM和Debian发行版已经使用数据和日志的自定义路径。 path.data 设置可以设置为多个路径，在这种情况下，所有路径将用于存储数据（属于单个分片的文件将全部存储在同一数据路径上）： 12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 cluster.name节点只能在群集与群集中的所有其他节点共享其cluster.name时才能加入群集。 默认名称为elasticsearch，但您应将其更改为描述集群用途的适当名称。cluster.name: logging-prod确保不要在不同的环境中重复使用相同的集群名称，否则可能会导致加入错误集群的节点。 node.name默认情况下，Elasticsearch将使用随机生成的uuid的第一个字符作为节点id。 请注意，节点ID是持久化的，并且在节点重新启动时不会更改，因此默认节点名称也不会更改。配置一个更有意义的名称是值得的，这是重启节点后也能一直保持的优势：node.name: prod-data-2node.name也可以设置为服务器的HOSTNAME，如下所示： 12node.name: $&#123;HOSTNAME&#125;bootstrap.memory_lock 没有JVM被交换到磁盘上这事对于节点的健康来说是至关重要的。一种实现方法是将bootstrap.memory_lock设置为true。要使此设置生效，需要首先配置其他系统设置。 有关如何正确设置内存锁定的更多详细信息，请参阅启用bootstrap.memory_lock。 network.host默认情况下，Elasticsearch仅仅绑定在本地回路地址——如：127.0.0.1与[::1]。这在一台服务器上跑一个开发节点是足够的。提示 事实上，多个节点可以在单个节点上相同的$ES_HOME位置一同运行。这可以用于测试Elasticsearch形成集群的能力,但这种配置方式不推荐用于生产环境。 为了将其它服务器上的节点形成一个可以相互通讯的集群，你的节点将不能绑定在一个回路地址上。 这里有更多的网路配置，通常你只需要配置network.host：network.host: 192.168.1.10network.host也可以配置成一些能识别的特殊的值，譬如：_local_、_site、_global_，它们可以结合指定:ip4与ip6来使用。更多相信信息请参见：网路配置 重要 👇 一旦你自定义了network.host的配置，Elasticsearch将假设你已经从开发模式转到了生产模式，并将升级系统检测的警告信息为异常信息。更多信息请参见：开发模式vs生产模式 discovery.zen.ping.unicast.hosts（单播发现）开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的回路地址，并扫描9300年到9305的端口去连接同一机器上的其他节点,试图连接到相同的服务器上运行的其他节点。它提供了不需要任何配置就能自动组建集群的体验。当与其它机器上的节点要形成一个集群时，你需要提供一个在线且可访问的节点列表。像如下来配置： 1234discovery.zen.ping.unicast.hosts: - 192.168.1.10:9300 - 192.168.1.11 #① - seeds.mydomain.com #② ① 未指定端口时，将使用默认的transport.profiles.default.port值，如果此值也为设置则使用transport.tcp.port ② 主机名将被尝试解析成能解析的多个IP discovery.zen.minimum_master_nodes为防止数据丢失，配置discovery-zen-minimum_master_nodes将非常重要，他规定了必须至少要有多少个master节点才能形成一个集群。没有此设置时，一个集群在发生网络问题是可能会分裂成多个集群——脑裂——这将导致数据丢失。更多详细信息请参见：通过minimum_master_nodes避免脑裂为避免脑裂，你需要根据master节点数来设置法定人数：(master_eligible_nodes / 2) + 1换句话说，如果你有三个master节点，最小的主节点数因被设置为(3/2)+1或者是2discovery.zen.minimum_master_nodes: 2 参考资料 elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch操作索引","date":"2021-07-05T13:11:01.000Z","path":"wiki/elasticsearch操作索引/","text":"创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 查看索引的mappingGET yj_visit_data/_mapping 1234567891011121314151617181920212223242526&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查询所有GET yj_visit_data/_search 12345&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 删除所有POST yj_visit_data/_delete_by_query 123456&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125; 通过文章删除POST yj_visit_data/_delete_by_query 1234567&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 根据文章查询GET yj_visit_data/_search 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 修改索引1234POST customer/_doc/1&#123; &quot;name&quot;:&quot;2&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch基础api","date":"2021-07-05T12:53:09.000Z","path":"wiki/elasticsearch基础cat_api/","text":"cat API集群健康状态GET _cat/health?v&amp;pretty 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1625489855 12:57:35 my-application yellow 1 1 35 35 0 0 23 0 - 60.3% 或者直接在服务器上调用rest接口：curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’ 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1475247709 17:01:49 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 我们可以看到我们名为 my-application 的集群与 yellow 的 status。 无论何时我们请求集群健康，我们可以获得 green，yellow，或者 red 的 status。Green 表示一切正常（集群功能齐全）， yellow 表示所有数据可用，但是有些副本尚未分配（集群功能齐全），red 意味着由于某些原因有些数据不可用。注意，集群是 red，它仍然具有部分功能（例如，它将继续从可用的分片中服务搜索请求），但是您可能需要尽快去修复它，因为您已经丢失数据了。 另外，从上面的响应中，我们可以看到共计 1 个 node（节点）和 0 个 shard（分片），因为我们还没有放入数据的。注意，因为我们使用的是默认的集群名称（elasticsearch），并且 Elasticsearch 默认情况下使用 unicast network（单播网络）来发现同一机器上的其它节点。有可能您不小心在您的电脑上启动了多个节点，然后它们全部加入到了单个集群。在这种情况下，你会在上面的响应中看到不止 1 个 node（节点）。 查看集群分布GET _cat/nodes?v&amp;pretty 12ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name172.19.0.1 20 61 15 0.02 0.04 0.29 cdhilmrstw * redtom-es-1 查看所有索引GET _cat/indices?v&amp;pretty 1234health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open rd-logstash-2021.06.19 p5iej71MQVW12s2awNv8nw 1 1 61236 0 16.3mb 16.3mbyellow open demo_index k6VTs7tdS0ysot-rPwxG9A 1 1 1 0 5.5kb 5.5kbgreen open kibana_sample_data_flights A7c5DViGSISii8FA0dNlGw 1 0 13059 0 5.6mb 5.6mb 查看所有索引的数量GET _cat/count?v&amp;pretty 12epoch timestamp count1625490245 13:04:05 838913 磁盘分配情况GET _cat/allocation?v&amp;pretty 123shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 35 308.7mb 20.1gb 215.9gb 236.1gb 8 172.19.0.1 172.19.0.1 redtom-es-1 23 UNASSIGNED 查看shard情况GET _cat/shards?v&amp;pretty 12345678index shard prirep state docs store ip nodeyj_visit_data 0 p STARTED 0 208b 172.19.0.1 redtom-es-1yj_visit_data 0 r UNASSIGNED demo_index 0 p STARTED 1 5.5kb 172.19.0.1 redtom-es-1demo_index 0 r UNASSIGNED rbtags 0 p STARTED 0 208b 172.19.0.1 redtom-es-1.kibana_1 0 p STARTED 280 11.5mb 172.19.0.1 redtom-es-1.kibana_task_manager_1 0 p STARTED 5 5.8mb 172.19.0.1 redtom-es-1 yj_visit_data 设置了一个副本分区，但是没有副节点，所以节点状态显示未分配； 参考资料 Elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"canal同步mysql数据到elasticsearch","date":"2021-07-05T03:26:50.000Z","path":"wiki/canal同步mysql数据到elasticsearch/","text":"首先安装elk推荐大家到elasic中文社区去下载 👉 【传送】⚠️ elastcisearch | logstash | kibana 的版本最好保持一直，否则会出现很多坑的，切记！ 安装ELK的步骤这里就不做介绍了，可以查看 👉 【TODO】 下载安装canal-adaptercanal github传送门 👉 【Alibaba Canal】 canal-client 模式可以参照canal给出的example项目和官方文档给出的例子来测试 依赖配置12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 创建maven项目保证canal-server 已经正确启动 👈 然后启动下面服务，操作数据库即可看到控制台的日志输出； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.redtom.canal.deploy;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import java.net.InetSocketAddress;import java.util.List;/** * @Author gaolei * @Date 2021/6/30 2:57 下午 * @Version 1.0 */@Slf4j@Componentclass CanalClient implements InitializingBean &#123; private final static int BATCH_SIZE = 1000; @Override public void afterPropertiesSet() throws Exception &#123; // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(&quot;***.***.***.***&quot;, 11111), &quot;example&quot;, &quot;canal&quot;, &quot;canal&quot;); try &#123; //打开连接 connector.connect(); //订阅数据库表,全部表 connector.subscribe(&quot;.*\\\\..*&quot;); //回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有ack的地方开始拿 connector.rollback(); while (true) &#123; // 获取指定数量的数据 Message message = connector.getWithoutAck(BATCH_SIZE); //获取批量ID long batchId = message.getId(); //获取批量的数量 int size = message.getEntries().size(); //如果没有数据 if (batchId == -1 || size == 0) &#123; try &#123; //线程休眠2秒 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; //如果有数据,处理数据 printEntry(message.getEntries()); &#125; //进行 batch id 的确认。确认之后，小于等于此 batchId 的 Message 都会被确认。 connector.ack(batchId); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; connector.disconnect(); &#125; &#125; /** * 打印canal server解析binlog获得的实体类信息 */ private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) &#123; //开启/关闭事务的实体类型，跳过 continue; &#125; //RowChange对象，包含了一行数据变化的所有特征 //比如isDdl 是否是ddl变更操作 sql 具体的ddl sql beforeColumns afterColumns 变更前后的数据字段等等 CanalEntry.RowChange rowChage; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;ERROR ## parser of eromanga-event has an error , data:&quot; + entry.toString(), e); &#125; //获取操作类型：insert/update/delete类型 CanalEntry.EventType eventType = rowChage.getEventType(); //打印Header信息 log.info(&quot;headers:&#123;&#125; &quot;, String.format(&quot;================》; binlog[%s:%s] , name[%s,%s] , eventType : %s&quot;, entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); //判断是否是DDL语句 if (rowChage.getIsDdl()) &#123; log.info(&quot;================》;isDdl: true,sql: &#123;&#125;&quot;, rowChage.getSql()); &#125; //获取RowChange对象里的每一行数据，打印出来 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; //如果是删除语句 if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); //如果是新增语句 &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); //如果是更新的语句 &#125; else &#123; //变更前的数据 log.info(&quot;-------&gt;; before&quot;); printColumn(rowData.getBeforeColumnsList()); //变更后的数据 log.info(&quot;-------&gt;; after&quot;); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; for (CanalEntry.Column column : columns) &#123; log.info(&quot; &#123;&#125; : &#123;&#125; update= &#123;&#125;&quot;, column.getName(), column.getValue(), column.getUpdated()); &#125; &#125;&#125; canal-adapter 模式adapter 配置文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546server: port: 8081spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1 batchSize: 1 retries: 0 timeout: accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 172.25.101.75:11111 canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 canal.tcp.username: canal.tcp.password: srcDataSources: defaultDS: url: jdbc:mysql://xxxx:pppp/database?useUnicode=true username: root password: pwd canalAdapters: - instance: example # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: es7 hosts: 172.25.101.75:9300 # 127.0.0.1:9200 for rest mode properties: mode: transport # or rest# # security.auth: test:123456 # only used for rest mode cluster.name: my-application# - name: kudu# key: kudu# properties:# kudu.master.address: 127.0.0.1 # &#x27;,&#x27; split multi address 我的elasticsearch是7.10.0版本的application.yml bootstrap.yml es6 es7 hbase kudu logback.xml META-INF rdb所以：👇 123cd es7biz_order.yml customer.yml mytest_user.ymlvim customer.yml customer.yml 配置文件如下： 123456789101112dataSourceKey: defaultDSdestination: examplegroupId: g1esMapping: _index: customer _id: id relations: customer_order: name: customer sql: &quot;select t.id, t.name, t.email from customer t&quot; etlCondition: &quot;where t.c_time&gt;=&#123;&#125;&quot; commitBatch: 3000 创建表结构12345678910CREATE TABLE `customer` ( `id` bigint(20) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, `email` varchar(255) DEFAULT NULL, `order_id` int(11) DEFAULT NULL, `order_serial` varchar(255) DEFAULT NULL, `order_time` datetime DEFAULT NULL, `customer_order` varchar(255) DEFAULT NULL, `c_time` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 测试canal-adapter同步效果创建一条记录122021-07-05 11:50:53.725 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;1&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457053000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457053724,&quot;type&quot;:&quot;INSERT&quot;&#125;Affected indexes: customer Elastcisearch 效果 1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;1&quot;, &quot;email&quot; : &quot;1&quot;, &quot;customer_order&quot; : &#123; &quot;name&quot; : &quot;customer&quot; &#125; &#125; &#125; ] &#125;&#125; 修改数据122021-07-05 11:54:36.402 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457275000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:[&#123;&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;customer_order&quot;:&quot;1&quot;&#125;],&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457276401,&quot;type&quot;:&quot;UPDATE&quot;&#125;Affected indexes: customer Elastcisearch 效果 删除一条数据122021-07-05 11:56:51.524 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457411000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457411523,&quot;type&quot;:&quot;DELETE&quot;&#125;Affected indexes: customer Elastcisearch 效果 参考资料 使用canal client-adapter完成mysql到es数据同步教程(包括全量和增量) es 同步问题 #1514 Github issue canal v1.1.4 文档手册 Sync es","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"mysql配置binlog","date":"2021-07-05T03:06:36.000Z","path":"wiki/binlog配置/","text":"开启binlog[mysqld]log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 查看binlog状态mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"mysql常用命令","date":"2021-07-05T03:06:36.000Z","path":"wiki/mysql常用命令/","text":"binlog相关命令mysql&gt; show variables like &#39;binlog_format&#39;; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like &#39;log_bin&#39;; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+ 用户&amp;权限创建用户并授权root用户登录： mysql -u root -p 然后输入密码创建用户： create user &#39;yjuser&#39;@&#39;%&#39; identified by &#39;u-bx.com&#39;;授权用户只读权限： grant SELECT on mirror.* to &#39;yjuser&#39;@&#39;%&#39; IDENTIFIED by &#39;u-bx.com&#39;;刷新权限：flush privileges; 查看当前用户select User(); Mac下配置文件首先，查看mysql读取配置文件的默认顺序 mysqld --help --verbose | more 查看帮助，下翻，会看到表示配置文件默认读取顺序，如下： Default options are read from the following files in the given order:/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf通常，这些位置上没有配置文件，所以需要创建文件，用下面的命令查找一个样例文件 ls $(brew --prefix mysql)/support-files/my-* 找到后，将这个配置文件拷贝到第一个默认读取目录下，用如下命令： cp /usr/local/opt/mysql/support-files/my-default.cnf /etc/my.cnf 或者 cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf 看你mysql的位置 然后，按需修改 my.cnf 文件。 修改完成后，需要重新启动mysql服务 brew services start mysql (启动)brew services stop mysql (停止)brew services restart mysql(重启)","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"flink简单上手","date":"2021-07-04T14:23:43.000Z","path":"wiki/flink简单上手/","text":"mac 安装 flink1、执行 brew install apache-flink 命令 123456789gaolei:/ gaolei$ brew install apache-flinkUpdating Homebrew...==&gt; Auto-updated Homebrew! Updated 1 tap (homebrew/services).No changes to formulae.==&gt; Downloading https://archive.apache.org/dist/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgz######################################################################## 100.0%🍺 /usr/local/Cellar/apache-flink/1.9.1: 166 files, 277MB, built in 15 minutes 29 seconds 2、执行flink启动脚本 12/usr/local/Cellar/apache-flink/1.9.1/libexec/bin./start-cluster.sh WordCount批处理Demo创建maven项目，导入依赖 注意自己的flink版本 👇👇 12345678910111213141516171819202122&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写批处理程序123456789101112131415161718192021222324public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125; &#125; 准备数据源文件123456hello sparkhello worldhello javahello flinkhow are youwhat is your name 执行结果123456789101112(is,1)(what,1)(you,1)(flink,1)(name,1)(world,1)(hello,4)(your,1)(are,1)(java,1)(how,1)(spark,1) flink 处理流式数据1、通过 nc -lk &lt;port&gt; 打开一个socket服务，监听7777端口 用于模拟实时的流数据 2、java代码处理流式数据 123456789101112131415161718192021222324252627public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数（设置成1即单线程处理） // env.setMaxParallelism(32); // 从文件中读取数据// String inputPath = &quot;/tmp/Flink_Tutorial/src/main/resources/hello.txt&quot;;// DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 4、在首次启动的时候遇到一个错误 ❌Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/streaming/api/datastream/DataStream处理方法可参照 参考资料 👇 参考资料 Exception in thread “main” java.lang.NoClassDefFoundError 解决方案 https://flink.apache.org/zh/downloads.html https://www.cnblogs.com/zlshtml/p/13796793.html","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Apache Flink","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink简介/","text":"官方地址请戳👉 【传送】 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink 是一个框架和分布式处理器引擎，用于对无界和有界数据进行状态计算； Why Flink 流数据更真实地反应了我们的生活方式 传统的数据架构是基于有限数据集的 我们的目标1、低延迟 毫秒级响应2、高吞吐 能够处理海量数据 分布式3、结果的准确性和良好的容错性 Where need Flink 电商和市场营销数据报表、广告投放、业务流程需要 物联网（IOT）传感器实时数据采集和显示，实时报警，交通运输业 电信业基站流量调配 银行和金融业实时结算和通知推送、实时检测异常行为 传统数据处理架构 传统的数据处理架构如上👆CRM(用户关系系统)， Order System(订单系统), Web App (用户点击时间)，当用户出发行为之后需要系统作出响应，首先由上层的计算层处理计算逻辑，计算层的逻辑计算依赖下面的存储层，计算层计算完成之后，将响应返回给客户端。这种基于传统数据库方式无法满足高并发场景，数据库的并发量都是很低的。 分析处理流程 分析处理流程架构如上👆，数据先有传统的关系数据库，经过提取，清洗过滤等，将数据存放到数据仓库，然后通过一些sql处理，生成数据报表和一些其他的查询。 问题也很明显，实时性太差了，处理流程太长，无法满足毫秒级需求 数据来源不唯一，能满足海量数据和高并发的需求，但是无法满足实时的需求 有状态的流式处理 把当前做流式计算所需要的数据不存放在数据库中，而是简单粗暴的直接放到本地内存中； 内存不稳定？周期性的检查点，数据存盘和故障检测； lambda架构用两台系统同时保障低延迟和结果准确； 这套架构分成两个流程，上面为批处理流程，数据收集到一定程序，交给批处理器处理，最终产生一个批处理结果 下面的流程为流式处理流程，保证能快速得到结果 最终有我们在应用层根据实际问题选择具体的处理结果交给应用程序这种架构有什么缺陷？可能得到的结果是不准确的，我们可以先快速的得到一个实时计算的结果，隔一段时间之后在来看批处理产生的结果。实现两台系统和维护两套系统，成本很大； 第三代流式处理架构Apache Flink 可以完美解决上面的问题👆Strom无法满足海量数据； Sparking Stream 无法满足低延迟； 基于事件驱动 （Event-driven） 处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。 无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流&lt;/&gt; 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 其他特点 支持事件时间（event-time）和处理时间（processing-time）语义 精确一次的状态一致性保证 低延迟 每秒处理数百万个事件，毫秒级延迟 与众多常用的存储系统链接 高可用，动态扩展，支持7*24全天运行 参考资料 1、尚硅谷 2021 Flink Java版2、Apache Flink Documentation","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Flink 运行时架构","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink运行时架构/","text":"Flink运行时组件 JobManager (作业管理器) JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成： ResourceManagerResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。 DispatcherDispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。 JobMasterJobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。 TaskManager （任务管理器） Flink中的工作进程，通常在flink中会有多个TaskManager运行，每一个TaskMaganer都包含一定数量的插槽（slots）. 插槽的数量限制了TaskManager能够执行的任务数量； 启动之后，TaskManager会向资源管理器注册他的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用，JobManager就可以向插槽分配任务（tasks）来执行了 在执行的过程中，一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据。 任务提交流程 任务调度原理Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。 Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run …中运行。 可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。 思考问题🤔 怎样实现并行计算？ 多线程 并行的任务，需要占用多少solt？ 一个流处理程序，到底包含多少个任务？ Tasks 和算子链并行度（Parallelism） 一个特定算子的子任务（subtask）的个数被称之为并行度； 一般情况下，一个Stream的并行度就是其所有算子中最大的并行度。整个流也有一个并行度，就是所有算子所有任务的并行度之和；对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。链行为是可以配置的；请参考链文档以获取详细信息。 TaskManager 和 Slots","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"拥塞避免","date":"2021-07-04T08:46:13.000Z","path":"wiki/拥塞避免/","text":"拥塞避免拥塞控制的慢启动是以指数方式快速的通过试探来扩大拥塞窗口的，但是一旦发生网络丢包，则肯定是很多报文段都会都是，因为窗口时称被增长的；为了解决这种问题，需要引入– 拥塞避免 什么是拥塞避免拥塞避免为了解决慢启动下，当拥塞窗口超出网络带宽时发生的大量丢包问题，它提出一个「慢启动阈值」的概念，当拥塞窗口到达这个阈值之后，不在以指数方式增长，而选择涨幅比较缓慢的「线性增长」，计算方式： cwnd += SMSS*SMSS/cwnd 当拥塞窗口在线性增长时发生丢包，将慢启动阈值设置为当前窗口的一半，慢启动窗口恢复初始窗口（init wnd）； 拥塞避免和慢启动是结合使用的，当发生网络丢包是，拥塞控制采用快速重传和快速启动来解决丢包问题！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-四次挥手/断开连接","date":"2021-07-04T08:34:11.000Z","path":"wiki/TCP-四次挥手-断开连接/","text":"TCP断开连接四次挥手 开始客户端和服务端都是处理【established】状态 客户端发送「FIN」报文之后，进入FIN-WAIT-1状态 服务端收到客户端的FIN之后，恢复一个ACK，同时进入CLOSE_WAIT状态 客户端接收到ACK之后，进入到FIN-WAIT-2状态 服务端接着发送FIN报文，同时进入LAST-ACK状态 客户端接收到服务端的FIN报文之后，发送ACK报文，并进入TIME_WAIT状态 客户端在经历2个MSL时间之后，进入CLOSE状态 服务端接收到客户端的ACK之后，进入CLOSE状态 并不是所有的四次挥手都是上述流程，当客户端和服务端同时发送关闭连接的请求如下👇： 可以看到双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN_WAIT_1 都进入到 CLOSING 这个过度状态然后再到 TIME_WAIT。 挥手一定需要四次吗？ 假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？ 为什么要有 TIME_WAIT? 断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。 那么为什么要等 2MSL 呢？ 就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。 假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。 等待 2MSL 会产生什么问题？ 如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"快速重传/快速恢复","date":"2021-07-04T08:33:35.000Z","path":"wiki/快速重传-快速恢复/","text":"快速重传和快速恢复快速重传 d 为何会接收到以个失序数据段？ 若报文丢失，将会产生连续的失序ACK段 若网络路径与设备导致数据段失序，将会产生少量的失序ACK段 若报文重复，将会产生少量的失序ACK段 当发送端发送pkt0是正常的，由于滑动窗口为满，发送方可以继续发送pkt1，pkt2； 加入pkt1发生了丢包，虽然pkt2接收端接收成功了，但是没有pkt1的数据段，接收端还是发送ACK1的确认报文； 在没有「快速重传」的情况下，发送端需要等到RTO之后，才可以重新发送pkt1 重传成功之后，接收端其实收到了pkt2之前的所有数据段，所以发送ACK3的确认报文 这种需要等待RTO才可以重传的方式效率是比较低的，因此需要快速重传来进行优化； 快速重传和累积确认 当发送方连续发送pkt3，pkt4，pkt5，pkt6四个数据端，但是pkt5在网络中丢包了，那后面发送的pkt6，pkt7，pkt8的确认报文都返回ACK5，希望发送方吃昂传pkt5的数据段；这个时候，发送方收到连续3个相同的确认报文，便立即重新发送pkt5的数据段； 接收方: 当接收到一个失序数据段时，立刻发送它所期待的缺口 ACK 序列号 当接收到填充失序缺口的数据段时，立刻发 送它所期待的下一个 ACK 序列号 发送方 当接收到3个重复的失序 ACK 段(4个相同的失序ACK段)时，不再等待重传定时器的触发，立刻基于快速重传机制重发报文段 当pkt5重新发送并被接收端接收之后，接收端发送ACK9的确认报文，而不是再分别发送ACK6，ACK7，ACK8，这个称谓「 累计确认 」。 快速恢复 快速重传下一定要进入慢启动吗? 接受端收到重复ACK，意味着网络仍在流动，而如果要重新进入慢启动，会导致网络突然减少数据流，拥塞窗口恢复初始窗口，所以，「在快速恢复下发生丢包的场景下」，应该使用快速恢复，简单的讲，就是将慢启动阈值设置成当前拥塞窗口的一半，而拥塞窗口也适当放低，而不是一下字恢复到初始窗口大小； 快速恢复的流程如上图👆所示！ 快速恢复的具体操作： 将 ssthresh 设置为当前拥塞窗口 cwnd 的一半，设当前 cwnd 为 ssthresh 加上 3*MSS 每收到一个重复 ACK，cwnd 增加 1 个 MSS 当新数据 ACK 到达后，设置 cwnd 为 ssthresh","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-拥塞控制之慢启动","date":"2021-07-04T08:33:19.000Z","path":"wiki/TCP-拥塞控制之慢启动/","text":"由于TCP是面向字节流的传输协议，可以发送不定长的字节流数据，TCP连接发送数据时会“先天性”尝试占用整个带宽，而当所有的TCP连接都尝试占用网络带宽时，就会造成网络的堵塞，而TCP慢启动算法则是为了解决这一场景； 全局思考 拥塞控制要面向整体思考，如上👆网络拓扑图，当左边的网络节点通过路由交换设备向右边的设备传输报文的时候，中间的某一链路的带宽肯定是一定的，这里假设1000M带宽，当左边R1以700Mb/s的速度向链路中发送数据，同时R2以600Mb/s的速率发送报文，那势必会有300Mb的数据报丢失；「路由交换设备基于存储转发来实现报文的发送」大量报文都是时，路由设备的缓冲队列肯定是慢的，这也会造成某些数据报在网络链路中停留时间过长，从而导致TCP通讯变慢，甚至网络瘫痪； 理想的情况下，当链路带宽占满以后，链路以最大带宽传输数据，当然显示中是不可能的，当发生轻度拥塞时，链路的吞吐量就开始下降了，发展到严重阻塞时，链路的吞吐量会严重地下降，甚至瘫痪； 那么，慢启动是如何发挥作用的呢？ 拥塞窗口 拥塞窗口cwnd(congestion window) 通告窗口rwnd(receiver‘s advertised window) 其实就是RCV.WND，标志在TCP首部的Window字段！ 发送窗口swnd = min(cwnd，rwnd) 前面学习滑动窗口的时候提到发送窗口大致等于接受窗口，当引入拥塞窗口时，发送窗口就是拥塞窗口和对方接受窗口的最小值 每收到一个ACK，cwnd扩充一倍 慢启动的窗口大小如何设置呢？如上所示，起初拥塞窗口设置成1个报文段大小，当发送端发送一个报文段并且没有发生丢包时，调整拥塞窗口为2个报文段大小，如果还没有发生丢包，一次类推，知道发生丢包停止；发送窗口以「指数」的方式扩大；慢启动是无法确知网络拥塞程度的情况下，以试探性地方式快速扩大拥塞窗口； 慢启动初始窗口慢启动的拥塞窗口真的就如上面所说的以一个报文段大小作为初始值吗？ 慢启动初始窗口 IW(Initial Window)的变迁 1 SMSS:RFC2001(1997) 2 - 4 SMSS:RFC2414(1998) IW = min (4SMSS, max (2SMSS, 4380 bytes)) 10 SMSS:RFC6928(2013) IW = min (10MSS, max (2MSS, 14600)) 其实在实际情况下，互联网中的网页都在10个mss左右，如果还是从1个mss开始，则会浪费3个RTT的时间；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-如何减少小报文提升网络效率","date":"2021-07-04T08:32:55.000Z","path":"wiki/TCP-如何减少小报文提升网络效率/","text":"如何减少小报文提升网络效率每一个TCP报文段都包含20字节的IP头部和20字节的TCP首部，如果报文段的数据部分很少的话，网络效率会很差； SWS(Silly Window syndrome) 糊涂窗口综合症 如上图👆所示场景，在之前的滑动窗口已经了解过，随着服务端处理连接数据能力越来越低，服务端的可用窗口不断压缩，最终导致窗口关闭； SWS 避免算法SWS 避免算法对发送方和接收方都做客 接收方 David D Clark 算法:窗口边界移动值小于 min(MSS, 缓存/2)时，通知窗口为 0 发送方 Nagle 算法:1、TCP_NODELAY 用于关闭 Nagle 算法2、没有已发送未确认报文段时，立刻发送数据3、存在未确认报文段时，直到:1-没有已发送未确认报文段，或者 2-数据长度达到MSS时再发送 TCP delayed acknowledgment 延迟确认实际情况下，没有携带任何数据的ACK报文也会造成网络效率低下的，因为确认报文也包含40字节的头部信息，但仅仅是为了传输ACK=1这样的信息，为了解决这种情况，TCP有一种机制，叫做延迟确认，如下👇： 当有响应数据要发送时,ack会随着响应数据立即发送给对方. 如果没有响应数据,ack的发送将会有一个延迟,以等待看是否有响应数据可以一起发送 如果在等待发送ack期间,对方的第二个数据段又到达了,这时要立即发送ack 那个延迟的时间如何设置呢？ 上面👆是Linux操作系统对于TCP延时的定义。 HZ是什呢？其实那是和操作系统的时钟相关的，具体的操作系统间各有差别；如何查看Linux操作系统下的HZ如何设置呢？ 1cat /boot/config- `-uname -r` | grep &#x27;^GONFIG_HZ=&#x27; TCP_CORK sendfile 零拷贝技术","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-滑动窗口","date":"2021-07-04T08:32:40.000Z","path":"wiki/TCP-滑动窗口/","text":"滑动窗口 之前学习了PAR方式的TCP超时和重传，其实在考虑发送方发送数据报的同时，也应该考虑接收方对于数据的处理能力，由此引出本次学习的主题 – 滑动窗口 发送端窗口滑动窗口按照传输数据方向分为两种，发送端窗口和接收端窗口；下面先看一下发送端窗口👇： 上图分为四个部分： 已发送并收到 Ack 确认的数据:1-31 字节 已发送未收到 Ack 确认的数据:32-45 字节 未发送但总大小在接收方处理范围内:46-51 字节 未发送但总大小超出接收方处理范围:52-字节 可用窗口和发送窗口 如上图这里可以引出两个概念：「可用窗口」和「发送窗口」 【 可用窗口 】： 就是上图中的第三部分，属于还未发送，但是在接收端可以处理范围内的部分；【 发送窗口 】： 就是发送端可以发送的最大报文大小，如上图中的第二部分+第三部分合成发送窗口； 可用窗口耗尽 可用窗口会在一个短暂的停留，当处于未发送并且接受端可以接受范围内的数据传输完成之后，可用窗口耗尽；当然上面仅仅说的一瞬时的状态，这个状态下，已经发送的报文段还没有确认，并且发送窗口大小没有发生变化，此时发送窗口达到最大状态； 窗口移动 如果在发送窗口中已经发送的报文段已经得到接受端确认之后，那部分数据就会被移除发送窗口，在发送窗口大小不发生变化的情况下，发送窗口向右➡️移动5个字节，因为左边已经发送的5个字节得到确认之后，被移除发送窗口； 可用窗口如何计算 再次引出三个概念： SND.WND SND 指的是发送端，WND指的是window，也就是发送端窗口的意思 SND.UNA UNA 就是un ACK的意思，指的是已经发送但是没有没有确认 它指向窗口的第一个字节处 SND.NXT NXT 是next的位置，是发送方接下来要发送的位置，它指向可用窗口的第一个字节处 那就很容易得出可用窗口的大小了，计算公式如下： Usable Window Size = SND.UNA + SND.WND - SND.NXT 接收端窗口上面介绍了发送端窗口的一些概念，下面👇是接收端窗口的学习： 已经接收并且已经确认 :28-31 字节 还未接收并且接收端可以接受:32-51 字节 还未接收并且超出接收处理能力:51-57 字节 这里引出两个概念： RCV.WND RCV是接收端的意思，WND是接受端窗口的大小 RCV.NXT NXT表示的是接受端接收窗口的开始位置，也就是接收方接下来处理的第一个字节； RCV.WND的大小接受端的内存以及缓冲区大小有关，在某种意义上说，接受端的窗口大小和发送端大小大致相同；接受端可接收的数据能力可以通过TCP首部的Window字段设置，但是接受端的处理能力是可能随时变化的，所以接受端和服务端的窗口大小大致是一样的； 流量控制下面👇根据一个例子来阐述流量控制，模拟一个GET请求，客户端向服务端请求一个260字节的文件，大致流程如下，比较繁琐： 这里假设MSS和窗口的大小不发生变化，同时客户端和发送端状态如下：【 客户端 】： 发送窗口默认360字节 接收窗口设定200字节【 服务端 】： 发送窗口设定200字节 接收窗口设定360字节 Step1： 客户端发送140字节的数据到服务端 【客户端】发送140字节，【SND.NXT】从1-&gt;141 【服务端】状态不变，等待接收客户端传输的140字节 Step2: 服务端接收140字节，发送80字节响应以及ACK 【 客户端 】发送140字节之后等待【 服务端 】的ACK 【 服务端 】可用窗口右移，【RCV.NXT】从1-&gt;141【 服务端 】发送80字节数据，【SND.NXT】从241-&gt;321 Step3: 客户端接收响应ACK，并且发送ACK 【 客户端 】发出的140字节得到确认，【SND.UNA】右移140字节【 客户端 】接收80字节数据，【RCV.NXT】右移80字节，从241-&gt;321 Step4: 服务端发送一个280字节的文件，但是280字节超出了客户端的接收窗口，所以客户端分成两部分传输，先传输120字节； 【 服务端 】发送120字节，【SND.NXT】向右移动120字节，从321-&gt;441 Step5: 客户端接收文件第一部分，并发送ACK 【 客户端 】接收120字节，【RCV.NXT】从321-&gt;441 Step6：服务端接收到第二步80字节的ACK [ 服务器 ] 80字节得到ACK 【SND.UNA】从241-&gt;321 Step7: 服务端接收到第4步的确认 【 服务端 】之前发送文件第一部分的120字节得到确认，【SND.UNA】右移动120，从321-&gt;441 Step8: 服务端发送文件第二部分的160字节 【 服务端 】： 发送160字节，【SND.NXT】向右移动160字节，从441-&gt;601 Step9: 客户端接收到文件第二部分160字节，同时发送ACK 【 客户端 】接收160字节，【RCV.NXT】向右移动160字节，从441-&gt;601 Step10: 服务端收到文件第二部分的ACK 【 服务端 】发送的160字节得到确认，【SND.UNA】向右一定160字节，从441-&gt;601；至此客户端收到服务端发送的完整的文件； 上面通过表格列举服务端和客户端每个状态在每个步骤的状态，如果不是很好理解，可以看如下示意图辅助理解： 客户端交互流程 服务端交互流程 上面👆是模拟一个GET请求，服务端发送一个280字节的文件给到客户端，客户端的接收窗口是200字节场景加，客户端和服务端的数据传输与交互流程，通过这个流程来学习滑动窗口的移动状态和流量控制的大致流程； 滑动窗口与操作系统缓冲区上面👆讲述的时候，都是假设窗口大小是不变的，而实际上，发送端和接受端的滑动窗口的字节数都吃存储在操作系统缓冲区的，操作系统的缓冲区受操作系统控制，当应用进程增加是，每个进程分配的内存减少，缓冲区减少，分配给每个连接的窗口就会压缩。**而且滑动窗口的大小也受应用进程读取缓冲区数据速度有关**； 应用进程读取缓冲区数据不及时造成窗口收缩step1: 客户端发送140字节 客户端发送到140字节之后，可用窗口收缩到220字节，发送窗口不变 Step2: 服务端接收140字节 但是应用进程仅仅读取40字节 服务端应用进程仅仅读取40字节，仍有100字节占用缓冲区大小，导致接受窗口收缩，服务端发送ACK报文时，在首部Window带上接收窗口的大小260 Step3: 客户端收到确认报文之后，发送窗口收缩到260 Step4: 客户端继续发送180字节数据 客户端发送180字节之后，可用窗口变成80字节 Step5: 服务端接收到180字节 假设应用程序仍然不读取这180字节，最终也导致服务端接收窗口再次收缩180字节，仅剩下80字节，在发送确认报文时，设置首部window=80 Step6: 客户端收到80字节的窗口时，调整发送窗口大小为80字节，可用窗口也是80字节 Step7: 客户端仍然发送80字节到服务端，此时可用窗口为空 Step8: 服务端应用进程继续不读区这80字节的缓冲区数据，最终导致服务端接收窗口大小为0，不能再接收任何数据，同时发送ACK报文； Step9：客户端收到确认报文之后，调整发送窗口大小为0，这个状态叫做「 窗口关闭 」 窗口收缩导致的丢包 Step1：客户端服务端开始的窗口大小都是360字节，客户端发送140字节数据 客户端发送140字节之后，可用窗口变成220字节 Step2：服务端应用进程骤增，进程缓存区平均分配，造成服务端接收窗口减少，从360变成240字节； 假设接收了140字节之后，应用进程没有读取，那个可用窗口进一步压缩，变成100字节； Step3：假设同一个连接在没有收到服务端确认之后，又发送了180个字节的数据（Retramission） 先发送了140字节，后发送了180字节，都没有得到确认，客户端可用窗口大小变成40字节 Step4：服务端收到上面👆第三步发送的180字节的数据，但是接受窗口的大小只有100字节，所以不能接收 服务端拒绝接收180字节 Step5：此时客户端才收到之前140字节的确认报文，才知道接收窗口发生了变化 客户端由于没有收到180字节的确认，加入客户端正在准备发送180字节数据，得到接受端的窗口大小是100字节之后，须强制将右侧窗口向左收缩80字节； 窗口关闭这个例子和上面的例子都发生了「 窗口关闭 」 窗口关闭： 发送端的发送窗口变成0的状态； 上面讲的两种情况一般不会发生的，因为操作系统不会既收缩窗口，同时减少连接缓存；而是一般先使用窗口收缩策略，之后在压缩缓冲区的方式来规避以上问题；发生窗口关闭之后，发送端不会被动的等待服务端的通知，而是会采用定时嗅探的方式去查看服务端接收窗口是否开放； Linux中对TCP缓冲区的调整方式 net.ipv4.tcp_rmem = 4096 87380 6291456 读缓存最小值、默认值、最大值，单位字节，覆盖 net.core.rmem_max net.ipv4.tcp_wmem = 4096 16384 4194304 写缓存最小值、默认值、最大值，单位字节，覆盖net.core.wmem_max net.ipv4.tcp_mem = 1541646 2055528 3083292 系统无内存压力、启动压力模式阀值、最大值，单位为页的数量 net.ipv4.tcp_moderate_rcvbuf = 1 开启自动调整缓存模式","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-RTO重传计数器的计算","date":"2021-07-04T08:32:27.000Z","path":"wiki/TCP-RTO重传计数器的计算/","text":"之前的文章已经介绍了TCP超时重传的过程中使用了定时器的策略，当定时器规定时间内未收到确认报文之后，就会触发报文的重传，同时定时器复位；那么定时器超时时间（RTO Retramission Timeout）是如何计算的呢？ 什么是RTT？了解RTO如何计算之前，首先明确一个概念「 RTT 」； 如上图所示，从client发送第一个「SYN」报文，到Server接受到报文，并且返回「SYN ACK」报文之后，client接受到Server的「ACK」报文之后，client所经历的时间，叫做1个RTT时间； 如何在重传下有效测量RTT？ 如上图两种情况：第一种，左侧a图所示，当一端发送的数据报丢失后要进行重传，到重传之后接收到确认报文之后，这种场景下该如何计算RTT呢？开始时间是按照第一次发送数据报时间呢还是按照重传数据报的时间呢？ 按照常理来说，如右侧b图所示，RTT时间应该以RTT2为准； 第二种，左侧b图所示，第一次发送数据报文时，由于网络时延导致RTO时间内没有收到接收段的确认报文，发送端进行重发，但是在刚刚重发之后就收到了第一次报文的确认报文，那这种情况RTT该如何计算呢？ 如右侧a图所示，RTT时间应该以RTT1为准； 就像上面提及的两种情况，一会以第一个RTT1为准，一会以RTT2为准，那么TCP协议如何正确的计算出RTT呢？ 使用Timestamp方式计算RTT之前的文章中在介绍TCP超时与重传的笔记中有介绍通过使用Timtstamp的方式来区分相同Seq序列号的不同报文，其实在TCP报文首部存储Timestamp的时候，会存储报文的发送时间和确认时间，如下所示： 如何计算RTO？上面👆说到了RTT如何计算，那个RTO和RTT有什么关系呢？ RTO的取值将会影响到TCP的传输效率以及网络的吞吐量； 通常来说RTO应该略大于RTT，如果RTO小于RTT，则会造成发送端频繁重发，可能会造成网络阻塞；如果RTO设置的过大，则接受端已经收到确认报文之后的一段时间内仍然不能发送其他报文，会造成两端性能的浪费和网络吞吐量的下降； 平滑RTO网络的RTT是不断的变化的，所以计算RTO的时候，应当考虑RTO的平滑性，尽量避免RTT波动带来的干扰，以抵挡瞬时变化； 平滑RTO在文档RFC793定义，给出如下计算方式： SRTT (smoothed round-trip time) = ( α * SRTT ) + ((1 - α) * RTT) α 从 0到 1(RFC 推荐 0.9)，越大越平滑 RTO = min[ UBOUND, max[ LBOUND, (β * SRTT) ] ] 如 UBOUND为1分钟，LBOUND为 1 秒钟， β从 1.3 到 2 之间 这种计算方式不适用于 RTT 波动大(方差大)的场景,如果网络的RTT波动很大，会造成RTO调整不及时； 追踪RTT方差计算RTO RFC6298(RFC2988)，其中α = 1/8， β = 1/4，K = 4，G 为最小时间颗粒: 首次计算 RTO，R为第 1 次测量出的 RTT123SRTT(smoothed round-trip time) = RRTTVAR(round-trip time variation) = R/2RTO = SRTT + max (G, K*RTTVAR) 后续计算 RTO，R’为最新测量出的 RTT123SRTT= (1-α)*SRTT+α*R’RTTVAR=(1-β)*RTTVAR+β*|SRTT-R’|RTO = SRTT + max (G, K*RTTVAR)","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP超时与重传","date":"2021-07-04T08:32:08.000Z","path":"wiki/TCP超时与重传/","text":"背景 如上图👆所示，设备A向设备B发送消息，消息在网络中会由于各种各样的问题导致丢失，那么该如何解决上述问题呢？ 采用定时器重传 PAR：Positive Acknowledgment with Retransmission 最简单的思路是在发送方设置「 定时器 」： 当设备A发送第一条消息之后，在定时器规定的时间内，如果收到设备B的确认报文，则设备A继续发送下一个报文，同时定时器复位； 如果第一条消息发送时间超出了定时器规定的时间，则设备A将重新发送第一条消息，同时重新设置定时器； 这种方式是串型发送的，只有第一个消息发送成功之后，才可以发送下一条消息，「 效率极差 」； 并发能力PAR 基于上述PAR效率低下的方式进行改造，在发送端采用并发+定时器的方式进行数据发送； 首先设备A可以同时发送多个消息或者报文段，每个报文段具有一个标志字段【#XX】去标志唯一，每个报文段连接具有自己的定时器； 设备B规定时间内收到设备A发送的数据之后并且设备A得到设备B的确认之后，设备A将定时器清除 同PAR一样，设备B没有在规定的时间内发送确认报文，设备A将这个报文所对应的定时器复位，重新发送这个报文 并发发送带来的问题采用并发的方式发送消息或者报文段固然提升了发送端的性能，但是发送端发送的消息可能接受端不能完全处理，这是双方报文处理速度或者效率不一致的问题； 所以对于接收端设备B，应该明确自己可能接受的数据量，并且在确认报文中同步到发送端设备A，设备A根据设备B的处理能力来调整发送数据的大小；也就是上图中的「 limit」； 继续延伸Sequment序列号和Ack序列号的设计理念或者设计初衷是「 解决应用层字节流的可靠发送 」 跟踪「应用层」的发送端数据是否送达 确定「接收端有序的」接收到「字节流」 序列号的值针对的是字节而不是报文 ⚠️⚠️⚠️ TCP的定位就是面向字节流的！ TCP序列号如何设计的 通过TCP报文头我们可以知道，Sequment序列号包括32位长度；也就是说一个Sequment可以发送2的32次方个字节，大约4G的数量，Sequment就无法表示了，当传输的数据超过“4G”之后，如果这个连接依然要使用的话，Sequment会重新复用；Sequment复用会产生一个问题，也就是序列号回绕；👇 序列号回绕 序列号回绕 (Protect Against Wrapped Sequence numbers) 当一个连接要发送6G的数据是，A、B、C、D分别发送1G的数据，如果继续使用此连接，E下一次发送数据1G，Seq序列号复用，E报文段的序列号和A报文段的序列号表示相同 按照上面的逻辑继续发送数据，F报文段的Seq标志和B报文段的是一样的； 加入B报文段在发送过程中丢失了，直到接受端接收了F报文段的同时B报文段到达接受端，接受端该如何区分相同Seq序列号不同数据的报文段呢？ 其实TCP解决这个问题很简单，就是在每个报文段上添加Tcp Timestamp时间戳，类似于版本号的理念； 接收端收到相同Seq序列号的报文段是可以根据时间戳来进行区分；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP最大报文段（MSS）","date":"2021-07-04T08:31:56.000Z","path":"wiki/TCP最大报文段（MSS）/","text":"MSS产生的背景我们都知道TCP协议是运输在传输层的协议，它是面向【字节流】的传输协议；它的上层，应用层传输的数据是无限制的，但是它的下层也就是网络层和链路层由于路由等转发设备有内存等限制是不可能无限制传输任何大小的报文的，它们一定会限制报文的长度，因此 TCP协议要完成的工作是将从应用层接受到的任意长度数据，切割成多个报文段，MSS就是如何切割报文段的依据。 什么是MSSMSS（Max Segment Size）：仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879 MSS 选择目的 尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率 防止 Segment 被某个设备的 IP 层基于 MTU 拆分 IP层基于MTU的数据拆分是效率极差的，一个报文段丢失，所有的报文段都要重传 MSS默认大小 默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部) MSS在什么时候使用 握手阶段协商 MSS 这个在TCP三次握手的文章中已经提及过了！ MSS 分类 发送方最大报文段: SMSS:SENDER MAXIMUM SEGMENT SIZE 接收方最大报文段: RMSS:RECEIVER MAXIMUM SEGMENT SIZE 在TCP常用选项中可以看到【MSS】的选项 TCP流与报文段在数据传输中的状态 从上图可以看到，左边客户端在发送字节流数据给到右边客户端，客户端发送一个连续的字节流，会在TCP层按照MSS大小规定进行拆分成多个小的报文段，分别传送到另一个客户端或者其他的接收端；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP三次握手","date":"2021-07-04T08:31:42.000Z","path":"wiki/TCP三次握手/","text":"握手🤝的目的 同步Sequence序列号 初始化序列号ISN （Inital Sequence Number） 交换TCP通讯的参数 比如最大报文段参数（MSS）、窗口比例因子（Window）、选择性确认（SACK）、制定校验和算法； 三次握手握手过程 TCP三次握手的大致流程图如上👆 使用tcpdump抓包分析三次🤝握手报文中Seq和Ack的变化 1tcpdump port 80 -c 3 -S 第一次握手🤝1IP upay.60734 &gt; 100.100.15.23.http: Flags [S], seq 3800409106, win 29200, options [mss 1460,sackOK,TS val 839851765 ecr 0,nop,wscale 7], length 0 客户端upay访问服务端80端口，发送一个「 seq=3800409106 」 ，同时标志位SYN=1，声明此次握手是要建立连接； 第二次握手🤝1IP 100.100.15.23.http &gt; upay.60734: Flags [S.], seq 1981710286, ack 3800409107, win 14600, options [mss 1440,nop,nop,sackOK,nop,wscale 7], length 0 第二次握手，服务端收到客户端的申请连接强求（SYN=1）之后，在服务端自己准备好的情况下，给客户端发送 「 ACK=1 SYN=1 」的确认报文，SYN=1同样也是声明此次报文是建立连接的报文请求； ack= 3800409107 也就是第一个客户端发给服务端的seq+1（ack是接收方下次期望接口报文的开始位置） 第三次握手握手1IP upay.60734 &gt; 100.100.15.23.http: Flags [.], ack 1981710287, win 229, length 0 客户端收到服务器返回的确认报文，确认可以进行连接，发送「 ack = 1981710287 」的确认报文，之后就完成了三次握手，TCP的连接就创建成功了，接下来双方就可以发送数据报了； TCP连接创建构成中状态的变更 首先客户端和服务端都是【CLOSED】状态，客户端发起连接请求之后，进入【SYN-SENT】状态，这个状态维持的时间很短，我们使用netstat去查看tcp连接状态的时候，基本上都不会看到这个状态，而服务端是在【LISTEN】状态，等待客户端的请求； 服务端收到客户端请求之后，发送「SYN ACK」确认报文，同时服务端进入【SYN-RECEIVED】状态，等待客户端的确认报文； 客户端收到服务端的同步确认请求之后，发送「ACK」确认报文，同时进入【ESTABLISHED】状态，准备后续的数据传输； 服务端收到三次握手最后的确认报文之后，进入【ESTABLISHED】状态，至此，一个TCP连接算是建立完成了，后面就是双方的通信了； TCB（Transmission Control Block） 保存连接使用的源端口、目的端口、目的 ip、序号、 应答序号、对方窗口大小、己方窗口大小、tcp 状态、tcp 输入/输出队列、应用层输出队 列、tcp 的重传有关变量等 TCP性能优化和安全问题 正如我们了解的TCP三次握手🤝的流程，当有大量SYN请求到达服务端时，会进入到【SYN队列】，服务端收到第二次确认报文之后，会进入【ESTABLISHED】状态，服务端操作系统内核会将连接放入到【ACCEPT】队列中，当Nginx或者Tomcat这些应用程序在调用accept（访问内核）的时候，就是在【ACCEPT】队列中取出连接进行处理； 由此可见，【SYN】队列和【ACCEPT】是会影响服务器连接性能的重要因素，所以对于高并发的场景下，这两个队列一定是要设置的比较大的； 如何设置SYN队列大小服务器端 SYN_RCV 状态 net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数 客户端 SYN_SENT 状态（服务端作为客户端，比如Ngnix转发等） net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数 net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围 Fast Open机制 TCP如何对连接的次数以及连接时间进行优化的呢？这里提到Fast Open机制；比如我们有一个Http Get请求，正常的三次握手🤝到收到服务端数据需要2个RTT的时间；FastOpen做出如下优化： 第一次创建连接的时候，也是要经历2个RTT时间，但是在服务端发送确认报文的时候，在报文中添加一个cookie； 等到下次客户端再需要创建请求的时候，直接将【SYN】和cookie一并带上，可以一次就创建连接，经过一个RTT客户端就可以收到服务端的数据； 如何Linux上打开TCP Fast Open net.ipv4.tcp_fastopen:系统开启 TFO 功能 0:关闭 1:作为客户端时可以使用 TFO 2:作为服务器时可以使用 TFO 3:无论作为客户端还是服务器，都可以使用 TFO SYN攻击什么是SYN攻击？ 正常的服务通讯都是由操作系统内核实现的请求报文来创建连接的，但是，可以人为伪造大量不同IP地址的SYN报文，也就是上面👆状态变更图中的SYN请求，但是收到服务端的ACK报文之后，却不发送对于服务端的ACK请求，也就是没有第三次挥手，这样会造成大量处于【SYN-RECEIVED】状态的TCP连接占用大量服务端资源，导致正常的连接无法创建，从而导致系统崩坏； SYN攻击如何查看1netstat -nap | grep SYN_RECV 如果存在大量【SYN-RECEIVED】的连接，就是发生SYN攻击了； 如何规避SYN攻击？ net.core.netdev_max_backlog 接收自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn_backlog SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接 设置SYN Timeout 由于SYN Flood攻击的效果取决于服务器上保持的SYN半连接数，这个值=SYN攻击的频度 x SYN Timeout，所以通过缩短从接收到SYN报文到确定这个报文无效并丢弃改连接的时间，例如设置为20秒以下，可以成倍的降低服务器的负荷。但过低的SYN Timeout设置可能会影响客户的正常访问。 设置SYN Cookie (net.ipv4.tcp_syncookies = 1) 就是给每一个请求连接的IP地址分配一个Cookie，如果短时间内连续受到某个IP的重复SYN报文，就认定是受到了攻击，并记录地址信息，以后从这个IP地址来的包会被一概丢弃。这样做的结果也可能会影响到正常用户的访问。 当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接 由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等 TCP_DEFER_ACCEPT这个是做什么呢？ 正如上面👆操作系统内核展示图所示，内核中维护两个队列【SYN】队列和【ACCEPT】队列，只有当收到客户端的ACK报文之后，连接会进入到【ACCEPT】，同时服务器的状态是【ESTABLISHED】状态，此时操作系统并不会去激活应用进程，而是会等待，知道收到真正的data分组之后，才会激活应用进程，这是为了提高应用进程的执行效率，避免应用进程的等待； TCP三次握手为什么不能是两次或者四次 参见文章：敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP头部","date":"2021-07-04T08:31:22.000Z","path":"wiki/TCP头部/","text":"带着问题学习 如何校验报文段是否损坏？ 如何CRC校验 seq和ack是如何计算的？ tcp校验位都有那些？ 6个 分别是什么含义？ tcp如何计算首部长度？ 偏移量 TCP Retransmission 重传？ tcp spurious retransmission 又是什么呢？ tcp dup ack 是什么？ ack与ACK有什么区别？ 分别有什么作用？ TCP头部结构 学习TCP协议首先要看一下它的报文段是如何组成的；TCP报文段组成由两部分，第一部分是报文头部，第二部分是数据部分； 先看一下报文头，也就是TCP首部的组成； 16位端口16位端口号：告知主机该报文段是来自哪里（源端口Source Port）以及传给哪个上层协议或应用程序（目的端口Destination Port）的。进行TCP通信时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号（比如DNS协议对应端口53，HTTP协议对应80，这些端口号可在/etc/services文件中找到）。 序列号（Seq）占32位，也就是4字节长度，序号范围自然也是是0~2^32-1。TCP是面向字节流的，TCP连接中传送的字节流中的每个字节都按顺序编号。整个要传送的字节流的起始序号必须要在连接建立时设置。首部中的序号字段值指的是本报文段所发送的数据的第一个字节的序号。 TCP用序列号对数据包进行标记，以便在到达目的地后重新重装，假设当前的序列号为 s，发送数据长度为 l，则下次发送数据时的序列号为 s + l。在建立连接时通常由计算机生成一个随机数作为序列号的初始值。 **这里存在一个疑问，第一次建立TCP连接的时候，网上一些博客上说seq是client随机生成的，也有的博客说是seq=1； 这里经过我抓包后，看到第一次创建TCP连接的时候，确实是1; ** 确认应答号（Ack）Ack占32位，4个字节长度，表示期望收到对方下一个报文段的序号值。 用作对另一方发送来的TCP报文段的响应。其值是收到的TCP报文段的序号值加1。假设主机A和主机B进行TCP通信，那么A发送出的TCP报文段不仅携带自己的序号，而且包含对B发送来的TCP报文段的确认号。反之，B发送出的TCP报文段也同时携带自己的序号和对A发送来的报文段的确认号。TCP的可靠性，是建立在「每一个数据报文都需要确认收到」的基础之上的。 就是说，通讯的任何一方在收到对方的一个报文之后，都要发送一个相对应的「确认报文」，来表达确认收到。 那么，确认报文，就会包含确认号。 若确认号=N，则表明：到序号N-1为止的所有数据都已正确收到。 数据偏移 Offset占 0.5 个字节 (4 位)。 这个字段实际上是指出了TCP报文段的首部长度 ，它指出了TCP报文段的数据起始处距离TCP报文的起始处有多远。 注意数据起始处和报文起始处的意思，上面👆已经写到，TCP报文段的组成有两部分，TCP报文首部和数据部分，偏移量记录的是报文段开始和数据开始的长度，也就是报文首部的长度； 一个数据偏移量 = 4 byte，由于4位二进制数能表示的最大十进制数字是 15，因此数据偏移的最大值是 60 byte，这也侧面限制了TCP首部的最大长度。 保留Reserved占 0.75 个字节 (6 位)。 保留为今后使用，但目前应置为 0。 标志位 TCP Flags标志位，一共有6个，分别占1位，共6位。 每一位的值只有 0 和 1，分别表达不同意思。 如上图是使用wireshard抓包展示截图； ACK(Acknowlegemt) ：确认序号有效 当 ACK = 1 的时候，确认号（Acknowledgemt Number）有效。 一般称携带 ACK 标志的 TCP 报文段为「确认报文段」。为0表示数据段不包含确认信息，确认号被忽略。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 设置为 1。 RST(Reset)：重置连接 当 RST = 1 的时候，表示 TCP 连接中出现严重错误，需要释放并重新建立连接。 一般称携带 RST 标志的 TCP 报文段为「复位报文段」。 SYN(SYNchronization)：发起了一个新连接 当 SYN = 1 的时候，表明这是一个请求连接报文段。 一般称携带 SYN 标志的 TCP 报文段为「同步报文段」。 在 TCP 三次握手中的第一个报文就是同步报文段，在连接建立时用来同步序号。对方若同意建立连接，则应在响应的报文段中使 SYN = 1 和 ACK = 1。 PSH (Push): 推送 当 PSH = 1 的时候，表示该报文段高优先级，接收方 TCP 应该尽快推送给接收应用程序，而不用等到整个 TCP 缓存都填满了后再交付。 FIN：释放一个连接 当 FIN = 1 时，表示此报文段的发送方的数据已经发送完毕，并要求释放 TCP 连接。一般称携带 FIN 的报文段为「结束报文段」。在 TCP 四次挥手释放连接的时候，就会用到该标志。 窗口大小 Window Size占16位。该字段明确指出了现在允许对方发送的数据量，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 窗口大小的值是指，从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量。 例如，假如确认号是701，窗口字段是 1000。这就表明，从 701 号算起，发送此报文段的一方还有接收 1000 （字节序号是 701 ~ 1700） 个字节的数据的接收缓存空间。 校验和 TCP Checksum占16位。 由发送端填充，接收端对TCP报文段执行【CRC算法】，以检验TCP报文段在传输过程中是否损坏，如果损坏这丢弃。 检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。 紧急指针 Urgent Pointer占 2 个字节。 仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。 当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 因此，紧急指针指出了紧急数据的末尾在报文段中的位置。 选项 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 下面是常用选项： MTU（最大传输单元）MTU（最大传输单元）是【链路层】中的网络对数据帧的一个限制，以以太网为例，MTU 为 1500 个字节。一个IP 数据报在以太网中传输，如果它的长度大于该 MTU 值，就要进行分片传输，使得每片数据报的长度小于MTU。分片传输的 IP 数据报不一定按序到达，但 IP 首部中的信息能让这些数据报片按序组装。IP 数据报的分片与重组是在网络层进完成的。 MSS （最大分段大小）MSS 是 TCP 里的一个概念（首部的选项字段中）。MSS 是 TCP 数据包每次能够传输的最大数据分段，TCP 报文段的长度大于 MSS 时，要进行分段传输。TCP 协议在建立连接的时候通常要协商双方的 MSS 值，每一方都有用于通告它期望接收的 MSS 选项（MSS 选项只出现在 SYN 报文段中，即 TCP 三次握手的前两次）。MSS 的值一般为 MTU 值减去两个首部大小（需要减去 IP 数据包包头的大小 20Bytes 和 TCP 数据段的包头 20Bytes）所以如果用链路层以太网，MSS 的值往往为 1460。而 Internet 上标准的 MTU 为 576，那么如果不设置，则MSS的默认值就为 536 个字节。TCP报文段的分段与重组是在运输层完成的。 seq和ack的计算逻辑 CRC校验参考资料TCP协议中的seq/ack序号是如何变化的？TCP协议详解TCP协议详解（一）：TCP头部结构TCP和UDP报文头格式TCP协议详解吃透TCP协议 传送门 👇 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP协议","date":"2021-07-04T08:30:55.000Z","path":"wiki/TCP协议/","text":"TCP协议学习笔记📒 下面是本人在学习TCP协议的过程中，记录的笔记，按照学习的过程从前到后整理在这里！可能会有很多的知识没有罗列，只是记录的大概框架，如果有问题或错误，欢迎指正！ 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手 学习资料 敖丙Github整理的笔记 有大概10篇左右的文章，都是高质量的，原地址请点击着👉 【Github】 极客时间《Web协议详解与抓包实战》– 陶辉老师 这门课程专门讲解网络协议的，包括Http/Https,TLS协议，TCP协议，IP协议等； 《计算机网络 自顶向下方法》第7版 很多名校计算机网络课程在使用的教材，非常权威！ 面试题https://www.nowcoder.com/index","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"Welcome to GeekIBLi","date":"2021-07-04T07:44:33.000Z","path":"wiki/index/","text":"道阻 且长不错的学习网站推荐掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io字节跳动后端面试题集知乎 Java快速进阶通道To Be Top Javaer - Java工程师成神之路Div-wangJava 全栈知识体系https://github.com/crossoverJie/JCSprouthttps://github.com/ZhongFuCheng3y/3y ‼️算法图文分析Laravel 学院https://gitee.com/veal98/CS-Wikihttps://github.com/Snailclimb/JavaGuidehttps://hadyang.com/interview/docs/basic/algo/kmp/中华石杉-Github 人工智能https://www.captainbed.net/blog-aflyun/ 工具网站ProcessOn示说 技术团队推荐小米信息部技术团队https://xiaomi-info.github.io/有赞技术团队美团技术团队 面试题合集2020年大厂Java面试前复习的正确姿势（800+面试题附答案解析）Java集合面试题（总结最全面的面试题）学妹字节、蘑菇街、阿里、莉莉丝…面经 - 敖丙2021大厂面试真题合集javaguide面试突击https://snailclimb.gitee.io/javaguide-interview/ 友情链接https://blog.csdn.net/weixin_43314519 数据结构与算法 👇 不错的总结 https://programmercarl.com/ 📖labuladong 的算法小抄 两年学说话 一生学闭嘴","tags":[],"categories":[{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}]},{"title":"面试知识点总结","date":"2021-01-27T13:27:07.000Z","path":"wiki/面试知识点总结/","text":"个人整理后端面试体系基础篇1、Java语言基础1.1 Java语言特性（继承 封装 多态 抽象）1.2 基本数据类型以及线程安全问题（double、long）1.3 接口、抽象类、普通类1.4 重要关键字 static 和 final1.5 异常和错误1.6 泛型1.7 异常1.8 线程安全问题1.9 Java中的集合类2、数据结构与算法2.1 了解基本的数据结构2.2 了解常用的常考的算法及其模版2.3 排序2.4 二分法2.53、计算机网络3.1 TCP协议相关（报文头、三次握手，超市重传，滑动窗口、拥塞避免、四次挥手）3.1 Http&amp;Https相关请求方式（GET、POST、PUT、DELETE、HEADER…）常见状态码（2XX、3XX、4XX、5XX…）Http各个版本的差异（1.0 、1.1、2.0）证书颁发过程对称加密和非对称加密SSL/TLS协议 3.3 IP协议3.4 ICMP协议4、数据库MySQL进阶篇1、Redis2、Kafka3、Spring框架4、Java虚拟机 5、设计模式 高级篇1、分布式事务2、分布式缓存3、微服务4、RPC发散篇1、设计一个秒杀系统 2、设计一个抢票系统 （保证高QPS和单库支撑） 用户看到剩余一张票，点击购买，提示没有票了，这个问题怎么解决？ 3、设计一个订单系统，保证数据安全，怎么做 4、海量数据寻找最值问题 4.1 海量数据 （日志）找出访问最多的IP","tags":[{"name":"面试","slug":"面试","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95/"}],"categories":[]},{"title":"2021年后端大厂-Spring","date":"2021-01-09T06:24:05.000Z","path":"wiki/2021年后端大厂-Spring/","text":"spring支持几种bean scope？Spring bean 支持 5 种 scope： Singleton - 每个 Spring IoC 容器仅有一个单实例。 Prototype - 每次请求都会产生一个新的实例。 Request - 每一次 HTTP 请求都会产生一个新的实例，并且该 bean 仅在当前 HTTP 请求内有效。 Session - 每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效。 Global-session - 类似于标准的 HTTP Session 作用域，不过它仅仅在基于portlet 的 web 应用中才有意义。 Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portlet 所共享。在 globalsession 作用域中定义的 bean 被限定于全局 portlet Session 的生命周期范围内。如果你在 web 中使用 global session 作用域来标识 bean，那么 web会自动当成 session 类型来使用。 仅当用户使用支持 Web 的 ApplicationContext 时，最后三个才可用。 Spring bean的生命周期是怎样的spring bean 容器的生命周期流程如下： （1）Spring 容器根据配置中的 bean 定义中实例化 bean。（2）Spring 使用依赖注入填充所有属性，如 bean 中所定义的配置。（3）如果 bean 实现BeanNameAware 接口，则工厂通过传递 bean 的 ID 来调用setBeanName()。（4）如果 bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 setBeanFactory()。（5）如果存在与 bean 关联的任何BeanPostProcessors，则调用preProcessBeforeInitialization() 方法。（6）如果为 bean 指定了 init 方法（ 的 init-method 属性），那么将调 用它。（7）最后，如果存在与 bean 关联的任何 BeanPostProcessors，则将调用 postProcessAfterInitialization() 方法。（8）如果 bean 实现DisposableBean 接口，当 spring 容器关闭时，会调用 destory()。（9）如果为bean 指定了 destroy 方法（ 的 destroy-method 属性），那么将 调用它。 什么是Spring的装配当 bean 在 Spring 容器中组合在一起时，它被称为装配或 bean 装配。Spring容器需要知道需要什么 bean 以及容器应该如何使用依赖注入来将 bean 绑定在一起，同时装配 bean。 自动装配有哪些方式在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。 在Spring框架xml配置中共有5种自动装配： （1）no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。 （2）byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。 （3）byType：通过参数的数据类型进行自动装配。 （4）constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。 （5）autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。 描述一下 DispatcherServlet 的工作流程（1）向服务器发送 HTTP 请求，请求被前端控制器 DispatcherServlet 捕获。 （2） DispatcherServlet 根据 -servlet.xml 中的配置对请求的 URL 进行解析，得到请求资源标识符（URI）。然后根据该 URI，调用 HandlerMapping获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以HandlerExecutionChain 对象的形式返回。 （3） DispatcherServlet 根据获得的 Handler，选择一个合适的HandlerAdapter。（附注：如果成功获得 HandlerAdapter 后，此时将开始执行拦截器的 preHandler(…)方法）。 （4）提取 Request 中的模型数据，填充 Handler 入参，开始执行 Handler（ Controller)。在填充 Handler 的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作： · HttpMessageConveter：将请求消息（如 Json、xml 等数据）转换成一个对象，将对象转换为指定的响应信息。 · 数据转换：对请求消息进行数据转换。如 String 转换成 Integer、Double 等。 · 数据根式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等。 · 数据验证：验证数据的有效性（长度、格式等），验证结果存储到BindingResult 或 Error 中。 （5）Handler(Controller)执行完成后，向 DispatcherServlet 返回一个ModelAndView 对象； （6）根据返回的 ModelAndView，选择一个适合的 ViewResolver（必须是已经注册到 Spring 容器中的 ViewResolver)返回给 DispatcherServlet。 （7） ViewResolver 结合 Model 和 View，来渲染视图。 （8）视图负责将渲染结果返回给客户端。 面试请不要再问我Spring Cloud底层原理https://juejin.cn/post/6844903705553174541 静态代理/动态代理狂神说 https://mp.weixin.qq.com/s/McxiyucxAQYPSOaJSUCCRQ 什么是Spring IOCSpring入门这一篇就够了 https://www.tianmaying.com/tutorial/spring-ioc Spring依赖注入Spring【依赖注入】就是这么简单 3y java后端开发三年！你还不了解Spring 依赖注入，凭什么给你涨薪 对象创建循环依赖问题如何解决循环依赖问题 一级缓存：Map&lt;String, Object&gt; singletonObjects第一级缓存的作用？ 用于存储单例模式下创建的Bean实例（已经创建完毕）。 该缓存是对外使用的，指的就是使用Spring框架的程序员。 存储什么数据？ K：bean的名称 V：bean的实例对象（有代理对象则指的是代理对象，已经创建完毕） 第二级缓存：Map&lt;String, Object&gt; earlySingletonObjects第二级缓存的作用？ 用于存储单例模式下创建的Bean实例（该Bean被提前暴露的引用,该Bean还在创建中）。 该缓存是对内使用的，指的就是Spring框架内部逻辑使用该缓存。 为了解决第一个classA引用最终如何替换为代理对象的问题（如果有代理对象）请爬楼参考演示案例 存储什么数据？ K：bean的名称 V：bean的实例对象（有代理对象则指的是代理对象，该Bean还在创建中） 第三级缓存：Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories第三级缓存的作用？ 通过ObjectFactory对象来存储单例模式下提前暴露的Bean实例的引用（正在创建中）。 该缓存是对内使用的，指的就是Spring框架内部逻辑使用该缓存。 此缓存是解决循环依赖最大的功臣 存储什么数据？ K：bean的名称 V：ObjectFactory，该对象持有提前暴露的bean的引用","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"categories":[]},{"title":"2021年后端大厂面试题合集","date":"2021-01-07T04:40:25.000Z","path":"wiki/2021年后端大厂面试题合集/","text":"记录2021年各互联网大厂面试问题以及答案链接，达到快速复习效果 面试题合集《我们一起进大厂》https://aobing.blog.csdn.net/category_9424379.html 1、Spring面试题1、Spring启动流程 https://aobing.blog.csdn.net/article/details/110383213 https://geekibli.github.io/wiki/Spring梳理启动脉络/ https://blog.csdn.net/hjukyjhg56/article/details/108529552 （面试总结版） 2、Spring的ioc和aop说一说 3、如果让你实现一个ioc，你要怎么做？ https://aobing.blog.csdn.net/article/details/110383213 https://geekibli.github.io/wiki/Spring梳理启动脉络/ 4、aop实现原理，以及jdk动态代理会遇到的问题 ，那cglib就没有什么问题了吗？ 2、Java虚拟机操作堆和栈谁比较快？为什么？ 3、网络面试题什么是加盐 Java基础为什么重写equals要重写Hashcode 10、逻辑推理字节最爱问的智力题，你会几道？（二）","tags":[],"categories":[]},{"title":"2021年后端大厂-计算机网络","date":"2021-01-01T06:17:33.000Z","path":"wiki/2021年后端大厂-计算机网络/","text":"计算机网络 面试合集链接优秀简历模板及计算机网络八股文 面试题目为什么需要TCP协议？TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 IP 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。 因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接收的网络包是无损坏、无间隔、非冗余和按序的。 URI和URL的区别URI(Uniform Resource Identifier)：中文全称为统一资源标志符，主要作用是唯一标识一个资源。 URL(Uniform Resource Location)：中文全称为统一资源定位符，主要作用是提供资源的路径。 DNS的工作流程 domain name system 集群式运行（高可用） 主机向本地域名服务器的查询一般是采用递归查询，而本地域名服务器向根域名的查询一般是采用迭代查询。 详情参见https://www.nowcoder.com/discuss/682094?source_id=profile_create_nctrack&amp;channel=-1 键入网址后，期间发生了什么？三歪问我：键入网址后，期间发生了什么？ 了解ARP协议吗?ARP协议属于网络层的协议，主要作用是实现从IP地址转换为MAC地址。在每个主机或者路由器中都建有一个ARP缓存表，表中有IP地址及IP地址对应的MAC地址。 有了IP地址，为什么还要用MAC地址？简单来说，标识网络中的一台计算机，比较常用的就是IP地址和MAC地址，但计算机的IP地址可由用户自行更改，管理起来相对困难，而MAC地址不可更改，所以一般会把IP地址和MAC地址组合起来使用。 说一下ping的过程ping是ICMP(网际控制报文协议)中的一个重要应用，ICMP是网络层的协议。ping的作用是测试两个主机的连通性。 ping的工作过程： 向目的主机发送多个ICMP回送请求报文 根据目的主机返回的回送报文的时间和成功响应的次数估算出数据包往返时间及丢包率。 路由器和交换机的区别？ 交换机 数据链库层 识别MAC地址并根据MAC地址转发数据帧 路由器 网络层 识别IP地址并根据IP地址转发数据包，维护数据表并基于数据表进行最佳路径选择 TCP和UDP协议的区别TCP建立连接，可靠传输，一对一，流量控制 UDP无需创建连接，尽量交付传输，支持一对多，没有流量控制，首部开销小 TCP协议如何保证可靠传输 主要有校验和、序列号、超时重传、流量控制及拥塞避免等几种方法。 三次握手和四次挥手敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题 为什么不是握手不是两次 三次握手才可以阻止历史重复连接的初始化（主要原因） 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费（避免建立多余的连接） 为什么是四次握手 1、被断开连接一方的ACK（响应主动断开连接的FIN）报文和自己的FIN报文要分开发送，因为中间报处理可能为处理完的数据，所以不能两个报文同时发送或者不能合并成一个报文发送 为什么需要TIME_WAIT 防止具有相同「四元组」的「旧」数据包被收到； 保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭； 这个比较好理解，就是为了确保被断开连接的一方能收到断开连接相应自己FIN报文的ACK报文，上图👆中的最后一个ACK报文。 为什么要等待两个MSL只有主动断开连接的一方在连接断开后要进行TIME_WAIT ， 那TIME_WAIT的时间为什么是2个MSL（Maximum Segment Lifetime 报文最大生存时间）。 TIME_WAIT 过多有什么危害？ 第一是内存资源占用； 第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口； 如何计算TCP数据长度IP报长度-IP首部长度-TCP首部长度 Socket编程https://mp.weixin.qq.com/s/rX3A_FA19n4pI9HicIEsXg 建立一个 socket 连接要经过哪些步骤 服务端和客户端初始化 socket，得到文件描述符； 服务端调用 bind，将绑定在 IP 地址和端口; 服务端调用 listen，进行监听； 服务端调用 accept，等待客户端连接； 客户端调用 connect，向服务器端的地址和端口发起连接请求； 服务端 accept 返回用于传输的 socket 的文件描述符； 客户端调用 write 写入数据；服务端调用 read 读取数据； 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。 这里需要注意的是，服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。 所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作监听 socket，一个叫作已完成连接 socket。 成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。 301 和 302区别301适合永久重定向 301比较常用的场景是使用域名跳转。 比如，我们访问 http://www.baidu.com 会跳转到 https://www.baidu.com，发送请求之后，就会返回301状态码，然后返回一个location，提示新的地址，浏览器就会拿着这个新的地址去访问。 注意： 301请求是可以缓存的， 即通过看status code，可以发现后面写着from cache。 或者你把你的网页的名称从php修改为了html，这个过程中，也会发生永久重定向。 302用来做临时跳转 比如未登陆的用户访问用户中心重定向到登录页面。访问404页面会重新定向到首页。 服务器500，501，502，503，504，505","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"categories":[]},{"title":"2021年后端大厂-MySQL","date":"2021-01-01T03:55:30.000Z","path":"wiki/2021年后端大厂-MySQL/","text":"没有别的目的，就是为了快速复习 Mysql面试合集什么是回表https://blog.csdn.net/xuyw10000/article/details/95971218 https://juejin.cn/post/6844904062329028621 覆盖索引的一个优化办法是将要查询的列添加到索引项中去","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"2021年后端大厂-算法面试合集","date":"2021-01-01T01:43:34.000Z","path":"wiki/2021年后端大厂-算法面试合集/","text":"算法类利用堆求 TopK 问题及其应用案例解决思路https://juejin.cn/post/6963096695333158926 https://juejin.cn/post/6844903831487152135","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"categories":[]}],"categories":[{"name":"draft","slug":"draft","permalink":"http://example.com/categories/draft/"},{"name":"Leetcode","slug":"Leetcode","permalink":"http://example.com/categories/Leetcode/"},{"name":"二分","slug":"Leetcode/二分","permalink":"http://example.com/categories/Leetcode/%E4%BA%8C%E5%88%86/"},{"name":"分布式事务","slug":"分布式事务","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"},{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"},{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"},{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"},{"name":"mybatis","slug":"Spring-Family/mybatis","permalink":"http://example.com/categories/Spring-Family/mybatis/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"},{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"},{"name":"MongoDB","slug":"DataBase/MongoDB","permalink":"http://example.com/categories/DataBase/MongoDB/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"},{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"},{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"},{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"},{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/tags/Elasticsearch/"},{"name":"Leetcode","slug":"Leetcode","permalink":"http://example.com/tags/Leetcode/"},{"name":"二分","slug":"二分","permalink":"http://example.com/tags/%E4%BA%8C%E5%88%86/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"},{"name":"LRU","slug":"LRU","permalink":"http://example.com/tags/LRU/"},{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"},{"name":"网络IO","slug":"网络IO","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9CIO/"},{"name":"epoll","slug":"epoll","permalink":"http://example.com/tags/epoll/"},{"name":"NIO","slug":"NIO","permalink":"http://example.com/tags/NIO/"},{"name":"Netty","slug":"Netty","permalink":"http://example.com/tags/Netty/"},{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://example.com/tags/LeetCode/"},{"name":"中等","slug":"中等","permalink":"http://example.com/tags/%E4%B8%AD%E7%AD%89/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"},{"name":"KMP","slug":"KMP","permalink":"http://example.com/tags/KMP/"},{"name":"高并发","slug":"高并发","permalink":"http://example.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"性能指标","slug":"性能指标","permalink":"http://example.com/tags/%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"},{"name":"服务发现","slug":"服务发现","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"位运算","slug":"位运算","permalink":"http://example.com/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"哈希表","slug":"哈希表","permalink":"http://example.com/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"排序","slug":"排序","permalink":"http://example.com/tags/%E6%8E%92%E5%BA%8F/"},{"name":"贪心","slug":"贪心","permalink":"http://example.com/tags/%E8%B4%AA%E5%BF%83/"},{"name":"动态规划","slug":"动态规划","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"双指针","slug":"双指针","permalink":"http://example.com/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"mybatis","slug":"mybatis","permalink":"http://example.com/tags/mybatis/"},{"name":"mongodb","slug":"mongodb","permalink":"http://example.com/tags/mongodb/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"},{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"面试","slug":"面试","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}]}