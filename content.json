{"pages":[{"title":"Tags","date":"2021-07-04T07:22:56.335Z","path":"tags/index.html","text":""},{"title":"About","date":"2021-07-04T09:22:18.003Z","path":"about/index.html","text":""},{"title":"Categories","date":"2021-07-04T07:22:56.334Z","path":"categories/index.html","text":""}],"posts":[{"title":"段合并","date":"2021-07-08T12:57:54.000Z","path":"wiki/段合并/","text":"参考资料 learnku.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"统计去重数据 (近似度量)","date":"2021-07-08T08:09:05.000Z","path":"wiki/统计去重数据/","text":"cardinality用法常用写法如下👇curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;months&quot; : &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125; &#125; &#125;&#125; 精度问题cardinality 度量是一个 近似算法。 它是基于 HyperLogLog++ （HLL）算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。 我们不需要理解技术细节， 但我们最好应该关注一下这个算法的 特性 ： 可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）。 小的数据集精度是非常高的。 我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 要配置精度，我们必须指定 precision_threshold 参数的值。 这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果。参考以下示例： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; ⚠️ ⚠️precision_threshold 接受 0–40000 之间的数字，更大的值还是会被当作 40000 来处理 示例会确保当字段唯一值在 100 以内时会得到非常准确的结果。尽管算法是无法保证这点的，但如果基数在阈值以下，几乎总是 100% 正确的。高于阈值的基数会开始节省内存而牺牲准确度，同时也会对度量结果带入误差。 对于指定的阈值，HLL 的数据结构会大概使用 precision_threshold * 8 字节的内存，所以就必须在牺牲内存和获得额外的准确度间做平衡。 在实际应用中， 100 的阈值可以在唯一值为百万的情况下仍然将误差维持 5% 以内 速度问题如果想要获得唯一值的数目， 通常 需要查询整个数据集合（或几乎所有数据）。 所有基于所有数据的操作都必须迅速，原因是显然的。 HyperLogLog 的速度已经很快了，它只是简单的对数据做哈希以及一些位操作。 但如果速度对我们至关重要，可以做进一步的优化。 因为 HLL 只需要字段内容的哈希值，我们可以在索引时就预先计算好。 就能在查询时跳过哈希计算然后将哈希值从 fielddata 直接加载出来。 预先计算哈希值只对内容很长或者基数很高的字段有用，计算这些字段的哈希值的消耗在查询时是无法忽略的。 尽管数值字段的哈希计算是非常快速的，存储它们的原始值通常需要同样（或更少）的内存空间。这对低基数的字符串字段同样适用，Elasticsearch 的内部优化能够保证每个唯一值只计算一次哈希。 基本上说，预先计算并不能保证所有的字段都更快，它只对那些具有高基数和/或者内容很长的字符串字段有作用。需要记住的是，预计算只是简单的将查询消耗的时间提前转移到索引时，并非没有任何代价，区别在于你可以选择在 什么时候 做这件事，要么在索引时，要么在查询时。 创建索引时添加如下配置： 1234567891011121314151617PUT /cars/&#123; &quot;mappings&quot;: &#123; &quot;transactions&quot;: &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 多值字段的类型是 murmur3 ，这是一个哈希函数。 现在当我们执行聚合时，我们使用 color.hash 字段而不是 color 字段：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 12345678910&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color.hash&quot; &#125; &#125; &#125;&#125; 现在 cardinality 度量会读取 “color.hash“ 里的值（预先计算的哈希值），取代动态计算原始值的哈希。 单个文档节省的时间是非常少的，但是如果你聚合一亿数据，每个字段多花费 10 纳秒的时间，那么在每次查询时都会额外增加 1 秒，如果我们要在非常大量的数据里面使用 cardinality ，我们可以权衡使用预计算的意义，是否需要提前计算 hash，从而在查询时获得更好的性能，做一些性能测试来检验预计算哈希是否适用于你的应用场景。。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"多桶排序","date":"2021-07-08T07:50:24.000Z","path":"wiki/多桶排序/","text":"多值桶（ terms 、 histogram 和 date_histogram ）动态生成很多桶。 Elasticsearch 是如何决定这些桶展示给用户的顺序呢？ 默认的，桶会根据 doc_count 降序排列。这是一个好的默认行为，因为通常我们想要找到文档中与查询条件相关的最大值：售价、人口数量、频率。但有些时候我们希望能修改这个顺序，不同的桶有着不同的处理方式。 内置排序这些排序模式是桶 固有的 能力：它们操作桶生成的数据 ，比如 doc_count 。 它们共享相同的语法，但是根据使用桶的不同会有些细微差别。 让我们做一个 terms 聚合但是按 doc_count 值的升序排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;_count&quot; : &quot;asc&quot; &#125; &#125; &#125; &#125;&#125;&#x27; 用关键字 _count ，我们可以按 doc_count 值的升序排序。 我们为聚合引入了一个 order 对象， 它允许我们可以根据以下几个值中的一个值进行排序： _count按文档数排序。对 terms 、 histogram 、 date_histogram 有效。 _term按词项的字符串值的字母顺序排序。只在 terms 内使用。 _key按每个桶的键值数值排序（理论上与 _term 类似）。 只在 histogram 和 date_histogram 内使用。 按度量排序有时，我们会想基于度量计算的结果值进行排序。 在我们的汽车销售分析仪表盘中，我们可能想按照汽车颜色创建一个销售条状图表，但按照汽车平均售价的升序进行排序。 我们可以增加一个度量，再指定 order 参数引用这个度量即可： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314151617181920&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125;&#x27; 计算每个桶的平均售价。 桶按照计算平均值的升序排序。 我们可以采用这种方式用任何度量排序，只需简单的引用度量的名字。不过有些度量会输出多个值。 extended_stats 度量是一个很好的例子：它输出好几个度量值。 如果我们想使用多值度量进行排序， 我们只需以关心的度量为关键词使用点式路径：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 使用 . 符号，根据感兴趣的度量进行排序。 深度度量排序在前面的示例中，度量是桶的直接子节点。平均售价是根据每个 term 来计算的。 在一定条件下，我们也有可能对 更深 的度量进行排序，比如孙子桶或从孙桶。 我们可以定义更深的路径，将度量用尖括号（ &gt; ）嵌套起来，像这样： my_bucket&gt;another_bucket&gt;metric 。 需要提醒的是嵌套路径上的每个桶都必须是 单值 的。 filter 桶生成 一个单值桶：所有与过滤条件匹配的文档都在桶中。 多值桶（如：terms ）动态生成许多桶，无法通过指定一个确定路径来识别。 目前，只有三个单值桶： filter 、 global 和 reverse_nested 。让我们快速用示例说明，创建一个汽车售价的直方图，但是按照红色和绿色（不包括蓝色）车各自的方差来排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;histogram&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;interval&quot;: 20000, &quot;order&quot;: &#123; &quot;red_green_cars&gt;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;red_green_cars&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123;&quot;color&quot;: [&quot;red&quot;, &quot;green&quot;]&#125;&#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123;&quot;extended_stats&quot;: &#123;&quot;field&quot; : &quot;price&quot;&#125;&#125; &#125; &#125; &#125; &#125; &#125;&#125;&#x27; 按照嵌套度量的方差对桶的直方图进行排序。 因为我们使用单值过滤器 filter ，我们可以使用嵌套排序。 按照生成的度量对统计结果进行排序。 本例中，可以看到我们如何访问一个嵌套的度量。 stats 度量是 red_green_cars 聚合的子节点，而 red_green_cars 又是 colors 聚合的子节点。 为了根据这个度量排序，我们定义了路径 red_green_cars&gt;stats.variance 。我们可以这么做，因为 filter 桶是个单值桶。","tags":[{"name":"elasticsaerch","slug":"elasticsaerch","permalink":"http://example.com/tags/elasticsaerch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"过滤和聚合","date":"2021-07-08T07:33:26.000Z","path":"wiki/过滤和聚合/","text":"过滤和聚合聚合范围限定还有一个自然的扩展就是过滤。因为聚合是在查询结果范围内操作的，任何可以适用于查询的过滤器也可以应用在聚合上。 过滤如果我们想找到售价在 $10,000 美元之上的所有汽车同时也为这些车计算平均售价， 可以简单地使用一个 constant_score 查询和 filter 约束： GET /cars/transactions/_search 12345678910111213141516171819&#123; &quot;size&quot; : 0, &quot;query&quot; : &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10000 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;single_avg_price&quot;: &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 从根本上讲，使用 non-scoring 查询和使用 match 查询没有任何区别。查询（包括了一个过滤器）返回一组文档的子集，聚合正是操作这些文档。使用 filtering query 会忽略评分，并有可能会缓存结果数据等等。 过滤桶但是如果我们只想对聚合结果过滤怎么办？ 假设我们正在为汽车经销商创建一个搜索页面， 我们希望显示用户搜索的结果，但是我们同时也想在页面上提供更丰富的信息，包括（与搜索匹配的）上个月度汽车的平均售价。 这里我们无法简单的做范围限定，因为有两个不同的条件。搜索结果必须是 ford ，但是聚合结果必须满足 ford AND sold &gt; now - 1M 。 为了解决这个问题，我们可以用一种特殊的桶，叫做 filter （注：过滤桶） 。 我们可以指定一个过滤桶，当文档满足过滤桶的条件时，我们将其加入到桶内。 查询结果如下：GET /cars/transactions/_search 1234567891011121314151617181920212223242526&#123; &quot;size&quot; : 0, &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;recent_sales&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold&quot;: &#123; &quot;from&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_price&quot;:&#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 使用 过滤 桶在 查询 范围基础上应用过滤器。 avg 度量只会对 ford 和上个月售出的文档计算平均售价。 因为 filter 桶和其他桶的操作方式一样，所以可以随意将其他桶和度量嵌入其中。所有嵌套的组件都会 “继承” 这个过滤，这使我们可以按需针对聚合过滤出选择部分。 后过滤器目前为止，我们可以同时对搜索结果和聚合结果进行过滤（不计算得分的 filter 查询），以及针对聚合结果的一部分进行过滤（ filter 桶）。 我们可能会想，”只过滤搜索结果，不过滤聚合结果呢？” 答案是使用 post_filter 。 它是接收一个过滤器的顶层搜索请求元素。这个过滤器在查询 之后 执行（这正是该过滤器的名字的由来：它在查询之后 post 执行）。正因为它在查询之后执行，它对查询范围没有任何影响，所以对聚合也不会有任何影响。 我们可以利用这个行为对查询条件应用更多的过滤器，而不会影响其他的操作，就如 UI 上的各个分类面。让我们为汽车经销商设计另外一个搜索页面，这个页面允许用户搜索汽车同时可以根据颜色来过滤。颜色的选项是通过聚合获得的： GET /cars/transactions/_search 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;post_filter&quot;: &#123; &quot;term&quot; : &#123; &quot;color&quot; : &quot;green&quot; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;all_colors&quot;: &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; post_filter 元素是 top-level 而且仅对命中结果进行过滤。 查询 部分找到所有的 ford 汽车，然后用 terms 聚合创建一个颜色列表。因为聚合对查询范围进行操作，颜色列表与福特汽车有的颜色相对应。 最后， post_filter 会过滤搜索结果，只展示绿色 ford 汽车。这在查询执行过 后 发生，所以聚合不受影响。 这通常对 UI 的连贯一致性很重要，可以想象用户在界面商选择了一类颜色（比如：绿色），期望的是搜索结果已经被过滤了，而 不是 过滤界面上的选项。如果我们应用 filter 查询，界面会马上变成 只 显示 绿色 作为选项，这不是用户想要的！ ⚠️ ⚠️ ⚠️ 性能考虑（Performance consideration）当你需要对搜索结果和聚合结果做不同的过滤时，你才应该使用 post_filter ， 有时用户会在普通搜索使用 post_filter 。 不要这么做！ post_filter 的特性是在查询 之后 执行，任何过滤对性能带来的好处（比如缓存）都会完全失去。 在我们需要不同过滤时， post_filter 只与聚合一起使用。 总结选择合适类型的过滤（如：搜索命中、聚合或两者兼有）通常和我们期望如何表现用户交互有关。选择合适的过滤器（或组合）取决于我们期望如何将结果呈现给用户。 在 filter 过滤中的 non-scoring 查询，同时影响搜索结果和聚合结果。 filter 桶影响聚合。 post_filter 只影响搜索结果。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 条形图","date":"2021-07-08T07:11:57.000Z","path":"wiki/聚合-条形图/","text":"参考资料","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"嵌套桶","date":"2021-07-08T07:01:16.000Z","path":"wiki/嵌套桶/","text":"两层嵌套在我们使用不同的嵌套方案时，聚合的力量才能真正得以显现。 在前例中，我们已经看到如何将一个度量嵌入桶中，它的功能已经十分强大了。 但真正令人激动的分析来自于将桶嵌套进 另外一个桶 所能得到的结果。 现在，我们想知道每个颜色的汽车制造商的分布： GET /cars/transactions/_search 12345678910111213141516171819202122&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;make&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注意前例中的 avg_price 度量仍然保持原位。 另一个聚合 make 被加入到了 color 颜色桶中。 这个聚合是 terms 桶，它会为每个汽车制造商生成唯一的桶。 这里发生了一些有趣的事。 首先，我们可能会观察到之前例子中的 avg_price 度量完全没有变化，还在原来的位置。 一个聚合的每个 层级 都可以有多个度量或桶， avg_price 度量告诉我们每种颜色汽车的平均价格。它与其他的桶和度量相互独立。 这对我们的应用非常重要，因为这里面有很多相互关联，但又完全不同的度量需要收集。聚合使我们能够用一次数据请求获得所有的这些信息。 另外一件值得注意的重要事情是我们新增的这个 make 聚合，它是一个 terms 桶（嵌套在 colors 、 terms 桶内）。这意味着它会为数据集中的每个唯一组合生成（ color 、 make ）元组。 让我们看看返回的响应（为了简单我们只显示部分结果）： 1234567891011121314151617181920212223242526&#123; &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125; &#125; &#125;&#125; 正如期望的那样，新的聚合嵌入在每个颜色桶中。 现在我们看见按不同制造商分解的每种颜色下车辆信息。 最终，我们看到前例中的 avg_price 度量仍然维持不变。 三层嵌套让我们回到话题的原点，在进入新话题之前，对我们的示例做最后一个修改， 为每个汽车生成商计算最低和最高的价格：GET /cars/transactions/_search 1234567891011121314151617181920212223&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;make&quot; &#125;, &quot;aggs&quot; : &#123; &quot;min_price&quot; : &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125;, &quot;max_price&quot; : &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 我们需要增加另外一个嵌套的 aggs 层级。 然后包括 min 最小度量。 以及 max 最大度量。 得到以下输出（只显示部分结果）： 12345678910111213141516171819202122232425262728293031323334353637&#123;... &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3, &quot;min_price&quot;: &#123; &quot;value&quot;: 10000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 20000 &#125; &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1, &quot;min_price&quot;: &#123; &quot;value&quot;: 80000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 80000 &#125; &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125;,... 有了这两个桶，我们可以对查询的结果进行扩展并得到以下信息： 有四辆红色车。红色车的平均售价是 $32，500 美元。其中三辆红色车是 Honda 本田制造，一辆是 BMW 宝马制造。最便宜的红色本田售价为 $10，000 美元。最贵的红色本田售价为 $20，000 美元。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 高级概念","date":"2021-07-08T06:52:35.000Z","path":"wiki/聚合-高级概念/","text":"桶桶 简单来说就是满足特定条件的文档的集合： 一个雇员属于 男性 桶或者 女性 桶 奥尔巴尼属于 纽约 桶 日期2014-10-28属于 十月 桶当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件。如果匹配到，文档将放入相应的桶并接着进行聚合操作。 桶也可以被嵌套在其他桶里面，提供层次化的或者有条件的划分方案。例如，辛辛那提会被放入俄亥俄州这个桶，而 整个 俄亥俄州桶会被放入美国这个桶。 Elasticsearch 有很多种类型的桶，能让你通过很多种方式来划分文档（时间、最受欢迎的词、年龄区间、地理位置 等等）。其实根本上都是通过同样的原理进行操作：基于条件来划分文档。 指标桶能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些指标的计算。分桶是一种达到目的的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。 大多数 指标 是简单的数学运算（例如最小值、平均值、最大值，还有汇总），这些是通过文档的值来计算。在实践中，指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据。 桶和指标的组合聚合 是由桶和指标组成的。 聚合可能只有一个桶，可能只有一个指标，或者可能两个都有。也有可能有一些桶嵌套在其他桶里面。例如，我们可以通过所属国家来划分文档（桶），然后计算每个国家的平均薪酬（指标）。 由于桶可以被嵌套，我们可以实现非常多并且非常复杂的聚合： 1.通过国家划分文档（桶） 2.然后通过性别划分每个国家（桶） 3.然后通过年龄区间划分每种性别（桶） 4.最后，为每个年龄区间计算平均薪酬（指标） 最后将告诉你每个 &lt;国家, 性别, 年龄&gt; 组合的平均薪酬。所有的这些都在一个请求内完成并且只遍历一次数据！","tags":[{"name":"elasticsaerch","slug":"elasticsaerch","permalink":"http://example.com/tags/elasticsaerch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"elasticsearch-overview","date":"2021-07-08T03:17:58.000Z","path":"wiki/elasticsearch-overview/","text":"学习资料 https://www.codingdict.com/ https://elasticsearch.cn/ https://www.elastic.co/guide/en/ 铭毅天下","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"关于 Elasticsearch 内存占用及分配","date":"2021-07-08T02:39:16.000Z","path":"wiki/关于-Elasticsearch-内存占用及分配/","text":"Elasticsearch 和 Lucene 对内存使用情况： Elasticsearch 限制的内存大小是 JAVA 堆空间的大小，不包括Lucene 缓存倒排索引数据空间。 Lucene 中的 倒排索引 segments 存储在文件中，为提高访问速度，都会把它加载到内存中，从而提高 Lucene 性能。所以建议至少留系统一半内存给Lucene。Node Query Cache (负责缓存f ilter 查询结果)，每个节点有一个，被所有 shard 共享，filter query查询结果要么是 yes 要么是no，不涉及 scores 的计算。集群中每个节点都要配置，默认为：indices.queries.cache.size:10% Indexing Buffer 索引缓冲区，用于存储新索引的文档，当其被填满时，缓冲区中的文档被写入磁盘中的 segments 中。节点上所有 shard 共享。缓冲区默认大小： indices.memory.index_buffer_size: 10%如果缓冲区大小设置了百分百则 indices.memory.min_index_buffer_size 用于这是最小值，默认为 48mb。indices.memory.max_index_buffer_size 用于最大大小，无默认值。 segmentssegments会长期占用内存，其初衷就是利用OS的cache提升性能。只有在Merge之后，才会释放掉标记为Delete的segments，释放部分内存。 Shard Request Cache 用于缓存请求结果，但之缓存request size为0的。比如说 hits.total, aggregations 和 suggestions.默认最大为indices.requests.cache.size:1% Field Data Cache 字段缓存重要用于对字段进行排序、聚合是使用。因为构建字段数据缓存代价昂贵，所以建议有足够的内训来存储。Fielddata 是 「 延迟 」 加载。如果你从来没有聚合一个分析字符串，就不会加载 fielddata 到内存中，也就不会使用大量的内存，所以可以考虑分配较小的heap给Elasticsearch。因为heap越小意味着Elasticsearch的GC会比较快，并且预留给Lucene的内存也会比较大。。如果没有足够的内存保存fielddata时，Elastisearch会不断地从磁盘加载数据到内存，并剔除掉旧的内存数据。剔除操作会造成严重的磁盘I/O，并且引发大量的GC，会严重影响Elastisearch的性能。 默认情况下Fielddata会不断占用内存，直到它触发了fielddata circuit breaker。fielddata circuit breaker会根据查询条件评估这次查询会使用多少内存，从而计算加载这部分内存之后，Field Data Cache所占用的内存是否会超过indices.breaker.fielddata.limit。如果超过这个值，就会触发fielddata circuit breaker，abort这次查询并且抛出异常，防止OOM。 1indices.breaker.fielddata.limit:60% (默认heap的60%) (es7之后改成70%) 如果设置了indices.fielddata.cache.size，当达到size时，cache会剔除旧的fielddata。 indices.breaker.fielddata.limit 必须大于 indices.fielddata.cache.size，否则只会触发fielddata circuit breaker，而不会剔除旧的fielddata。 配置Elasticsearch堆内存Elasticsearch默认安装后设置的内存是 1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10g 注意点给es分配内存时要注意，至少要分配一半儿内存留给 Lucene。分配给 es 的内存最好不要超过 32G ，因为如果堆大小小于 32 GB，JVM 可以利用指针压缩，这可以大大降低内存的使用：每个指针 4 字节而不是 8 字节。如果大于32G 每个指针占用 8字节，并且会占用更多的内存带宽，降低了cpu性能。 还有一点， 要关闭 swap 内存交换空间，禁用swapping。频繁的swapping 对服务器来说是致命的。总结：给es JVM栈的内存最好不要超过32G，留给Lucene的内存越大越好，Lucene把所有的segment都缓存起来，会加快全文检索。 关闭交换区这应该显而易见了，但仍然需要明确的写出来：把内存换成硬盘将毁掉服务器的性能，想象一下：涉及内存的操作是需要快速执行的。如果介质从内存变为了硬盘，一个10微秒的操作变成需要10毫秒。而且这种延迟发生在所有本该只花费10微秒的操作上，就不难理解为什么交换区对于性能来说是噩梦。 最好的选择是禁用掉操作系统的交换区。可以用以下命令： 1sudo swapoff -a 来禁用，你可能还需要编辑 /etc/fstab 文件。细节可以参考你的操作系统文档。 如果实际环境不允许禁用掉 swap，你可以尝试降低 swappiness。此值控制操作系统使用交换区的积极性。这可以防止在正常情况下使用交换区，但仍允许操作系统在紧急情况下将内存里的东西放到交换区。 对于大多数Linux系统来说，这可以用 sysctl 值来配置： 1vm.swappiness = 1 # 将此值配置为1会比0好，在kernal内核的某些版本中，0可能会引起OOM异常。 最后，如果两种方法都不可用，你应该在ElasticSearch的配置中启用 mlockall.file。这允许JVM锁定其使用的内存，而避免被放入操作系统交换区。 在elasticsearch.yml中，做如下设置： 1bootstrap.mlockall: true 查看node节点数据GET /_cat/nodes?v&amp;h=id,ip,port,v,master,name,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count GET /_cat/nodes?v&amp;h=id,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size GET /_cat/nodes?v&amp;h=id,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count 参考文章","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"cross-fields跨字段查询","date":"2021-07-07T06:41:42.000Z","path":"wiki/cross-fields跨字段查询/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » cross-fields 跨字段查询","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"copy_to参数","date":"2021-07-07T06:34:42.000Z","path":"wiki/copy-to参数/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 自定义 _all 字段 Docs » Mapping parameters（映射参数） » Mapping(映射) » copy_to（合并参数）","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"字符串排序与多字段","date":"2021-07-07T03:05:31.000Z","path":"wiki/字符串排序与多字段/","text":"被解析的字符串字段也是多值字段， 但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如 fine old art ， 这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Elasticsearch 在排序过程中没有这样的信息。 你可以使用 min 和 max 排序模式（默认是 min ），但是这会导致排序以 art 或是 old ，任何一个都不是所希望的。 为了以字符串字段进行排序，这个字段应仅包含一项： 整个 not_analyzed 字符串。 但是我们仍需要 analyzed 字段，这样才能以全文进行查询 一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段： analyzed 用于搜索， not_analyzed 用于排序 但是保存相同的字符串两次在 _source 字段是浪费空间的。 我们真正想要做的是传递一个 单字段 但是却用两种方式索引它。所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个 fields 参数 该参数允许你转化一个简单的映射如： 1234&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;&#125; 为一个多字段映射如： 12345678910&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; tweet 主字段与之前的一样: 是一个 analyzed 全文字段。 新的 tweet.raw 子字段是 not_analyzed. 现在，至少只要我们重新索引了我们的数据，使用 tweet 字段用于搜索，tweet.raw 字段用于排序：curl -X GET &quot;localhost:9200/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;, &quot;sort&quot;: &quot;tweet.raw&quot;&#125;","tags":[],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"推荐系统-Overview","date":"2021-07-07T02:08:05.000Z","path":"wiki/推荐系统-Overview/","text":"博客资料 深度解析京东个性化推荐系统演进史 用 Mahout 和 Elasticsearch 实现推荐系统 美团推荐算法实践 58同城推荐系统设计与实现 微博推荐系统的架构演进之路 Flink 在小红书推荐系统中的应用 小红书大数据在推荐系统中的应用 快看漫画个性化推荐探索与实践 数据仓库系列篇——唯品会大数据架构 推荐系统基本概念和架构 PAI平台搭建企业级个性化推荐系统 - Aliyun 参考资料","tags":[{"name":"推荐","slug":"推荐","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90/"}],"categories":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"}]},{"title":"flink 提交任务","date":"2021-07-06T15:57:04.000Z","path":"wiki/flink-提交任务/","text":"下面演示如何通过admin页面提交任务 👇 准备task jar1234567891011121314151617181920public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 执行mvn install -DskipTest 可以得到相应的jar admin提交jar 提交完jar包之后，需要设置相关参数，这个根据自己的实际情况来设置，下面是参考样例： Enter Class : com.ibli.flink.StreamWordCount也就是程序入口，我们这是写了一个main方法，如果是程序的话，可以写对应bootstrap的启动类 Program Arguments : –host localhost –port 7777 点击 submit 之后查看提交的任务状态 查看任务 可以看到是有两个任务，并且都是在执行状态；点击一个任务，还可以查看任务详情信息，和一些其他的信息，非常全面； 查看运行时任务列表 查看任务管理列表 点击任务可以跳转到详情页面 👇 下面是执行日志 我们还可以看到任务执行的标准输出结果✅ 任务源数据通过nc 输入数据，由程序读取7777端口输入流并解析数据 123gaolei:geekibli gaolei$ nc -lk 7777hello javahello flink 取消任务如下 再次查看已完成任务列表 如下：","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"apache-flink-overview","date":"2021-07-06T15:39:54.000Z","path":"wiki/apache-flink-overview/","text":"学习初衷推荐系统数据需要实时处理，使用Apache Flink实时计算用户数据，分析用户行为，达到实时业务数据分析和实现业务相关推荐； 学习资料 ashiamd.github.io 尚硅谷2021最新Java版Flink 武老师清华硕士，原IBM-CDL负责人 Apache Flink® — Stateful Computations over Data Streams Apache Flink® - 数据流上的有状态计算","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"most_fields类型","date":"2021-07-06T12:54:23.000Z","path":"wiki/most-fields类型/","text":"多字段映射首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍： DELETE /my_index 1234567891011121314151617181920PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; title 字段使用 english 英语分析器来提取词干。 title.std 字段使用 standard 标准分析器，所以没有词干提取。 接着索引一些文档： 12345PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; 这里用一个简单 match 查询 title 标题字段是否包含 jumping rabbits （跳跃的兔子）： 12345678GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;jumping rabbits&quot; &#125; &#125;&#125; 因为有了 english 分析器，这个查询是在查找以 jump 和 rabbit 这两个被提取词的文档。两个文档的 title 字段都同时包括这两个词，所以两个文档得到的评分也相同： 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125; ]&#125; 如果只是查询 title.std 字段，那么只有文档 2 是匹配的。尽管如此，如果同时查询两个字段，然后使用 bool 查询将评分结果 合并 ，那么两个文档都是匹配的（ title 字段的作用），而且文档 2 的相关度评分更高（ title.std 字段的作用）： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; 我们希望将所有匹配字段的评分合并起来，所以使用 most_fields 类型。这让 multi_match 查询用 bool 查询将两个字段语句包在里面，而不是使用 dis_max (最佳字段) 查询。 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8226396, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.10741998, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125; ]&#125; 文档 2 现在的评分要比文档 1 高。 用广度匹配字段 title 包括尽可能多的文档——以提升召回率——同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。 每个字段对于最终评分的贡献可以通过自定义值 boost 来控制。比如，使 title 字段更为重要，这样同时也降低了其他信号字段的作用： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title^10&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; title 字段的 boost 的值为 10 使它比 title.std 更重要。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 多数字段","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"multi_match 查询","date":"2021-07-06T12:37:26.000Z","path":"wiki/multi-match-查询/","text":"multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。 📒 📒 📒 multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即：best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。 默认情况下，查询的类型是 best_fields ，这表示它会为每个字段生成一个 match 查询，然后将它们组合到 dis_max 查询的内部，如下： 1234567891011121314151617181920212223&#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, ], &quot;tie_breaker&quot;: 0.3 &#125;&#125; 上面这个查询用 multi_match 重写成更简洁的形式： 123456789&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;body&quot; ], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125;&#125; ⚠️ ⚠️ ⚠️ best_fields 类型是默认值，可以不指定。 如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。 查询字段名称的模糊匹配字段名称可以用 模糊匹配 的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中，例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: &quot;*_title&quot; &#125;&#125; 提升单个字段的权重可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: [ &quot;*_title&quot;, &quot;chapter_title^2&quot; ] &#125;&#125; chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » multi_match 查询 Elasticsearch Guide [7.x] » Query DSL » Full text queries » Multi-match query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"dis_max查询","date":"2021-07-06T12:09:23.000Z","path":"wiki/dis-max查询/","text":"假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例： 1234567891011PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;&#125; 用户输入词组 Brown fox 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词： 现在运行以下 bool 查询： 12345678910&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 但是我们发现查询的结果是文档 1 的评分更高： 为了理解导致这样的原因，需要回想一下 bool 是如何计算评分的： 它会执行 should 语句中的两个查询。加和两个查询的评分。乘以匹配语句的总数。除以所有语句总数（这里为：2）。 文档 1 的两个字段都包含 brown 这个词，所以两个 match 语句都能成功匹配并且有一个评分。文档 2 的 body 字段同时包含 brown 和 fox 这两个词，但 title 字段没有包含任何词。这样， body 查询结果中的高分，加上 title 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。 在本例中， title 和 body 字段是相互竞争的关系，所以就需要找到单个 最佳匹配 的字段。 如果不是简单将每个字段的评分结果加在一起，而是将 最佳匹配 字段的评分作为查询的整体评分，结果会怎样？这样返回的结果可能是： 同时 包含 brown 和 fox 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。 dis_max 查询不使用 bool 查询，可以使用 dis_max 即分离 最大化查询 （Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ： 12345678910&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 得到我们想要的结果为： Top-level parameters for dis_maxedit queries(Required, array of query objects) Contains one or more query clauses. Returned documents must match one or more of these queries. If a document matches multiple queries, Elasticsearch uses the highest relevance score. tie_breaker(Optional, float) Floating point number between 0 and 1.0 used to increase the relevance scores of documents matching multiple query clauses. Defaults to 0.0. You can use the tie_breaker value to assign higher relevance scores to documents that contain the same term in multiple fields than documents that contain this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields. If a document matches multiple clauses, the dis_max query calculates the relevance score for the document as follows: Take the relevance score from a matching clause with the highest score.Multiply the score from any other matching clauses by the tie_breaker value.Add the highest score to the multiplied scores.If the tie_breaker value is greater than 0.0, all matching clauses count, but the clause with the highest score counts most. dis_max，只是取分数最高的那个query的分数而已，完全不考虑其他query的分数，这种一刀切的做法，可能导致在有其他query的影响下，score不准确的情况，这时为了使用结果更准确，最好还是要考虑到其他query的影响;使用 tie_breaker 将其他query的分数也考虑进去, tie_breaker 参数的意义，将其他query的分数乘以tie_breaker，然后综合考虑后与最高分数的那个query的分数综合在一起进行计算，这样做除了取最高分以外，还会考虑其他的query的分数。tie_breaker的值，设置在在0~1之间，是个小数就行，没有固定的值 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 最佳字段 Elasticsearch中文文档 Elasticsearch Guide [7.x] » Query DSL » Compound queries » Disjunction max query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch被破坏的相似度","date":"2021-07-06T11:50:11.000Z","path":"wiki/elasticsearch被破坏的相似度/","text":"在讨论更复杂的 多字段搜索 之前，让我们先快速解释一下为什么只在主分片上 创建测试索引 。 用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤：用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。 为了理解为什么会这样，可以设想，我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 foo 。可能是分片 1 有其中 3 个 foo 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。 在 什么是相关度？中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 词频/逆向文档频率 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 逆向文档频率 将 某个词在索引内所有文档出现的百分数 考虑在内，出现的频率越高，它的权重就越低。 但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。 因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 foo 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， foo 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。 在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。 为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 match 查询 里介绍的那样，如果只有一个分片，那么本地的 IDF 就是 全局的 IDF。 第二个方式就是在搜索请求后添加 ?search_type=dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。 不要在生产环境上使用 dfs_query_then_fetch 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。 参考资料 Elasticsearch: 权威指南 » 基础入门 » 排序与相关性 » 什么是相关性? Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 被破坏的相关度！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"canal同步es后部分字段为null","date":"2021-07-06T08:11:15.000Z","path":"wiki/canal同步es后部分字段为null/","text":"现象 配置文件如下： 123456789101112131415dataSourceKey: defaultDS # 源数据源的key, 对应上面配置的srcDataSources中的值destination: example # cannal的instance或者MQ的topicgroupId: g1 # 对应MQ模式下的groupId, 只会同步对应groupId的数据esMapping: _index: rd_member_fans_info # es 的索引名称 _type: _doc # es 的doc名称 _id: _id # es 的_id, 如果不配置该项必须配置下面的pk项_id则会由es自动分配# pk: id # 如果不需要_id, 则需要指定一个属性为主键属性# # sql映射 sql: &#x27;SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.create_time,t.update_time,t.`status` FROM rd_member_fans_info t&#x27;# objFields:# _labels: array:; # 数组或者对象属性, array:; 代表以;字段里面是以;分隔的# _obj: object # json对象 etlCondition: &quot;where t.update_time&gt;=&#123;&#125;&quot; # etl 的条件参数 commitBatch: 3000 # 提交批大小 ⚠️ ⚠️sql执行是没有问题的！ canal-adapter 获取binlog数据也没有问题，显示日志如下： 12021-07-06 15:39:24.588 [pool-1-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:3,&quot;redtom_id&quot;:1,&quot;fans_redtom_id&quot;:1,&quot;fans_username&quot;:&quot;1&quot;,&quot;fans_introduce&quot;:&quot;1&quot;,&quot;fans_avatar&quot;:&quot;1&quot;,&quot;is_each_following&quot;:1,&quot;follow_channel&quot;:1,&quot;create_time&quot;:1625556851000,&quot;update_time&quot;:1625556851000,&quot;status&quot;:2&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625557164000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;rd_member_fans_info&quot;,&quot;ts&quot;:1625557164587,&quot;type&quot;:&quot;INSERT&quot;&#125; 然后看一下我创建索引的mapping 解决方法调整sql如下： SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.status as is_deleted , t.create_time,t.update_time FROM rd_member_fans_info t 调整了那些东西呢？ status 的顺序提前而已！ 测试执行一下命令：curl http://127.0.0.1:8081/etl/es7/rd_member_fans_info.yml -X POST canal-adapter 日志如下： 122021-07-06 16:21:33.519 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member_fans_info2021-07-06 16:21:33.527 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - 数据全量导入完成, 一共导入 3 条数据, 耗时: 7 查看es数据：","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch-analyzer","date":"2021-07-06T07:02:01.000Z","path":"wiki/elasticsearch-analyzer/","text":"测试常见分析器GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 12345&#123; &quot;analyzer&quot;: &quot;english&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;stop&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; 默认分析器虽然我们可以在字段层级指定分析器，但是如果该层级没有指定任何的分析器，那么我们如何能确定这个字段使用的是哪个分析器呢？ 分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下： 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 在搜索时，顺序有些许不同： 查询自己定义的 analyzer ，否则 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 有时，在索引时和搜索时使用不同的分析器是合理的。我们可能要想为同义词建索引（例如，所有 quick 出现的地方，同时也为 fast 、 rapid 和 speedy 创建索引）。但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 quick 、 fast 、 rapid 或 speedy 。 为了区分，Elasticsearch 也支持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。 如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样： 查询自己定义的 analyzer ，否则字段映射里定义的 search_analyzer ，否则字段映射里定义的 analyzer ，否则索引设置中名为 default_search 的分析器，默认为索引设置中名为 default 的分析器，默认为standard 标准分析器 保持简单多数情况下，会提前知道文档会包括哪些字段。最简单的途径就是在创建索引或者增加类型映射时，为每个全文字段设置分析器。这种方式尽管有点麻烦，但是它让我们可以清楚的看到每个字段每个分析器是如何设置的。 通常，多数字符串字段都是 not_analyzed 精确值字段，比如标签（tag）或枚举（enum），而且更多的全文字段会使用默认的 standard 分析器或 english 或其他某种语言的分析器。这样只需要为少数一两个字段指定自定义分析：或许标题 title 字段需要以支持 输入即查找（find-as-you-type） 的方式进行索引。 可以在索引级别设置中，为绝大部分的字段设置你想指定的 default 默认分析器。然后在字段级别设置中，对某一两个字段配置需要指定的分析器。 📒 📒 📒对于和时间相关的日志数据，通常的做法是每天自行创建索引，由于这种方式不是从头创建的索引，仍然可以用 索引模板（Index Template） 为新建的索引指定配置和映射。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 控制分析","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"Parameter index out of range (1 > number of parameters, which is 0).","date":"2021-07-06T05:03:16.000Z","path":"wiki/Parameter-index-out-of-range-1-number-of-parameters-which-is-0/","text":"问题记录123456789101112132021-07-06 12:39:31.179 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:39:31.186 [http-nio-8081-exec-2] ERROR com.alibaba.otter.canal.client.adapter.support.Util - sqlRs has error, sql: SELECT COUNT(1) FROM ( select t.redtom_id as id, t.username, t.nickname, t.avatar, t.status, t.mobile, t.mobile_region_no, t.email, t.gender, t.password,t.salt,t.birthday,t.introduce,t.country,t.region,t.level,t.is_vip,t.follows ,t.fans,t.likes_num, t.collects_num, t.instagram_account, t.youtube_account, t.facebook_account, t.twitter_account,t.create_ip, t.create_time,t.update_time from rd_member r where t.create_time&gt;=&#x27;&#123;0&#125;&#x27;) _CNT2021-07-06 12:39:31.188 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0).java.lang.RuntimeException: java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0). at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:65) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:62) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] 如何解决我执行的操作如下：👇curl http://127.0.0.1:8081/etl/es7/customer.yml -X POST -d &quot;params=2019-08-31 00:00:00&quot; 但是我的 es7/rd_member.yml的配置文件如下： etlCondition:&quot;where a.c_time&gt;=&#39;&#123;0&#125;&#39;&quot; # etl 的条件参数 应该改成：etlCondition:&quot;where a.c_time&gt;=&#123;&#125;&quot; # etl 的条件参数","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"field name is null or empty","date":"2021-07-06T04:53:44.000Z","path":"wiki/field-name-is-null-or-empty/","text":"canal adapter 报错信息123456789101112131415161718192021222021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO o.a.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring FrameworkServlet &#x27;dispatcherServlet&#x27;2021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization started2021-07-06 12:46:31.968 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization completed in 9 ms2021-07-06 12:46:31.995 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:46:32.027 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - field name is null or emptyjava.lang.IllegalArgumentException: field name is null or empty at org.elasticsearch.index.query.BaseTermQueryBuilder.&lt;init&gt;(BaseTermQueryBuilder.java:113) ~[na:na] at org.elasticsearch.index.query.TermQueryBuilder.&lt;init&gt;(TermQueryBuilder.java:75) ~[na:na] at org.elasticsearch.index.query.QueryBuilders.termQuery(QueryBuilders.java:202) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.lambda$executeSqlImport$1(ESEtlService.java:141) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:60) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.executeSqlImport(ESEtlService.java:64) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:105) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) ~[na:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke 问题排查操作是向数据库中插入一条数据，通过canal-adapter同步到elasticsearch中，接口发生以上错误！现象是canal-adapter检测到和mysql的数据变化，但是同步到es的时候发生了错误；猜想大概是某个为空导致存到es的时候发生异常； 然后查看es7下的mapping配置： 发现我的sql查id的时候写错了，别名应该写成_id,对应elasticsearch的_id","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"elasticsearch映射","date":"2021-07-05T14:54:05.000Z","path":"wiki/elasticsearch映射/","text":"Elasticsearch 支持如下简单域类型： 字符串: string （es7之后编程text） 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 查看索引的mappingGET /gb/_mapping/tweet 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 自定义域映射尽管在很多情况下基本域数据类型已经够用，但你经常需要为单独域自定义映射，特别是字符串域。自定义映射允许你执行下面的操作： 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： 12345&#123; &quot;number_of_clicks&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;&#125; 默认， string (text) 类型域会被认为包含全文。就是说，它们的值在索引前，会通过一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。 string 域映射的两个最重要属性是 index 和 analyzer 。 indexindex 属性控制怎样索引字符串。它可以是下面三个值： analyzed首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。 no不索引这个域。这个域不会被搜索到。 (比如一些隐私信息) string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ： ⚠️ ⚠️其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。 analyzer对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english; 123456&#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 更新映射当你首次创建一个索引的时候，可以指定类型的映射。你也可以使用 /_mapping 为新类型（或者为存在的类型更新映射）增加映射。⚠️ ⚠️尽管你可以 增加 一个存在的映射，你不能 修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。 我们可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed 。 为了描述指定映射的两种方式，我们先删除 gd 索引：DELETE /gb然后创建一个新索引，指定 tweet 域使用 english 分析器： 12345678910111213141516171819202122PUT /gb &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 稍后，我们决定在 tweet 映射增加一个新的名为 tag 的 not_analyzed 的文本域，使用 _mapping ： 123456789PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中 测试映射你可以使用 analyze API 测试字符串域的映射。比较下面两个请求的输出： 1234567891011GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125; tweet 域产生两个词条 black 和 cat ， tag 域产生单独的词条 Black-cats 。换句话说，我们的映射正常工作。 参考资料 Elasticsearch权威指南","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"分析与分析器","date":"2021-07-05T14:43:44.000Z","path":"wiki/分析与分析器/","text":"分析包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 ，之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。 内置分析器但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：&quot;Set the shape to semi-transparent by calling set_trans(5)&quot; 标准分析器标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。 什么时候使用分析器当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。 全文查询，理解每个域是如何定义的，因此它们可以做正确的事： 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。 测试分析器有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本： 12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条： 12345678910111213141516171819202122232425&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。 每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于​keep_types token 过滤器​。 analyze API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。 你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域—​不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。 要做到这一点，我们必须手动指定这些域的映射。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"倒排索引","date":"2021-07-05T14:17:00.000Z","path":"wiki/倒排索引/","text":"Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容： The quick brown fox jumped over the lazy dogQuick brown foxes leap over lazy dogs in summer为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。 如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如： Quick 可以小写化为 quick 。foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的， dogs 可以为提取为 dog 。jumped 和 leap 是同义词，可以索引为相同的单词 jump 。现在索引看上去像这样：这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！ 这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch分页查询","date":"2021-07-05T14:08:23.000Z","path":"wiki/elasticsearch分页查询/","text":"和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size显示应该返回的结果数量，默认是 10from显示应该跳过的初始结果数量，默认是 0如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果： GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 ⚠️ ⚠️ ⚠️考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。 在分布式系统中深度分页 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页—​结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因 参考资料 elasticsearch权威指南 干货 | 全方位深度解读 Elasticsearch 分页查询 Paginate search results","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"多索引多类型搜索","date":"2021-07-05T14:02:34.000Z","path":"wiki/多索引多类型搜索/","text":"如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。 然而，经常的情况下，你想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示： /_search在所有的索引中搜索所有的类型/gb/_search在 gb 索引中搜索所有的类型/gb,us/_search在 gb 和 us 索引中搜索所有的文档/g*,u*/_search在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search在所有的索引中搜索 user 和 tweet 类型当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的—​只是会涉及到更多的分片。 注意 ⚠️搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch重要配置","date":"2021-07-05T13:22:32.000Z","path":"wiki/elasticsearch重要配置/","text":"虽然Elasticsearch仅需要很少的配置，但有许多设置需要手动配置，并且在进入生产之前绝对必须进行配置。 path.data 和 path.logscluster.namenode.namebootstrap.memory_locknetwork.hostdiscovery.zen.ping.unicast.hostsdiscovery.zen.minimum_master_nodespath.data 和 path.logs如果使用.zip或.tar.gz归档，则数据和日志目录是$ES_HOME的子文件夹。 如果这些重要的文件夹保留在其默认位置，则存在将Elasticsearch升级到新版本时被删除的高风险。 在生产使用中，肯定得更改数据和日志文件夹的位置： 123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch RPM和Debian发行版已经使用数据和日志的自定义路径。 path.data 设置可以设置为多个路径，在这种情况下，所有路径将用于存储数据（属于单个分片的文件将全部存储在同一数据路径上）： 12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 cluster.name节点只能在群集与群集中的所有其他节点共享其cluster.name时才能加入群集。 默认名称为elasticsearch，但您应将其更改为描述集群用途的适当名称。cluster.name: logging-prod确保不要在不同的环境中重复使用相同的集群名称，否则可能会导致加入错误集群的节点。 node.name默认情况下，Elasticsearch将使用随机生成的uuid的第一个字符作为节点id。 请注意，节点ID是持久化的，并且在节点重新启动时不会更改，因此默认节点名称也不会更改。配置一个更有意义的名称是值得的，这是重启节点后也能一直保持的优势：node.name: prod-data-2node.name也可以设置为服务器的HOSTNAME，如下所示： 12node.name: $&#123;HOSTNAME&#125;bootstrap.memory_lock 没有JVM被交换到磁盘上这事对于节点的健康来说是至关重要的。一种实现方法是将bootstrap.memory_lock设置为true。要使此设置生效，需要首先配置其他系统设置。 有关如何正确设置内存锁定的更多详细信息，请参阅启用bootstrap.memory_lock。 network.host默认情况下，Elasticsearch仅仅绑定在本地回路地址——如：127.0.0.1与[::1]。这在一台服务器上跑一个开发节点是足够的。提示 事实上，多个节点可以在单个节点上相同的$ES_HOME位置一同运行。这可以用于测试Elasticsearch形成集群的能力,但这种配置方式不推荐用于生产环境。 为了将其它服务器上的节点形成一个可以相互通讯的集群，你的节点将不能绑定在一个回路地址上。 这里有更多的网路配置，通常你只需要配置network.host：network.host: 192.168.1.10network.host也可以配置成一些能识别的特殊的值，譬如：_local_、_site、_global_，它们可以结合指定:ip4与ip6来使用。更多相信信息请参见：网路配置 重要 👇 一旦你自定义了network.host的配置，Elasticsearch将假设你已经从开发模式转到了生产模式，并将升级系统检测的警告信息为异常信息。更多信息请参见：开发模式vs生产模式 discovery.zen.ping.unicast.hosts（单播发现）开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的回路地址，并扫描9300年到9305的端口去连接同一机器上的其他节点,试图连接到相同的服务器上运行的其他节点。它提供了不需要任何配置就能自动组建集群的体验。当与其它机器上的节点要形成一个集群时，你需要提供一个在线且可访问的节点列表。像如下来配置： 1234discovery.zen.ping.unicast.hosts: - 192.168.1.10:9300 - 192.168.1.11 #① - seeds.mydomain.com #② ① 未指定端口时，将使用默认的transport.profiles.default.port值，如果此值也为设置则使用transport.tcp.port ② 主机名将被尝试解析成能解析的多个IP discovery.zen.minimum_master_nodes为防止数据丢失，配置discovery-zen-minimum_master_nodes将非常重要，他规定了必须至少要有多少个master节点才能形成一个集群。没有此设置时，一个集群在发生网络问题是可能会分裂成多个集群——脑裂——这将导致数据丢失。更多详细信息请参见：通过minimum_master_nodes避免脑裂为避免脑裂，你需要根据master节点数来设置法定人数：(master_eligible_nodes / 2) + 1换句话说，如果你有三个master节点，最小的主节点数因被设置为(3/2)+1或者是2discovery.zen.minimum_master_nodes: 2 参考资料 elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch操作索引","date":"2021-07-05T13:11:01.000Z","path":"wiki/elasticsearch操作索引/","text":"创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 查看索引的mappingGET yj_visit_data/_mapping 1234567891011121314151617181920212223242526&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查询所有GET yj_visit_data/_search 12345&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 删除所有POST yj_visit_data/_delete_by_query 123456&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125; 通过文章删除POST yj_visit_data/_delete_by_query 1234567&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 根据文章查询GET yj_visit_data/_search 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 修改索引1234POST customer/_doc/1&#123; &quot;name&quot;:&quot;2&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch基础api","date":"2021-07-05T12:53:09.000Z","path":"wiki/elasticsearch基础cat_api/","text":"cat API集群健康状态GET _cat/health?v&amp;pretty 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1625489855 12:57:35 my-application yellow 1 1 35 35 0 0 23 0 - 60.3% 或者直接在服务器上调用rest接口：curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’ 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1475247709 17:01:49 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 我们可以看到我们名为 my-application 的集群与 yellow 的 status。 无论何时我们请求集群健康，我们可以获得 green，yellow，或者 red 的 status。Green 表示一切正常（集群功能齐全）， yellow 表示所有数据可用，但是有些副本尚未分配（集群功能齐全），red 意味着由于某些原因有些数据不可用。注意，集群是 red，它仍然具有部分功能（例如，它将继续从可用的分片中服务搜索请求），但是您可能需要尽快去修复它，因为您已经丢失数据了。 另外，从上面的响应中，我们可以看到共计 1 个 node（节点）和 0 个 shard（分片），因为我们还没有放入数据的。注意，因为我们使用的是默认的集群名称（elasticsearch），并且 Elasticsearch 默认情况下使用 unicast network（单播网络）来发现同一机器上的其它节点。有可能您不小心在您的电脑上启动了多个节点，然后它们全部加入到了单个集群。在这种情况下，你会在上面的响应中看到不止 1 个 node（节点）。 查看集群分布GET _cat/nodes?v&amp;pretty 12ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name172.19.0.1 20 61 15 0.02 0.04 0.29 cdhilmrstw * redtom-es-1 查看所有索引GET _cat/indices?v&amp;pretty 1234health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open rd-logstash-2021.06.19 p5iej71MQVW12s2awNv8nw 1 1 61236 0 16.3mb 16.3mbyellow open demo_index k6VTs7tdS0ysot-rPwxG9A 1 1 1 0 5.5kb 5.5kbgreen open kibana_sample_data_flights A7c5DViGSISii8FA0dNlGw 1 0 13059 0 5.6mb 5.6mb 查看所有索引的数量GET _cat/count?v&amp;pretty 12epoch timestamp count1625490245 13:04:05 838913 磁盘分配情况GET _cat/allocation?v&amp;pretty 123shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 35 308.7mb 20.1gb 215.9gb 236.1gb 8 172.19.0.1 172.19.0.1 redtom-es-1 23 UNASSIGNED 查看shard情况GET _cat/shards?v&amp;pretty 12345678index shard prirep state docs store ip nodeyj_visit_data 0 p STARTED 0 208b 172.19.0.1 redtom-es-1yj_visit_data 0 r UNASSIGNED demo_index 0 p STARTED 1 5.5kb 172.19.0.1 redtom-es-1demo_index 0 r UNASSIGNED rbtags 0 p STARTED 0 208b 172.19.0.1 redtom-es-1.kibana_1 0 p STARTED 280 11.5mb 172.19.0.1 redtom-es-1.kibana_task_manager_1 0 p STARTED 5 5.8mb 172.19.0.1 redtom-es-1 yj_visit_data 设置了一个副本分区，但是没有副节点，所以节点状态显示未分配； 参考资料 Elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"canal同步mysql数据到elasticsearch","date":"2021-07-05T03:26:50.000Z","path":"wiki/canal同步mysql数据到elasticsearch/","text":"首先安装elk推荐大家到elasic中文社区去下载 👉 【传送】⚠️ elastcisearch | logstash | kibana 的版本最好保持一直，否则会出现很多坑的，切记！ 安装ELK的步骤这里就不做介绍了，可以查看 👉 【TODO】 下载安装canal-adaptercanal github传送门 👉 【Alibaba Canal】 canal-client 模式可以参照canal给出的example项目和官方文档给出的例子来测试 依赖配置12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 创建maven项目保证canal-server 已经正确启动 👈 然后启动下面服务，操作数据库即可看到控制台的日志输出； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.redtom.canal.deploy;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import java.net.InetSocketAddress;import java.util.List;/** * @Author gaolei * @Date 2021/6/30 2:57 下午 * @Version 1.0 */@Slf4j@Componentclass CanalClient implements InitializingBean &#123; private final static int BATCH_SIZE = 1000; @Override public void afterPropertiesSet() throws Exception &#123; // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(&quot;***.***.***.***&quot;, 11111), &quot;example&quot;, &quot;canal&quot;, &quot;canal&quot;); try &#123; //打开连接 connector.connect(); //订阅数据库表,全部表 connector.subscribe(&quot;.*\\\\..*&quot;); //回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有ack的地方开始拿 connector.rollback(); while (true) &#123; // 获取指定数量的数据 Message message = connector.getWithoutAck(BATCH_SIZE); //获取批量ID long batchId = message.getId(); //获取批量的数量 int size = message.getEntries().size(); //如果没有数据 if (batchId == -1 || size == 0) &#123; try &#123; //线程休眠2秒 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; //如果有数据,处理数据 printEntry(message.getEntries()); &#125; //进行 batch id 的确认。确认之后，小于等于此 batchId 的 Message 都会被确认。 connector.ack(batchId); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; connector.disconnect(); &#125; &#125; /** * 打印canal server解析binlog获得的实体类信息 */ private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) &#123; //开启/关闭事务的实体类型，跳过 continue; &#125; //RowChange对象，包含了一行数据变化的所有特征 //比如isDdl 是否是ddl变更操作 sql 具体的ddl sql beforeColumns afterColumns 变更前后的数据字段等等 CanalEntry.RowChange rowChage; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;ERROR ## parser of eromanga-event has an error , data:&quot; + entry.toString(), e); &#125; //获取操作类型：insert/update/delete类型 CanalEntry.EventType eventType = rowChage.getEventType(); //打印Header信息 log.info(&quot;headers:&#123;&#125; &quot;, String.format(&quot;================》; binlog[%s:%s] , name[%s,%s] , eventType : %s&quot;, entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); //判断是否是DDL语句 if (rowChage.getIsDdl()) &#123; log.info(&quot;================》;isDdl: true,sql: &#123;&#125;&quot;, rowChage.getSql()); &#125; //获取RowChange对象里的每一行数据，打印出来 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; //如果是删除语句 if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); //如果是新增语句 &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); //如果是更新的语句 &#125; else &#123; //变更前的数据 log.info(&quot;-------&gt;; before&quot;); printColumn(rowData.getBeforeColumnsList()); //变更后的数据 log.info(&quot;-------&gt;; after&quot;); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; for (CanalEntry.Column column : columns) &#123; log.info(&quot; &#123;&#125; : &#123;&#125; update= &#123;&#125;&quot;, column.getName(), column.getValue(), column.getUpdated()); &#125; &#125;&#125; canal-adapter 模式adapter 配置文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546server: port: 8081spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1 batchSize: 1 retries: 0 timeout: accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 172.25.101.75:11111 canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 canal.tcp.username: canal.tcp.password: srcDataSources: defaultDS: url: jdbc:mysql://xxxx:pppp/database?useUnicode=true username: root password: pwd canalAdapters: - instance: example # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: es7 hosts: 172.25.101.75:9300 # 127.0.0.1:9200 for rest mode properties: mode: transport # or rest# # security.auth: test:123456 # only used for rest mode cluster.name: my-application# - name: kudu# key: kudu# properties:# kudu.master.address: 127.0.0.1 # &#x27;,&#x27; split multi address 我的elasticsearch是7.10.0版本的application.yml bootstrap.yml es6 es7 hbase kudu logback.xml META-INF rdb所以：👇 123cd es7biz_order.yml customer.yml mytest_user.ymlvim customer.yml customer.yml 配置文件如下： 123456789101112dataSourceKey: defaultDSdestination: examplegroupId: g1esMapping: _index: customer _id: id relations: customer_order: name: customer sql: &quot;select t.id, t.name, t.email from customer t&quot; etlCondition: &quot;where t.c_time&gt;=&#123;&#125;&quot; commitBatch: 3000 创建表结构12345678910CREATE TABLE `customer` ( `id` bigint(20) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, `email` varchar(255) DEFAULT NULL, `order_id` int(11) DEFAULT NULL, `order_serial` varchar(255) DEFAULT NULL, `order_time` datetime DEFAULT NULL, `customer_order` varchar(255) DEFAULT NULL, `c_time` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 测试canal-adapter同步效果创建一条记录122021-07-05 11:50:53.725 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;1&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457053000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457053724,&quot;type&quot;:&quot;INSERT&quot;&#125;Affected indexes: customer Elastcisearch 效果 1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;1&quot;, &quot;email&quot; : &quot;1&quot;, &quot;customer_order&quot; : &#123; &quot;name&quot; : &quot;customer&quot; &#125; &#125; &#125; ] &#125;&#125; 修改数据122021-07-05 11:54:36.402 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457275000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:[&#123;&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;customer_order&quot;:&quot;1&quot;&#125;],&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457276401,&quot;type&quot;:&quot;UPDATE&quot;&#125;Affected indexes: customer Elastcisearch 效果 删除一条数据122021-07-05 11:56:51.524 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457411000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457411523,&quot;type&quot;:&quot;DELETE&quot;&#125;Affected indexes: customer Elastcisearch 效果 参考资料 使用canal client-adapter完成mysql到es数据同步教程(包括全量和增量) es 同步问题 #1514 Github issue canal v1.1.4 文档手册 Sync es","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"mysql配置binlog","date":"2021-07-05T03:06:36.000Z","path":"wiki/binlog配置/","text":"开启binlog[mysqld]log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 查看binlog状态mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQl","slug":"DataBase/MySQl","permalink":"http://example.com/categories/DataBase/MySQl/"}]},{"title":"mysql常用命令","date":"2021-07-05T03:06:36.000Z","path":"wiki/mysql常用命令/","text":"binlog相关命令mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQl","slug":"DataBase/MySQl","permalink":"http://example.com/categories/DataBase/MySQl/"}]},{"title":"flink简单上手","date":"2021-07-04T14:23:43.000Z","path":"wiki/flink简单上手/","text":"mac 安装 flink1、执行 brew install apache-flink 命令 123456789gaolei:/ gaolei$ brew install apache-flinkUpdating Homebrew...==&gt; Auto-updated Homebrew! Updated 1 tap (homebrew/services).No changes to formulae.==&gt; Downloading https://archive.apache.org/dist/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgz######################################################################## 100.0%🍺 /usr/local/Cellar/apache-flink/1.9.1: 166 files, 277MB, built in 15 minutes 29 seconds 2、执行flink启动脚本 12/usr/local/Cellar/apache-flink/1.9.1/libexec/bin./start-cluster.sh WordCount批处理Demo创建maven项目，导入依赖 注意自己的flink版本 👇👇 12345678910111213141516171819202122&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写批处理程序123456789101112131415161718192021222324public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125; &#125; 准备数据源文件123456hello sparkhello worldhello javahello flinkhow are youwhat is your name 执行结果123456789101112(is,1)(what,1)(you,1)(flink,1)(name,1)(world,1)(hello,4)(your,1)(are,1)(java,1)(how,1)(spark,1) flink 处理流式数据1、通过 nc -lk &lt;port&gt; 打开一个socket服务，监听7777端口 用于模拟实时的流数据 2、java代码处理流式数据 123456789101112131415161718192021222324252627public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数（设置成1即单线程处理） // env.setMaxParallelism(32); // 从文件中读取数据// String inputPath = &quot;/tmp/Flink_Tutorial/src/main/resources/hello.txt&quot;;// DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 3、在终端数据数据，如下： 4、在首次启动的时候遇到一个错误 ❌Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/streaming/api/datastream/DataStream处理方法可参照 参考资料 👇 参考资料 Exception in thread “main” java.lang.NoClassDefFoundError 解决方案","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Apache Flink","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink简介/","text":"官方地址请戳👉 【传送】 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink 是一个框架和分布式处理器引擎，用于对无界和有界数据进行状态计算； Why Flink 流数据更真实地反应了我们的生活方式 传统的数据架构是基于有限数据集的 我们的目标1、低延迟 毫秒级响应2、高吞吐 能够处理海量数据 分布式3、结果的准确性和良好的容错性 Where need Flink 电商和市场营销数据报表、广告投放、业务流程需要 物联网（IOT）传感器实时数据采集和显示，实时报警，交通运输业 电信业基站流量调配 银行和金融业实时结算和通知推送、实时检测异常行为 传统数据处理架构 传统的数据处理架构如上👆CRM(用户关系系统)， Order System(订单系统), Web App (用户点击时间)，当用户出发行为之后需要系统作出响应，首先由上层的计算层处理计算逻辑，计算层的逻辑计算依赖下面的存储层，计算层计算完成之后，将响应返回给客户端。这种基于传统数据库方式无法满足高并发场景，数据库的并发量都是很低的。 分析处理流程 分析处理流程架构如上👆，数据先有传统的关系数据库，经过提取，清洗过滤等，将数据存放到数据仓库，然后通过一些sql处理，生成数据报表和一些其他的查询。 问题也很明显，实时性太差了，处理流程太长，无法满足毫秒级需求 数据来源不唯一，能满足海量数据和高并发的需求，但是无法满足实时的需求 有状态的流式处理 把当前做流式计算所需要的数据不存放在数据库中，而是简单粗暴的直接放到本地内存中； 内存不稳定？周期性的检查点，数据存盘和故障检测； lambda架构用两台系统同时保障低延迟和结果准确； 这套架构分成两个流程，上面为批处理流程，数据收集到一定程序，交给批处理器处理，最终产生一个批处理结果 下面的流程为流式处理流程，保证能快速得到结果 最终有我们在应用层根据实际问题选择具体的处理结果交给应用程序这种架构有什么缺陷？可能得到的结果是不准确的，我们可以先快速的得到一个实时计算的结果，隔一段时间之后在来看批处理产生的结果。实现两台系统和维护两套系统，成本很大； 第三代流式处理架构Apache Flink 可以完美解决上面的问题👆Strom无法满足海量数据； Sparking Stream 无法满足低延迟； 基于事件驱动 （Event-driven） 处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。 无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流&lt;/&gt; 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 其他特点 支持事件时间（event-time）和处理时间（processing-time）语义 精确一次的状态一致性保证 低延迟 每秒处理数百万个事件，毫秒级延迟 与众多常用的存储系统链接 高可用，动态扩展，支持7*24全天运行 参考资料 1、尚硅谷 2021 Flink Java版2、Apache Flink Documentation","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Flink 运行时架构","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink运行时架构/","text":"Flink运行时组件 JobManager (作业管理器)JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成： ResourceManagerResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。 DispatcherDispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。 JobMasterJobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。 TaskManager （任务管理器） Flink中的工作进程，通常在flink中会有多个TaskManager运行，每一个TaskMaganer都包含一定数量的插槽（slots）. 插槽的数量限制了TaskManager能够执行的任务数量； 启动之后，TaskManager会向资源管理器注册他的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用，JobManager就可以向插槽分配任务（tasks）来执行了 在执行的过程中，一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据。 任务提交流程 任务调度原理Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。 Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run …中运行。 可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。 思考问题🤔 怎样实现并行计算？ 多线程 并行的任务，需要占用多少solt？ 一个流处理程序，到底包含多少个任务？ Tasks 和算子链并行度（Parallelism） 一个特定算子的子任务（subtask）的个数被称之为并行度； 一般情况下，一个Stream的并行度就是其所有算子中最大的并行度。整个流也有一个并行度，就是所有算子所有任务的并行度之和；对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。链行为是可以配置的；请参考链文档以获取详细信息。 TaskManager 和 Slots","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"拥塞避免","date":"2021-07-04T08:46:13.000Z","path":"wiki/拥塞避免/","text":"拥塞避免拥塞控制的慢启动是以指数方式快速的通过试探来扩大拥塞窗口的，但是一旦发生网络丢包，则肯定是很多报文段都会都是，因为窗口时称被增长的；为了解决这种问题，需要引入– 拥塞避免 什么是拥塞避免拥塞避免为了解决慢启动下，当拥塞窗口超出网络带宽时发生的大量丢包问题，它提出一个「慢启动阈值」的概念，当拥塞窗口到达这个阈值之后，不在以指数方式增长，而选择涨幅比较缓慢的「线性增长」，计算方式： w cwnd += SMSS*SMSS/cwnd 当拥塞窗口在线性增长时发生丢包，将慢启动阈值设置为当前窗口的一半，慢启动窗口恢复初始窗口（init wnd）； i 拥塞避免和慢启动是结合使用的，当发生网络丢包是，拥塞控制采用快速重传和快速启动来解决丢包问题！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-四次挥手/断开连接","date":"2021-07-04T08:34:11.000Z","path":"wiki/TCP-四次挥手-断开连接/","text":"TCP断开连接四次挥手 开始客户端和服务端都是处理【established】状态 客户端发送「FIN」报文之后，进入FIN-WAIT-1状态 服务端收到客户端的FIN之后，恢复一个ACK，同时进入CLOSE_WAIT状态 客户端接收到ACK之后，进入到FIN-WAIT-2状态 服务端接着发送FIN报文，同时进入LAST-ACK状态 客户端接收到服务端的FIN报文之后，发送ACK报文，并进入TIME_WAIT状态 客户端在经历2个MSL时间之后，进入CLOSE状态 服务端接收到客户端的ACK之后，进入CLOSE状态 i 并不是所有的四次挥手都是上述流程，当客户端和服务端同时发送关闭连接的请求如下👇： 可以看到双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN_WAIT_1 都进入到 CLOSING 这个过度状态然后再到 TIME_WAIT。 i 挥手一定需要四次吗？ 假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？ i 为什么要有 TIME_WAIT? 断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。 w 那么为什么要等 2MSL 呢？ 就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。 假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。 i 等待 2MSL 会产生什么问题？ 如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"快速重传/快速恢复","date":"2021-07-04T08:33:35.000Z","path":"wiki/快速重传-快速恢复/","text":"快速重传和快速恢复快速重传 d 为何会接收到以个失序数据段？ 若报文丢失，将会产生连续的失序ACK段 若网络路径与设备导致数据段失序，将会产生少量的失序ACK段 若报文重复，将会产生少量的失序ACK段 当发送端发送pkt0是正常的，由于滑动窗口为满，发送方可以继续发送pkt1，pkt2； 加入pkt1发生了丢包，虽然pkt2接收端接收成功了，但是没有pkt1的数据段，接收端还是发送ACK1的确认报文； 在没有「快速重传」的情况下，发送端需要等到RTO之后，才可以重新发送pkt1 重传成功之后，接收端其实收到了pkt2之前的所有数据段，所以发送ACK3的确认报文 w 这种需要等待RTO才可以重传的方式效率是比较低的，因此需要快速重传来进行优化； 快速重传和累积确认 当发送方连续发送pkt3，pkt4，pkt5，pkt6四个数据端，但是pkt5在网络中丢包了，那后面发送的pkt6，pkt7，pkt8的确认报文都返回ACK5，希望发送方吃昂传pkt5的数据段；这个时候，发送方收到连续3个相同的确认报文，便立即重新发送pkt5的数据段； i 接收方: 当接收到一个失序数据段时，立刻发送它所期待的缺口 ACK 序列号 当接收到填充失序缺口的数据段时，立刻发 送它所期待的下一个 ACK 序列号 i 发送方 当接收到3个重复的失序 ACK 段(4个相同的失序ACK段)时，不再等待重传定时器的触发，立刻基于快速重传机制重发报文段 当pkt5重新发送并被接收端接收之后，接收端发送ACK9的确认报文，而不是再分别发送ACK6，ACK7，ACK8，这个称谓「 累计确认 」。 快速恢复 i 快速重传下一定要进入慢启动吗? 接受端收到重复ACK，意味着网络仍在流动，而如果要重新进入慢启动，会导致网络突然减少数据流，拥塞窗口恢复初始窗口，所以，「在快速恢复下发生丢包的场景下」，应该使用快速恢复，简单的讲，就是将慢启动阈值设置成当前拥塞窗口的一半，而拥塞窗口也适当放低，而不是一下字恢复到初始窗口大小； 快速恢复的流程如上图👆所示！ w 快速恢复的具体操作： 将 ssthresh 设置为当前拥塞窗口 cwnd 的一半，设当前 cwnd 为 ssthresh 加上 3*MSS 每收到一个重复 ACK，cwnd 增加 1 个 MSS 当新数据 ACK 到达后，设置 cwnd 为 ssthresh","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-拥塞控制之慢启动","date":"2021-07-04T08:33:19.000Z","path":"wiki/TCP-拥塞控制之慢启动/","text":"由于TCP是面向字节流的传输协议，可以发送不定长的字节流数据，TCP连接发送数据时会“先天性”尝试占用整个带宽，而当所有的TCP连接都尝试占用网络带宽时，就会造成网络的堵塞，而TCP慢启动算法则是为了解决这一场景； 全局思考 拥塞控制要面向整体思考，如上👆网络拓扑图，当左边的网络节点通过路由交换设备向右边的设备传输报文的时候，中间的某一链路的带宽肯定是一定的，这里假设1000M带宽，当左边R1以700Mb/s的速度向链路中发送数据，同时R2以600Mb/s的速率发送报文，那势必会有300Mb的数据报丢失；「路由交换设备基于存储转发来实现报文的发送」大量报文都是时，路由设备的缓冲队列肯定是慢的，这也会造成某些数据报在网络链路中停留时间过长，从而导致TCP通讯变慢，甚至网络瘫痪； 理想的情况下，当链路带宽占满以后，链路以最大带宽传输数据，当然显示中是不可能的，当发生轻度拥塞时，链路的吞吐量就开始下降了，发展到严重阻塞时，链路的吞吐量会严重地下降，甚至瘫痪； 那么，慢启动是如何发挥作用的呢？ 拥塞窗口 s 拥塞窗口cwnd(congestion window) 通告窗口rwnd(receiver‘s advertised window) 其实就是RCV.WND，标志在TCP首部的Window字段！ 发送窗口swnd = min(cwnd，rwnd) 前面学习滑动窗口的时候提到发送窗口大致等于接受窗口，当引入拥塞窗口时，发送窗口就是拥塞窗口和对方接受窗口的最小值 i 每收到一个ACK，cwnd扩充一倍 慢启动的窗口大小如何设置呢？如上所示，起初拥塞窗口设置成1个报文段大小，当发送端发送一个报文段并且没有发生丢包时，调整拥塞窗口为2个报文段大小，如果还没有发生丢包，一次类推，知道发生丢包停止；发送窗口以「指数」的方式扩大；慢启动是无法确知网络拥塞程度的情况下，以试探性地方式快速扩大拥塞窗口； 慢启动初始窗口慢启动的拥塞窗口真的就如上面所说的以一个报文段大小作为初始值吗？ w 慢启动初始窗口 IW(Initial Window)的变迁 1 SMSS:RFC2001(1997) 2 - 4 SMSS:RFC2414(1998) IW = min (4SMSS, max (2SMSS, 4380 bytes)) 10 SMSS:RFC6928(2013) IW = min (10MSS, max (2MSS, 14600)) w 其实在实际情况下，互联网中的网页都在10个mss左右，如果还是从1个mss开始，则会浪费3个RTT的时间；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-如何减少小报文提升网络效率","date":"2021-07-04T08:32:55.000Z","path":"wiki/TCP-如何减少小报文提升网络效率/","text":"如何减少小报文提升网络效率每一个TCP报文段都包含20字节的IP头部和20字节的TCP首部，如果报文段的数据部分很少的话，网络效率会很差； SWS(Silly Window syndrome) 糊涂窗口综合症 如上图👆所示场景，在之前的滑动窗口已经了解过，随着服务端处理连接数据能力越来越低，服务端的可用窗口不断压缩，最终导致窗口关闭； SWS 避免算法SWS 避免算法对发送方和接收方都做客 接收方 i David D Clark 算法:窗口边界移动值小于 min(MSS, 缓存/2)时，通知窗口为 0 发送方 w Nagle 算法:1、TCP_NODELAY 用于关闭 Nagle 算法2、没有已发送未确认报文段时，立刻发送数据3、存在未确认报文段时，直到:1-没有已发送未确认报文段，或者 2-数据长度达到MSS时再发送 TCP delayed acknowledgment 延迟确认实际情况下，没有携带任何数据的ACK报文也会造成网络效率低下的，因为确认报文也包含40字节的头部信息，但仅仅是为了传输ACK=1这样的信息，为了解决这种情况，TCP有一种机制，叫做延迟确认，如下👇： 当有响应数据要发送时,ack会随着响应数据立即发送给对方. 如果没有响应数据,ack的发送将会有一个延迟,以等待看是否有响应数据可以一起发送 如果在等待发送ack期间,对方的第二个数据段又到达了,这时要立即发送ack 那个延迟的时间如何设置呢？ 上面👆是Linux操作系统对于TCP延时的定义。 HZ是什呢？其实那是和操作系统的时钟相关的，具体的操作系统间各有差别；如何查看Linux操作系统下的HZ如何设置呢？ 1cat /boot/config- `-uname -r` | grep &#x27;^GONFIG_HZ=&#x27; TCP_CORK sendfile 零拷贝技术","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-滑动窗口","date":"2021-07-04T08:32:40.000Z","path":"wiki/TCP-滑动窗口/","text":"滑动窗口 i 之前学习了PAR方式的TCP超时和重传，其实在考虑发送方发送数据报的同时，也应该考虑接收方对于数据的处理能力，由此引出本次学习的主题 – 滑动窗口 发送端窗口滑动窗口按照传输数据方向分为两种，发送端窗口和接收端窗口；下面先看一下发送端窗口👇： 上图分为四个部分： 已发送并收到 Ack 确认的数据:1-31 字节 已发送未收到 Ack 确认的数据:32-45 字节 未发送但总大小在接收方处理范围内:46-51 字节 未发送但总大小超出接收方处理范围:52-字节 可用窗口和发送窗口 如上图这里可以引出两个概念：「可用窗口」和「发送窗口」 s 【 可用窗口 】： 就是上图中的第三部分，属于还未发送，但是在接收端可以处理范围内的部分；【 发送窗口 】： 就是发送端可以发送的最大报文大小，如上图中的第二部分+第三部分合成发送窗口； 可用窗口耗尽 可用窗口会在一个短暂的停留，当处于未发送并且接受端可以接受范围内的数据传输完成之后，可用窗口耗尽；当然上面仅仅说的一瞬时的状态，这个状态下，已经发送的报文段还没有确认，并且发送窗口大小没有发生变化，此时发送窗口达到最大状态； 窗口移动 如果在发送窗口中已经发送的报文段已经得到接受端确认之后，那部分数据就会被移除发送窗口，在发送窗口大小不发生变化的情况下，发送窗口向右➡️移动5个字节，因为左边已经发送的5个字节得到确认之后，被移除发送窗口； 可用窗口如何计算 再次引出三个概念： SND.WND i SND 指的是发送端，WND指的是window，也就是发送端窗口的意思 SND.UNA i UNA 就是un ACK的意思，指的是已经发送但是没有没有确认 它指向窗口的第一个字节处 SND.NXT i NXT 是next的位置，是发送方接下来要发送的位置，它指向可用窗口的第一个字节处 那就很容易得出可用窗口的大小了，计算公式如下： i Usable Window Size = SND.UNA + SND.WND - SND.NXT 接收端窗口上面介绍了发送端窗口的一些概念，下面👇是接收端窗口的学习： 已经接收并且已经确认 :28-31 字节 还未接收并且接收端可以接受:32-51 字节 还未接收并且超出接收处理能力:51-57 字节 这里引出两个概念： RCV.WND i RCV是接收端的意思，WND是接受端窗口的大小 RCV.NXT i NXT表示的是接受端接收窗口的开始位置，也就是接收方接下来处理的第一个字节； RCV.WND的大小接受端的内存以及缓冲区大小有关，在某种意义上说，接受端的窗口大小和发送端大小大致相同；接受端可接收的数据能力可以通过TCP首部的Window字段设置，但是接受端的处理能力是可能随时变化的，所以接受端和服务端的窗口大小大致是一样的； 流量控制下面👇根据一个例子来阐述流量控制，模拟一个GET请求，客户端向服务端请求一个260字节的文件，大致流程如下，比较繁琐： s 这里假设MSS和窗口的大小不发生变化，同时客户端和发送端状态如下：【 客户端 】： 发送窗口默认360字节 接收窗口设定200字节【 服务端 】： 发送窗口设定200字节 接收窗口设定360字节 Step1： 客户端发送140字节的数据到服务端 i 【客户端】发送140字节，【SND.NXT】从1-&gt;141 w 【服务端】状态不变，等待接收客户端传输的140字节 Step2: 服务端接收140字节，发送80字节响应以及ACK i 【 客户端 】发送140字节之后等待【 服务端 】的ACK w【 服务端 】可用窗口右移，【RCV.NXT】从1-&gt;141【 服务端 】发送80字节数据，【SND.NXT】从241-&gt;321 Step3: 客户端接收响应ACK，并且发送ACK i 【 客户端 】发出的140字节得到确认，【SND.UNA】右移140字节【 客户端 】接收80字节数据，【RCV.NXT】右移80字节，从241-&gt;321 Step4: 服务端发送一个280字节的文件，但是280字节超出了客户端的接收窗口，所以客户端分成两部分传输，先传输120字节； w 【 服务端 】发送120字节，【SND.NXT】向右移动120字节，从321-&gt;441 Step5: 客户端接收文件第一部分，并发送ACK i 【 客户端 】接收120字节，【RCV.NXT】从321-&gt;441 Step6：服务端接收到第二步80字节的ACK w [ 服务器 ] 80字节得到ACK 【SND.UNA】从241-&gt;321 Step7: 服务端接收到第4步的确认 w 【 服务端 】之前发送文件第一部分的120字节得到确认，【SND.UNA】右移动120，从321-&gt;441 Step8: 服务端发送文件第二部分的160字节 w 【 服务端 】： 发送160字节，【SND.NXT】向右移动160字节，从441-&gt;601 Step9: 客户端接收到文件第二部分160字节，同时发送ACK i 【 客户端 】接收160字节，【RCV.NXT】向右移动160字节，从441-&gt;601 Step10: 服务端收到文件第二部分的ACK w 【 服务端 】发送的160字节得到确认，【SND.UNA】向右一定160字节，从441-&gt;601；至此客户端收到服务端发送的完整的文件； 上面通过表格列举服务端和客户端每个状态在每个步骤的状态，如果不是很好理解，可以看如下示意图辅助理解： 客户端交互流程 服务端交互流程 上面👆是模拟一个GET请求，服务端发送一个280字节的文件给到客户端，客户端的接收窗口是200字节场景加，客户端和服务端的数据传输与交互流程，通过这个流程来学习滑动窗口的移动状态和流量控制的大致流程； 滑动窗口与操作系统缓冲区上面👆讲述的时候，都是假设窗口大小是不变的，而实际上，发送端和接受端的滑动窗口的字节数都吃存储在操作系统缓冲区的，操作系统的缓冲区受操作系统控制，当应用进程增加是，每个进程分配的内存减少，缓冲区减少，分配给每个连接的窗口就会压缩。**而且滑动窗口的大小也受应用进程读取缓冲区数据速度有关**； 应用进程读取缓冲区数据不及时造成窗口收缩step1: 客户端发送140字节 i 客户端发送到140字节之后，可用窗口收缩到220字节，发送窗口不变 Step2: 服务端接收140字节 但是应用进程仅仅读取40字节 w 服务端应用进程仅仅读取40字节，仍有100字节占用缓冲区大小，导致接受窗口收缩，服务端发送ACK报文时，在首部Window带上接收窗口的大小260 Step3: 客户端收到确认报文之后，发送窗口收缩到260 Step4: 客户端继续发送180字节数据 i 客户端发送180字节之后，可用窗口变成80字节 Step5: 服务端接收到180字节 w 假设应用程序仍然不读取这180字节，最终也导致服务端接收窗口再次收缩180字节，仅剩下80字节，在发送确认报文时，设置首部window=80 Step6: 客户端收到80字节的窗口时，调整发送窗口大小为80字节，可用窗口也是80字节 Step7: 客户端仍然发送80字节到服务端，此时可用窗口为空 Step8: 服务端应用进程继续不读区这80字节的缓冲区数据，最终导致服务端接收窗口大小为0，不能再接收任何数据，同时发送ACK报文； Step9：客户端收到确认报文之后，调整发送窗口大小为0，这个状态叫做「 窗口关闭 」 窗口收缩导致的丢包 Step1：客户端服务端开始的窗口大小都是360字节，客户端发送140字节数据 i 客户端发送140字节之后，可用窗口变成220字节 Step2：服务端应用进程骤增，进程缓存区平均分配，造成服务端接收窗口减少，从360变成240字节； w 假设接收了140字节之后，应用进程没有读取，那个可用窗口进一步压缩，变成100字节； Step3：假设同一个连接在没有收到服务端确认之后，又发送了180个字节的数据（Retramission） i 先发送了140字节，后发送了180字节，都没有得到确认，客户端可用窗口大小变成40字节 Step4：服务端收到上面👆第三步发送的180字节的数据，但是接受窗口的大小只有100字节，所以不能接收 w 服务端拒绝接收180字节 Step5：此时客户端才收到之前140字节的确认报文，才知道接收窗口发生了变化 i 客户端由于没有收到180字节的确认，加入客户端正在准备发送180字节数据，得到接受端的窗口大小是100字节之后，须强制将右侧窗口向左收缩80字节； 窗口关闭这个例子和上面的例子都发生了「 窗口关闭 」 s 窗口关闭： 发送端的发送窗口变成0的状态； 上面讲的两种情况一般不会发生的，因为操作系统不会既收缩窗口，同时减少连接缓存；而是一般先使用窗口收缩策略，之后在压缩缓冲区的方式来规避以上问题；发生窗口关闭之后，发送端不会被动的等待服务端的通知，而是会采用定时嗅探的方式去查看服务端接收窗口是否开放； Linux中对TCP缓冲区的调整方式 net.ipv4.tcp_rmem = 4096 87380 6291456 读缓存最小值、默认值、最大值，单位字节，覆盖 net.core.rmem_max net.ipv4.tcp_wmem = 4096 16384 4194304 写缓存最小值、默认值、最大值，单位字节，覆盖net.core.wmem_max net.ipv4.tcp_mem = 1541646 2055528 3083292 系统无内存压力、启动压力模式阀值、最大值，单位为页的数量 net.ipv4.tcp_moderate_rcvbuf = 1 开启自动调整缓存模式","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-RTO重传计数器的计算","date":"2021-07-04T08:32:27.000Z","path":"wiki/TCP-RTO重传计数器的计算/","text":"i 之前的文章已经介绍了TCP超时重传的过程中使用了定时器的策略，当定时器规定时间内未收到确认报文之后，就会触发报文的重传，同时定时器复位；那么定时器超时时间（RTO Retramission Timeout）是如何计算的呢？ 什么是RTT？了解RTO如何计算之前，首先明确一个概念「 RTT 」； 如上图所示，从client发送第一个「SYN」报文，到Server接受到报文，并且返回「SYN ACK」报文之后，client接受到Server的「ACK」报文之后，client所经历的时间，叫做1个RTT时间； 如何在重传下有效测量RTT？ 如上图两种情况：第一种，左侧a图所示，当一端发送的数据报丢失后要进行重传，到重传之后接收到确认报文之后，这种场景下该如何计算RTT呢？开始时间是按照第一次发送数据报时间呢还是按照重传数据报的时间呢？ w 按照常理来说，如右侧b图所示，RTT时间应该以RTT2为准； 第二种，左侧b图所示，第一次发送数据报文时，由于网络时延导致RTO时间内没有收到接收段的确认报文，发送端进行重发，但是在刚刚重发之后就收到了第一次报文的确认报文，那这种情况RTT该如何计算呢？ w 如右侧a图所示，RTT时间应该以RTT1为准； 就像上面提及的两种情况，一会以第一个RTT1为准，一会以RTT2为准，那么TCP协议如何正确的计算出RTT呢？ 使用Timestamp方式计算RTT之前的文章中在介绍TCP超时与重传的笔记中有介绍通过使用Timtstamp的方式来区分相同Seq序列号的不同报文，其实在TCP报文首部存储Timestamp的时候，会存储报文的发送时间和确认时间，如下所示： 如何计算RTO？上面👆说到了RTT如何计算，那个RTO和RTT有什么关系呢？ RTO的取值将会影响到TCP的传输效率以及网络的吞吐量； s 通常来说RTO应该略大于RTT，如果RTO小于RTT，则会造成发送端频繁重发，可能会造成网络阻塞；如果RTO设置的过大，则接受端已经收到确认报文之后的一段时间内仍然不能发送其他报文，会造成两端性能的浪费和网络吞吐量的下降； 平滑RTO网络的RTT是不断的变化的，所以计算RTO的时候，应当考虑RTO的平滑性，尽量避免RTT波动带来的干扰，以抵挡瞬时变化； 平滑RTO在文档RFC793定义，给出如下计算方式： SRTT (smoothed round-trip time) = ( α * SRTT ) + ((1 - α) * RTT) w α 从 0到 1(RFC 推荐 0.9)，越大越平滑 RTO = min[ UBOUND, max[ LBOUND, (β * SRTT) ] ] w 如 UBOUND为1分钟，LBOUND为 1 秒钟， β从 1.3 到 2 之间 这种计算方式不适用于 RTT 波动大(方差大)的场景,如果网络的RTT波动很大，会造成RTO调整不及时； 追踪RTT方差计算RTO i RFC6298(RFC2988)，其中α = 1/8， β = 1/4，K = 4，G 为最小时间颗粒: 首次计算 RTO，R为第 1 次测量出的 RTT123SRTT(smoothed round-trip time) = RRTTVAR(round-trip time variation) = R/2RTO = SRTT + max (G, K*RTTVAR) 后续计算 RTO，R’为最新测量出的 RTT123SRTT= (1-α)*SRTT+α*R’RTTVAR=(1-β)*RTTVAR+β*|SRTT-R’|RTO = SRTT + max (G, K*RTTVAR)","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP超时与重传","date":"2021-07-04T08:32:08.000Z","path":"wiki/TCP超时与重传/","text":"背景 如上图👆所示，设备A向设备B发送消息，消息在网络中会由于各种各样的问题导致丢失，那么该如何解决上述问题呢？ 采用定时器重传 i PAR：Positive Acknowledgment with Retransmission 最简单的思路是在发送方设置「 定时器 」： 当设备A发送第一条消息之后，在定时器规定的时间内，如果收到设备B的确认报文，则设备A继续发送下一个报文，同时定时器复位； 如果第一条消息发送时间超出了定时器规定的时间，则设备A将重新发送第一条消息，同时重新设置定时器； 这种方式是串型发送的，只有第一个消息发送成功之后，才可以发送下一条消息，「 效率极差 」； 并发能力PAR i 基于上述PAR效率低下的方式进行改造，在发送端采用并发+定时器的方式进行数据发送； 首先设备A可以同时发送多个消息或者报文段，每个报文段具有一个标志字段【#XX】去标志唯一，每个报文段连接具有自己的定时器； 设备B规定时间内收到设备A发送的数据之后并且设备A得到设备B的确认之后，设备A将定时器清除 同PAR一样，设备B没有在规定的时间内发送确认报文，设备A将这个报文所对应的定时器复位，重新发送这个报文 并发发送带来的问题采用并发的方式发送消息或者报文段固然提升了发送端的性能，但是发送端发送的消息可能接受端不能完全处理，这是双方报文处理速度或者效率不一致的问题； 所以对于接收端设备B，应该明确自己可能接受的数据量，并且在确认报文中同步到发送端设备A，设备A根据设备B的处理能力来调整发送数据的大小；也就是上图中的「 limit」； 继续延伸Sequment序列号和Ack序列号的设计理念或者设计初衷是「 解决应用层字节流的可靠发送 」 跟踪「应用层」的发送端数据是否送达 确定「接收端有序的」接收到「字节流」 序列号的值针对的是字节而不是报文 ⚠️⚠️⚠️ i TCP的定位就是面向字节流的！ TCP序列号如何设计的 通过TCP报文头我们可以知道，Sequment序列号包括32位长度；也就是说一个Sequment可以发送2的32次方个字节，大约4G的数量，Sequment就无法表示了，当传输的数据超过“4G”之后，如果这个连接依然要使用的话，Sequment会重新复用；Sequment复用会产生一个问题，也就是序列号回绕；👇 序列号回绕 i 序列号回绕 (Protect Against Wrapped Sequence numbers) 当一个连接要发送6G的数据是，A、B、C、D分别发送1G的数据，如果继续使用此连接，E下一次发送数据1G，Seq序列号复用，E报文段的序列号和A报文段的序列号表示相同 按照上面的逻辑继续发送数据，F报文段的Seq标志和B报文段的是一样的； 加入B报文段在发送过程中丢失了，直到接受端接收了F报文段的同时B报文段到达接受端，接受端该如何区分相同Seq序列号不同数据的报文段呢？ 其实TCP解决这个问题很简单，就是在每个报文段上添加Tcp Timestamp时间戳，类似于版本号的理念； 接收端收到相同Seq序列号的报文段是可以根据时间戳来进行区分；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP最大报文段（MSS）","date":"2021-07-04T08:31:56.000Z","path":"wiki/TCP最大报文段（MSS）/","text":"MSS产生的背景我们都知道TCP协议是运输在传输层的协议，它是面向【字节流】的传输协议；它的上层，应用层传输的数据是无限制的，但是它的下层也就是网络层和链路层由于路由等转发设备有内存等限制是不可能无限制传输任何大小的报文的，它们一定会限制报文的长度，因此 TCP协议要完成的工作是将从应用层接受到的任意长度数据，切割成多个报文段，MSS就是如何切割报文段的依据。 什么是MSSMSS（Max Segment Size）：仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879 MSS 选择目的 尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率 防止 Segment 被某个设备的 IP 层基于 MTU 拆分 s IP层基于MTU的数据拆分是效率极差的，一个报文段丢失，所有的报文段都要重传 MSS默认大小 s 默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部) MSS在什么时候使用 s 握手阶段协商 MSS 这个在TCP三次握手的文章中已经提及过了！ MSS 分类 发送方最大报文段: SMSS:SENDER MAXIMUM SEGMENT SIZE 接收方最大报文段: RMSS:RECEIVER MAXIMUM SEGMENT SIZE 在TCP常用选项中可以看到【MSS】的选项 TCP流与报文段在数据传输中的状态 从上图可以看到，左边客户端在发送字节流数据给到右边客户端，客户端发送一个连续的字节流，会在TCP层按照MSS大小规定进行拆分成多个小的报文段，分别传送到另一个客户端或者其他的接收端；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP三次握手","date":"2021-07-04T08:31:42.000Z","path":"wiki/TCP三次握手/","text":"握手🤝的目的 同步Sequence序列号 i 初始化序列号ISN （Inital Sequence Number） 交换TCP通讯的参数 i 比如最大报文段参数（MSS）、窗口比例因子（Window）、选择性确认（SACK）、制定校验和算法； 三次握手握手过程 TCP三次握手的大致流程图如上👆 使用tcpdump抓包分析三次🤝握手报文中Seq和Ack的变化 1tcpdump port 80 -c 3 -S 第一次握手🤝1IP upay.60734 &gt; 100.100.15.23.http: Flags [S], seq 3800409106, win 29200, options [mss 1460,sackOK,TS val 839851765 ecr 0,nop,wscale 7], length 0 客户端upay访问服务端80端口，发送一个「 seq=3800409106 」 ，同时标志位SYN=1，声明此次握手是要建立连接； 第二次握手🤝1IP 100.100.15.23.http &gt; upay.60734: Flags [S.], seq 1981710286, ack 3800409107, win 14600, options [mss 1440,nop,nop,sackOK,nop,wscale 7], length 0 第二次握手，服务端收到客户端的申请连接强求（SYN=1）之后，在服务端自己准备好的情况下，给客户端发送 「 ACK=1 SYN=1 」的确认报文，SYN=1同样也是声明此次报文是建立连接的报文请求； ack= 3800409107 也就是第一个客户端发给服务端的seq+1（ack是接收方下次期望接口报文的开始位置） 第三次握手握手1IP upay.60734 &gt; 100.100.15.23.http: Flags [.], ack 1981710287, win 229, length 0 客户端收到服务器返回的确认报文，确认可以进行连接，发送「 ack = 1981710287 」的确认报文，之后就完成了三次握手，TCP的连接就创建成功了，接下来双方就可以发送数据报了； TCP连接创建构成中状态的变更 首先客户端和服务端都是【CLOSED】状态，客户端发起连接请求之后，进入【SYN-SENT】状态，这个状态维持的时间很短，我们使用netstat去查看tcp连接状态的时候，基本上都不会看到这个状态，而服务端是在【LISTEN】状态，等待客户端的请求； 服务端收到客户端请求之后，发送「SYN ACK」确认报文，同时服务端进入【SYN-RECEIVED】状态，等待客户端的确认报文； 客户端收到服务端的同步确认请求之后，发送「ACK」确认报文，同时进入【ESTABLISHED】状态，准备后续的数据传输； 服务端收到三次握手最后的确认报文之后，进入【ESTABLISHED】状态，至此，一个TCP连接算是建立完成了，后面就是双方的通信了； TCB（Transmission Control Block） i 保存连接使用的源端口、目的端口、目的 ip、序号、 应答序号、对方窗口大小、己方窗口大小、tcp 状态、tcp 输入/输出队列、应用层输出队 列、tcp 的重传有关变量等 TCP性能优化和安全问题 正如我们了解的TCP三次握手🤝的流程，当有大量SYN请求到达服务端时，会进入到【SYN队列】，服务端收到第二次确认报文之后，会进入【ESTABLISHED】状态，服务端操作系统内核会将连接放入到【ACCEPT】队列中，当Nginx或者Tomcat这些应用程序在调用accept（访问内核）的时候，就是在【ACCEPT】队列中取出连接进行处理； w 由此可见，【SYN】队列和【ACCEPT】是会影响服务器连接性能的重要因素，所以对于高并发的场景下，这两个队列一定是要设置的比较大的； 如何设置SYN队列大小服务器端 SYN_RCV 状态 net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数 客户端 SYN_SENT 状态（服务端作为客户端，比如Ngnix转发等） net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数 net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围 Fast Open机制 TCP如何对连接的次数以及连接时间进行优化的呢？这里提到Fast Open机制；比如我们有一个Http Get请求，正常的三次握手🤝到收到服务端数据需要2个RTT的时间；FastOpen做出如下优化： 第一次创建连接的时候，也是要经历2个RTT时间，但是在服务端发送确认报文的时候，在报文中添加一个cookie； 等到下次客户端再需要创建请求的时候，直接将【SYN】和cookie一并带上，可以一次就创建连接，经过一个RTT客户端就可以收到服务端的数据； 如何Linux上打开TCP Fast Open net.ipv4.tcp_fastopen:系统开启 TFO 功能 0:关闭 1:作为客户端时可以使用 TFO 2:作为服务器时可以使用 TFO 3:无论作为客户端还是服务器，都可以使用 TFO SYN攻击什么是SYN攻击？ 正常的服务通讯都是由操作系统内核实现的请求报文来创建连接的，但是，可以人为伪造大量不同IP地址的SYN报文，也就是上面👆状态变更图中的SYN请求，但是收到服务端的ACK报文之后，却不发送对于服务端的ACK请求，也就是没有第三次挥手，这样会造成大量处于【SYN-RECEIVED】状态的TCP连接占用大量服务端资源，导致正常的连接无法创建，从而导致系统崩坏； SYN攻击如何查看1netstat -nap | grep SYN_RECV w 如果存在大量【SYN-RECEIVED】的连接，就是发生SYN攻击了； 如何规避SYN攻击？ net.core.netdev_max_backlog 接收自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn_backlog SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接 设置SYN Timeout 由于SYN Flood攻击的效果取决于服务器上保持的SYN半连接数，这个值=SYN攻击的频度 x SYN Timeout，所以通过缩短从接收到SYN报文到确定这个报文无效并丢弃改连接的时间，例如设置为20秒以下，可以成倍的降低服务器的负荷。但过低的SYN Timeout设置可能会影响客户的正常访问。 设置SYN Cookie (net.ipv4.tcp_syncookies = 1) 就是给每一个请求连接的IP地址分配一个Cookie，如果短时间内连续受到某个IP的重复SYN报文，就认定是受到了攻击，并记录地址信息，以后从这个IP地址来的包会被一概丢弃。这样做的结果也可能会影响到正常用户的访问。 当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接 w 由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等 TCP_DEFER_ACCEPT这个是做什么呢？ 正如上面👆操作系统内核展示图所示，内核中维护两个队列【SYN】队列和【ACCEPT】队列，只有当收到客户端的ACK报文之后，连接会进入到【ACCEPT】，同时服务器的状态是【ESTABLISHED】状态，此时操作系统并不会去激活应用进程，而是会等待，知道收到真正的data分组之后，才会激活应用进程，这是为了提高应用进程的执行效率，避免应用进程的等待； i TCP三次握手为什么不能是两次或者四次 参见文章：敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP头部","date":"2021-07-04T08:31:22.000Z","path":"wiki/TCP头部/","text":"带着问题学习 如何校验报文段是否损坏？ 如何CRC校验 seq和ack是如何计算的？ tcp校验位都有那些？ 6个 分别是什么含义？ tcp如何计算首部长度？ 偏移量 TCP Retransmission 重传？ tcp spurious retransmission 又是什么呢？ tcp dup ack 是什么？ ack与ACK有什么区别？ 分别有什么作用？ TCP头部结构 学习TCP协议首先要看一下它的报文段是如何组成的；TCP报文段组成由两部分，第一部分是报文头部，第二部分是数据部分； 先看一下报文头，也就是TCP首部的组成； 16位端口16位端口号：告知主机该报文段是来自哪里（源端口Source Port）以及传给哪个上层协议或应用程序（目的端口Destination Port）的。进行TCP通信时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号（比如DNS协议对应端口53，HTTP协议对应80，这些端口号可在/etc/services文件中找到）。 序列号（Seq）占32位，也就是4字节长度，序号范围自然也是是0~2^32-1。TCP是面向字节流的，TCP连接中传送的字节流中的每个字节都按顺序编号。整个要传送的字节流的起始序号必须要在连接建立时设置。首部中的序号字段值指的是本报文段所发送的数据的第一个字节的序号。 TCP用序列号对数据包进行标记，以便在到达目的地后重新重装，假设当前的序列号为 s，发送数据长度为 l，则下次发送数据时的序列号为 s + l。在建立连接时通常由计算机生成一个随机数作为序列号的初始值。 **这里存在一个疑问，第一次建立TCP连接的时候，网上一些博客上说seq是client随机生成的，也有的博客说是seq=1； 这里经过我抓包后，看到第一次创建TCP连接的时候，确实是1; ** 确认应答号（Ack）Ack占32位，4个字节长度，表示期望收到对方下一个报文段的序号值。 用作对另一方发送来的TCP报文段的响应。其值是收到的TCP报文段的序号值加1。假设主机A和主机B进行TCP通信，那么A发送出的TCP报文段不仅携带自己的序号，而且包含对B发送来的TCP报文段的确认号。反之，B发送出的TCP报文段也同时携带自己的序号和对A发送来的报文段的确认号。TCP的可靠性，是建立在「每一个数据报文都需要确认收到」的基础之上的。 就是说，通讯的任何一方在收到对方的一个报文之后，都要发送一个相对应的「确认报文」，来表达确认收到。 那么，确认报文，就会包含确认号。 若确认号=N，则表明：到序号N-1为止的所有数据都已正确收到。 数据偏移 Offset占 0.5 个字节 (4 位)。 这个字段实际上是指出了TCP报文段的首部长度 ，它指出了TCP报文段的数据起始处距离TCP报文的起始处有多远。 注意数据起始处和报文起始处的意思，上面👆已经写到，TCP报文段的组成有两部分，TCP报文首部和数据部分，偏移量记录的是报文段开始和数据开始的长度，也就是报文首部的长度； 一个数据偏移量 = 4 byte，由于4位二进制数能表示的最大十进制数字是 15，因此数据偏移的最大值是 60 byte，这也侧面限制了TCP首部的最大长度。 保留Reserved占 0.75 个字节 (6 位)。 保留为今后使用，但目前应置为 0。 标志位 TCP Flags标志位，一共有6个，分别占1位，共6位。 每一位的值只有 0 和 1，分别表达不同意思。 如上图是使用wireshard抓包展示截图； ACK(Acknowlegemt) ：确认序号有效 当 ACK = 1 的时候，确认号（Acknowledgemt Number）有效。 一般称携带 ACK 标志的 TCP 报文段为「确认报文段」。为0表示数据段不包含确认信息，确认号被忽略。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 设置为 1。 RST(Reset)：重置连接 当 RST = 1 的时候，表示 TCP 连接中出现严重错误，需要释放并重新建立连接。 一般称携带 RST 标志的 TCP 报文段为「复位报文段」。 SYN(SYNchronization)：发起了一个新连接 当 SYN = 1 的时候，表明这是一个请求连接报文段。 一般称携带 SYN 标志的 TCP 报文段为「同步报文段」。 在 TCP 三次握手中的第一个报文就是同步报文段，在连接建立时用来同步序号。对方若同意建立连接，则应在响应的报文段中使 SYN = 1 和 ACK = 1。 PSH (Push): 推送 当 PSH = 1 的时候，表示该报文段高优先级，接收方 TCP 应该尽快推送给接收应用程序，而不用等到整个 TCP 缓存都填满了后再交付。 FIN：释放一个连接 当 FIN = 1 时，表示此报文段的发送方的数据已经发送完毕，并要求释放 TCP 连接。一般称携带 FIN 的报文段为「结束报文段」。在 TCP 四次挥手释放连接的时候，就会用到该标志。 窗口大小 Window Size占16位。该字段明确指出了现在允许对方发送的数据量，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 窗口大小的值是指，从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量。 例如，假如确认号是701，窗口字段是 1000。这就表明，从 701 号算起，发送此报文段的一方还有接收 1000 （字节序号是 701 ~ 1700） 个字节的数据的接收缓存空间。 校验和 TCP Checksum占16位。 由发送端填充，接收端对TCP报文段执行【CRC算法】，以检验TCP报文段在传输过程中是否损坏，如果损坏这丢弃。 检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。 紧急指针 Urgent Pointer占 2 个字节。 仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。 当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 因此，紧急指针指出了紧急数据的末尾在报文段中的位置。 选项 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 下面是常用选项： MTU（最大传输单元）MTU（最大传输单元）是【链路层】中的网络对数据帧的一个限制，以以太网为例，MTU 为 1500 个字节。一个IP 数据报在以太网中传输，如果它的长度大于该 MTU 值，就要进行分片传输，使得每片数据报的长度小于MTU。分片传输的 IP 数据报不一定按序到达，但 IP 首部中的信息能让这些数据报片按序组装。IP 数据报的分片与重组是在网络层进完成的。 MSS （最大分段大小）MSS 是 TCP 里的一个概念（首部的选项字段中）。MSS 是 TCP 数据包每次能够传输的最大数据分段，TCP 报文段的长度大于 MSS 时，要进行分段传输。TCP 协议在建立连接的时候通常要协商双方的 MSS 值，每一方都有用于通告它期望接收的 MSS 选项（MSS 选项只出现在 SYN 报文段中，即 TCP 三次握手的前两次）。MSS 的值一般为 MTU 值减去两个首部大小（需要减去 IP 数据包包头的大小 20Bytes 和 TCP 数据段的包头 20Bytes）所以如果用链路层以太网，MSS 的值往往为 1460。而 Internet 上标准的 MTU 为 576，那么如果不设置，则MSS的默认值就为 536 个字节。TCP报文段的分段与重组是在运输层完成的。 seq和ack的计算逻辑 CRC校验参考资料TCP协议中的seq/ack序号是如何变化的？TCP协议详解TCP协议详解（一）：TCP头部结构TCP和UDP报文头格式TCP协议详解吃透TCP协议","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP协议","date":"2021-07-04T08:30:55.000Z","path":"wiki/TCP协议/","text":"TCP协议学习笔记📒 w 下面是本人在学习TCP协议的过程中，记录的笔记，按照学习的过程从前到后整理在这里！可能会有很多的知识没有罗列，只是记录的大概框架，如果有问题或错误，欢迎指正！ 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手 学习资料 i 敖丙Github整理的笔记 有大概10篇左右的文章，都是高质量的，原地址请点击着👉 【Github】 i 极客时间《Web协议详解与抓包实战》– 陶辉老师 这门课程专门讲解网络协议的，包括Http/Https,TLS协议，TCP协议，IP协议等； i 《计算机网络 自顶向下方法》第7版 很多名校计算机网络课程在使用的教材，非常权威！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"Welcome to GeekIBLi","date":"2021-07-04T07:44:33.000Z","path":"wiki/index/","text":"I am geekibli; I come from HBU. I like code. 😄 😄 😄 😄 道阻 且长 Java： Java并发编程的挑战 Java函数式编程 Java虚拟机 Java IO （Netty） Spring框架： Spring框架核心（Bean MVC Config） SpringBoot SpringCloud (Fegin、Consul、gateWay) Spring + Mybatis (MybatisPlus) 数据库 Mysql Redis Elasticsearch 计算机网络 网络层协议 网络加密算法 操作系统 Linux (用户、文件、网关、服务) 设计模式 设计模式的应用场景 常见的设计模式 分布式 分布式锁（MySQL Redis Zookeeper） 分布式事务(Seats) 分布式任务调度 （ElasticJob、XXL-Job） 分布式ID(百度、美团、滴滴) 分布式数据库（TiDB） 中间件 消息中间件（Kafka） 数据中间件（ShardingJDBC、Canal） 连接池（HiKariCP） 微服务的挑战 服务发现与注册 服务治理 服务通信 开发工具（Git、Jenkins、Maven） 运维 容器（Docker、K8s） 不错的学习网站推荐 掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io示说 「 提供了很多优质的PPT 还有很多大厂的沙龙视屏以及材料」 技术团队推荐 小米信息部技术团队有赞技术团队美团技术团队 两年学说话 一生学闭嘴","tags":[],"categories":[{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}]}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"},{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"},{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQl","slug":"DataBase/MySQl","permalink":"http://example.com/categories/DataBase/MySQl/"},{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"},{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"},{"name":"elasticsaerch","slug":"elasticsaerch","permalink":"http://example.com/tags/elasticsaerch/"},{"name":"推荐","slug":"推荐","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90/"},{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}]}