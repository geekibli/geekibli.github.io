{"pages":[{"title":"Categories","date":"2021-08-11T02:49:25.950Z","path":"categories/index.html","text":""},{"title":"潜心一技 直到极致","date":"2021-08-11T02:45:43.229Z","path":"about/index.html","text":"道阻 且长不错的学习网站推荐掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io字节跳动后端面试题集知乎 Java快速进阶通道To Be Top Javaer - Java工程师成神之路Div-wangJava 全栈知识体系https://github.com/crossoverJie/JCSprouthttps://github.com/ZhongFuCheng3y/3y ‼️ 工具网站ProcessOn示说 「 提供了很多优质的PPT 还有很多大厂的沙龙视屏以及材料」 技术团队推荐小米信息部技术团队有赞技术团队美团技术团队 面试题合集2020年大厂Java面试前复习的正确姿势（800+面试题附答案解析）Java集合面试题（总结最全面的面试题） 两年学说话 一生学闭嘴"},{"title":"Tags","date":"2021-08-11T02:44:22.789Z","path":"tags/index.html","text":""}],"posts":[{"title":"Arthas-Java诊断神器","date":"2021-08-20T08:28:32.000Z","path":"wiki/Arthas-Java诊断神器/","text":"Arthas-Java诊断神器 官方文档地址 👉 https://arthas.aliyun.com/doc/index.html# 1. 安装arthaswget https://arthas.aliyun.com/arthas-boot.jar java -jar arthas-boot.jar --target-ip 0.0.0.0 如果你的机器没有任何java进程在运行，会提示如下错误 👇 正常启动如下： ⚠️ 由于我们仅仅启动了一个java进程，所有这里就只有一个。输入1回车即可。 42423就是我们的java进程号 2. 查看JVM信息2.1 syspropsysprop 可以打印所有的System Properties信息。 也可以指定单个key： sysprop java.version 也可以通过grep来过滤： sysprop | grep user 可以设置新的value： sysprop testKey testValue 2.2 sysenvsysenv 命令可以获取到环境变量。和sysprop命令类似。 2.3 jvmjvm 命令会打印出JVM的各种详细信息。 2.4 dashboarddashboard 命令可以查看当前系统的实时数据面板。 输入 Q 或者 Ctrl+C 可以退出dashboard命令。 3. 查看线程相关3.1 查看线程列表thread 3.2 查看线程栈信息thread 18 3.3 查看5秒内的CPU使用率top n线程栈thread -n 3 -i 5000 3.4 查找线程是否有阻塞thread -b 4. sc/sm 查看已加载的类下面介绍Arthas里查找已加载类的命令。 4.1 sc 查找到所有JVM已经加载到的类如果搜索的是接口，还会搜索所有的实现类。比如查看所有的Filter实现类： sc javax.servlet.Filter 通过-d参数，可以打印出类加载的具体信息，很方便查找类加载问题。 sc -d javax.servlet.Filter sc支持通配，比如搜索所有的StringUtils： sc *StringUtils 4.2 sm 查找类的具体函数sm java.math.RoundingMode 通过-d参数可以打印函数的具体属性： sm -d java.math.RoundingMode 也可以查找特定的函数，比如查找构造函数： sm java.math.RoundingMode &lt;init&gt; 5. Jad反编译可以通过 jad 命令来反编译代码： jad com.example.demo.arthas.user.UserController 通过--source-only参数可以只打印出在反编译的源代码： jad --source-only com.example.demo.arthas.user.UserController 6. Ognl动态代码在Arthas里，有一个单独的ognl命令，可以动态执行代码。 6.1 调用static函数ognl &#39;@java.lang.System@out.println(&quot;hello ognl&quot;)&#39; 可以检查Terminal里的进程输出，可以发现打印出了hello ognl。 6.2 查找UserController的ClassLoader123sc -d com.example.demo.arthas.user.UserController | grep classLoaderHash$ sc -d com.example.demo.arthas.user.UserController | grep classLoaderHash classLoaderHash 1be6f5c3 注意hashcode是变化的，需要先查看当前的ClassLoader信息，提取对应ClassLoader的hashcode。 如果你使用-c，你需要手动输入hashcode：-c &lt;hashcode&gt; $ ognl -c 1be6f5c3 @com.example.demo.arthas.user.UserController@logger 对于只有唯一实例的ClassLoader可以通过--classLoaderClass指定class name，使用起来更加方便： 123456$ ognl --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader @org.springframework.boot.SpringApplication@logger@Slf4jLocationAwareLog[ FQCN=@String[org.apache.commons.logging.LogAdapter$Slf4jLocationAwareLog], name=@String[org.springframework.boot.SpringApplication], logger=@Logger[Logger[org.springframework.boot.SpringApplication]],] --classLoaderClass 的值是ClassLoader的类名，只有匹配到唯一的ClassLoader实例时才能工作，目的是方便输入通用命令，而-c &lt;hashcode&gt;是动态变化的。 6.3 获取静态类的静态字段获取UserController类里的logger字段： 1ognl --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader @com.example.demo.arthas.user.UserController@logger 还可以通过-x参数控制返回值的展开层数。比如： 1ognl --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader -x 2 @com.example.demo.arthas.user.UserController@logger 6.4 执行多行表达式，赋值给临时变量，返回一个List123456ognl &#x27;#value1=@System@getProperty(&quot;java.home&quot;), #value2=@System@getProperty(&quot;java.runtime.name&quot;), &#123;#value1, #value2&#125;&#x27;$ ognl &#x27;#value1=@System@getProperty(&quot;java.home&quot;), #value2=@System@getProperty(&quot;java.runtime.name&quot;), &#123;#value1, #value2&#125;&#x27;@ArrayList[ @String[/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre], @String[Java(TM) SE Runtime Environment],] 6.5 更多在Arthas里ognl表达式是很重要的功能，在很多命令里都可以使用ognl表达式。 一些更复杂的用法，可以参考： OGNL特殊用法请参考：https://github.com/alibaba/arthas/issues/71 OGNL表达式官方指南：https://commons.apache.org/proper/commons-ognl/language-guide.html 7. Watch查看命令7.1 如何使用watch com.example.demo.arthas.user.UserController * &#39;&#123;params, throwExp&#125;&#39; 执行完之后，会阻塞，此时如果有请求进来，发生一场的话，就会看到异常信息。 如果想把获取到的结果展开，可以用-x参数： watch com.example.demo.arthas.user.UserController * &#39;&#123;params, throwExp&#125;&#39; -x 2 7.2 返回值表达式在上面的例子里，第三个参数是返回值表达式，它实际上是一个ognl表达式，它支持一些内置对象： loader clazz method target params returnObj throwExp isBefore isThrow isReturn 你可以利用这些内置对象来组成不同的表达式。比如返回一个数组： 1watch com.example.demo.arthas.user.UserController * &#x27;&#123;params[0], target, returnObj&#125;&#x27; 更多参考： https://arthas.aliyun.com/doc/advice-class.html 7.3 条件表达式watch命令支持在第4个参数里写条件表达式，比如： watch com.example.demo.arthas.user.UserController * returnObj &#39;params[0] &gt; 100&#39; 当访问 localhost:80/user/1时，watch命令没有输出 当访问localhost:80/user/101时，watch会打印出结果。 7.4 当异常时捕获watch命令支持-e选项，表示只捕获抛出异常时的请求： watch com.example.demo.arthas.user.UserController * &quot;&#123;params[0],throwExp&#125;&quot; -e 7.5 按照耗时进行过滤watch命令支持按请求耗时进行过滤，比如： watch com.example.demo.arthas.user.UserController * &#39;&#123;params, returnObj&#125;&#39; &#39;#cost&gt;200&#39; 8. 热更新代码下面介绍通过jad/mc/redefine 命令实现动态更新代码的功能。 目前，访问 http://localhost/user/0 ，会返回500异常： 12curl http://localhost/user/0&#123;&quot;timestamp&quot;:1550223186170,&quot;status&quot;:500,&quot;error&quot;:&quot;Internal Server Error&quot;,&quot;exception&quot;:&quot;java.lang.IllegalArgumentException&quot;,&quot;message&quot;:&quot;id &lt; 1&quot;,&quot;path&quot;:&quot;/user/0&quot;&#125; 下面通过热更新代码，修改这个逻辑。 8.1 jad反编译UserController在arthas中执行jad命令 👇 jad --source-only com.example.demo.arthas.user.UserController &gt; /tmp/UserController.java jad反编译的结果保存在 /tmp/UserController.java文件里了。 在【 机器 】上然后用vim来编辑/tmp/UserController.java： vim /tmp/UserController.java 比如当 user id 小于1时，也正常返回，不抛出异常： 123456789@GetMapping(value=&#123;&quot;/user/&#123;id&#125;&quot;&#125;)public User findUserById(@PathVariable Integer id) &#123; logger.info(&quot;id: &#123;&#125;&quot;, (Object)id); if (id != null &amp;&amp; id &lt; 1) &#123; return new User(id, &quot;name&quot; + id); // throw new IllegalArgumentException(&quot;id &lt; 1&quot;); &#125; return new User(id.intValue(), &quot;name&quot; + id);&#125; 8.2 sc查找加载UserController的ClassLoader123sc -d *UserController | grep classLoaderHash$ sc -d *UserController | grep classLoaderHash classLoaderHash 1be6f5c3 可以发现是 springbootLaunchedURLClassLoader@1be6f5c3 加载的。 记下classLoaderHash，后面需要使用它。在这里，它是 1be6f5c3。 8.3 mc编译java文件编译java文件，类似于javac。 保存好/tmp/UserController.java之后，使用mc(Memory Compiler)命令来编译，并且通过-c或者--classLoaderClass参数指定ClassLoader： 12345mc --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader /tmp/UserController.java -d /tmp$ mc --classLoaderClass org.springframework.boot.loader.LaunchedURLClassLoader /tmp/UserController.java -d /tmpMemory compiler output:/tmp/com/example/demo/arthas/user/UserController.classAffect(row-cnt:1) cost in 346 ms 也可以通过mc -c &lt;classLoaderHash&gt; /tmp/UserController.java -d /tmp，使用-c参数指定ClassLoaderHash: 1$ mc -c 1be6f5c3 /tmp/UserController.java -d /tmp 8.4 redefine加载class文件再使用redefine命令重新加载新编译好的UserController.class： 123redefine /tmp/com/example/demo/arthas/user/UserController.class$ redefine /tmp/com/example/demo/arthas/user/UserController.classredefine success, size: 1 8.5 热修改代码结果redefine成功之后，再次访问 localhost:80/user/0 ，结果是： 1234&#123; &quot;id&quot;: 0, &quot;name&quot;: &quot;name0&quot;&#125; 9. Exit/Stop9.1 resetArthas在 watch/trace 等命令时，实际上是修改了应用的字节码，插入增强的代码。显式执行 reset 命令，可以清除掉这些增强代码。 9.2 退出Arthas用 exit 或者 quit 命令可以退出Arthas。 退出Arthas之后，还可以再次用 java -jar arthas-boot.jar 来连接。 9.3 彻底退出Arthasexit/quit命令只是退出当前session，arthas server还在目标进程中运行。 想完全退出Arthas，可以执行 stop 命令。 10. arthas-boot支持的参数arthas-boot.jar 支持很多参数，可以执行 java -jar arthas-boot.jar -h 来查看。 10.1 允许外部访问默认情况下， arthas server侦听的是 127.0.0.1 这个IP，如果希望远程可以访问，可以使用--target-ip的参数。 java -jar arthas-boot.jar --target-ip 10.2 列出所有的版本java -jar arthas-boot.jar --versions 使用指定版本： java -jar arthas-boot.jar --use-version 3.1.0 10.3 只侦听Telnet端口，不侦听HTTP端口java -jar arthas-boot.jar --telnet-port 9999 --http-port -1 10.4 打印运行的详情java -jar arthas-boot.jar -v 11. Web Console","tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}],"categories":[]},{"title":"LeetCode-只出现一次的数字","date":"2021-08-19T13:12:54.000Z","path":"wiki/LeetCode-只出现一次的数字/","text":"只出现一次的数字https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x21ib6/ 给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。 说明： 你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 示例 1: 输入: [2,2,1]输出: 1示例 2: 输入: [4,1,2,1,2]输出: 4相关标签位运算 分析仅仅出现一个，很显然，这个数和前后都不一样，然后特殊判断一下头部和尾部就可以了。 代码1234567891011121314151617181920212223class Solution &#123; public int singleNumber(int[] nums) &#123; if(nums == null || nums.length == 0)&#123; return 0; &#125; if(nums.length == 1)&#123; return nums[0]; &#125; Arrays.sort(nums); for(int i = 2; i &lt; nums.length; i++)&#123; if(nums[i - 2] != nums[i - 1] &amp;&amp; nums[i - 1] != nums[i])&#123; return nums[i - 1]; &#125; if(i == nums.length - 1 &amp;&amp; nums[i] != nums[i - 1])&#123; return nums[i]; &#125; if(i == 2 &amp;&amp; nums[i - 2] != nums[i - 1])&#123; return nums[i - 2]; &#125; &#125; return 0; &#125;&#125; 分析二使用异或运算，将所有值进行异或异或运算，相异为真，相同为假，所以 a^a = 0 ;0^a = a因为异或运算 满足交换律 a^b^a = a^a^b = b 所以数组经过异或运算，单独的值就剩下了 https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x21ib6/?discussion=Mo9fKT 代码二123456789class Solution &#123; public int singleNumber(int[] nums) &#123; int reduce = 0; for (int num : nums) &#123; reduce = reduce ^ num; &#125; return reduce; &#125;&#125;","tags":[{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"位运算","slug":"位运算","permalink":"http://example.com/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}],"categories":[]},{"title":"LeetCode-存在重复元素","date":"2021-08-19T12:57:35.000Z","path":"wiki/LeetCode-存在重复元素/","text":"存在重复元素https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x248f5/ 给定一个整数数组，判断是否存在重复元素。 如果存在一值在数组中出现至少两次，函数返回 true 。如果数组中每个元素都不相同，则返回 false 。 示例 1: 输入: [1,2,3,1]输出: true示例 2: 输入: [1,2,3,4]输出: false示例 3: 输入: [1,1,1,3,3,4,3,2,4,2]输出: true 代码1、双重循环是很容易想到，但是会超出时间限制 123456789101112131415class Solution &#123; public boolean containsDuplicate(int[] nums) &#123; if(nums == null || nums.length == 0)&#123; return false; &#125; for(int i = 0; i &lt; nums.length; i++)&#123; for(int j = i + 1; j &lt; nums.length ; j++)&#123; if(nums[i] == nums[j])&#123; return true; &#125; &#125; &#125; return false; &#125;&#125; 2、先排序，然后判断相邻两个元素是否相等 1234567891011class Solution &#123; public boolean containsDuplicate(int[] nums) &#123; Arrays.sort(nums); for(int i = 1; i &lt; nums.length; i++) &#123; if (nums[i] == nums[i - 1]) &#123; return true; &#125; &#125; return false; &#125;&#125;","tags":[{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"哈希表","slug":"哈希表","permalink":"http://example.com/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"排序","slug":"排序","permalink":"http://example.com/tags/%E6%8E%92%E5%BA%8F/"}],"categories":[]},{"title":"LeetCode-买卖股票的最佳时机II","date":"2021-08-19T12:35:49.000Z","path":"wiki/LeetCode-买卖股票的最佳时机II/","text":"买卖股票的最佳时机IIhttps://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x2zsx1/ 给定一个数组 prices ，其中 prices[i] 是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: prices = [7,1,5,3,6,4]输出: 7解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。示例 2: 输入: prices = [1,2,3,4,5]输出: 4解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。示例 3: 输入: prices = [7,6,4,3,1]输出: 0解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 ⏰提示： 1 &lt;= prices.length &lt;= 3 * 1040 &lt;= prices[i] &lt;= 104 分析[7,1,5,3,6,4][1,2,3,4,5] 其实规律很简单，就比较当天和前一天的大小关系就好了，一次循环就下来了。 一开始想的比较复杂，用的指针滑动，比如1，2，3，4，5这种，一开始想的是 5 - 1 ，其实 1 + 1 + 1 + 1就行了。 代码12345678910111213141516171819202122class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices == null || prices.length == 0)&#123; return 0; &#125; if(prices.length == 1)&#123; return 0; &#125; int count = 0; int index = 0; // [7,1,5,3,6,4] // [1,2,3,4,5] for(int i = 1; i &lt; prices.length ; i++)&#123; if(prices[i] &gt; prices[i - 1])&#123; count += prices[i] - prices[i - 1]; &#125; &#125; return count; &#125;&#125;","tags":[{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"贪心","slug":"贪心","permalink":"http://example.com/tags/%E8%B4%AA%E5%BF%83/"},{"name":"动态规划","slug":"动态规划","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"categories":[]},{"title":"LeetCode-删除排序数组中的重复项","date":"2021-08-19T11:28:03.000Z","path":"wiki/LeetCode-删除排序数组中的重复项/","text":"删除排序数组中的重复项https://leetcode-cn.com/leetbook/read/top-interview-questions-easy/x2gy9m/ 给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。 不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。 说明: 为什么返回数值是整数，但输出的答案是数组呢? 请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。 你可以想象内部操作如下: 1234567// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝int len = removeDuplicates(nums);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; 示例 1： 输入：nums = [1,1,2]输出：2, nums = [1,2]解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。示例 2： 输入：nums = [0,0,1,1,1,2,2,3,3,4]输出：5, nums = [0,1,2,3,4]解释：函数应该返回新的长度 5 ， 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4 。不需要考虑数组中超出新长度后面的元素。 提示： 0 &lt;= nums.length &lt;= 3 * 104-104 &lt;= nums[i] &lt;= 104nums 已按升序排列 思路假如输入 [0,0,1,1,1,2,2,3,3,4] 最终的结果应该是[0,1,2,3,4,2,2,3,3,4] count 变量用来存储不重复数组的个数 offset 作为当前游标记录判断之后不重复的数据的位置 解法123456789101112131415161718class Solution &#123; public int removeDuplicates(int[] nums) &#123; if (nums == null || nums.length == 0)&#123; return 0; &#125; int result = 1; int offset = 1; for (int i = 1;i&lt;nums.length;i++)&#123; if (nums[i] == nums[i - 1])&#123; continue; &#125; nums[offset] = nums[i]; offset++; result++; &#125; return result; &#125;&#125;","tags":[{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"双指针","slug":"双指针","permalink":"http://example.com/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"}],"categories":[]},{"title":"MySQL中的锁🔒","date":"2021-08-19T07:17:31.000Z","path":"wiki/MySQL中的锁🔒/","text":"乐观锁和悲观锁 表锁和行锁 表锁： 开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低 表锁按照数据操作可以分成两种： 表读锁（Table Read Lock） 表写锁（Table Write Lock 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁 读写阻塞：当前用户在读数据，其他的用户不能修改当前用户读的数据，会加锁！ 写写阻塞：当前用户在修改数据，其他的用户不能修改当前用户正在修改的数据，会加锁！ 行锁： 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高 InnoDB实现了以下两种类型的行锁。 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 也叫做读锁：读锁是共享的，多个客户可以同时读取同一个资源，但不允许其他客户修改。 排他锁（X锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 也叫做写锁：写锁是排他的，写锁会阻塞其他的写锁和读锁。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁也是数据库隐式帮我们做了，不需要程序员操心！ ⚠️ InnoDB行锁和表锁都支持！MyISAM只支持表锁！ innoDB什么时候会使用到行锁？InnoDB只有通过索引条件检索数据才使用行级锁，否则，InnoDB将使用表锁 参考资料数据库两大神器【索引和锁】 基于数据表乐观锁实现分布式锁","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"MySQL性能调优","date":"2021-08-19T06:06:45.000Z","path":"wiki/MySQL性能调优/","text":"1、使用【 覆盖索引 】避免回表锁造成的时间消耗1、查询语句的时候避免使用select * 2、创建索引添加适当的列避免回表 2、使用【 联合索引 】区分度比较高的列放到前面注意联合索引的最左匹配原则 3、对索引进行函数计算或者表达式计算会导致索引失效 🔒4、利用子查询优化超多分页场景5、explain命令查询执行计划 show profile查询执行的性能消耗面试前必须知道的MySQL命令【expalin】 -3y 6、在事务开始后，事务内尽可能只操作数据库，减少锁持有时间7、尽量避免字符串查询，如果允许的话可以使用Elasticsearch8、如果优化都做了还是查询的很慢，可以做一些聚合表，线上的业务直接查聚合之后的数据9、读写瓶颈问题9.1 如果是单库的情况下，可以考虑读写分离，提升读/写的性能 9.2 主从结构下还是存在瓶颈的话，可以考虑分库分表 注意 分库分表下的id尽量保证递增","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"kafka-基础知识","date":"2021-08-18T12:05:29.000Z","path":"wiki/kafka-基础知识/","text":"kafka是什么kafka运行时架构kafka为什么能承载高并发kafka的确认机制是什么kafka如何保证消息准确kafka会丢消息吗？ kafka会重复消费消息吗 幂等性 消息顺序消费问题参考资料kafka基础知识 - yyy","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[]},{"title":"Spring-事务隔离","date":"2021-08-18T03:10:21.000Z","path":"wiki/Spring-事务隔离/","text":"参考资料https://juejin.cn/post/6854573219916021767","tags":[],"categories":[]},{"title":"Flink-物理分区函数","date":"2021-08-12T08:38:31.000Z","path":"wiki/Flink-物理分区函数/","text":"Flink提供的8种分区函数GlobalPartitioner该分区器会将所有的数据都发送到下游的某个算子实例(subtask id = 0) 12345678910111213// 数据会被分发到下游算子的第一个实例中进行处理 public static void global() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setMaxParallelism(8); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); dataStream.flatMap(new RichFlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String s, Collector&lt;String&gt; collector) throws Exception &#123; collector.collect(s + &quot;_**&quot;); &#125; &#125;).setParallelism(2).global().print(&quot;global : &quot;); env.execute(); &#125; ShufflePartitioner随机选择一个下游算子实例进行发送 12345678//数据会被随机分发到下游算子的每一个实例中进行 public static void shuffle() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); DataStream&lt;String&gt; broadcast = dataStream.shuffle(); broadcast.print(&quot;shuffle : &quot;); env.execute(); &#125; BroadcastPartitioner发送到下游所有的算子实例 12345678//广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做Jion的场景 public static void broadcast() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); DataStream&lt;String&gt; broadcast = dataStream.broadcast(); broadcast.print(&quot;broadcast : &quot;); env.execute(); &#125; RebalancePartitioner通过循环的方式依次发送到下游的task 123456789101112public static void rebalance() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setParallelism(4); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); dataStream.map(new RichMapFunction&lt;String, String&gt;() &#123; @Override public String map(String s) throws Exception &#123; return s + &quot;_**&quot;; &#125; &#125;).setParallelism(1).rebalance().print(&quot;rebalance : &quot;); env.execute(); &#125; RescalePartitioner123456789101112131415/** * 这种分区器会根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。这里有点难以理解，假设上游并行度为 2，编号为 A 和 B。下游并行度为 4，编号为 1，2，3，4。那么 A 则把数据循环发送给 1 和 2，B 则把数据循环发送给 3 和 4。假设上游并行度为 4，编号为 A，B，C，D。下游并行度为 2，编号为 1，2。那么 A 和 B 则把数据发送给 1，C 和 D 则把数据发送给 2。 */ public static void rescale() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setParallelism(4); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); dataStream.map(new RichMapFunction&lt;String, String&gt;() &#123; @Override public String map(String s) throws Exception &#123; return s + &quot;_**&quot;; &#125; &#125;).setParallelism(1).rescale().print(&quot;rescale : &quot;); env.execute(); &#125; ForwardPartitioner发送到下游对应的第一个task，保证上下游算子并行度一致，即上有算子与下游算子是1:1的关系 12345678//用于将记录输出到下游本地的算子实例。它要求上下游算子并行度一样。简单的说，ForwardPartitioner用来做数据的控制台打印。 public static void forward() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setParallelism(1); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;ggg&quot;, &quot;fff&quot;, &quot;ddd&quot;, &quot;sss&quot;, &quot;aaa&quot;, &quot;qqq&quot;, &quot;www&quot;); DataStream&lt;String&gt; broadcast = dataStream.shuffle(); broadcast.print(&quot;shuffle : &quot;); env.execute(); &#125; ⚠️ 在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner，对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常。 KeyByPartitioner根据key的分组索引选择发送到相对应的下游subtask 123456789101112public static void keyBy() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setMaxParallelism(8); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhh&quot;, &quot;hhh&quot;, &quot;hhh&quot;, &quot;hhh&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;www&quot;); dataStream.flatMap(new RichFlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String s, Collector&lt;String&gt; collector) throws Exception &#123; collector.collect(s + &quot;_**&quot;); &#125; &#125;).keyBy(String::toString).print(&quot;keyBy : &quot;); env.execute(); &#125; CustomPartitionerWrapper通过Partitioner实例的partition方法(自定义的)将记录输出到下游。 1234567891011121314151617181920212223public static void custom() throws Exception &#123; StreamExecutionEnvironment env = getEnv().setMaxParallelism(8); DataStreamSource&lt;String&gt; dataStream = env.fromElements(&quot;hhhh&quot;, &quot;hhhss&quot;, &quot;hhh&quot;, &quot;hhh&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;sss&quot;, &quot;www&quot;); dataStream.flatMap(new RichFlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String s, Collector&lt;String&gt; collector) throws Exception &#123; collector.collect(s + &quot;_**&quot;); &#125; &#125;).partitionCustom(new CustomPartitioner(),String::toString) .print(&quot;custom :&quot;); env.execute(); &#125; public static class CustomPartitioner implements Partitioner&lt;String&gt; &#123; // key: 根据key的值来分区 // numPartitions: 下游算子并行度 @Override public int partition(String key, int numPartitions) &#123; return key.length() % numPartitions;//在此处定义分区策略 &#125; &#125; Flink的八种分区策略源码解读 Apache Flink 中文文档","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-UDF函数类","date":"2021-08-12T06:36:09.000Z","path":"wiki/Flink-UDF函数类/","text":"函数类比如说我们常用的MapFunction，FilterFunction，ProcessFunction等，每一步操作都基本上都对应一个Function。 12345678910public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125;&#125; ⚠️ 简单滚动聚合函数，比如sum，max是不需要Function。 好处： 1、通用型强，可复用 2、可抽象方法，代码简洁 匿名函数不需要单独定义Function，直接在Stream的操作中直接实现，效果和上面👆的完全一样。 1234567DataStream&lt;SensorReading&gt; dataStream = unionStream.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Double(strings[1]), new Double(strings[2])); &#125;&#125;); 富函数‘‘富函数’’是DataStream API 提供的一个函数类的接口，所有Flink函数类都有其Rich版本，它与常规函数的不同在于，可以获取运行环境的上下文，并包含一些声明周期方法，所以可以实现更加复杂的功能。 RichMapFunction，RichFlatMapFunction等等 Rich Function有一个生命周期的概念，典型的生命周期方法有 👇 open() 方法是rich function的初始化方法，当一个算子比如map被调用之前被调用。 close()方法是生命周期中最后一个被调用的方法，做一些清理工作。 如果有多个分区的话，每个分区的open方法和close方法都会执行一次 getRuntimeContext()获取运行时上下文。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class RichFunctionDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; dataSource = env.socketTextStream(&quot;localhost&quot;, 9999); DataStream&lt;SensorReading&gt; dataStream = dataSource.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Long(strings[1]), new Double(strings[2])); &#125; &#125;); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; stream = dataStream.map(new MyMapFunction()); stream.print(); env.execute(); &#125; // 富函数是抽象类，这里要用继承 public static class MyMapFunction extends RichMapFunction&lt;SensorReading, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void open(Configuration parameters) throws Exception &#123; System.err.println(&quot;invoke open&quot;); // 一般定义状态，或者链接数据库操作 super.open(parameters); &#125; @Override public Tuple2&lt;String, Integer&gt; map(SensorReading sensorReading) throws Exception &#123; RuntimeContext runtimeContext = this.getRuntimeContext(); System.err.println(&quot;runtimeContext.getTaskName() : &quot; + runtimeContext.getTaskName()); return new Tuple2&lt;&gt;(sensorReading.getSersorId(), runtimeContext.getIndexOfThisSubtask()); &#125; @Override public void close() throws Exception &#123; super.close(); System.err.println(&quot;invoke close method&quot;); &#125; &#125;&#125;","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-到底支持多少种数据类型","date":"2021-08-11T11:50:41.000Z","path":"wiki/Flink-到底支持多少中数据类型/","text":"Flink支持所有的Java和Scala基础数据类型以及其包装类型 支持Tuple元组类型，Flink在Java API中定义了很多Tuple的实现类，从Tuple0 ~ Tuple25类型 Scala样例类 case class，对应Java中的POJO类对象(必须提供无参构造方法 get/set) 其他，比如 Arrays , Lists, Maps, Enums等都是支持的","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-如何读取数据源（集合｜文件｜自定义｜Kafka等）","date":"2021-08-11T11:50:11.000Z","path":"wiki/Flink-如何读取数据源（集合｜文件｜自定义｜Kafka等）/","text":"读取文件这里是以txt文件为例，实现WordCount，其他文件类型同理。 12345678910111213141516171819202122232425 public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125;&#125; 实现自定义数据源需要自己写一个类，实现SourceFunction接口的run方法和cancle方法，注意⚠️，SourceFunction的泛型类型必须要写上，不然会报错的。 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(1); DataStreamSource dataStreamSource = env.addSource(new MySourceFunction()); dataStreamSource.print(); env.execute(); &#125; // 实现自定义的source public static class MySourceFunction implements SourceFunction&lt;SensorReading&gt; &#123; // 定义标识位 控制数据产生 private boolean running = true; public void run(SourceContext ctx) throws Exception &#123; // 定义各个随机数生成器 HashMap&lt;String, Double&gt; sensorMap = new HashMap&lt;String, Double&gt;(10); for (int i = 0; i &lt; 10; i++) &#123; sensorMap.put(&quot;sensor_&quot; + (i + 1), 60 + new Random().nextGaussian() * 20); &#125; while (running) &#123; for (String sensor : sensorMap.keySet()) &#123; double newtemp = sensorMap.get(sensor) + new Random().nextGaussian(); sensorMap.put(sensor, newtemp); ctx.collect(new SensorReading(sensor, System.currentTimeMillis(), newtemp)); &#125; Thread.sleep(10000); &#125; &#125; public void cancel() &#123; running = false; &#125; &#125;","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-你所知道的算子都在这","date":"2021-08-11T11:49:20.000Z","path":"wiki/Flink-你所知道的算子都在这/","text":"好了，看到这的话，Apache Flink基础概念啥的都应该了解差不多了吧，我们几天就See一下，平时用到的StreamApi中各式各样的算子都有什么，然后，我们搞点Demo试一下。 📒 我也是边学边实现一些Demo,这样呢可以方便自己理解，形成体系以后也应该能帮到别人快速学习吧。 这就是地址了👉 https://github.com/geekibli/flink-study 欢迎star！ 下面的Demo都是可以直接运行的 如果是通过socket获取数据的话，确认现开启端口啊，我用的Mac系统，可以使用以下命令 nc -lk 9999 提供一个全局获取环境的方法我们一个静态方法getEnv(), 不然每次还要new，挺麻烦的； 123private static StreamExecutionEnvironment getEnv() &#123; return StreamExecutionEnvironment.getExecutionEnvironment();&#125; POJO类12345public class SensorReading implements Serializable &#123; private String sersorId; private double timestamp; private double newtemp;&#125; map12345678910111213141516public static void mapTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); ArrayList&lt;Integer&gt; nums = Lists.newArrayList(); nums.add(1); nums.add(2); nums.add(3); DataStreamSource&lt;Integer&gt; source = env.fromCollection(nums); SingleOutputStreamOperator&lt;Integer&gt; map = source.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer integer) throws Exception &#123; return integer * integer; &#125; &#125;); map.print(); env.execute();&#125; keyBy123456789101112131415161718192021public static void keyByTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements( new Tuple2&lt;String, Integer&gt;(&quot;age&quot;, 1), new Tuple2&lt;String, Integer&gt;(&quot;name&quot;, 2), new Tuple2&lt;String, Integer&gt;(&quot;name&quot;, 3), new Tuple2&lt;String, Integer&gt;(&quot;name&quot;, 3)); source.map( new MapFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; map(Tuple2&lt;String, Integer&gt; stringIntegerTuple2) throws Exception &#123; Integer f1 = stringIntegerTuple2.f1; stringIntegerTuple2.setField(f1 + 10, 1); return stringIntegerTuple2; &#125; &#125;) .keyBy(1) .print(); env.execute(); &#125; reduce12345678910111213141516171819public static void reduceTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); env.fromElements( Tuple2.of(2L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(2L, 4L), Tuple2.of(1L, 5L)) .keyBy(1) .reduce(new ReduceFunction&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123; @Override public Tuple2&lt;Long, Long&gt; reduce(Tuple2&lt;Long, Long&gt; longLongTuple2, Tuple2&lt;Long, Long&gt; t1) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(t1.f0, longLongTuple2.f1 + t1.f1); &#125; &#125;) .print(); env.execute();&#125; 还有一个栗子🌰 123456789101112131415161718192021222324public static void reduce() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataSource = env.socketTextStream(&quot;localhost&quot;, 9999); DataStream&lt;SensorReading&gt; dataStream = dataSource.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Long(strings[1]), new Double(strings[2])); &#125; &#125;); DataStream&lt;SensorReading&gt; sersorId = dataStream.keyBy(&quot;sersorId&quot;) .reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; @Override public SensorReading reduce(SensorReading sensorReading, SensorReading t1) throws Exception &#123; String id = t1.getSersorId(); Double time = t1.getTimestamp(); return new SensorReading(id, time, Math.max(sensorReading.getNewtemp(), t1.getNewtemp())); &#125; &#125;); sersorId.print(); env.execute();&#125; split|select12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public static void splitTest() throws Exception &#123; StreamExecutionEnvironment env = getEnv(); DataStreamSource&lt;String&gt; dataSource = env.socketTextStream(&quot;localhost&quot;, 9999); DataStream&lt;SensorReading&gt; dataStream = dataSource.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Double(strings[1]), new Double(strings[2])); &#125; &#125;); SplitStream&lt;SensorReading&gt; split = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading value) &#123; return (value.getNewtemp() &gt; 30) ? Collections.singleton(&quot;high&quot;) : Collections.singleton(&quot;low&quot;); &#125; &#125;); DataStream&lt;SensorReading&gt; low = split.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; high = split.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; all = split.select(&quot;high&quot;, &quot;low&quot;); // connect DataStream&lt;Tuple2&lt;String, Double&gt;&gt; highStream = high.map(new MapFunction&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(SensorReading sensorReading) throws Exception &#123; return new Tuple2&lt;&gt;(sensorReading.getSersorId(), sensorReading.getNewtemp()); &#125; &#125;); // 链接之后的stream ConnectedStreams&lt;Tuple2&lt;String, Double&gt;, SensorReading&gt; connect = highStream.connect(low); SingleOutputStreamOperator&lt;Object&gt; resultStream = connect.map(new CoMapFunction&lt;Tuple2&lt;String, Double&gt;, SensorReading, Object&gt;() &#123; @Override public Object map1(Tuple2&lt;String, Double&gt; stringDoubleTuple2) throws Exception &#123; return new Tuple3&lt;&gt;(stringDoubleTuple2.f0, stringDoubleTuple2.f0, &quot;high temp warning&quot;); &#125; @Override public Object map2(SensorReading sensorReading) throws Exception &#123; return new Tuple2&lt;&gt;(sensorReading.getSersorId(), &quot;normal temp&quot;); &#125; &#125;); resultStream.print(); env.execute(); &#125; connect | coMap如上split方法下面我们是有操作connect的api的 union12345678910111213141516171819public static void unionTest() throws Exception &#123; // 必须是数据类型相同 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; firstStream = env.socketTextStream(&quot;localhost&quot;, 9999); DataStreamSource&lt;String&gt; secondStream = env.socketTextStream(&quot;localhost&quot;, 7777); DataStream&lt;String&gt; unionStream = firstStream.union(secondStream); DataStream&lt;SensorReading&gt; dataStream = unionStream.map(new MapFunction&lt;String, SensorReading&gt;() &#123; @Override public SensorReading map(String s) throws Exception &#123; String[] strings = s.split(&quot;,&quot;); return new SensorReading(strings[0], new Double(strings[1]), new Double(strings[2])); &#125; &#125;); dataStream.print(); env.execute();&#125; // TODO 不断学习 不断补充","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"Flink-核心之Windows窗口","date":"2021-08-10T08:55:40.000Z","path":"wiki/Flink-核心之Windows窗口/","text":"什么是窗口 窗口分配器有几种 窗口如何实现增量计算和全量计算","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[]},{"title":"设计模式之美-门面模式","date":"2021-08-07T12:27:47.000Z","path":"wiki/设计模式之美-门面模式/","text":"门面模式门面模式又叫做外观模式，提供统一的一个接口，用来访问子系统中的一群接口；门面模式定义了一个高层接口，让子系统更容易使用；门面模式属于结构型模式； 类图 Facade 门面类12345678910111213141516public class Facade &#123; // 继承各个子系统功能，进行封装，一定程度上不遵循单一职责原则 SubSystemA subSystemA = new SubSystemA(); SubSystemB subSystemB = new SubSystemB(); SubSystemC subSystemC = new SubSystemC(); public void doA()&#123; subSystemA.doA(); &#125; public void doB()&#123; subSystemB.doB(); &#125; public void doC()&#123; subSystemC.doC(); &#125;&#125; 子系统A123public class SubSystemA &#123; public void doA()&#123;&#125;&#125; 子系统B123public class SubSystemB &#123; public void doB()&#123;&#125;&#125; 子系统C123public class SubSystemC &#123; public void doC()&#123;&#125;&#125; 以上Facade类集成了三个子系统的类，在自己定义的方法中，并不是Facade自己实现的逻辑，而是调用了对应子系统的方法，这种实现方式叫做门面模式；是不是很简单； 看到这是不是有点似曾相识呢，没错，我们天天都在写的Controller,Service,Dao不就是门面模式吗，没错，只不过把这种方式形成方法论，也就有了所谓的门面模式！ 举个栗子一些商业博客会有一个功能，就是发表文章或者评论点赞会获得一些积分啊，虚拟币啊,然后会有积分商城，在里面可以免费的兑换商品，其实很难凑的够积分，不够费劲的… 好了，结合伪代码来体验门面模式👇👇👇： 下面是一些演示所需要的类： 关系图如下 PaymentService 支付服务123456public class PaymentService &#123; public boolean pay(GiftInfo giftInfo)&#123; System.out.println(&quot;扣减&quot; + giftInfo.getName() + &quot;积分成功！&quot;); return true; &#125;&#125; QualityService 库存服务123456public class QualityService &#123; public boolean isAvailable(GiftInfo giftInfo)&#123; System.out.println(&quot;校验&quot; + giftInfo.getName() + &quot;积分通过，库存充足！&quot;); return true; &#125;&#125; ShipService 物流服务123456public class ShipService &#123; public String doShip(GiftInfo giftInfo)&#123; System.out.println(giftInfo.getName() + &quot;生成物流订单&quot;); return String.valueOf(System.currentTimeMillis()); &#125;&#125; 客户端 非门面模式写法12345678910111213141516171819public static void main(String[] args) &#123; QualityService qualityService = new QualityService(); PaymentService paymentService = new PaymentService(); ShipService shipService = new ShipService(); GiftInfo giftInfo = new GiftInfo(&quot; 《Java编程思想》 &quot;); if (!qualityService.isAvailable(giftInfo))&#123; System.err.println(&quot;Quality not enough!&quot;); &#125; if (!paymentService.pay(giftInfo))&#123; System.err.println(&quot;Pay error!&quot;); &#125; String shipNo = shipService.doShip(giftInfo); System.err.println(&quot;Order shipNo:&quot; + shipNo);&#125; 这种写法会将库存，支付和物流等服务都暴露给调用方，是很不安全的，而且造成客户端依赖严重，代码臃肿； 门面模式写法123456public static void main(String[] args) &#123; FacadeService facadeService = new FacadeService(); GiftInfo giftInfo = new GiftInfo(&quot; 《Java编程思想》 &quot;); String shipNo = facadeService.doOrder(giftInfo); System.err.println(&quot;Order shipNo:&quot; + shipNo); &#125; 上面这种就是门面模式的写法👆 ， 相信大家应该很熟悉吧，这样的话，暴露给客户端就一个订单服务就可以了！ 应用场景 子系统越来越复杂，增加门面模式提供简单的接口，给用户使用； 构建多层的系统接口，利用门面对象作为每层的入口，简化层之间的调用 应用Spring JdbcUtilsMybatis configurationTomcat requestFacade responseFacade 优点 简化了调用过程，无需深入了解子系统，以防止给子系统带来风险 根据上面礼品兑换的逻辑，用户根本不care你底层的兑换逻辑，什么库存啊，支付状态啊，生成订单逻辑等等，对于用户来说，我只需要一步下单即可； 减少系统依赖，松耦合 这一点也是相对客户端来说，客户端只关心的的订单服务就好了，其他的库存，供应链等都不关系； 更好的划分访问层次，提高了安全性 合理的划分层次，减少底层系统的暴露，仅仅暴露一些必要的状态和接口，这一点大家应该都知道的，像service层调用Dao层，而不能在service层直接访问数据库； 遵循迪米特法则 缺点 当子系统的功能需要扩展或者修改的时候，上层封装可能要面临修改的风险，这样增加了后期的维护成本，也不遵循开闭原则 可能会违背单一职责原则 门面模式和代理模式的区别简单来说，门面模式就是一种代理模式，是属于静态代理的模式；但是和静态代理又有一些区别，门面模式的侧重点在于对底层的封装，而静态代理则终于对代理对象的增强，除了调用受委托对象的方法之外，可以扩展额外的功能；很多时候会把门面模式注入成单例，比如一些全局的Util,还有我们常见的一些Controller等等；","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-适配器模式","date":"2021-08-07T12:23:59.000Z","path":"wiki/设计模式之美-适配器模式/","text":"适配器模式适配器模式（Adapter Pattern）又叫做变压器模式，它的功能是将一个类的接口变成客户端所期望的另一个接口，从而使得原本因接口不匹配而导致无法在一起工作的两个类能够在一起工作，属于结构性设计模式的一种； 在软件开发的过程中，基本上任何问题都可以通过一个中间层解决。适配器模式其实就是一个中间层，适配器模式起着转化/委托的作用，将一种接口转化为两一种服务功能或需求的接口。 下面举个例子来分析一下👇 类图 如图是一个适配器模式类图所在👆 代码实现一个文件上传的功能，我们有很多中选择，亚马逊的AWS,阿里的OSS等等吧，但是不同的厂商有自己的标准或者API,但是在我们系统中体现的就是一个putObject方法，所以需要定义一个CloudSDK标准，然后不同的厂商来适配我们自己的标准，在我们的putObject和三方的SDK中间加一层适配层；对于客户端来说，仅仅是完成文件上传的动作，至于你服务端到底使用亚马逊的服务还是阿里的服务，它是不care的，这也体现的策略模式； 所以这个例子兼备工厂模式+策略模式+适配器模式； CloudController12345678910111213141516171819public class CloudController &#123; // 如果是Spring项目这里直接注入就可以了，没必要这么麻烦 public CloudController(CloudService cloudService) &#123; this.cloudService = cloudService; &#125; private CloudService cloudService; public void uploadFile(String fileName)&#123; System.out.println(&quot;invoke upload file service!&quot; + fileName); cloudService.uploadFile(fileName); &#125; public static void main(String[] args) &#123; CloudController cloudController = new CloudController(new CloudService(&quot;ali&quot;)); cloudController.uploadFile(&quot;think in java&quot;); &#125;&#125; CloudService1234567891011121314public class CloudService &#123; private CloudSDK cloudSDK; public CloudService(String cloudStorage) &#123; // 使用工厂来创建具体的SDK this.cloudSDK = CloudFactory.create(cloudStorage); &#125; public void uploadFile(String fileName) &#123; cloudSDK.putObject(fileName); &#125;&#125; CloudSDK 定义SDK标准123public interface CloudSDK &#123; void putObject(String fileName);&#125; AWSSDKAdapter 适配CloudSDK标准123456789101112public class AWSSDKAdapter implements CloudSDK &#123; private AWSSDK awssdk; public AWSSDKAdapter() &#123; this.awssdk = new AWSSDK(); &#125; @Override public void putObject(String fileName) &#123; awssdk.putObject(fileName); &#125;&#125; AliSDKAdapter适配CloudSDK标准1234567891011121314public class AliSDKAdapter implements CloudSDK &#123; private AliSDK aliSDK; public AliSDKAdapter() &#123; this.aliSDK = new AliSDK(); &#125; // 这个方法就是适配器的标准 不管你的三方服务需要多少接口，是要实现云上传，统一通过putObject这个接口实现就可以 @Override public void putObject(String fileName) &#123; aliSDK.setBucket(); aliSDK.uploadFile(fileName); &#125;&#125; AWSSDK 三方SDK123456public class AWSSDK &#123; // 一般以JAR的方式引入到项目中，我们也不可能去修改三方的SDK,这是各个厂商制定的自己的标准 public void putObject(String fileName)&#123; System.out.println(&quot;aws upload file &quot; + fileName); &#125;&#125; AliSDK 三方SDK12345678910public class AliSDK &#123; // 一般以JAR的方式引入到项目中，我们也不可能去修改三方的SDK,这是各个厂商制定的自己的标准 public void setBucket() &#123; System.out.println(&quot;Ali oss set bucket!&quot;); &#125; public void uploadFile(String fileName) &#123; System.out.println(&quot;Ali oss upload file!&quot; + fileName); &#125;&#125; CloudFactory12345678910111213141516public class CloudFactory &#123; // 这里是写死了 生产环境中可以通过配置文件的方式或者启动加载等等方式实现，有很多 private static final Map&lt;String, CloudSDK&gt; sdkMap = new HashMap&lt;&gt;(); // 总之要符合开闭原则 对修改关闭 static &#123; sdkMap.put(&quot;ali&quot;, new AliSDKAdapter()); sdkMap.put(&quot;aws&quot;, new AWSSDKAdapter()); &#125; // 通过不同的策略生成具体的SDK实例 public static CloudSDK create(String storage) &#123; return sdkMap.get(storage); &#125;&#125; 适配器和装饰器模式的区别适配器模式和装饰器模式都是包装器模式，装饰器模式其实是一种特殊的代理模式； 对比维度 适配器模式 装饰器模式 形式 没有层级关系 一种非常特别的代理模式，具有层级关系 定义 适配器和被适配者没有必然的联系，通常采用继承或代理的方式进行包装 装饰器和被装饰者都实现同一个接口，主要目的是扩展之后依旧保留OOP关系 关系 没有满足has-a的关系 满足is-a关系 功能 注重兼容、转换 注重覆盖、扩展 设计 后置考虑 前置考虑 适配器功能的优点 ✅ 能提高类的透明性和复用，但现有的类复用不需要改变 适配器类和原角色解耦，投稿程序的扩展性 在很多业务中符合开闭原则 适配器的缺点 适配器编写过程需要结合业务场景综合考虑，可能会增加系统的复杂性 增加代码阅读难度，降低代码可读性，过多的适配器会使得系统代码变得紊乱","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-责任链模式","date":"2021-08-07T12:21:01.000Z","path":"wiki/设计模式之美-责任链模式/","text":"责任链模式责任链模式（Chain of Responsibility Patter）是将链中的每一个节点看成是一个对象，每个节点处理的请求均不相同，并且内部自动维护下一个节点对象。当一个请求从链的头部发出时，会沿着链的路径一次传递给每一个节点对象，直到有节点处理这个请求为止；责任链模式属于行为型模式 类图 Handler 责任链抽象类 链式结构12345678910public abstract class Handler &#123; protected Handler nextHandler; public void setNextHandler(Handler nextHandler) &#123; this.nextHandler = nextHandler; &#125; public abstract void handleRequest(String request);&#125; ConcreteHandlerA 节点对象A1234567891011121314public class ConcreteHandlerA extends Handler &#123; @Override public void handleRequest(String request) &#123; if (&quot;reqA&quot;.equals(request)) &#123; System.err.println(this.getClass().getSimpleName() + &quot;deal this request &quot; + request); return; &#125; if (this.nextHandler != null) &#123; this.nextHandler.handleRequest(request); &#125; &#125;&#125; ConcreteHandlerB 节点对象B123456789101112public class ConcreteHandlerB extends Handler &#123; @Override public void handleRequest(String request) &#123; if (&quot;reqB&quot;.equals(request)) &#123; System.err.println(this.getClass().getSimpleName() + &quot;deal this request &quot; + request); return; &#125; if (this.nextHandler != null) &#123; this.nextHandler.handleRequest(request); &#125; &#125;&#125; DemoTest 测试类12345678910public class DemoTest &#123; public static void main(String[] args) &#123; Handler handlerA = new ConcreteHandlerA(); Handler handlerB = new ConcreteHandlerB(); handlerA.setNextHandler(handlerB); handlerA.handleRequest(&quot;reqB&quot;); &#125;&#125;// 输出结果：//ConcreteHandlerBdeal this request reqB 业务下面我们举一个业务场景的例子🌰来展示一下什么是责任链模式：我们登录时，肯定要校验用户名和密码有没有传参，如果参数都是空的话，那也没有必要执行后面的登录部分，直接返回了；如果参数是合法的，那我们就要校验数据库是否存在这个用户了，如果不存在该用户，那就直接返回；如果存在，那就要校验用户有没有权限访问正在请求的资源，如果有权限，则可以正常访问资源，如果没有权限，则抛出异常； 非责任链模式写法12345678910111213141516// 以下是伪代码public void login(String userName,String pwd)&#123; if (StringUtils.isEmpty(userName) || StringUtils.isEmpty(pwd))&#123; throw new IllegalArgumentException(); &#125; Member member = selectUser(userName,pwd); if (Objects.isNull(member))&#123; throw new IllegalArgumentException(); &#125; if (!validateRoot(member))&#123; throw new IllegalArgumentException(); &#125; System.err.println(&quot;Login success!&quot;); &#125; 这种写法虽然简单，但是将所有的操作全部杂糅在一起了，结构是十分混乱的；下面👇看一下如果使用责任链模式该如何实现： Handler 抽象责任链处理器12345678910public abstract class Handler &#123; protected Handler next; public void next(Handler next)&#123; this.next = next; &#125; public abstract void doHandler(Member member);&#125; ValidateHandler 参数校验处理器1234567891011public class ValidateHandler extends Handler&#123; @Override public void doHandler(Member member) &#123; if (StringUtils.isEmpty(member.getLoginName()) || StringUtils.isEmpty(member.getLoginPass()))&#123; System.err.println(&quot;登录名或者密码为空！&quot;); return; &#125; System.err.println(&quot;参数校验成功！&quot;); next.doHandler(member); &#125;&#125; 登录处理器12345678public class LoginHandler extends Handler &#123; @Override public void doHandler(Member member) &#123; System.err.println(&quot;login success!&quot;); member.setRoleName(&quot;root&quot;); next.doHandler(member); &#125;&#125; 鉴权处理器12345678910public class AuthHandler extends Handler&#123; @Override public void doHandler(Member member) &#123; if (&quot;root&quot;.equals(member.getRoleName()))&#123; System.err.println(&quot;Auth success!&quot;); return; &#125; System.err.println(&quot;Auth failed!&quot;); &#125;&#125; Member成员类12345678910111213public class Member &#123; private String loginName; private String loginPass; private String roleName; public Member(String loginName, String loginPass) &#123; this.loginName = loginName; this.loginPass = loginPass; &#125; // GETTER SETTER ....&#125; DemoTest 测试类123456789101112public class DemoTest &#123; // 首先创建所有的处理器，设置后它们的先后顺序，设置next节点，完成之后在执行链路操作； public static void main(String[] args) &#123; ValidateHandler validateHandler = new ValidateHandler(); LoginHandler loginHandler = new LoginHandler(); validateHandler.next(loginHandler); AuthHandler authHandler = new AuthHandler(); loginHandler.next(authHandler); Member member = new Member(&quot;xiaoming&quot;, &quot;222&quot;); validateHandler.doHandler(member); &#125;&#125; 上面的写法，看起来有没有比非责任链写法要B格高一些，但是，这样每次创建节点对象，好像又显得不太爽，有些臃肿的意思！而且调整节点之间的顺序时也比较复杂，容易改错！ 那我们就进一步优化以下吧！ 责任链模式+建造者模式 优化添加建造器builder12345678910111213141516171819202122232425262728293031public abstract class Handler&lt;T&gt; &#123; protected Handler next; public void next(Handler next) &#123; this.next = next; &#125; public abstract void doHandler(Member member); // 暴露建造器，将节点对象以链表的形式串起来 public static class Builder&lt;T&gt; &#123; private Handler&lt;T&gt; head; private Handler&lt;T&gt; tail; public Handler&lt;T&gt; build() &#123; return this.head; &#125; public Builder&lt;T&gt; addHandler(Handler&lt;T&gt; handler) &#123; if (this.head == null) &#123; this.head = this.tail = handler; return this; &#125; this.tail.next(handler); this.tail = handler; return this; &#125; &#125;&#125; 测试123456789public static void main(String[] args) &#123; Member member = new Member(&quot;xiaoming&quot;, null); Handler.Builder&lt;Handler&gt; builder = new Handler.Builder(); builder.addHandler(new ValidateHandler()) .addHandler(new LoginHandler()) .addHandler(new AuthHandler()) .build() .doHandler(member); &#125; 通过添加建造器之后，责任链的组装与调用是不是显得很清晰，每个节点对象各司其职，如果需要本节点执行，则执行，如果不是，则交给下一个节点继续！ 责任链适用的场景 多个对象处理同一个请求，但具体由那个对象处理则在运动时动态决定 在不明确🈯指定接受者的情况下，向多个对象的一个提交请求 可以动态指定一组对象处理请求 举一些实际的例子：javax.servlet.Filter的doFilter方法就是使用的责任链模式; org.springframework.web.filter.CompositeFilter#doFilter123public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; (new CompositeFilter.VirtualFilterChain(chain, this.filters)).doFilter(request, response); &#125; 这是Spring框架中的一个实现(javax.servlet.Filter),下面展开具体的代码： org.springframework.web.filter.CompositeFilter12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class CompositeFilter implements Filter &#123; //Filter 这里的Filter相当于处理节点，存放在List中，和我们上面的Handler存放在建造者的链式结构中，异曲同工。 private List&lt;? extends Filter&gt; filters = new ArrayList(); public CompositeFilter() &#123; &#125; public void setFilters(List&lt;? extends Filter&gt; filters) &#123; this.filters = new ArrayList(filters); &#125; public void init(FilterConfig config) throws ServletException &#123; Iterator var2 = this.filters.iterator(); while(var2.hasNext()) &#123; Filter filter = (Filter)var2.next(); filter.init(config); &#125; &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; (new CompositeFilter.VirtualFilterChain(chain, this.filters)).doFilter(request, response); &#125; public void destroy() &#123; int i = this.filters.size(); while(i-- &gt; 0) &#123; Filter filter = (Filter)this.filters.get(i); filter.destroy(); &#125; &#125; private static class VirtualFilterChain implements FilterChain &#123; private final FilterChain originalChain; private final List&lt;? extends Filter&gt; additionalFilters; private int currentPosition = 0; public VirtualFilterChain(FilterChain chain, List&lt;? extends Filter&gt; additionalFilters) &#123; this.originalChain = chain; this.additionalFilters = additionalFilters; &#125; public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException &#123; if (this.currentPosition == this.additionalFilters.size()) &#123; this.originalChain.doFilter(request, response); // 通过currentPosition的移动，转移到链路上的不同处理节点，这就是责任链模式的体现 &#125; else &#123; ++this.currentPosition; Filter nextFilter = (Filter)this.additionalFilters.get(this.currentPosition - 1); nextFilter.doFilter(request, response, this); &#125; &#125; &#125;&#125; 像上面的例子还有很多很多，比如Spring Security和Shiro中的拦截器就是很典型的责任链模式，还有Netty中的 io.netty.channel.ChannelPipeline等等，都是责任链模式； 责任链模式的优点👍👍👍 将请求与处理解耦 请求处理者（节点对象）只需要关注自己感兴趣的请求进行处理即可，对于不感兴趣的请求，直接转发给下一个节点对象 具备链式传递处理请求功能，请求发送者无需知晓链路结构，只需等待处理请求结果 链路结构灵活，可以通过改变链路结果动态的新增和删除责任 易于扩展新的请求节点（符合开闭原则） 责任链模式的缺点 责任链太长或者处理时间太长，导致系统性能下降 如果节点对象存在循环♻️引用时，会造成死循环，导致系统崩溃，所以这个在设计的时候一定要注意不能形成闭环⚠️⚠️⚠️","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-观察者模式","date":"2021-08-07T12:18:40.000Z","path":"wiki/设计模式之美-观察者模式/","text":"观察者模式观察者模式指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。 属于行为型模式 类图 ISubject12345678public interface ISubject&lt;E&gt; &#123; boolean attach(IObserver&lt;E&gt; observer); boolean detach(IObserver&lt;E&gt; observer); void notify(E event);&#125; ConcreteSubject12345678910111213141516171819202122232425public class ConcreteSubject&lt;E&gt; implements ISubject&lt;E&gt; &#123; private List&lt;IObserver&lt;E&gt;&gt; observers = new ArrayList&lt;&gt;(); @Override public boolean attach(IObserver&lt;E&gt; observer) &#123; if (this.observers.contains(observer)) &#123; return false; &#125; observers.add(observer); return true; &#125; @Override public boolean detach(IObserver&lt;E&gt; observer) &#123; return this.observers.remove(observer); &#125; @Override public void notify(E event) &#123; for (IObserver observer : observers) &#123; observer.update(event); &#125; &#125;&#125; IObserver1234public interface IObserver&lt;E&gt; &#123; void update(E event);&#125; ConcreteObserver123456public class ConcreteObserver&lt;E&gt; implements IObserver&lt;E&gt; &#123; @Override public void update(E event) &#123; System.err.println(&quot;receive event &quot; + event); &#125;&#125; DemoTest12345678910public class DemoTest &#123; public static void main(String[] args) &#123; ISubject&lt;String&gt; subject = new ConcreteSubject&lt;String&gt;(); subject.attach(new ConcreteObserver&lt;String&gt;()); subject.attach(new ConcreteObserver&lt;String&gt;()); subject.attach(new ConcreteObserver&lt;String&gt;()); subject.notify(&quot;test&quot;); &#125;&#125; JDK实现观察者模式 比如有一个博客系统提供了问题社区，一个人提出问题，会有其他人收到这个消息；我们使用JDK来模拟一个简单的场景； Blog 被观察对象12345678910111213141516171819public class Blog extends Observable &#123; private Blog() &#123; &#125; private static final Blog blog = new Blog(); private String name = &quot;IBLi Blog&quot;; public static Blog getInstance() &#123; return blog; &#125; public void question(Question question) &#123; System.out.println(question.getUserName() + &quot; 在 blog 提交了问题 ： &quot; + question.getContent()); setChanged(); notifyObservers(question); &#125;&#125; Question1234567891011121314151617181920212223242526public class Question &#123; private String userName; private String content; public Question(String userName, String content) &#123; this.userName = userName; this.content = content; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125;&#125; Student 学生类 被通知对象123456789101112131415161718192021222324252627282930313233public class Student implements Observer &#123; private String name; public Student(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; /** * This method is called whenever the observed object is changed. An * application calls an &lt;tt&gt;Observable&lt;/tt&gt; object&#x27;s * &lt;code&gt;notifyObservers&lt;/code&gt; method to have all the object&#x27;s * observers notified of the change. * * @param o the observable object. * @param arg an argument passed to the &lt;code&gt;notifyObservers&lt;/code&gt; */ @Override public void update(Observable o, Object arg) &#123; Blog blog = (Blog) o; Question question = (Question) arg; System.out.println(&quot;******************&quot;); System.out.println(this.name + &quot; 看到了 &quot; + question.getUserName() + &quot; 的问题 \\n&quot; + &quot;问题是： &quot; + question.getContent()); &#125;&#125; Client 测试1234567891011public class Client &#123; public static void main(String[] args) &#123; Blog blog = Blog.getInstance(); blog.addObserver(new Student(&quot;Tom&quot;)); blog.addObserver(new Student(&quot;John&quot;)); Question question = new Question(&quot;小明&quot;,&quot;观察者模式是什么？&quot;); blog.question(question); &#125;&#125; 测试结果1234567小明 在 blog 提交了问题 ： 观察者模式是什么？******************John看到了 小明 的问题 问题是： 观察者模式是什么？******************Tom看到了 小明 的问题 问题是： 观察者模式是什么？ 基于Guava实现观察者模式Student 被观察对象123456789101112131415161718192021222324252627public class Student &#123; private String name; public Student(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; @Subscribe public void Observer(Student str) &#123; System.err.println(&quot;student invoke method + &quot; + str.toString()); &#125;&#125; Teacher 被通知对象12345678910111213141516171819202122232425262728public class Teacher &#123; private String name; public Teacher(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Teacher&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; @Subscribe public void Observer(Teacher str) &#123; System.err.println(&quot;teacher invoke method + &quot; + str.toString()); &#125;&#125; Client 测试类123456789101112public class Client &#123; public static void main(String[] args) &#123; EventBus eventBus = new EventBus(); Student student = new Student(&quot;Ibli&quot;); Teacher teacher = new Teacher(&quot;laoshi&quot;); eventBus.register(student); eventBus.register(teacher); eventBus.post(new Student(&quot;xuesheng&quot;)); eventBus.post(new Teacher(&quot;teacher&quot;)); &#125;&#125; 测试结果12student invoke method + Student&#123;name=&#x27;xuesheng&#x27;&#125;teacher invoke method + Teacher&#123;name=&#x27;teacher&#x27;&#125; 观察者模式使用场景1、当一个抽象模型包含两个方面内容，其中一个方面依赖另一个方面2、其他一个或多个对象的变化依赖另一个对象的变化3、实现类似广播机制的功能，无需知道具体收听者，只需要广播。系统中感兴趣的对象会自动接口该广播4、多层级嵌套机制，形成一种链式出发机制，是的时间具备跨域（跨越两种观察者类型）通知 观察者模式优点✅1、降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。2、目标与观察者之间建立了一套触发机制。 观察者模式缺点目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-装饰器模式","date":"2021-08-07T12:15:15.000Z","path":"wiki/设计模式之美-装饰器模式/","text":"装饰器模式（Decorator Pattern）装饰器模式也叫做包装模式，是指在不改变原有对象的基础上，将功能附加到对象上，提供比继承更有弹性的替代方案（扩展原有对象的功能），强调一点是基于已有对象的功能增强；装饰器模式属于结构性模式; 装饰器类图 Component 抽象构件 定义一个抽象接口以规范准备接受附加责任的对象 123public abstract class Component &#123; public abstract void operation();&#125; ConcreteComponent 具体的构件 实现抽象构件,通过装饰角色为其添加一些职责 123456public class ConcreteComponent extends Component&#123; @Override public void operation() &#123; System.err.println(&quot;Do some biz event!&quot;); &#125;&#125; Decorator 抽象装饰角色 继承抽象构件,并包含具体构件的实例,可以通过其子类扩展具体构件的功能 123456789101112131415public abstract class Decorator extends Component&#123; //持有组件对象 protected Component component; public Decorator(Component component) &#123; this.component = component; &#125; @Override public void operation() &#123; //转发请求给组件对象,可以在转发前后执行一些附加动作 component.operation(); &#125;&#125; ConcreteDecoratorA 实现抽象装饰的相关方法,并给具体构件对象添加附加的责任 123456789101112131415161718192021public class ConcreteDecoratorA extends Decorator &#123; public ConcreteDecoratorA(Component component) &#123; super(component); &#125; public void operationFirst() &#123; System.err.println(&quot;ConcreteDecoratorA first!&quot;); &#125; public void operationLast() &#123; System.err.println(&quot;ConcreteDecoratorA last!&quot;); &#125; @Override public void operation() &#123; operationFirst(); super.operation(); operationLast(); &#125;&#125; Client 测试类1234567public class Client &#123; public static void main(String[] args) &#123; Component component = new ConcreteComponent(); Decorator decoratorA = new ConcreteDecoratorA(component); decoratorA.operation(); &#125;&#125; 举个例子买煎饼的时候,我们可以直接买一个煎饼,但有的时候觉得味道单一或者吃不饱,我们可以加一些东西,比如烤肠,鸡蛋,鸡排等等; 先看看非装饰器模式的写法 BatterCake 就一个普通的煎饼类； 123456public class BatterCake &#123; protected String getMsg()&#123;return &quot;煎饼&quot;;&#125; public int getPrice()&#123; return 10; &#125;&#125; BatterCakeWithEgg 添加一个鸡蛋的煎饼； 123456789public class BatterCakeWithEgg extends BatterCake&#123; protected String getMsg()&#123; return super.getMsg() + &quot; 加一个鸡蛋&quot;; &#125; public int getPrice()&#123; return super.getPrice() + 1; &#125;&#125; BatterCakeWithEggAndSauage 加一个鸡蛋 再加一跟烤肠的煎饼； 123456789public class BatterCakeWithEggAndSauage extends BatterCakeWithEgg&#123; protected String getMsg()&#123; return super.getMsg() + &quot; 加一个烤肠&quot;; &#125; public int getPrice()&#123; return super.getPrice() + 2; &#125;&#125; Client 客户123456789101112public class Client &#123; public static void main(String[] args) &#123; BatterCake batterCake = new BatterCake(); System.err.println(batterCake.getMsg() + &quot;价格 &quot; + batterCake.getPrice()); BatterCakeWithEgg batterCakeWithEgg = new BatterCakeWithEgg(); System.err.println(batterCakeWithEgg.getMsg() + &quot;价格 &quot; + batterCakeWithEgg.getPrice()); BatterCakeWithEggAndSauage batterCakeWithEggAndSauage = new BatterCakeWithEggAndSauage(); System.err.println(batterCakeWithEggAndSauage.getMsg() + &quot;价格 &quot; + batterCakeWithEggAndSauage.getPrice()); &#125;&#125; 看到这种写法，应该能看出它的弊端了，就是实现很简单，适合于需求固定的业务和多样性比较简单的业务；一旦客户需要一个鸡蛋，两根烤肠，那是不是还要在BatterCakeWithEggAndSauage类上继续扩展呢，这其实是很low的设计；而且非常不灵活，比如客户只需要一跟烤肠的煎饼。这个时候怎么解决呢； 使用装饰器模式优化 BatterCake 抽象组件1234public abstract class BatterCake &#123; protected abstract String getMsg(); protected abstract int getPrice();&#125; 基础类 实现抽象接口1234567891011public class BaseBatterCake extends BatterCake&#123; @Override protected String getMsg() &#123; return &quot;煎饼&quot;; &#125; @Override protected int getPrice() &#123; return 5; &#125;&#125; BatterCakeDecorator 装饰器12345678910111213141516171819public class BatterCakeDecorator extends BatterCake&#123; private BatterCake batterCake; public BatterCakeDecorator(BatterCake batterCake) &#123; this.batterCake = batterCake; &#125; @Override protected String getMsg() &#123; return this.batterCake.getMsg(); &#125; @Override protected int getPrice() &#123; return this.batterCake.getPrice(); &#125;&#125; EggDecorator 具体的装饰器对象12345678910111213141516public class EggDecorator extends BatterCakeDecorator&#123; public EggDecorator(BatterCake batterCake) &#123; super(batterCake); &#125; @Override protected String getMsg() &#123; return super.getMsg() + &quot; 加一个鸡蛋&quot;; &#125; @Override protected int getPrice() &#123; return super.getPrice() + 1; &#125;&#125; SauageDecorator 具体的装饰对象12345678910111213141516public class SauageDecorator extends BatterCakeDecorator&#123; public SauageDecorator(BatterCake batterCake) &#123; super(batterCake); &#125; @Override protected String getMsg() &#123; return super.getMsg() + &quot; 加一个烤肠&quot;; &#125; @Override protected int getPrice() &#123; return super.getPrice() + 2; &#125;&#125; 客户 测试类123456789101112131415161718public class Client &#123; public static void main(String[] args) &#123; BatterCake batterCake = new BaseBatterCake(); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); batterCake = new EggDecorator(batterCake); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); batterCake = new SauageDecorator(batterCake); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); batterCake = new EggDecorator(batterCake); System.err.println(batterCake.getMsg() + &quot;总计 : &quot; + batterCake.getPrice()); &#125;&#125; 上面这种写法是运用了装饰器模式的写法，这样会增加程序的灵活性，EggDecorator返回一个BatterCake，在EggDecorator中的getMsg方法和getPrice方法中添加关于Egg的逻辑来实现对BatterCake的增强，同理SauageDecorator也是，在它自己的getMsg方法和getPrice方法中添加自己的逻辑，当然，都是基于调用super方法的基础上添加自己的逻辑，同时具体的装饰对象返回父类类型的对象； 1234@Override protected int getPrice() &#123; return super.getPrice() + 2; &#125; 这里进行一下总结：1、具体装饰对象(EggDecorator)一定是继承自装饰组件(BatterCakeDecorator)2、为了实现对象增强，子类中的方法一定是基于super方法的基础上，添加自己的逻辑的 使用场景 用于扩展一个类的功能或给一个类添加附加职责 动态地给一个对象添加功能,这些功能可以在动态的撤销 实际应用Spring TransactionAwareCacheManagerJDK FileInputStream 装饰器模式与代理模式 装饰器模式是一种特殊的代理模式 装饰器模式强调自身的功能扩展,透明的,动态的扩展与增强 透明指的是功能的扩展由客户端控制 代理模式强调代理过程的控制 装饰器模式的优点1、装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用2、通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果3、装饰器模式完全遵守开闭原则 装饰器模式的缺点1、增加了一些子类，系统代码会显得臃肿。2、组合方式容易出错，代码可读性比较差。 参考文档1、包装模式就是这么简单啦2、装饰器模式（装饰设计模式）详解3、java中的装饰设计模式，浅谈与继承之间的区别4、JDK IO中的适配器模式和装饰者模式","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-策略模式","date":"2021-08-07T12:09:10.000Z","path":"wiki/设计模式之美-策略模式/","text":"策略模式策略模式（Strategy Pattern）也叫做政策模式（Policy Pattern）,它是将定义的算法封装起来，让它们之间可以相互替换，从而让算法的变化不影响到使用算法的用户；它属于行为型模式。可以在一定程度上规避if-else/switch等策略模式使用的面向对象的继承和多态机制，从而实现同一行为在不同的场景下具备不同的实现； 应用策略模式在实际应用场景中有很多的应用，凡是设计到选择的场景基本都可以使用策略模式来实现；比如购买一个商品选择支付方式，是选择银行卡还是快捷支付，微信？支付宝等； 假如一个系统中有很多类，而它们的区别仅仅在与它们的行为不同； 一个系统需要动态的从几种算法中选择一种；一些平台型产品中肯定会用到的 需要屏蔽算法规则； 策略模式类图 通过上图可以看到，策略模式主要包含3个角色： 上下文角色（Context）: 用来操作策略的上下文环境，屏蔽高层模块（客户端）对策略/算法的直接访问，封装可能存在的变化 抽象策略角色（IStrategy）: 规定策略或算法的行为 具体的策略角色（ConcreteStrategy）: 具体的策略逻辑或算法 这里的上下文角色仅仅是一个称谓，大家只要知道他在策略模式中的作用就可以了，就是桥接客户端和策略/方法的作用； 举个例子 背景： 电商场景下，用户购买一个商品，支付时可以选择一种优惠策略，如果没有优惠，则使用默认的策略，即无任何优惠； 1、上面两层是定义的一个营销策略的接口，然后提供不同的策略来实现；2、第三层是策略生成所使用的工厂和客户端调用策略中间的上下文，承接上下，桥接模式；3、最下层的Demo可以看成是客户端； 定义策略接口 制定标准方法123public interface IPromotionStrategy &#123; void doPromote(); &#125; CashBackStrategy 现金折返策略123456public class CashBackStrategy implements IPromotionStrategy&#123; @Override public void doPromote() &#123; System.out.println(&quot;直接返现&quot;); &#125;&#125; CouponStrategy 优惠券策略1234567public class CouponStrategy implements IPromotionStrategy&#123; // 优惠券策略 @Override public void doPromote() &#123; System.out.println(&quot;使用优惠券抵扣&quot;); &#125;&#125; GroupBuyStrategy 团购优惠策略123456public class GroupBuyStrategy implements IPromotionStrategy&#123; @Override public void doPromote() &#123; System.out.println(&quot;团购 5人成团&quot;); &#125;&#125; EmptyStrategy 默认策略123456public class EmptyStrategy implements IPromotionStrategy&#123; @Override public void doPromote() &#123; System.out.println(&quot;无任何优惠&quot;); &#125;&#125; PromoteActivity 客户端和具体算法之间的上下文，用于桥接123456789101112public class PromoteActivity &#123; private IPromotionStrategy promotionStrategy; public PromoteActivity(IPromotionStrategy promotionStrategy) &#123; this.promotionStrategy = promotionStrategy; &#125; public void execute()&#123; promotionStrategy.doPromote(); &#125;&#125; PromoteStrategyFactory 策略工厂123456789101112131415161718192021222324252627282930313233343536public class PromoteStrategyFactory &#123; private PromoteStrategyFactory() &#123; &#125; // 策略容器 private static Map&lt;String, IPromotionStrategy&gt; promotionStrategyMap = new HashMap&lt;&gt;(); // 默认空策略-没有任何优惠 private static IPromotionStrategy emptyStrategy = new EmptyStrategy(); // 这个可以放到配置文件或者放到数据库，在项目启动的时候加载到服务中就可以了 static &#123; promotionStrategyMap.put(PromoteKey.CASH_BACK, new CashBackStrategy()); promotionStrategyMap.put(PromoteKey.COUPON, new CouponStrategy()); promotionStrategyMap.put(PromoteKey.CROUP_BUY, new GroupBuyStrategy()); &#125; // 简单工厂创建营销策略 public static IPromotionStrategy createPromoteStrategy(String key) &#123; IPromotionStrategy promotionStrategy = promotionStrategyMap.get(key); if (promotionStrategy == null) &#123; return emptyStrategy; &#125; return promotionStrategy; &#125; // 所有定义好的策略 private interface PromoteKey &#123; String COUPON = &quot;coupon&quot;; String CROUP_BUY = &quot;groupBuy&quot;; String CASH_BACK = &quot;cashBack&quot;; &#125; // 因为策略模式需要用户知道所有可用的策略，所以这个方法暴露给客户端 public static Set&lt;String&gt; getPromoteKeySet() &#123; return promotionStrategyMap.keySet(); &#125;&#125; 策略模式下客户端写法 12345public static void main(String[] args) &#123; String strategy = PromoteStrategyFactory.getPromoteKeySet().stream().findAny().get(); IPromotionStrategy promoteStrategy = PromoteStrategyFactory.createPromoteStrategy(strategy); promoteStrategy.doPromote();&#125; 非策略模式下客户端写法1234567891011121314public static void main(String[] args) &#123; String strategy = &quot;客户选择的策略&quot;; IPromotionStrategy promotionStrategy; if (&quot;团购&quot;.equals(strategy))&#123; promotionStrategy = new GroupBuyStrategy(); &#125;else if (&quot;优惠券&quot;.equals(strategy))&#123; promotionStrategy = new CouponStrategy(); &#125;else if (&quot;现金折返&quot;.equals(strategy))&#123; promotionStrategy = new CashBackStrategy(); &#125;else &#123; promotionStrategy = new EmptyStrategy(); &#125; promotionStrategy.doPromote();&#125; 以上的对比，哪种方式比较优雅，立见高下了吧😄😄😄 策略模式的优点 策略模式是符合开闭原则的 避免使用多重条件转移语句 if-else语句、switch语句等 使用策略模式可以提高算法的保密性和安全性（客户端通过上下文组建来调用具体的算法-桥接） 策略模式的缺点 客户端必须要知道全部可用的策略，然后由用户决定使用那个策略，决定权在于客户 代码中会产生很多的策略类，增加代码量和系统的维护难度 👤个人体会1、结合业务场景来设计，体会设计模式的思路，学设计模式最重要的是思想；2、设计模式一般在框架中体现的比较多，大家可以多学习一些框架源码的设计理念，如果你是初级，可能一下子理解不了，慢慢体会吧，当你工作一段时间之后，见过一些工业生产的项目，看过一些优秀的框架源码的时候，会有一个 “柳暗花明又一村”的阶段；3、举个例子，Controller我们都写过的，一个项目中的接口路径是唯一的，项目启动的时候，由Spring加载到容器中，当由请求进来时是，Spring是根据path来返回具体的控制器，也就是Controller具体的方法；4、还有Comparator比较器，不同的容器内部做数据排序的时候由不同的实现，这也是策略模式的体现；5、本文中的例子虽然简单，但是包含了简单工厂模式，桥接模式，策略模式。任何一种设计模式都难以独立存在，这个大家要注意；","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-桥接模式","date":"2021-08-07T11:59:32.000Z","path":"wiki/设计模式之美-桥接模式/","text":"桥接模式（Bridge Pattern）桥接模式也称为桥梁模式，接口（Integer）模式或者柄体（Handle and Body）模式，是将抽象部分和它的具体实现部分分离，是它们都可以独立地变化； 通过组合的方式建立两个类之间的联系，而不是继承； 桥接模式属于结构型模式； 继承一般来作为多继承的备用方案； 桥接模式的结构 桥接模式主要包括一下几个角色： 1、抽象角色（Abstraction）： 定义抽象类，并包含一个对实现化对象的引用，正是这个引用，起着桥梁性的作用；2、扩展抽象化（Refined Abstraction）角色： Abstraction123456789101112public abstract class Abstraction &#123; protected IImplementor iImplementor; public Abstraction(IImplementor iImplementor) &#123; this.iImplementor = iImplementor; &#125; public void operation()&#123; this.iImplementor.operation(); &#125;&#125; IImplementor123public interface IImplementor &#123; void operation();&#125; ConcreteImplementA1234567public class ConcreteImplementA implements IImplementor&#123; @Override public void operation() &#123; &#125;&#125; ConcreteImplementB1234567public class ConcreteImplementB implements IImplementor&#123; @Override public void operation() &#123; &#125;&#125; RefinedAbstraction1234567891011public class RefinedAbstraction extends Abstraction&#123; public RefinedAbstraction(IImplementor iImplementor) &#123; super(iImplementor); &#125; @Override public void operation() &#123; super.operation(); &#125;&#125; 举个例子 AbstractMessage123456789101112public abstract class AbstractMessage &#123; private IMessage message; public AbstractMessage(IMessage message) &#123; this.message = message; &#125; void sendMsg(String msg,String toUser)&#123; this.message.send(msg,toUser); &#125;&#125; IMessage123public interface IMessage &#123; void send(String msg,String toUser);&#125; SmsMessage123456public class SmsMessage implements IMessage&#123; @Override public void send(String msg, String toUser) &#123; System.err.println(&quot;使用短信消息发送 &quot; + msg + &quot; 发送给 &quot; + toUser); &#125;&#125; EmailMessage123456public class EmailMessage implements IMessage&#123; @Override public void send(String msg, String toUser) &#123; System.err.println(&quot;使用邮件消息发送 &quot; + msg + &quot; 发送给 &quot; + toUser); &#125;&#125; NormalMessage1234567891011public class NormalMessage extends AbstractMessage&#123; public NormalMessage(IMessage message) &#123; super(message); &#125; @Override void sendMsg(String msg, String toUser) &#123; super.sendMsg(msg, toUser); &#125;&#125; UegencyMessage12345678910public class UegencyMessage extends AbstractMessage&#123; void sendMsg(String msg,String toUser)&#123; msg = &quot;[ 加急]&quot; + msg; super.sendMsg(msg,toUser); &#125; public UegencyMessage(IMessage message) &#123; super(message); &#125;&#125; DemoTest123456789101112public class DemoTest &#123; public static void main(String[] args) &#123; IMessage message = new SmsMessage(); AbstractMessage abstractMessage = new NormalMessage(message); abstractMessage.sendMsg(&quot;加班申请&quot;,&quot;老大&quot;); message = new EmailMessage(); abstractMessage = new UegencyMessage(message); abstractMessage.sendMsg(&quot;调休申请&quot;,&quot;老总&quot;); &#125;&#125; 12使用短信消息发送 加班申请 发送给 老大使用邮件消息发送 [ 加急]调休申请 发送给 老总 桥接模式的适用场景1、在抽象和具体实现之间需要增加更多的灵活性的场景；2、一个类在两个或者多个独立变化的维度，而这两个或多个维度都需要独立的进行拓展3、不希望使用继承，或者因为多层继承导致系统类的个数剧增； 桥接模式的优点1、遵循软件设计原则，分离抽象部分和具体实现部分2、提高了系统的扩展性3、符合开闭原则4、符合合成复用原则 不使用继承而使用组合 桥接模式1、增加了系统代码可读性和复杂性2、需要正确识别交接的不同维度","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-建造者模式","date":"2021-08-07T11:55:36.000Z","path":"wiki/设计模式之美-建造者模式/","text":"建造者模式 建造者模式（Builder Pattern）将一个复杂的对象的构建过程与它的表示，使得同样的构建过程可以创建出不同的表示。 建造者模式属于创建型模式； 对于用户而言，使用建造者模式只需要指定需要创建的类型就可以获取对象，创建的过程以及细节不需要了解，根据建造者模式的定义，可以简单的理解为两层含义：1、构建与表示分离；构建代表对象创建，表示代表对象行为，方法，也就是将对象的创建与行为分离；（对应到Java代码，其实就是使用接口规范行为，然后由具体的实现进行构建）2、创建不同的表示；也就是具备同样的行为，但是却由于构建的行为顺序不同或其他原因可以创建出不同的表示； Course1234567891011@Data@Getter@Setterpublic class Course &#123; private String name; private String ppt; private String video; private String note; private String homework; &#125; CourseBuilder123456789101112131415161718192021222324252627282930public class CourseBuilder &#123; private Course course = new Course(); public CourseBuilder addName(String name)&#123; course.setName(name); return this; &#125; public CourseBuilder addHomework(String homework)&#123; course.setHomework(homework); return this; &#125; public CourseBuilder addVideo(String video)&#123; course.setVideo(video); return this; &#125; public CourseBuilder addPpt(String ppt)&#123; course.setPpt(ppt); return this; &#125; public CourseBuilder addNote(String note)&#123; course.setNote(note); return this; &#125; public Course builder()&#123; return course; &#125; &#125; DomoTest 测试123456789public class DomoTest &#123; public static void main(String[] args) &#123; CourseBuilder courseBuilder = new CourseBuilder(); courseBuilder.addHomework(&quot;课后作业111&quot;) .addName(&quot;设计模式&quot;) .addNote(&quot;课堂笔记&quot;); System.out.println(&quot;courseBuilder = &quot; + courseBuilder.builder()); &#125;&#125; 输出结果： 1courseBuilder = Course(name=设计模式, ppt=null, video=null, note=课堂笔记, homework=课后作业111) 建造者在JDK中的应用StringBuilder 123public final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125; java.lang.StringBuilder.append(java.lang.CharSequence)12345@Overridepublic StringBuilder append(CharSequence s) &#123; super.append(s); return this;&#125; 这里的StringBuilder就是一个实现构造器，只不过它的上级还有一个抽象的构造器AbstractStringBuilder;看到这源码是不是和我们上面举的例子类似呢，这就是JDK源码中很典型的建造者模式的应用；处理上面的StringBuilder之外，还有像Mybatis框架中的CacheBuilder缓存构造器，还有像SqlSessionFactory装载时的openSession方法都是建造者模式； 建造者模式的使用场景 适用于创建对象需要很多步骤，但是步骤的顺序不一定是固定的 如果一个对象有非常复杂的内部结构（有很多的成员变量或属性） 把复杂对象的创建和使用分离 建造者模式的优点 封装行很好，使得创建过程和使用分离开 扩展性好，建造类之间独立，一定程度上解耦 便于控制细节，建造者可以对创建过程逐步细化，而不对其他模块产生任何影响 建造者模式的缺点 产生了对于的Builder对象，造成了类的冗余 如果产品内部发生变化，建造者都要修改，维护成本比较大；不适合经常变动的对象，这样也是不符合开闭原则的 建造者模式和工厂模式的对比 建造者模式更加注重方法的调用顺序，工厂模式注重于对象的创建 创建对象的力度不同，建造者模式创建复杂的对象，由各种复杂的组件组成，工厂模式创建出来对象的都一样 关注点不同，工厂模式只需要吧对象创建出来就可以了，而建造者模式中不仅要创建出这个对象，还要知道这个对象由那些组件组成 建造者模式根据建造过程中的顺序不一样，最终的对象不见组成也不一样，对象的每个部件的设置都是很灵活的","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-工厂模式","date":"2021-08-07T11:40:56.000Z","path":"wiki/设计模式之美-工厂模式/","text":"1. 简单工厂模式简单工厂模式是指有一个工厂对象决定创建出哪一个产品类的实例；属于创建型模式，但它不属于GOF 23种设计模式。 UML类图 组成要素1、一个抽象产品类2、具体产品类3、一个工厂 肥宅喜爱的各种快乐水（产品接口） 123public interface Kls &#123; String name();&#125; 肥宅快乐水-可乐（具体产品） 123456public class Coke implements Kls &#123; @Override public String name() &#123; return &quot;肥宅快乐水-可乐&quot;; &#125;&#125; 快乐水-雪碧（具体产品） 123456public class Sprite implements Kls &#123; @Override public String name() &#123; return &quot;快乐水-雪碧&quot;; &#125;&#125; 快乐水生产工厂(工厂类) 1234567891011121314public class KlsFactory &#123; public static Kls getFzs(String type) throws Exception &#123; Kls fzs = null; if (&quot;coke&quot;.equalsIgnoreCase(type)) &#123; fzs = new Coke(); &#125; else if (&quot;sprite&quot;.equalsIgnoreCase(type)) &#123; fzs = new Sprite(); &#125; if (Objects.isNull(fzs)) &#123; throw new RuntimeException(&quot;没找到快乐水~&quot;); &#125; return fzs; &#125;&#125; 客户端使用 12345678910111213public class Fz &#123; @Test public void drink() throws Exception &#123; // 制造可乐 Kls coke = KlsFactory.getFzs(&quot;coke&quot;); System.out.println(&quot;肥宅开始喝：&quot; + coke.name()); // 制造雪碧 Kls sprite = KlsFactory.getFzs(&quot;sprite&quot;); System.out.println(&quot;肥宅开始喝：&quot; + sprite.name()); &#125;&#125; 1.1 优点 对客户端隐藏的具体的实现，客户端只需要知道要创建什么对象即可，不用关心对象是怎么创建的 解耦，客户端不需要通过new来创建对象，如果后期产品类需要修改，则只需要修改工厂类即可，不用整个项目遍地去寻找哪里有new 1.2 缺点 由一个专门的工厂负责生产，如果业务变得复杂，这个类将变得十分臃肿 工厂类生产什么产品都是写死在工厂中的，如果增加新的产品，还要修改工厂类的生产逻辑 2. 工厂方法模式工厂方法模式（Factory Method）是简单工厂的仅一步深化， 在工厂方法模式中，我们不再提供一个统一的工厂类来创建所有的对象，而是针对不同的对象提供不同的工厂。也就是说每个对象都有一个与之对应的工厂。 组成要素1、一个抽象产品类2、多个具体产品类3、一个抽象工厂4、多个具体工厂 - 每一个具体产品对应一个具体工厂5、符合 - OCP开放封闭原则 接着上面快乐水的例子。将 快乐水工厂 （KlsFactory） 抽象出共有方法，再分别实现具体的快乐水生产工厂。 快乐水总工厂 12345678public interface Factory &#123; /** * 制造快乐水 * * @return Kls */ Kls create();&#125; 可乐工厂 123456public class CokeFactory implements Factory &#123; @Override public Kls create() &#123; return new Coke(); &#125;&#125; 雪碧工厂 123456public class SpriteFactory implements Factory &#123; @Override public Kls create() &#123; return new Sprite(); &#125;&#125; 肥宅(客户端) 1234567891011121314public class Fz &#123; @Test public void drink() throws Exception &#123; // 制造可乐 CokeFactory cokeFactory = new CokeFactory(); Kls coke = cokeFactory.create(); System.out.println(&quot;肥宅开始喝：&quot; + coke.name()); // 制造雪碧 SpriteFactory spriteFactory = new SpriteFactory(); Kls sprite = spriteFactory.create(); System.out.println(&quot;肥宅开始喝：&quot; + sprite.name()); &#125;&#125; 扩展芬达快乐水 123456public class Fanta implements Kls &#123; @Override public String name() &#123; return &quot;快乐水-芬达&quot;; &#125;&#125; 复制代码芬达工厂 123456public class FantaFactory implements Factory &#123; @Override public Kls create() &#123; return new Fanta(); &#125;&#125; 肥宅使用添加 123FantaFactory fantaFactory = new FantaFactory();Kls fanta = fantaFactory.create();System.out.println(&quot;肥宅开始喝：&quot; + fanta.name()); 2.1 优点1、降低了代码耦合度，对象的生成交给子类去完成（这里的耦合度是相对于简单工厂模式的工厂类比较的）2、降低了代码耦合度，对象的生成交给子类去完成 2.2 缺点1、增加了代码量，每个具体产品都需要一个具体工厂（在具体的业务中可能会产生大量的重复代码）2、当增加抽象产品 也就是添加一个其他产品族 需要修改工厂 违背OCP 3. 抽象工厂模式 组成要素1、多个抽象产品类2、具体产品类3、抽象工厂类 - 声明(一组)返回抽象产品的方法4、具体工厂类 - 生成(一组)具体产品 一个抽象产品类和两个具体的产品（可乐） 1234567891011121314151617public abstract class Coke &#123; public abstract void doCreate();&#125;public class LocalCoke extends Coke&#123; @Override public void doCreate() &#123; System.err.println(&quot;生产本土可乐&quot;); &#125;&#125;public class ForeignCoke extends Coke&#123; @Override public void doCreate() &#123; System.out.println(&quot;生产外国可乐&quot;); &#125;&#125; 一个抽象产品类和两个具体的产品（雪碧） 1234567891011121314151617public abstract class Sprite &#123; public abstract void doCreate();&#125;public class LocalSprite extends Sprite&#123; @Override public void doCreate() &#123; System.out.println(&quot;生产本地雪碧&quot;); &#125;&#125;public class ForeignSprite extends Sprite&#123; @Override public void doCreate() &#123; System.err.println(&quot;生产外国雪碧&quot;); &#125;&#125; 一个抽象工厂和两个具体的工厂 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public interface IAbstractFactory &#123; /** * 成产可乐 * * @return 声明(一组)返回抽象产品的方法 */ Coke createCoke(); /** * 生产雪碧 * * @return 声明(一组)返回抽象产品的方法 */ Sprite createSprite();&#125;public class LocalFactory implements IAbstractFactory&#123; /** * 成产可乐 * * @return 生成(一组)具体产品 */ @Override public LocalCoke createCoke() &#123; return new LocalCoke(); &#125; /** * 生产雪碧 * * @return 生成(一组)具体产品 */ @Override public LocalSprite createSprite() &#123; return new LocalSprite(); &#125;&#125;public class ForeignFactory implements IAbstractFactory&#123; /** * 成产可乐 * * @return 生成(一组)具体产品 */ @Override public ForeignCoke createCoke() &#123; return new ForeignCoke(); &#125; /** * 生产雪碧 * * @return 生成(一组)具体产品 */ @Override public ForeignSprite createSprite() &#123; return new ForeignSprite(); &#125;&#125; 3.1 抽象工厂的使用1234567891011public class FactoryTest &#123; public static void main(String[] args) &#123; LocalFactory localFactory = new LocalFactory(); LocalCoke localCoke = localFactory.createCoke(); localCoke.doCreate(); ForeignFactory foreignFactory = new ForeignFactory(); ForeignSprite foreignSprite = foreignFactory.createSprite(); foreignSprite.doCreate(); &#125;&#125; 3.2 优点1、代码解耦2、很好的满足OCP开放封闭原则3、抽象工厂模式中我们可以定义实现不止一个接口，一个工厂也可以生成不止一个产品类 对于复杂对象的生产相当灵活易扩展(相对于工厂方法模式的优化) 3.3 缺点1、扩展产品是需要修改所有工厂，违反违反OCP原则2、整体实现也比较复杂 参考文章 设计模式-简单工厂、工厂方法模式、抽象工厂模式 设计模式 —— 工厂模式 还有多少个十年 继续做热血少年","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-原型模式","date":"2021-08-07T11:31:43.000Z","path":"wiki/设计模式之美-原型模式/","text":"原型模式原型模式（Prototype Pattern）指原型实例指定创建对象的种类，并且通过复制这些原型创建新的对象，属于创建型模式； 原型模式的核心在于复制原型对象。以系统中已存在的一个对象为原型，直接基于内存二进制流进行复制，不需要再精力耗时的对象初始化过程（不调用构造函数），性能提升很多。当对象的构造过程比较耗时时，可以把当前系统已存在的对象作为原型，对其进行复制（一般是基于二进制流的复制），躲避初始化过程，使得新对象的创建时间大大缩短； 原型模式类图 IPrototype 定义克隆的方法 类似于JDK自带的Cloneable123public interface IPrototype&lt;T&gt; &#123; T clone();&#125; ConcretePrototype 具体的要克隆的对象1234567891011121314151617181920212223242526272829303132333435363738public class ConcretePrototype implements IPrototype &#123; private int age; private String name; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; // 可能我们平时都这么去复制对象 @Override public ConcretePrototype clone() &#123; ConcretePrototype concretePrototype = new ConcretePrototype(); concretePrototype.setAge(this.age); concretePrototype.setName(this.name); return concretePrototype; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; Client 客户端 测试123456789101112public static void main(String[] args) &#123; //创建原型对象 ConcretePrototype prototype = new ConcretePrototype(); prototype.setAge(18); prototype.setName(&quot;Tom&quot;); System.out.println(prototype); //拷贝原型对象 ConcretePrototype cloneType = prototype.clone(); System.out.println(cloneType); System.err.println(cloneType == prototype); &#125; 运行结果： 123ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;&#125;ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;&#125;false 实现JDK Cloneable的克隆对象写法1234567891011121314151617181920212223242526272829303132333435363738394041public class ConcretePrototype1 implements Cloneable &#123; private int age; private String name; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public ConcretePrototype1 clone() &#123; Object clone = null; try &#123; clone = super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); System.err.println(&quot;Clone Error!&quot;); &#125; return (ConcretePrototype1) clone; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 输出结果同上；以上的两个属性都是基本数据类型和String，并没有引用类型，下面我们添加一个引用类型的属性测试以下👇👇 ConcretePrototype添加一个List类型属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class ConcretePrototype implements IPrototype &#123; private int age; private String name; private List&lt;String&gt; hobbies; public List&lt;String&gt; getHobbies() &#123; return hobbies; &#125; public void setHobbies(List&lt;String&gt; hobbies) &#123; this.hobbies = hobbies; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public ConcretePrototype clone() &#123; ConcretePrototype concretePrototype = new ConcretePrototype(); concretePrototype.setAge(this.age); concretePrototype.setName(this.name); concretePrototype.setHobbies(this.hobbies); return concretePrototype; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, hobbies=&quot; + hobbies + &#x27;&#125;&#x27;; &#125;&#125; SETTER方法实现浅克隆 浅克隆也叫浅拷贝：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 123456789101112131415161718192021222324public static void main(String[] args) &#123; //创建原型对象 ConcretePrototype1 prototype = new ConcretePrototype1(); prototype.setAge(18); prototype.setName(&quot;Tom&quot;); ArrayList&lt;String&gt; strings = Lists.newArrayList(&quot;数学&quot;, &quot;英语&quot;); prototype.setHobbies(strings); System.out.println(prototype); //拷贝原型对象 ConcretePrototype1 cloneType = prototype.clone(); cloneType.getHobbies().add(&quot;语文&quot;); System.out.println(cloneType); System.out.println(prototype); System.err.println(cloneType == prototype); System.err.println(cloneType.getHobbies() == prototype.getHobbies()); &#125;输出结果：ConcretePrototype1&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语]&#125;ConcretePrototype1&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语, 语文]&#125;ConcretePrototype1&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语, 语文]&#125;false// 这里为什么结果是true看到上面画的图应该可以看懂吧，set方法其实是将list的引用设置过去，并不是创建一个新的list再赋值！true 通过内存字节流”克隆”对象，实现深克隆 深克隆也叫深拷贝：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class ConcretePrototypeDeep implements Cloneable,Serializable &#123; private int age; private String name; private List&lt;String&gt; hobbies; public List&lt;String&gt; getHobbies() &#123; return hobbies; &#125; public void setHobbies(List&lt;String&gt; hobbies) &#123; this.hobbies = hobbies; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; // 基于内存字节流生成对象 public ConcretePrototypeDeep deepClone() &#123; try &#123; //将当前对象信息写到内存 ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream); objectOutputStream.writeObject(this); //从内存中读取对象信息，强制转换成当前类型 ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray()); ObjectInputStream objectInputStream = new ObjectInputStream(byteArrayInputStream); return (ConcretePrototypeDeep) objectInputStream.readObject(); &#125; catch (IOException ioException) &#123; ioException.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public String toString() &#123; return &quot;ConcretePrototype&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, hobbies=&quot; + hobbies + &#x27;&#125;&#x27;; &#125;&#125; 输出结果： 12345ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语]&#125;ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语, 语文]&#125;ConcretePrototype&#123;age=18, name=&#x27;Tom&#x27;, hobbies=[数学, 英语]&#125;falsefalse 在Java语言中，如果需要实现深克隆，可以通过覆盖Object类的clone()方法实现，也可以通过序列化(Serialization)等方式来实现。 要实现深拷贝，必须实现Cloneable接口，去重写clone方法，否则会抛出CloneNotSupportedException异常，但是如果对象中包含很多引用类型的属性，这样去覆盖clone方法其实是很麻烦的，可以优先使用序列化的方式实现！ 克隆破坏单例模式如果我们克隆的目标的对象是单例对象，这便意味着，深克隆会破坏单例。解决以上问题的思路： 禁止深克隆 在单例对象的getInstance方法，返回当前对象，而不是去新创建一个对象或者通过内存字节流等方法生成对象 原型模式在Java中的应用ArrayList底层是基于数组结果的，它的动态扩容过程是创建一个新的数组，并把数组中的元素拷贝过去，用新的数组来继续存放元素；在创建新数组的过程中便使用了原型模式。 java.util.ArrayList.clone1234567891011public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // this shouldn&#x27;t happen, since we are Cloneable throw new InternalError(e); &#125;&#125; java.util.Arrays.copyOf(T[], int)123public static &lt;T&gt; T[] copyOf(T[] original, int newLength) &#123; return (T[]) copyOf(original, newLength, original.getClass()); &#125; java.util.Arrays.copyOf(U[], int, java.lang.Class&lt;? extends T[]&gt;)123456789public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; &#125; 最终还是System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength));这个方法来完成数组的拷贝！像我们使用的 com.alibaba.fastjson.JSON#parseObject org.springframework.beans.BeanUtils#copyProperties(java.lang.Object, java.lang.Object)` 等都是原型模式； 原型模式适用场景 类初始化消耗资源过多 new一个对象需要很多繁琐的过程（数据准备，访问权限等） 构造函数比较复杂 循环体内产生大量对象时 原型模式的优点 Java自带的原型模式是基于内存二进制流的复制，在性能上比直接创建一个对象更加优良 可以使用深克隆的方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用，可辅助实现撤销操作。 原型模式的缺点 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-单例模式","date":"2021-08-07T11:22:11.000Z","path":"wiki/设计模式之美-单例模式/","text":"单例模式概念单例模式：指一个类在任何情况下都绝对只有一个实例，并提供一个全局访问点（getInstance方法）。大概实现就是隐藏其构造方法，单例模式属于创建型模式。一些实际的应用场景比如，DBpool, ServletContext,ServletConfig等 单例模式写法饿汉式单例在单例类首次加载时创建实例； 123456789public class HungrySingleton &#123; private static final HungrySingleton hungrySingleton = new HungrySingleton(); private HungrySingleton()&#123;&#125; public static HungrySingleton getInstance()&#123; return hungrySingleton; &#125;&#125; 优点执行效率高，没有加任何锁 缺点类加载的时候就初始化，在某些情况下，可能会造成内存浪费；如果出现类的数量很多的时候，会初始化很多类，占用大量内存； 局限性Spring就不能使用，Spring启动的时候，会有大量的类加载。 饿汉式的第二种写法 1234567891011121314public class HungryStaticSingleton &#123; private static final HungryStaticSingleton hungrySingleton ; static &#123; hungrySingleton = new HungryStaticSingleton(); &#125; private HungryStaticSingleton()&#123;&#125; public static HungryStaticSingleton getInstance()&#123; return hungrySingleton; &#125;&#125; 区别仅仅实在与类加载的顺序不同。👇 懒汉式单例被外部类调用时才创建实例； 123456789101112131415public class LazySingleton &#123; private static LazySingleton instance = null; private LazySingleton() &#123; &#125; public static LazySingleton getInstance() &#123; if (instance == null) &#123; instance = new LazySingleton(); &#125; return instance; &#125;&#125; 优点节省了内存 缺点不能保证真实单例，线程不安全 线程不安全的原因 后面的线程覆盖掉前面线程创建的实例 同时进入判断条件，按顺序返回，没有覆盖的时候，就返回实例 解决方法： 123456public static synchorized LazySingleton getInstance() &#123; if (instance == null) &#123; instance = new LazySingleton(); &#125; return instance;&#125; 但是getInstance方法添加上锁之后，性能下降，如果有很多请求访问，除了获得锁的线程之外，其他线程都要等待。 如何优化？ 12345678910111213141516public class LazyDclSingleton &#123; private static LazyDclSingleton instance = null; private LazyDclSingleton() &#123; &#125; // 后面的线程覆盖掉前面线程创建的实例 public static LazyDclSingleton getInstance() &#123; if (instance == null) &#123; synchronized (LazyDclSingleton.class) &#123; instance = new LazyDclSingleton(); &#125; &#125; return instance; &#125;&#125; 双重检查锁1234567891011121314151617181920public class LazyDclSingleton &#123; private static volatile LazyDclSingleton instance = null; private LazyDclSingleton() &#123; &#125; public static LazyDclSingleton getInstance() &#123; // 检查是否要阻塞 if (instance == null) &#123; synchronized (LazyDclSingleton.class) &#123; // 检查是否要创建实例 if (instance == null) &#123; instance = new LazyDclSingleton(); &#125; &#125; &#125; return instance; &#125;&#125; 局限性会出现指令重排序的问题，有可能返回一个不完整的实例解决方案：private static volatile LazyDclSingleton instance = null;（volatile禁止指令重排序） 优点性能高，能保证线程安全 缺点代码可读性查，不够美观，代码不够优雅 静态内部类写法12345678910111213141516171819/** * 静态内部类 * 静态内部类在使用时才进行构建 * classPath: ../LazyInnerClassSingleton.class * ../LazyInnerClassSingleton$LazyHolder.class * * 优点：写法优雅，利用了java语言语法，性能也高，避免内存的浪费 * 缺点：能够被反射破坏 */public class LazyInnerClassSingleton &#123; private LazyInnerClassSingleton()&#123;&#125; public static LazyInnerClassSingleton getInstance()&#123; return LazyHolder.instance; &#125; private static class LazyHolder&#123; private static final LazyInnerClassSingleton instance = new LazyInnerClassSingleton(); &#125;&#125; 优点1、写法优雅，利用了java语言语法2、性能也高3、避免内存的浪费 缺点1、能够被反射破坏单例 12345678910111213/** * 反射破坏单例模式 */public class ReflectTest &#123; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Class&lt;?&gt; clazz = LazyInnerClassSingleton.class; Constructor&lt;?&gt; declaredConstructor = clazz.getDeclaredConstructor(null); declaredConstructor.setAccessible(true); Object instance = declaredConstructor.newInstance(); System.err.println(instance); &#125;&#125;// 打印结果 com.ibli.javaBase.pattern.singleton.LazyInnerClassSingleton@38af3868 解决办法： 在构造器中添加一个判断，如果实例已经创建，则直接抛出异常终止创建； 12345private LazyInnerClassSingleton()&#123; if (LazyHolder.instance != null)&#123; throw new IllegalArgumentException(); &#125; &#125; 注册式单例 将每一个实例都缓存到一个容器中，使用唯一标志获取实例 枚举写法123456789101112131415161718public enum EnumSingleton &#123; INSTANCE; private Object object; public Object getObject() &#123; return object; &#125; public void setObject(Object object) &#123; this.object = object; &#125; public static EnumSingleton getInstance()&#123; return INSTANCE; &#125;&#125; 使用与测试 123456789101112131415public class EnumSingletonTest &#123; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; EnumSingleton enumSingleton = EnumSingleton.getInstance(); enumSingleton.setObject(new Object()); // 尝试使用反射破坏 Class&lt;?&gt; clazz = EnumSingleton.class; Constructor&lt;?&gt; declaredConstructor = clazz.getDeclaredConstructor(String.class,int.class); System.err.println(declaredConstructor); declaredConstructor.setAccessible(true); Object object = declaredConstructor.newInstance(); System.err.println(object); &#125;&#125; 测试结果： 1234private com.ibli.javaBase.pattern.singleton.EnumSingleton(java.lang.String,int)Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Cannot reflectively create enum objects at java.lang.reflect.Constructor.newInstance(Constructor.java:417) at com.ibli.javaBase.pattern.singleton.EnumSingletonTest.main(EnumSingletonTest.java:17) 原因在JDK底层源码中已经做了限制 12345678910111213141516171819202122@CallerSensitive public T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, null, modifiers); &#125; &#125; // 不能创建枚举类型 if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException(&quot;Cannot reflectively create enum objects&quot;); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) &#123; ca = acquireConstructorAccessor(); &#125; @SuppressWarnings(&quot;unchecked&quot;) T inst = (T) ca.newInstance(initargs); return inst; &#125; 优点写法优雅，使用方便 缺点和饿汉式一样，在某些情况下会造成大量内存浪费 容器式单例写法12345678910111213141516171819202122public class ContainerSingleton &#123; private ContainerSingleton() &#123; &#125; private static Map&lt;String, Object&gt; ioc = new ConcurrentHashMap&lt;&gt;(); public static Object getInstance(String className) &#123; Object instance = null; if (!ioc.containsKey(className)) &#123; try &#123; instance = Class.forName(className).newInstance(); ioc.put(className, instance); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return instance; &#125; else &#123; return ioc.get(className); &#125; &#125;&#125; 测试类： 1234567public class ContainerSingletonTest &#123; public static void main(String[] args) &#123; Object o1 = ContainerSingleton.getInstance(&quot;com.ibli.javaBase.pattern.singleton.Pojo&quot;); Object o2 = ContainerSingleton.getInstance(&quot;com.ibli.javaBase.pattern.singleton.Pojo&quot;); System.err.println(o1 == o2); // true &#125;&#125; 容器式单例写法适合创建大量单例实例的场景，类似与Spring的IOC容器。当然上面的写法也会存在一个线程安全问题 序列化破坏单例模式123456789101112131415/** * 序列化：把内存中对象的状态转换为字节码的形式，然后在把字节码以IO输出流写到磁盘上 * 反序列化： 将持久化的字节码内容，通过IO流的方式读取到内存中，然后在转换成Java对象 */public class SerializableSingleton implements Serializable &#123; private static final SerializableSingleton serializableSingleton = new SerializableSingleton(); private SerializableSingleton() &#123; &#125; public static SerializableSingleton getInstance() &#123; return serializableSingleton; &#125;&#125; 测试类： 123456789101112131415161718192021222324252627public class SerializableSingletonTest &#123; public static void main(String[] args) &#123; SerializableSingleton s1; SerializableSingleton s2 = SerializableSingleton.getInstance(); FileOutputStream fos; try &#123; fos = new FileOutputStream(&quot;SerializableSingleton.obj&quot;); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(s2); oos.flush(); oos.close(); FileInputStream fis = new FileInputStream(&quot;SerializableSingleton.obj&quot;); ObjectInputStream ois = new ObjectInputStream(fis); s1 = (SerializableSingleton) ois.readObject(); ois.close(); System.out.println(s1); System.out.println(s2); System.out.println(s1 == s2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 结果： 123com.ibli.javaBase.pattern.singleton.SerializableSingleton@20ad9418com.ibli.javaBase.pattern.singleton.SerializableSingleton@681a9515false 解决方法：在SerializableSingleton中添加一个方法 1234// 桥接模式private Object readResolve() &#123; return serializableSingleton;&#125; 结果: 123com.ibli.javaBase.pattern.singleton.SerializableSingleton@681a9515com.ibli.javaBase.pattern.singleton.SerializableSingleton@681a9515true 原因：ois.readObject();方法底层有对readResolve方法的判断，如果不存在这个方法，会利用反射生成一个新的实例； ThreadLocal单例下面介绍一种比较少见的一种单例模式 12345678910111213141516public class ThreadLocalSingleton &#123; private static final ThreadLocal&lt;ThreadLocalSingleton&gt; threadLocalSingleton = new ThreadLocal&lt;ThreadLocalSingleton&gt;() &#123; @Override protected ThreadLocalSingleton initialValue() &#123; return new ThreadLocalSingleton(); &#125; &#125;; private ThreadLocalSingleton()&#123;&#125; public static ThreadLocalSingleton getInstance()&#123; return threadLocalSingleton.get(); &#125;&#125; 123456public class ThreadLocalExector implements Runnable&#123; @Override public void run() &#123; System.err.println(ThreadLocalSingleton.getInstance()); &#125;&#125; 测试： 123456789101112public class ThreadLocalSingletonTest &#123; public static void main(String[] args) &#123; System.out.println(ThreadLocalSingleton.getInstance()); System.out.println(ThreadLocalSingleton.getInstance()); Thread thread1 = new Thread(new ThreadLocalExector()); Thread thread2 = new Thread(new ThreadLocalExector()); thread1.start(); thread2.start();; &#125;&#125; 结果： 12345com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@38af3868 1com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@38af3868 2com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@10c69a60 3com.ibli.javaBase.pattern.singleton.ThreadLocalSingleton@2f1eeb2f 4 以上3和4的结果虽然不一样，但是其实也是实现了【单例】的效果。 山脚太拥挤 我们更高处见。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"设计模式之美-享元模式","date":"2021-08-07T11:16:35.000Z","path":"wiki/设计模式之美-享元模式/","text":"享元模式（Flyweight Pattern）前言在面向对象程序设计过程中，有时会面临要创建大量相同或相似对象实例的问题。创建那么多的对象将会耗费很多的系统资源，它是系统性能提高的一个瓶颈。 定义享元模式，又称为轻量级模式，运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。它是对象池的一种实现，类似于线程池，线程池可以避免不停的创建个销毁多个对象，消耗性能（用户态和内核态）；【宗旨】：共享细粒度对象，将多个对同一对象的访问集中起来；属于结构性模式； 内部状态和外部状态在共享对象的过程中，又两种状态：内部状态： 内部状态指对象共享出来的信息，存储在享元信息内部，并且不回随环境的改变而改变； 就是对象所共有的信息，比如火车票，多有从北京-上海的对象可以共用一个，而不是有1000张票，创建1000个对象； 外部状态： 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享； 也是不叫好理解的，当火车票被你购买之后，就和你的身份证号（唯一）绑定了，这属于外部状态； 使用场景1、常常应用于系统底层的开发，以便解决系统的性能问题；2、系统有大量相似对象，需要缓冲池的地方； 举个例子🌰享元实体类123456789101112131415161718192021222324252627282930313233public class Examination &#123; private String name; private String phone; private String subject; public Examination(String subject) &#123; super(); this.subject = subject; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPhone() &#123; return phone; &#125; public void setPhone(String phone) &#123; this.phone = phone; &#125; @Override public String toString() &#123; return &quot;Examination [name=&quot; + name + &quot;, phone=&quot; + phone + &quot;, subject=&quot; + subject + &quot;]&quot;; &#125;&#125; 享元工厂1234567891011121314151617181920212223public class ExaminationFactory &#123; // 对象池 private static Map&lt;String, Examination&gt; pool = new HashMap&lt;&gt;(); public static Examination getexExamination(String name, String phone, String subject) &#123; // 通过学科判断是否存在这个对象 if (pool.containsKey(subject)) &#123; Examination examination = pool.get(subject); examination.setName(name); examination.setPhone(phone); System.out.println(&quot;在缓存池取用对象:&quot; + examination.toString()); return examination; &#125; else &#123; Examination examination = new Examination(subject); pool.put(subject, examination); examination.setName(name); examination.setPhone(phone); System.out.println(&quot;新建对象:&quot; + examination.toString()); return examination; &#125; &#125;&#125; 测试类123456789101112public class Client &#123; public static void main(String[] args) &#123; Examination A = ExaminationFactory.getexExamination(&quot;A&quot;, &quot;13812345678&quot;, &quot;软件工程&quot;); Examination B = ExaminationFactory.getexExamination(&quot;B&quot;, &quot;13812341234&quot;, &quot;软件工程&quot;); Examination C = ExaminationFactory.getexExamination(&quot;C&quot;, &quot;13812341328&quot;, &quot;电子信息工程&quot;); Examination D = ExaminationFactory.getexExamination(&quot;D&quot;, &quot;13812345111&quot;, &quot;桥梁工程工程&quot;); Examination E = ExaminationFactory.getexExamination(&quot;E&quot;, &quot;13812345444&quot;, &quot;软件工程&quot;); System.err.println(A.hashCode()); System.err.println(B.hashCode()); &#125;&#125; 测试结果1234567新建对象:Examination [name=A, phone=13812345678, subject=软件工程]在缓存池取用对象:Examination [name=B, phone=13812341234, subject=软件工程]新建对象:Examination [name=C, phone=13812341328, subject=电子信息工程]新建对象:Examination [name=D, phone=13812345111, subject=桥梁工程工程]在缓存池取用对象:Examination [name=E, phone=13812345444, subject=软件工程]15289025771528902577 实际应用1、JDK – String 123456789101112131415161718192021222324252627282930313233public static void main(String[] args) &#123; // &quot;hello&quot;是编译器常量， String s1是运行时，把常量地址赋值给他&lt;br&gt; String s1 = &quot;hello&quot;; String s2 = &quot;hello&quot;; // &quot;he&quot; + &quot;llo&quot; 两个常量相加，会在编译期处理&lt;br&gt; String s3 = &quot;he&quot; + &quot;llo&quot;; // &quot;hel&quot; &quot;lo&quot; new String 共建了3个空间，然后拼接起来是一个新的空间（why?） String s4 = &quot;hel&quot; + new String(&quot;lo&quot;); String s5 = new String(&quot;hello&quot;); // s5存放的是堆中中间 String s6 = s5.intern(); //拿到常量中的地址， String s7 = &quot;h&quot;; String s8 = &quot;ello&quot;; String s9 = s7 + s8; //为什么这个不一样，因为是变量相加所以编译期没有做优化 System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;); System.out.println(&quot;s1 &quot; + System.identityHashCode(s1)); System.out.println(&quot;s2 &quot; + System.identityHashCode(s2)); System.out.println(&quot;s3 &quot; + System.identityHashCode(s3)); System.out.println(&quot;s4 &quot; + System.identityHashCode(s4)); System.out.println(&quot;s5 &quot; + System.identityHashCode(s5)); //s6为s5.intern()拿到的是常量池里的“hello” System.out.println(&quot;s6 &quot; + System.identityHashCode(s6)); System.out.println(&quot;s7 &quot; + System.identityHashCode(s7)); System.out.println(&quot;s8 &quot; + System.identityHashCode(s8)); System.out.println(&quot;s9 &quot; + System.identityHashCode(s9)); System.out.print(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;); System.out.println(s1==s2);//true System.out.println(s1==s3);//true System.out.println(s1==s4);//false System.out.println(s1==s9);//false System.out.println(s4==s5);//false System.out.println(s1==s6);//true &#125; 2、JDK – Integer 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125; &#125; 3、线程池4、Tomcat连接池 享元模式的优点✅1、减少对象的创建，降低内存中对象的数量，降低系统的内存使用，提升效率；2、减少内存之外的其他资源占用，IO,带宽等； 享元模式的缺点1、关注内部，外部状态，关注程序线程安全问题；2、使系统、程序的逻辑变得复杂； 参考资料1、[设计模式] - 享元模式2、设计模式-享元模式3、设计模式-享元设计模式","tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[]},{"title":"Java并发编程之ConcurrentHashMap实现原理","date":"2021-08-05T07:35:46.000Z","path":"wiki/Java并发编程之ConcurrentHashMap实现原理/","text":"ConcurrentHashMapMap应该是我们平时开发过程中除了List使用的第二频繁的数据结构了吧，我们都知道HashMap无法在多线程环境下保证安全，那我们可以使用什么来代替HashMap呢，有两个选择，HashTable和ConcurrentHashMap,由于HashTable的性能相对比较低，我们一般都使用ConcurrentHashMap来代替HashMap。 前言HashTable&amp;HashMapHashTable的put, get,remove等方法是通过synchronized来修饰保证其线程安全性的。 HashTable是 不允许key跟value为null的。 问题是synchronized是个关键字级别的==重量锁==，在get数据的时候任何写入操作都不允许。相对来说性能不好。 ConcurrentHashMap概述在ConcurrentHashMap中通过一个Node&lt;K,V&gt;[]数组来保存添加到map中的键值对，而在同一个数组位置是通过链表和红黑树的形式来保存的。但是这个数组只有在第一次添加元素的时候才会初始化，否则只是初始化一个ConcurrentHashMap对象的话，只是设定了一个sizeCtl变量，这个变量用来判断对象的一些状态和是否需要扩容，后面会详细解释。 第一次添加元素的时候，默认初期长度为16，当往map中继续添加元素的时候，通过hash值跟数组长度取与来决定放在数组的哪个位置，如果出现放在同一个位置的时候，优先以链表的形式存放，在同一个位置的个数又达到了8个以上，如果数组的长度还小于64的时候，则会扩容数组。如果数组的长度大于等于64了的话，在会将该节点的链表转换成树。 通过扩容数组的方式来把这些节点给分散开。然后将这些元素复制到扩容后的新的数组中，同一个链表中的元素通过hash值的数组长度位来区分，是还是放在原来的位置还是放到扩容的长度的相同位置去 。在扩容完成之后，如果某个节点的是树，同时现在该节点的个数又小于等于6个了，则会将该树转为链表。 取元素的时候，相对来说比较简单，通过计算hash来确定该元素在数组的哪个位置，然后在通过遍历链表或树来判断key和key的hash，取出value值。 ConcurrentHashMap 原理解析ConcurrentHashMap属性123456789101112131415161718private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;private static final int DEFAULT_CAPACITY = 16;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;static final int MOVED = -1; // 表示正在转移static final int TREEBIN = -2; // 表示已经转换成树static final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashtransient volatile Node&lt;K,V&gt;[] table;//默认没初始化的数组，用来保存元素private transient volatile Node&lt;K,V&gt;[] nextTable;//转移的时候用的数组/** * 用来控制表初始化和扩容的，默认值为0，当在初始化的时候指定了大小，这会将这个大小保存在sizeCtl中，大小为数组的0.75 * 当为负的时候，说明表正在初始化或扩张， * -1表示初始化 * -(1+n) n:表示活动的扩张线程 */private transient volatile int sizeCtl; Node&lt;K,V&gt;,这是构成每个元素的基本类。12345678910111213141516static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //key的hash值 final K key; //key volatile V val; //value volatile Node&lt;K,V&gt; next; //表示链表中的下一个节点 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125;&#125; TreeNode，构造树的节点12345678910111213static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125;&#125; TreeBin 用作树的头结点只存储root和first节点，不存储节点的key、value值。 12345678910static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock&#125; ForwardingNode在转移的时候放在头部的节点，是一个空节点 1234567static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; ConcurrentHashMap几个重要方法123456789101112131415161718192021/* * 用来返回节点数组的指定位置的节点的原子操作 */@SuppressWarnings(&quot;unchecked&quot;)static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;/* * cas原子操作，在指定位置设定值 */static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;/* * 原子操作，在指定位置设定值 */static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; ConcurrentHashMap的初始化先看一下ConcurrentHashMap的构造器 1234567891011121314151617//空的构造public ConcurrentHashMap() &#123;&#125;//如果在实例化对象的时候指定了容量，则初始化sizeCtlpublic ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125;//当出入一个Map的时候，先设定sizeCtl为默认容量，在添加元素public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125; 可以看到，在任何一个构造方法中，都没有对存储Map元素Node的table变量进行初始化。而是在第一次put操作的时候在进行初始化。 下面来看看数组的初始化方法initTable 1234567891011121314151617181920212223242526272829/** * 初始化数组table， * 如果sizeCtl小于0，说明别的数组正在进行初始化，则让出执行权 * 如果sizeCtl大于0的话，则初始化一个大小为sizeCtl的数组 * 否则的话初始化一个默认大小(16)的数组 * 然后设置sizeCtl的值为数组长度的3/4 */private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //第一次put的时候，table还没被初始化，进入while if ((sc = sizeCtl) &lt; 0) //sizeCtl初始值为0，当小于0的时候表示在别的线程在初始化表或扩展表 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //SIZECTL：表示当前对象的内存偏移量，sc表示期望值，-1表示要替换的值，设定为-1表示要初始化表了 try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; //指定了大小的时候就创建指定大小的Node数组，否则创建指定大小(16)的Node数组 @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; //初始化后，sizeCtl长度为数组长度的3/4 &#125; break; &#125; &#125; return tab;&#125; ConcurrentHashMap的put操作详解👇下面是put方法的源码： 12345678/* * 单纯的额调用putVal方法，并且putVal的第三个参数设置为false * 当设置为false的时候表示这个value一定会设置 * true的时候，只有当这个key的value为空的时候才会设置 */ public V put(K key, V value) &#123; return putVal(key, value, false); &#125; 下面看一下putVal方法实现： 当添加一对键值对的时候，首先会去判断保存这些键值对的数组是不是初始化了，如果没有的话就初始化数组, 然后通过计算hash值来确定放在数组的哪个位置。 如果这个位置为空则直接添加，如果不为空的话，则取出这个节点来，取出来的节点的hash值是MOVED(-1)的话，则表示当前正在对这个数组进行扩容，复制到新的数组，则当前线程也去帮助复制。 如果这个节点，不为空，也不在扩容，则通过synchronized来加锁，进行添加操作。 然后判断当前取出的节点位置存放的是链表还是树，如果是链表的话，则遍历整个链表，直到取出来的节点的key来个要放的key进行比较，如果key相等，并且key的hash值也相等的话，则说明是同一个key，则覆盖掉value，否则的话则添加到链表的末尾。如果是树的话，则调用putTreeVal方法把这个元素添加到树中去。 最后在添加完成之后，会判断在该节点处共有多少个节点（注意是添加前的个数），如果达到8个以上了的话，则调用treeifyBin方法来尝试将处的链表转为树，或者扩容数组。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293final V putVal (K key, V value,boolean onlyIfAbsent)&#123; if (key == null || value == null) throw new NullPointerException(); //取得key的hash值 int hash = spread(key.hashCode()); //用来计算在这个节点总共有多少个元素，用来控制扩容或者转移为树 int binCount = 0; for (Node&lt;K, V&gt;[] tab = table; ; ) &#123; Node&lt;K, V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) //第一次put的时候table没有初始化，则初始化table tab = initTable(); //通过哈希计算出一个表中的位置因为n是数组的长度，所以(n-1)&amp;hash肯定不会出现数组越界 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果这个位置没有元素的话，则通过cas的方式尝试添加，注意这个时候是没有加锁的 if (casTabAt(tab, i, null, //创建一个Node添加到数组中区，null表示的是下一个节点为空 new Node&lt;K, V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; /* * 如果检测到某个节点的hash值是MOVED，则表示正在进行数组扩张的数据复制阶段， * 则当前线程也会参与去复制，通过允许多线程复制的功能，一次来减少数组的复制所带来的性能损失 */ else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; /* * 如果在这个位置有元素的话，就采用synchronized的方式加锁， * 如果是链表的话(hash大于0)，就对这个链表的所有元素进行遍历， * 如果找到了key和key的hash值都一样的节点，则把它的值替换到 * 如果没找到的话，则添加在链表的最后面 * 否则，是树的话，则调用putTreeVal方法添加到树中去 * * 在添加完之后，会对该节点上关联的的数目进行判断， * 如果在8个以上的话，则会调用treeifyBin方法，来尝试转化为树，或者是扩容 */ V oldVal = null; synchronized (f) &#123; //再次取出要存储的位置的元素，跟前面取出来的比较 if (tabAt(tab, i) == f) &#123; //取出来的元素的hash值大于0，当转换为树之后，hash值为-2 if (fh &gt;= 0) &#123; binCount = 1; //遍历这个链表 for (Node&lt;K, V&gt; e = f; ; ++binCount) &#123; K ek; //要存的元素的hash，key跟要存储的位置的节点的相同的时候，替换掉该节点的value即可 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; //当使用putIfAbsent的时候，只有在这个key没有设置值得时候才设置 if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K, V&gt; pred = e; //如果不是同样的hash，同样的key的时候，则判断该节点的下一个节点是否为空， if ((e = e.next) == null) &#123; //为空的话把这个要加入的节点设置为当前节点的下一个节点 pred.next = new Node&lt;K, V&gt;(hash, key, value, null); break; &#125; &#125; //表示已经转化成红黑树类型了 &#125; else if (f instanceof TreeBin) &#123; Node&lt;K, V&gt; p; binCount = 2; //调用putTreeVal方法，将该元素添加到树中去 if ((p = ((TreeBin&lt;K, V&gt;) f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //当在同一个节点的数目达到8个的时候，则扩张数组或将给节点的数据转为tree if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); //计数 return null;&#125; ConcurrentHashMap的扩容详解在put方法的详解中，我们可以看到，在同一个节点的个数超过8个的时候，会调用treeifyBin方法来看看是扩容还是转化为一棵树，同时在每次添加完元素的addCount方法中，也会判断当前数组中的元素是否达到了sizeCtl的量，如果达到了的话，则会进入transfer方法去扩容。 12345678910111213141516171819202122232425262728293031/** * Replaces all linked nodes in bin at given index unless table is * too small, in which case resizes instead. * 当数组长度小于64的时候，扩张数组长度一倍，否则的话把链表转为树 */private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; System.out.println(&quot;treeifyBin方\\t==&gt;数组长：&quot;+tab.length); if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //MIN_TREEIFY_CAPACITY 64 tryPresize(n &lt;&lt; 1); // 数组扩容 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; //使用synchronized同步器，将该节点出的链表转为树 if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //hd：树的头(head) for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) //把Node组成的链表，转化为TreeNode的链表，头结点任然放在相同的位置 hd = p; //设置head else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd));//把TreeNode的链表放入容器TreeBin中 &#125; &#125; &#125; &#125;&#125; 可以看到当需要扩容的时候，调用的时候tryPresize方法，看看trePresize的源码 trePresize的源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * 扩容表为指可以容纳指定个数的大小（总是2的N次方） * 假设原来的数组长度为16，则在调用tryPresize的时候，size参数的值为16&lt;&lt;1(32)，此时sizeCtl的值为12 * 计算出来c的值为64,则要扩容到sizeCtl≥为止 * 第一次扩容之后 数组长：32 sizeCtl：24 * 第二次扩容之后 数组长：64 sizeCtl：48 * 第二次扩容之后 数组长：128 sizeCtl：94 --&gt; 这个时候才会退出扩容 */private final void tryPresize(int size) &#123; /* * MAXIMUM_CAPACITY = 1 &lt;&lt; 30 * 如果给定的大小大于等于数组容量的一半，则直接使用最大容量， * 否则使用tableSizeFor算出来 * 后面table一直要扩容到这个值小于等于sizeCtrl(数组长度的3/4)才退出扩容 */ int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // printTable(tab); 调试用的 /* * 如果数组table还没有被初始化，则初始化一个大小为sizeCtrl和刚刚算出来的c中较大的一个大小的数组 * 初始化的时候，设置sizeCtrl为-1，初始化完成之后把sizeCtrl设置为数组长度的3/4 * 为什么要在扩张的地方来初始化数组呢？这是因为如果第一次put的时候不是put单个元素， * 而是调用putAll方法直接put一个map的话，在putALl方法中没有调用initTable方法去初始化table， * 而是直接调用了tryPresize方法，所以这里需要做一个是不是需要初始化table的判断 */ if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //初始化tab的时候，把sizeCtl设为-1 try &#123; if (table == tab) &#123; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; /* * 一直扩容到的c小于等于sizeCtl或者数组长度大于最大长度的时候，则退出 * 所以在一次扩容之后，不是原来长度的两倍，而是2的n次方倍 */ else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) &#123; break; //退出扩张 &#125; else if (tab == table) &#123; int rs = resizeStamp(n); /* * 如果正在扩容Table的话，则帮助扩容 * 否则的话，开始新的扩容 * 在transfer操作，将第一个参数的table中的元素，移动到第二个元素的table中去， * 虽然此时第二个参数设置的是null，但是，在transfer方法中，当第二个参数为null的时候， * 会创建一个两倍大小的table */ if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; /* * transfer的线程数加一,该线程将进行transfer的帮忙 * 在transfer的时候，sc表示在transfer工作的线程数 */ if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; /* * 没有在初始化或扩容，则开始扩容 */ else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) &#123; transfer(tab, null); &#125; &#125; &#125;&#125; 在tryPresize方法中，并没有加锁，允许多个线程进入，如果数组正在扩张，则当前线程也去帮助扩容。 transfer方法数组扩容的主要方法就是transfer方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. * 把数组中的节点复制到新的数组的相同位置，或者移动到扩张部分的相同位置 * 在这里首先会计算一个步长，表示一个线程处理的数组长度，用来控制对CPU的使用， * 每个CPU最少处理16个长度的数组元素,也就是说，如果一个数组的长度只有16，那只有一个线程会对其进行扩容的复制移动操作 * 扩容的时候会一直遍历，知道复制完所有节点，没处理一个节点的时候会在链表的头部设置一个fwd节点，这样其他线程就会跳过他， * 复制后在新数组中的链表不是绝对的反序的 */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) //MIN_TRANSFER_STRIDE 用来控制不要占用太多CPU stride = MIN_TRANSFER_STRIDE; // subdivide range //MIN_TRANSFER_STRIDE=16 /* * 如果复制的目标nextTab为null的话，则初始化一个table两倍长的nextTab * 此时nextTable被设置值了(在初始情况下是为null的) * 因为如果有一个线程开始了表的扩张的时候，其他线程也会进来帮忙扩张， * 而只是第一个开始扩张的线程需要初始化下目标数组 */ if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; /* * 创建一个fwd节点，这个是用来控制并发的，当一个节点为空或已经被转移之后，就设置为fwd节点 * 这是一个空的标志节点 */ ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; //是否继续向前查找的标志位 boolean finishing = false; // to ensure sweep(清扫) before committing nextTab,在完成之前重新在扫描一遍数组，看看有没完成的没 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) &#123; advance = false; &#125; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; //已经完成转移 nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //设置sizeCtl为扩容后的0.75 return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) &#123; return; &#125; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) //数组中把null的元素设置为ForwardingNode节点(hash值为MOVED[-1]) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; synchronized (f) &#123; //加锁操作 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; //该节点的hash值大于等于0，说明是一个Node节点 /* * 因为n的值为数组的长度，且是power(2,x)的，所以，在&amp;操作的结果只可能是0或者n * 根据这个规则 * 0--&gt; 放在新表的相同位置 * n--&gt; 放在新表的（n+原来位置） */ int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; /* * lastRun 表示的是需要复制的最后一个节点 * 每当新节点的hash&amp;n -&gt; b 发生变化的时候，就把runBit设置为这个结果b * 这样for循环之后，runBit的值就是最后不变的hash&amp;n的值 * 而lastRun的值就是最后一次导致hash&amp;n 发生变化的节点(假设为p节点) * 为什么要这么做呢？因为p节点后面的节点的hash&amp;n 值跟p节点是一样的， * 所以在复制到新的table的时候，它肯定还是跟p节点在同一个位置 * 在复制完p节点之后，p节点的next节点还是指向它原来的节点，就不需要进行复制了，自己就被带过去了 * 这也就导致了一个问题就是复制后的链表的顺序并不一定是原来的倒序 */ for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; //n的值为扩张前的数组的长度 if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; /* * 构造两个链表，顺序大部分和原来是反的 * 分别放到原来的位置和新增加的长度的相同位置(i/n+i) */ for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) /* * 假设runBit的值为0， * 则第一次进入这个设置的时候相当于把旧的序列的最后一次发生hash变化的节点(该节点后面可能还有hash计算后同为0的节点)设置到旧的table的第一个hash计算后为0的节点下一个节点 * 并且把自己返回，然后在下次进来的时候把它自己设置为后面节点的下一个节点 */ ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else /* * 假设runBit的值不为0， * 则第一次进入这个设置的时候相当于把旧的序列的最后一次发生hash变化的节点(该节点后面可能还有hash计算后同不为0的节点)设置到旧的table的第一个hash计算后不为0的节点下一个节点 * 并且把自己返回，然后在下次进来的时候把它自己设置为后面节点的下一个节点 */ hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; //否则的话是一个树节点 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; /* * 在复制完树节点之后，判断该节点处构成的树还有几个节点， * 如果≤6个的话，就转回为一个链表 */ ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 到这里，ConcurrentHashMap的put操作和扩容都介绍的差不多了， 下面的两点一定要注意： 1、复制之后的新链表不是旧链表的绝对倒序 2、在扩容的时候每个线程都有处理的步长，最少为16，在这个步长范围内的数组节点只有自己一个线程来处理 ConcurrentHashMap的get操作详解1234567891011121314151617181920212223242526/* * 相比put方法，get就很单纯了，支持并发操作， * 当key为null的时候回抛出NullPointerException的异常 * get操作通过首先计算key的hash值来确定该元素放在数组的哪个位置 * 然后遍历该位置的所有节点 * 如果不存在的话返回null */public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 链表转为红黑树的过程前面在讲解tryifyBin的源码的时候讲到过，如果在当个bin上的元素超过了8个的时候，就会尝试去扩容数组或者是将链表转为红黑树。 源码：👇 12345678910111213141516171819202122232425262728private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //MIN_TREEIFY_CAPACITY 64 tryPresize(n &lt;&lt; 1); // 数组扩容 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; //使用synchronized同步器，将该节点出的链表转为树 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //hd：树的头(head) for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); //把Node组成的链表，转化为TreeNode的链表，头结点任然放在相同的位置 if ((p.prev = tl) == null) hd = p; //设置head else tl.next = p; tl = p; &#125; //把TreeNode的链表放入容器TreeBin setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd));中 &#125; &#125; &#125; &#125;&#125; 首先将Node的链表转化为一个TreeNode的链表，然后将TreeNode链表的头结点来构造一个TreeBin。下面是TreeBin构造方法的源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748TreeBin(TreeNode&lt;K,V&gt; b) &#123; //创建的TreeBin是一个空节点，hash值为TREEBIN（-2） super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125;// else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; //x代表的是转换为树之前的顺序遍历到链表的位置的节点，r代表的是根节点 for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) // dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) //当key不可以比较，或者相等的时候采取的一种排序措施 dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; //在这里判断要放的left/right是否为空，不为空继续用left/right节点来判断 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; //每次插入一个元素的时候都调用balanceInsertion来保持红黑树的平衡 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root);&#125; ConcurrentHashMap的同步机制前面分析了下ConcurrentHashMap的源码，那么，对于一个映射集合来说，ConcurrentHashMap是如果来做到并发安全，又是如何做到高效的并发的呢？ 首先是读操作，从源码中可以看出来，在get操作中，根本没有使用同步机制，也没有使用unsafe方法，所以读操作是支持并发操作的。那么写操作呢？ 分析这个之前，先看看什么情况下会引起数组的扩容，扩容是通过transfer方法来进行的。而调用transfer方法的只有trePresize、helpTransfer和addCount三个方法。 这三个方法又是分别在什么情况下进行调用的呢？ tryPresize是在treeIfybin和putAll方法中调用，treeIfybin主要是在put添加元素完之后，判断该数组节点相关元素是不是已经超过8个的时候，如果超过则会调用这个方法来扩容数组或者把链表转为树。 helpTransfer是在当一个线程要对table中元素进行操作的时候，如果检测到节点的HASH值为MOVED的时候，就会调用helpTransfer方法，在helpTransfer中再调用transfer方法来帮助完成数组的扩容 addCount是在当对数组进行操作，使得数组中存储的元素个数发生了变化的时候会调用的方法。 所以引起数组扩容的情况如下： 1、只有在往map中添加元素的时候，在某一个节点的数目已经超过了8个，同时数组的长度又小于64的时候，才会触发数组的扩容。 2、当数组中元素达到了sizeCtl的数量的时候，则会调用transfer方法来进行扩容 那么在扩容的时候，可以不可以对数组进行读写操作呢？ 事实上是可以的。当在进行数组扩容的时候，如果当前节点还没有被处理（也就是说还没有设置为fwd节点），那就可以进行设置操作。如果该节点已经被处理了，则当前线程也会加入到扩容的操作中去。 那么，多个线程又是如何同步处理的呢？ 在ConcurrentHashMap中，同步处理主要是通过Synchronized和unsafe两种方式来完成的。 1、在取得sizeCtl、某个位置的Node的时候，使用的都是unsafe的方法，来达到并发安全的目的 2、当需要在某个位置设置节点的时候，则会通过Synchronized的同步机制来锁定该位置的节点。 3、在数组扩容的时候，则通过处理的步长和fwd节点来达到并发安全的目的，通过设置hash值为MOVED 4、当把某个位置的节点复制到扩张后的table的时候，也通过Synchronized的同步机制来保证线程安全 参考资料https://juejin.cn/search?query=concurrenthashmap&amp;utm_source=gold_browser_extension&amp;utm_medium=search https://juejin.cn/post/6844904136937308168 https://www.cnblogs.com/zerotomax/p/8687425.html#go0 https://juejin.cn/post/6844903520957644808 https://processon.com/view/6049998ae401fd39d6fffbaa?fromnew=1 https://juejin.cn/post/6871793103020556295 https://juejin.cn/post/6844903641866846222 https://juejin.cn/post/6844903602423595015 ConcurrentHashMap基于JDK1.8","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之CountDownLatch工具","date":"2021-08-04T09:00:07.000Z","path":"wiki/Java并发编程之CountDownLatch工具/","text":"CountDownLatch你要问什么是CountDownLatch? 那我可有的说了。 之前干活的时候，有很多处理数据的任务，但是呢，数据量很大，写的java脚本执行下来肯定会比较慢，那怎么办呢，想起来刚毕业那会，有个同事写了一个并发调用的工具，当时感觉碉堡了。 当我查看这个工具的具体实现时，发现它是基于CountDownLatch来封装的，咱当时也没用过CountDownLatch，感觉应该挺难，就直接用了那个工具。 后来发现那个工具使用起来有些繁琐，就比如我刷数据这个事，CountDownLatch直接干是最简单的。 CountDownLatch是什么A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. 按照官方API文档上的介绍呢，CountDownLatch就是一个同步机制，用来实现一个或多个线程一直wait知道另一个线程完成一系列动作。 CountDownLatch使用1234567891011121314151617181920212223242526272829public class CountDownLatchDemo &#123; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(2); new Thread(()-&gt;&#123; try &#123; Thread.sleep(1000); latch.countDown(); System.out.println(Thread.currentThread().getName() + &quot; execute 111&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(()-&gt;&#123; try &#123; Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + &quot; execute 222&quot;); latch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); System.out.println(&quot;main thread invoke await&quot;); latch.await(); System.out.println(&quot;subThread execute end&quot;); &#125;&#125; 执行结果如下： 1234main thread invoke awaitThread-0 execute 111Thread-1 execute 222subThread execute end 下面我们就从countDown和await两个方法解析CountDownLatch的运行机制吧 CountDownLatch实现原理和ReentrantLock实现独占锁不同的是，CountDownLatch是典型的共享锁。 值得注意的是，CountDownLatch的静态内部类Sync继承了AbstractQueuedSynchronizer并实现了tryAcquireShared方法和tryReleaseShared方法。 下面先从构造方法入手开始学习 👇 1234public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); this.sync = new Sync(count);&#125; 初始化count字段，其值是设置在AQS的state字段上面的，当每个线程执行了countDown()之后，state = state - 1 当state = 0 时，唤醒之前await的线程。 await()下面是await方法： 123public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125; AQS#acquireSharedInterruptibly(int arg) 12345678public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) // 获取中断标志，把中断标志复位，然后把中断异常往上层抛 throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125; tryAcquireShared(arg)这个方法和之前学习ReentrantLock时是一样的，这是AQS提供的模版方法。 AQS提供模版方法，有每个子类自己去实现逻辑，然后再由AQS本身调用。 CountDownLatch#tryAcquireShared(int acquires) 123protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1;&#125; getState()返回的是AQS的state值，第一个线程获取是肯定不是0 如果getState()方法返回-1的话，会执行下面的方法： AQS#doAcquireSharedInterruptibly 1234567891011121314151617181920212223242526272829private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 表示aqs state = 0 // 需要把当前线程设置成头节点，并向下传播 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 避免一直空转，将前一个节点状态设置成SIGNAL,然后挂起当前线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 如果线程中断，则直接抛出异常 throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 当countDownLatch的count变成0的时候，主线程await完成，然后被唤醒，继续执行。 setHeadAndPropagate(Node node, int propagate) 123456789101112131415161718192021222324252627private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus either before * or after setHead) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don&#x27;t know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) // 如果后续节点是shard节点，释放 doReleaseShared(); &#125;&#125; countDown()123public void countDown() &#123; sync.releaseShared(1);&#125; AQS#releaseShared(int arg) 释放共享锁 12345678public final boolean releaseShared(int arg) &#123; // 有子类实现 if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; CountDownLatch#tryReleaseShared(int releases) 这个方法比较简单，每执行一次countDown(), state = state - 1 最后返回state是否等于0 如果不等于0 说明共享锁不能释放 1234567891011protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; doReleaseShared() 释放共享锁方法 12345678910111213141516171819202122232425262728293031private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; // 获取头结点的等待状态 int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 释放后继结点 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; unparkSuccessor()执行线程唤醒的方法 1234567891011121314151617181920212223242526private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 参考文章https://www.jianshu.com/p/128476015902 https://segmentfault.com/a/1190000015807573","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之搞懂线程池","date":"2021-08-04T08:43:19.000Z","path":"wiki/Java并发编程之搞懂线程池-1/","text":"ThreadPool1. 为什么存在线程池1.1 降低资源消耗通过复用已存在的线程和降低线程关闭的次数来尽可能降低系统性能损耗；（享元模式） 1.2 提升系统响应速度通过复用线程，省去创建线程的过程，因此整体上提升了系统的响应速度； 1.3 提高线程的可管理性线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，因此，需要使用线程池来管理线程。 至于为什么不允许手动创建线程池，请参见https://dayarch.top/p/why-we-need-to-use-threadpool.html 2. 线程池的工作流程线程池顾名思义，就是由很多线程构成的池子，来一个任务，就从池子中取一个线程，处理这个任务。这是一个简单的理解，实际上线程池的实现和运转是一个非常复杂的过程。 例如线程池肯定不会无限扩大的，否则资源会耗尽；当线程数到达一个阶段，提交的任务会被暂时存储在一个队列中，如果队列内容可以不断扩大，极端下也会耗尽资源，那选择什么类型的队列，当队列满如何处理任务，都有涉及很多内容。线程池总体的工作过程如下图： 线程池内的线程数的大小相关的概念有两个，一个是核心池大小，还有最大池大小。如果当前的线程个数比核心池个数小，当任务到来，会优先创建一个新的线程并执行任务。当已经到达核心池大小，则把任务放入队列，为了资源不被耗尽，队列的最大容量可能也是有上限的，如果达到队列上限则考虑继续创建新线程执行任务，如果此刻线程的个数已经到达最大池上限，则考虑把任务丢弃。 在 java.util.concurrent 包中，提供了 ThreadPoolExecutor 的实现。 12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;&#125; 3. 线程池参数既然有了刚刚对线程池工作原理对概述，这些参数就很容易理解了： 3.1 corePoolSize 核心池大小，既然如前原理部分所述。需要注意的是在初创建线程池时线程不会立即启动，直到有任务提交才开始启动线程并逐渐时线程数目达到corePoolSize。若想一开始就创建所有核心线程需调用prestartAllCoreThreads方法。 3.2 maximumPoolSize池中允许的最大线程数。需要注意的是当核心线程满且阻塞队列也满时才会判断当前线程数是否小于最大线程数，并决定是否创建新线程。 3.3 keepAliveTime当线程数大于核心时，多于的空闲线程最多存活时间 3.4 unitkeepAliveTime 参数的时间单位。 3.5 workQueue当线程数目超过核心线程数时用于保存任务的队列。主要有3种类型的BlockingQueue可供选择：无界队列，有界队列和同步移交。将在下文中详细阐述。从参数中可以看到，此队列仅保存实现Runnable接口的任务。 别看这个参数位置很靠后，但是真的很重要，因为楼主的坑就因这个参数而起，这些细节有必要仔细了解清楚。 3.6 threadFactory执行程序创建新线程时使用的工厂。 3.7 handler阻塞队列已满且线程数达到最大值时所采取的饱和策略。java默认提供了4种饱和策略的实现方式：中止、抛弃、抛弃最旧的、调用者运行。将在下文中详细阐述。 4. 可选择的阻塞队列BlockingQueue详解再重复一下新任务进入时线程池的执行策略：1、如果运行的线程少于corePoolSize，则 Executor始终首选添加新的线程，而不进行排队。（如果当前运行的线程小于corePoolSize，则任务根本不会存入queue中，而是直接运行） 2、如果运行的线程大于等于 corePoolSize，则 Executor始终首选将请求加入队列，而不添加新的线程。如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。主要有3种类型的BlockingQueue： 4.1 无界队列队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而楼主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 4.2 有界队列常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue与有界的LinkedBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 在我们的修复方案中，选择的就是这个类型的队列，虽然会有部分任务被丢失，但是我们线上是排序日志搜集任务，所以对部分对丢失是可以容忍的。 4.3 同步移交队列如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 5. 可选择的饱和策略RejectedExecutionHandler详解JDK主要提供了4种饱和策略供选择。4种策略都做为静态内部类在ThreadPoolExcutor中进行实现。 5.1 AbortPolicy中止策略该策略是默认饱和策略。 12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125; 使用该策略时在饱和时会抛出RejectedExecutionException（继承自RuntimeException），调用者可捕获该异常自行处理。 5.2 DiscardPolicy抛弃策略12public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;&#125; 如上所示，什么都不做。 5.3 DiscardOldestPolicy抛弃旧任务策略123456public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125;&#125; 如代码，先将阻塞队列中的头元素出队抛弃，再尝试提交任务。如果此时阻塞队列使用PriorityBlockingQueue优先级队列，将会导致优先级最高的任务被抛弃，因此不建议将该种策略配合优先级队列使用。 5.4 CallerRunsPolicy调用者运行12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125;&#125; 既不抛弃任务也不抛出异常，直接运行任务的run方法，换言之将任务回退给调用者来直接运行。使用该策略时线程池饱和后将由调用线程池的主线程自己来执行任务，因此在执行任务的这段时间里主线程无法再提交新任务，从而使线程池中工作线程有时间将正在处理的任务处理完成。 6. Java提供的四种常用线程池解析既然楼主踩坑就是使用了 JDK 的默认实现，那么再来看看这些默认实现到底干了什么，封装了哪些参数。简而言之 Executors 工厂方法Executors.newCachedThreadPool() 提供了无界线程池，可以进行自动线程回收；Executors.newFixedThreadPool(int) 提供了固定大小线程池，内部使用无界队列；Executors.newSingleThreadExecutor() 提供了单个后台线程。 详细介绍一下上述四种线程池。 6.1 newCachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 在newCachedThreadPool中如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。初看该构造函数时我有这样的疑惑：核心线程池为0，那按照前面所讲的线程池策略新任务来临时无法进入核心线程池，只能进入 SynchronousQueue中进行等待，而SynchronousQueue的大小为1，那岂不是第一个任务到达时只能等待在队列中，直到第二个任务到达发现无法进入队列才能创建第一个线程？这个问题的答案在上面讲SynchronousQueue时其实已经给出了，要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。因此即便SynchronousQueue一开始为空且大小为1，第一个任务也无法放入其中，因为没有线程在等待从SynchronousQueue中取走元素。因此第一个任务到达时便会创建一个新线程执行该任务。 6.2 newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 看代码一目了然了，线程数量固定，使用无限大的队列。 6.3 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 123public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 在来看看ScheduledThreadPoolExecutor（）的构造函数 1234public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); &#125; ScheduledThreadPoolExecutor的父类即ThreadPoolExecutor，因此这里各参数含义和上面一样。值得关心的是DelayedWorkQueue这个阻塞对列，在上面没有介绍，它作为静态内部类就在ScheduledThreadPoolExecutor中进行了实现。简单的说，DelayedWorkQueue是一个无界队列，它能按一定的顺序对工作队列中的元素进行排列。 6.4 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 1234public static ScheduledExecutorService newSingleThreadScheduledExecutor() &#123; return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); &#125; 首先new了一个线程数目为 1 的ScheduledThreadPoolExecutor，再把该对象传入DelegatedScheduledExecutorService中，看看DelegatedScheduledExecutorService的实现代码： 1234DelegatedScheduledExecutorService(ScheduledExecutorService executor) &#123; super(executor); e = executor;&#125; 在看看它的父类 123DelegatedExecutorService(ExecutorService executor) &#123; e = executor; &#125; 其实就是使用装饰模式增强了ScheduledExecutorService（1）的功能，不仅确保只有一个线程顺序执行任务，也保证线程意外终止后会重新创建一个线程继续执行任务。 7. 为什么禁止使用 Executors 创建线程池? 7.1 实验证明Executors缺陷12345678910111213141516171819202122public class ExecutorsDemo &#123; private static ExecutorService executor = Executors.newFixedThreadPool(15); public static void main(String[] args) &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; executor.execute(new SubThread()); &#125; &#125; &#125; class SubThread implements Runnable &#123; @Override public void run() &#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; //do nothing &#125; &#125; &#125; &#125; 通过指定JVM参数:-Xmx8m -Xms8m运行以上代码，会抛出OOM: 1234Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceededat java.util.concurrent.LinkedBlockingQueue.offer(LinkedBlockingQueue. java:416)at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor. java:1371)at com.hollis.ExecutorsDemo.main(ExecutorsDemo.java:16) 以上代码指出，ExecutorsDemo.java 的第 16 行，就是代码中的 execu- tor.execute(new SubThread());。 7.2 Executors 为什么存在缺陷通过上面的例子，我们知道了 Executors 创建的线程池存在 OOM 的风险，那 么到底是什么原因导致的呢?我们需要深入 Executors 的源码来分析一下。 其实，在上面的报错信息中，我们是可以看出蛛丝马迹的，在以上的代码中其实 已经说了，真正的导致 OOM 的其实是 LinkedBlockingQueue.offer 方法。 如果读者翻看代码的话，也可以发现，其实底层确实是通过 LinkedBlock- ingQueue 实现的: 123public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); 如果读者对 Java 中的阻塞队列有所了解的话，看到这里或许就能够明白原因了。 Java 中 的 BlockingQueue 主 要 有 两 种 实 现， 分 别 是 ArrayBlockingQ- ueue 和 LinkedBlockingQueue。 ArrayBlockingQueue 是一个用数组实现的有界阻塞队列，必须设置容量。 LinkedBlockingQueue 是一个用链表实现的有界阻塞队列，容量可以选择 进行设置，不设置的话，将是一个无边界的阻塞队列，最大长度为 Integer.MAX_ VALUE。 这里的问题就出在:不设置的话，将是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE。也就是说，如果我们不设置 LinkedBlockingQueue 的 容量的话，其默认容量将会是 Integer.MAX_VALUE。 而 newFixedThreadPool 中创建 LinkedBlockingQueue 时，并未指定容 量。此时，LinkedBlockingQueue 就是一个无边界队列，对于一个无边界队列 来说，是可以不断的向队列中加入任务的，这种情况下就有可能因为任务过多而导 致内存溢出问题。 上面提到的问题主要体现在newFixedThreadPool 和 newSingleThreadExecutor 两个工厂方法上，并不是说 newCachedThreadPool 和 newScheduledThreadPool 这两个方法就安全了，这两种方式创建的最大线程数可能是 Integer.MAX_VALUE，而创建这么多线程，必然就有可能导致 OOM。 7.3 创建线程池的正确姿势避免使用 Executors 创建线程池，主要是避免使用其中的默认实现，那么我们 可以自己直接调用 ThreadPoolExecutor 的构造函数来自己创建线程池。在创建的 同时，给 BlockQueue 指定容量就可以了。 12private static ExecutorService executor = new ThreadPoolExecutor(10, 10, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue(10)); 这种情况下，一旦提交的线程数超过当前可用线程数时，就会抛出 java.util. concurrent.RejectedExecutionException，这是因为当前线程池使用的队列 是有边界队列，队列已经满了便无法继续处理新的请求。但是异常(Exception)总比 发生错误(Error)要好。 除了自己定义 ThreadPoolExecutor 外。还有其他方法。这个时候第一时间 就应该想到开源类库，如 apache 和 guava 等。 作者推荐使用 guava 提供的 ThreadFactoryBuilder 来创建线程池。 12345678910public class ExecutorsDemo &#123; private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor. AbortPolicy()); public static void main(String[] args) &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; pool.execute(new SubThread()); &#125; &#125;&#125; 通过上述方式创建线程时，不仅可以避免 OOM 的问题，还可以自定义线程名 称，更加方便的出错的时候溯源。 8、ThreadPoolExecutor源码解析8.1 ThreadPoolExecutor类重要属性1234567891011121314151617//这个属性是用来存放 当前运行的worker数量以及线程池状态的//int是32位的，这里把int的高3位拿来记录线程池状态的标志位,后29位拿来记录当前运行worker的数量private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//存放任务的阻塞队列private final BlockingQueue&lt;Runnable&gt; workQueue;//worker的集合,用set来存放private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();//历史达到的worker数最大值private int largestPoolSize;//当队列满了并且worker的数量达到maxSize的时候,执行具体的拒绝策略private volatile RejectedExecutionHandler handler;//超出coreSize的worker的生存时间private volatile long keepAliveTime;//常驻worker的数量private volatile int corePoolSize;//最大worker的数量,一般当workQueue满了才会用到这个参数private volatile int maximumPoolSize; 8.2 ThreadPoolExecutor定义的内部状态12345678910111213141516private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 其中AtomicInteger变量ctl的功能非常强大: 利用低29位表示线程池中线程数，通过高3位表示线程池的运行状态: RUNNING: -1 &lt;&lt; COUNT_BITS，即高3位为111，该状态的线程池会接收新任务，并处理阻塞队列中的任务； SHUTDOWN: 0 &lt;&lt; COUNT_BITS，即高3位为000，该状态的线程池不会接收新任务，但会处理阻塞队列中的任务； STOP : 1 &lt;&lt; COUNT_BITS，即高3位为001，该状态的线程不会接收新任务，也不会处理阻塞队列中的任务，而且会中断正在运行的任务； TIDYING : 2 &lt;&lt; COUNT_BITS，即高3位为010, 所有的任务都已经终止； TERMINATED: 3 &lt;&lt; COUNT_BITS，即高3位为011, terminated()方法已经执行完成 8.3 execute源码解析1234567891011121314151617181920212223242526public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加任务队列并创建核心线程，然后在执行 if (addWorker(command, true)) return; c = ctl.get(); &#125; // 线程池是运行状态并且任务成功添加到队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // recheck and if necessary 回滚到入队操作前，即倘若线程池shutdown状态，就remove(command) //如果线程池没有RUNNING，成功从阻塞队列中删除任务，执行reject方法处理任务 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) // 使用普通线程运行任务 addWorker(null, false); &#125; // 往线程池中创建新的线程失败，则reject任务 else if (!addWorker(command, false)) reject(command);&#125; 思考🤔 为什么需要double check线程池的状态? 在多线程环境下，线程池的状态时刻在变化，而ctl.get()是非原子操作，很有可能刚获取了线程池状态后线程池状态就改变了。判断是否将command加入workque是线程池之前的状态。倘若没有double check，万一线程池处于非running状态(在多线程环境下很有可能发生)，那么command永远不会执行。 addWorker方法 先看看addWorker方法的注释，方便我们理解源码 1234567891011121314151617181920212223242526272829303132Checks if a new worker can be added with respect to current * pool state and the given bound (either core or maximum). If so, * the worker count is adjusted accordingly, and, if possible, a * new worker is created and started, running firstTask as its * first task. This method returns false if the pool is stopped or * eligible to shut down. It also returns false if the thread * factory fails to create a thread when asked. If the thread * creation fails, either due to the thread factory returning * null, or due to an exception (typically OutOfMemoryError in * Thread.start()), we roll back cleanly. * // 大概翻译如下： //1、首先先检查线程池的状态和线程数量是否超过界限 //2、如果可以创建的话，需要更新任务的数量，然后运行任务 //3、有两种情况这个方法会返回false，线程池stop状态或者shut down状态 //还有一种情况是共创创建线程失败 4、不管是发生什么异常，例如线程工厂返回null或者是发生了OOM,直接回滚 * @param firstTask the task the new thread should run first (or * null if none). Workers are created with an initial first task * (in method execute()) to bypass queuing when there are fewer * than corePoolSize threads (in which case we always start one), * or when the queue is full (in which case we must bypass queue). * Initially idle threads are usually created via * prestartCoreThread or to replace other dying workers. * * @param core if true use corePoolSize as bound, else * maximumPoolSize. (A boolean indicator is used here rather than a * value to ensure reads of fresh values after checking other pool * state). * @return true if successful 下面是源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // cas修改c的值 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); // largestPoolSize记录的最大workers长度 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 线程启动，执行任务(Worker.thread(firstTask).start()); t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 线程start之后会执行如下run方法： 1234 /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this);&#125; 下面是runWorker方法 12345678910111213141516171819202122232425262728293031323334353637383940414243final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125; &#125; 通过getTask方法从阻塞队列中获取等待的任务，如果队列中没有任务，getTask方法会被阻塞并挂起，不会占用cpu资源； getTask()方法源码如下 下面来看一下getTask()方法，这里面涉及到keepAliveTime的使用，从这个方法我们可以看出先吃池是怎么让超过corePoolSize的那部分worker销毁的。 12345678910111213141516171819202122232425262728293031323334353637private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; allowCoreThreadTimeOut为false，线程即使空闲也不会被销毁；倘若为ture，在keepAliveTime内仍空闲则会被销毁。 如果线程允许空闲等待而不被销毁timed == false，workQueue.take任务: 如果阻塞队列为空，当前线程会被挂起等待；当队列中有任务加入时，线程被唤醒，take方法返回任务，并执行； 如果线程不允许无休止空闲timed == true, workQueue.poll任务: 如果在keepAliveTime时间内，阻塞队列还是没有任务，则返回null；","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之Blocking Queue","date":"2021-08-03T08:48:32.000Z","path":"wiki/Java并发编程之Blocking-Queue/","text":"Blocking QueueA blocking queue is a queue that blocks when you try to dequeue from it and the queue is empty, or if you try to enqueue items to it and the queue is already full. A thread trying to dequeue from an empty queue is blocked until some other thread inserts an item into the queue. A thread trying to enqueue an item in a full queue is blocked until some other thread makes space in the queue, either by dequeuing one or more items or clearing the queue completely. 阻塞队列两大特性： 当队列满时，如果生产者线程向队列 put 元素，队列会一直阻塞生产者线程，直到队列可用或者响应中断退出 当队列为空时，如果消费者线程 从队列里面 take 元素，队列会阻塞消费者线程，直到队列不为空 阻塞队列最常使用在生产者和消费者模型中，生产者生产数据，将数据存放在队列中，消费者消费数据，在队列中取出数据。 阻塞队列在不可用时，下面是各种处理操作的结果：👇 方法/处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e, time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 add 抛出异常IllegalStateException12345678public class ArrayBlockingQueueDemo &#123; public static void main(String[] args) &#123; ArrayBlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(1); queue.add(&quot;a&quot;); queue.add(&quot;b&quot;); System.err.println(&quot;queue size -&gt; &quot; + queue.size()); &#125;&#125; 异常信息： 1234Exception in thread &quot;main&quot; java.lang.IllegalStateException: Queue full at java.util.AbstractQueue.add(AbstractQueue.java:98) at java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:312) at com.ibli.note.ArrayBlockingQueueDemo.main(ArrayBlockingQueueDemo.java:15) element抛出异常NoSuchElementException1234567public class ArrayBlockingQueueDemo &#123; public static void main(String[] args) &#123; ArrayBlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(1); System.err.println(&quot;queue size -&gt; &quot; + queue.size()); queue.element(); &#125;&#125; 异常信息： 1234queue size -&gt; 0Exception in thread &quot;main&quot; java.util.NoSuchElementException at java.util.AbstractQueue.element(AbstractQueue.java:136) at com.ibli.note.ArrayBlockingQueueDemo.main(ArrayBlockingQueueDemo.java:14) ArrayBlockingQueue底层由数组实现的有界的阻塞队列，它的容量在创建的时候就已经确认了，并且不能修改。 12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125; 默认情况下，ArrayBlockingQueue是不保证线程公平访问队列的，这里所谓的公平与否是指，阻塞的线程能否按照阻塞的先后顺序访问队列，先阻塞先访问，后阻塞后访问。 思考为什么默认情况下是非公平的方式访问呢？ 🤔 这个是为了增加系统资源利用率，在不保证公平的情况下，多线程之间之间执行的效率要比公平模式下高的多。 ArrayBlovkingQueue#put方法 下面是put方法源码： 12345678910111213141516public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; // 加锁 lock.lockInterruptibly(); try &#123; while (count == items.length) // 队列满了之后，阻塞 notFull.await(); // 向队列中添加元素 enqueue(e); &#125; finally &#123; // 执行完最后释放锁 lock.unlock(); &#125;&#125; 下面是添加数据的方法源码： 1234567891011private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; // 数据添加完之后，唤醒等待队列中的线程到同步队列 notEmpty.signal();&#125; ‼️唤醒的线程能够抢到锁是不确定的，signal会添加节点到同步队列中等待获取锁。这个可以看一下Condition那篇文章。 ArrayBlockingQueue更多详细细节以及原理跳转链接https://www.jianshu.com/p/a636b3d83911 LinkedBlockingQueueLinkedBlockingQueue是用链表实现的有界阻塞队列，同样满足FIFO的特性，与ArrayBlockingQueue相比起来具有更高的吞吐量，为了防止LinkedBlockingQueue容量迅速增，损耗大量内存。通常在创建LinkedBlockingQueue对象时，会指定其大小，如果未指定，容量等于Integer.MAX_VALUE; Executors.newFixedThreadPool 阿里巴巴禁止使用Executors来创建线程池 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 属性信息12345678910111213141516171819202122232425262728293031323334353637/** * 节点类，用于存储数据 */static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125;&#125;/** 阻塞队列的大小，默认为Integer.MAX_VALUE */private final int capacity;/** 当前阻塞队列中的元素个数 */private final AtomicInteger count = new AtomicInteger();/** * 阻塞队列的头结点 */transient Node&lt;E&gt; head;/** * 阻塞队列的尾节点 */private transient Node&lt;E&gt; last;/** 获取并移除元素时使用的锁，如take, poll, etc */private final ReentrantLock takeLock = new ReentrantLock();/** notEmpty条件对象，当队列没有数据时用于挂起执行删除的线程 */private final Condition notEmpty = takeLock.newCondition();/** 添加元素时使用的锁如 put, offer, etc */private final ReentrantLock putLock = new ReentrantLock();/** notFull条件对象，当队列数据已满时用于挂起执行添加的线程 */private final Condition notFull = putLock.newCondition(); 构造函数123456789101112131415161718192021222324252627282930public LinkedBlockingQueue() &#123; // 默认大小为Integer.MAX_VALUE this(Integer.MAX_VALUE);&#125;public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125;public LinkedBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this(Integer.MAX_VALUE); final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; int n = 0; for (E e : c) &#123; if (e == null) throw new NullPointerException(); if (n == capacity) throw new IllegalStateException(&quot;Queue full&quot;); enqueue(new Node&lt;E&gt;(e)); ++n; &#125; count.set(n); &#125; finally &#123; putLock.unlock(); &#125;&#125; LinkedBlockingQueue#put方法1234567891011121314151617181920212223242526public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; // 获取锁中断 putLock.lockInterruptibly(); try &#123; //判断队列是否已满，如果已满阻塞等待 while (count.get() == capacity) &#123; notFull.await(); &#125; // 把node放入队列中 enqueue(node); c = count.getAndIncrement(); // 再次判断队列是否有可用空间，如果有唤醒下一个线程进行添加操作 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; // 如果队列中有一条数据，唤醒消费线程进行消费 if (c == 0) signalNotEmpty();&#125; 队列已满，阻塞等待。 队列未满，创建一个node节点放入队列中，如果放完以后队列还有剩余空间，继续唤醒下一个添加线程进行添加。如果放之前队列中没有元素，放完以后要唤醒消费线程进行消费。 ArrayBlockingQueue与LinkedBlockingQueue的比较相同点：ArrayBlockingQueue和LinkedBlockingQueue都是通过condition通知机制来实现可阻塞式插入和删除元素，并满足线程安全的特性； 不同点： 1、ArrayBlockingQueue底层是采用的数组进行实现，而LinkedBlockingQueue则是采用链表数据结构； 2、ArrayBlockingQueue插入和删除数据，只采用了一个lock，而LinkedBlockingQueue则是在插入和删除分别采用了putLock和takeLock，这样可以降低线程由于线程无法获取到lock而进入WAITING状态的可能性，从而提高了线程并发执行的效率 更多LinkedBlockingQueue的实现细节参见https://blog.csdn.net/tonywu1992/article/details/83419448 PriorityBlockingQueuePriorityBlockingQueue是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现compareTo()方法来指定元素排序规则，或者初始化时通过构造器参数Comparator来指定排序规则。 123456789public PriorityBlockingQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator) &#123; if (initialCapacity &lt; 1) throw new IllegalArgumentException(); this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); this.comparator = comparator; this.queue = new Object[initialCapacity]; &#125; 使用优先级队列需要注意的点： 1、队列中不允许出现null值，也不允许出现不能排序的元素。 2、队列容量是没有上限的，但是如果插入的元素超过负载，有可能会引起OutOfMemory异常。 当我们使用无界队列是都应该注意的点，不能在队列中无限存放数据 3、PriorityBlockingQueue由于是无界的，所以put方法是非阻塞的。 123public void put(E e) &#123; offer(e); // never need to block 请自行对照上面表格&#125; 可以给定初始容量，这个容量会按照一定的算法自动扩充 123456// Default array capacity.private static final int DEFAULT_INITIAL_CAPACITY = 11;public PriorityBlockingQueue() &#123; this(DEFAULT_INITIAL_CAPACITY, null);&#125; 这里默认的容量是 11，由于也是基于数组。 4、内部只有一个Lock，所以生产消费者不能同时作业 详情可以参照https://www.cnblogs.com/wyq1995/p/12289462.html DelayQueueDelayQueue顾名思义，具有延时作用的队列。 记得第一次接触延时队列的时候是在看分布式任务调度时看到底层有关延时队列的实现。 DelayQueue 也是一个无界阻塞队列，使用时要注意OOM。 只有delay时间小于0的元素才能够被取出。 生产者消费者模型创建一个类，实现Delayed方法，重写getDelay方法和compareTo方法； 12345678910111213141516171819202122232425262728293031323334353637public class DelayData implements Delayed &#123; private long second; private String val; public DelayData(long second, String val) &#123; long l = System.currentTimeMillis(); System.err.println(second + &quot; &quot; + l); this.second = second + l; this.val = val; &#125; public long getSecond() &#123; return second; &#125; @Override public long getDelay(TimeUnit unit) &#123; long diffTime = second - System.currentTimeMillis(); return unit.convert(diffTime,TimeUnit.MILLISECONDS); &#125; @Override public int compareTo(Delayed o) &#123; DelayData tmp = (DelayData) o; long result = second - tmp.getSecond() ; return (int) result; &#125; @Override public String toString() &#123; return &quot;DelayData&#123;&quot; + &quot;second=&quot; + second + &quot;, val=&#x27;&quot; + val + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 然后创建两个线程模拟生产者和消费者 123456789101112131415161718192021222324252627282930313233public class DelayQueueDemo &#123; public static void main(String[] args) &#123; DelayQueue&lt;DelayData&gt; delayQueue = new DelayQueue&lt;DelayData&gt;(); new Thread(() -&gt; &#123; delayQueue.put(new DelayData(5000, &quot;a&quot;)); delayQueue.put(new DelayData(10000, &quot;b&quot;)); delayQueue.put(new DelayData(15000, &quot;c&quot;)); &#125;).start(); new Thread(() -&gt; &#123; boolean flag = true; while (true &amp;&amp; flag) &#123; try &#123; Thread.sleep(1000); System.err.println(&quot;执行一次循环 队列长度&quot; + delayQueue.size()); DelayData poll = delayQueue.take(); if (poll != null)&#123; System.err.println(poll.toString()); &#125; if (delayQueue.size() == 0)&#123; flag = false; break; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125; SynchronousQueueSynchronousQueue实际上它不是一个真正的队列，因为它不会维护队列中元素的存储空间，与其他队列不同的是，它维护一组线程，这些线程在等待把元素加入或移除队列。适用于生产者少消费者多的情况。 SynchronousQueue是生产者直接把数据给消费者（消费者直接从生产者这里拿数据）。换句话说，每一个插入操作必须等待一个线程对应的移除操作。SynchronousQueue又有两种模式： 1、公平模式 采用公平锁，并配合一个FIFO队列（Queue）来管理多余的生产者和消费者 2、非公平模式 采用非公平锁，并配合一个LIFO栈（Stack）来管理多余的生产者和消费者，这也是SynchronousQueue默认的模式 构造方法123456 public SynchronousQueue() &#123; this(false); &#125;public SynchronousQueue(boolean fair) &#123; transferer = fair ? new TransferQueue() : new TransferStack();&#125; transferer 是一个内部类用于在生产者和消费者之间传递数据 实现生产者消费者下面模拟一个生产者生产数据，两个消费者消费数据。 123456789101112131415161718192021222324252627282930313233343536373839404142public class SynchronousQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; SynchronousQueue queue = new SynchronousQueue(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(2000L); while (true) &#123; Thread.sleep(100); long l = System.currentTimeMillis(); queue.put(l); System.out.println(Thread.currentThread().getName() + &quot; 生产者生产数据 :&quot; + l); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); while (true) &#123; Thread.sleep(300); System.out.println(Thread.currentThread().getName() + &quot;消费者消费数据 ： &quot; + queue.take()); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); while (true) &#123; Thread.sleep(300); System.out.println(Thread.currentThread().getName() + &quot;消费者消费数据 ： &quot; + queue.take()); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 代码运行结果： 1234567Thread-0 生产者生产数据 :1628055947404Thread-1消费者消费数据 ： 1628055947404Thread-0 生产者生产数据 :1628055947506Thread-2消费者消费数据 ： 1628055947506Thread-0 生产者生产数据 :1628055947608Thread-2消费者消费数据 ： 1628055947608Thread-0 生产者生产数据 :1628055947713 SynchronousQueue详细实现细节参见https://blog.csdn.net/yanyan19880509/article/details/52562039 参考资料https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html http://tutorials.jenkov.com/java-concurrency/index.html https://www.baeldung.com/java-blocking-queue https://blog.csdn.net/tonywu1992/article/details/83419448","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之中断机制","date":"2021-08-03T07:56:37.000Z","path":"wiki/Java并发编程之中断机制/","text":"中断机制Java语言提供一种机制来试图“终止”一些特殊的线程，比如一下空转的线程一直消耗系统资源，可以使用中断的方式来停止这一类的线程，这就是Java中断机制。 1、中断注意的地方1、Java中线程间是协作式，而非抢占式. 调用一个线程的interrupt() 方法中断一个线程，并不是强行关闭这个线程，只是跟这个线程打个招呼，将线程的中断标志位置为true，线程是否中断，由线程本身决定。 2、isInterrupted() 判定当前线程是否处于中断状态。 3、静态方法 interrupted() 判定当前线程是否处于中断状态，同时中断标志位改为 false。 4、如果方法里如果抛出中断异常 InterruptedException，则线程的中断标志位会被复位成false，如果确实是需要中断线程，要求我们自己在catch语句块里再次调用interrupt()。 5、Java 中所有的阻塞方法都会抛出 InterruptedException，比如wait(), join(),sleep()。 2、Java中断提供的方法在Java中提供了3个有关中断的方法： Thread.currentThread().isInterrupted() 判断当前的线程是否被中断 thread.interrupt(); 中断一个线程，将中断标志设置成true 1234567891011121314public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; Thread.interrupted()123public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; 判断线程是否被中断，并清除中断标志，改成false； 验证一下就可以了 👇 12345678public static void main(String[] args) &#123; System.err.println(Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); System.err.println(Thread.currentThread().isInterrupted()); boolean interrupted = Thread.interrupted(); System.err.println(&quot;interrupted &quot; + interrupted); System.err.println(Thread.currentThread().isInterrupted());&#125; 3、中断例子1234567891011121314151617181920212223242526272829public class InterrupterDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (true &amp;&amp; !Thread.currentThread().isInterrupted())&#123; System.err.println(1); System.err.println(Thread.interrupted()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; System.err.println(&quot;after sleep &quot;+ Thread.currentThread().isInterrupted()); Thread.currentThread().interrupt(); boolean interrupted = Thread.interrupted(); System.err.println(&quot;interrupted &quot;+ Thread.currentThread().isInterrupted() + &quot;/ &quot; + interrupted); Thread.currentThread().interrupt(); System.err.println(&quot;final sleep &quot;+ Thread.currentThread().isInterrupted()); break; &#125; System.err.println(2); &#125; &#125;); thread.start(); Thread.sleep(3000); thread.interrupt(); &#125;&#125; 注意，中断一场不要【 吞掉 】，要不在程序中相应中断一场，进行相应的逻辑处理，或者将一场继续向上抛，由上层处理。 参考资料https://dayarch.top/p/java-concurrency-interrupt-mechnism.html","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之Condition机制底层","date":"2021-08-03T04:00:16.000Z","path":"wiki/Java并发编程之Condition机制底层/","text":"Lock框架中的Condition机制还是看一下之前ReentrantLock中调用condition方法的流程图 👇 任何一个java对象都天然继承于Object类，在线程间实现通信的往往会应用到Object的几个方法，比如wait(),wait(long timeout),wait(long timeout, int nanos)与notify(),notifyAll()几个方法实现等待/通知机制，同样的， 在java Lock体系下依然会有同样的方法实现等待/通知机制。 从整体上来看Object的wait和notify/notify是与对象监视器配合完成线程间的等待/通知机制，而Condition与Lock配合完成等待通知机制，前者是java底层级别的，后者是语言级别的，具有更高的可控制性和扩展性。两者除了在使用方式上不同外，在功能特性上还是有很多的不同： Condition能够支持不响应中断，而通过使用Object方式不支持； Condition能够支持多个等待队列（new 多个Condition对象），而Object方式只能支持一个； Condition能够支持超时时间的设置，而Object不支持 1. Condition接口提供的方法1.1 await方法void await() throws InterruptedException 当前线程进入等待状态，如果其他线程调用condition的signal或者signalAll方法并且当前线程获取Lock从await方法返回，如果在等待状态中被中断会抛出被中断异常； long awaitNanos(long nanosTimeout) 当前线程进入等待状态直到被通知，中断或者超时； boolean await(long time, TimeUnit unit)throws InterruptedException 同第二种，支持自定义时间单位 boolean awaitUntil(Date deadline) throws InterruptedException 当前线程进入等待状态直到被通知，中断或者到了某个时间 1.2 signal方法void signal() 唤醒一个等待在condition上的线程，将该线程从等待队列中转移到同步队列中，如果在同步队列中能够竞争到Lock则可以从等待方法中返回。 void signalAll() 与1的区别在于能够唤醒所有等待在condition上的线程。 2. Condition在ReentrantLock中的使用下面先通过一个例子看一下Condition的使用 👇 1、大致流程就是线程1先获取lock之后，执行线程1的方法，然后调用condition.await();方法阻塞当前线程；同时加入Condition等待队列 2、线程1释放lock之后，线程2而已经在同步队列中了，线程2获取lock执行权，执行condition.signal()方法唤醒线程1 3、线程1被唤醒之后，node节点重新添加到同步队列中，等待获取执行权限，在线程2调用了unlock()方法之后，线程1重新获取到lock之后，执行后续流程。 123456789101112131415161718192021222324252627282930313233343536public class ReentrantLockDemo &#123; static Lock lock = new ReentrantLock(); public static void main(String[] args) &#123; Condition condition = lock.newCondition(); new Thread(()-&gt;&#123; System.err.println(&quot;enter thread 1 &quot;); lock.lock(); try &#123; try &#123; System.err.println(&quot;thread 1 invoke await&quot;); condition.await(); System.err.println(&quot;thread 1 invoked signal&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.err.println(&quot;exit thread 1 &quot;); &#125;finally &#123; lock.unlock(); &#125; &#125;).start(); new Thread(()-&gt;&#123; System.err.println(&quot;enter thread 2 &quot;); lock.lock(); try &#123; System.err.println(&quot;thread 2 invoke signal&quot;); condition.signal(); System.err.println(&quot;exit thread 2 &quot;); &#125;finally &#123; lock.unlock(); &#125; &#125;).start(); &#125;&#125; 上面代码的执行结果可以猜想一下 1234567enter thread 1 thread 1 invoke awaitenter thread 2 thread 2 invoke signalexit thread 2 thread 1 invoked signalexit thread 1 3. Condition等待/通知实现原理要想能够深入的掌握condition还是应该知道它的实现原理，现在我们一起来看看condiiton的源码。创建一个condition对象是通过lock.newCondition(),而这个方法实际上是会new出一个ConditionObject对象，该类是AQS的一个内部类，和Node类一样，非常重要。 condition是要和lock配合使用的也就是condition和Lock是绑定在一起的，而lock的实现原理又依赖于AQS，自然而然ConditionObject作为AQS的一个内部类无可厚非。 我们知道在锁机制的实现上，AQS内部维护了一个同步队列，如果是独占式锁的话，所有获取锁失败的线程的尾插入到同步队列，同样的，condition内部也是使用同样的方式，内部维护了一个 等待队列，所有调用condition.await方法的线程会加入到等待队列中，并且线程状态转换为等待状态。 另外注意到ConditionObject中有两个成员变量： private transient Node firstWaiter; private transient Node lastWaiter; 在AQS中condition队列可以存在多个如下所示，但是同步队列之可能是一个，值得注意的是，同步队列是一个双向链表队列，而等待队列是一个单向的队列。 下面从await方法入手来学习Condition的机制是如何运转的。 3.1 等待awaitpublic class ConditionObject implements Condition AQS#ConditionObject内部类实现了Condition接口的await方法： 1234567891011121314151617181920212223242526public final void await() throws InterruptedException &#123; // 判断线程是否中断 if (Thread.interrupted()) throw new InterruptedException(); // 将节点添加到等待队列 Node node = addConditionWaiter(); // 进入等待队列中的线程需要释放lock让给别的线程 int savedState = fullyRelease(node); int interruptMode = 0; // 如果节点不在同步队列，则挂起当前线程，知道进入同步队列或者被中断 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 调用await的线程会一直阻塞在上面的while循环，知道被唤醒或者相应中断，才会执行下面的方法 // 进入同步队列尝试获取lock，和之前一样，为了限制一直空转，会在第二次循环之后，park此节点，知道队列中轮到这个线程出队 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled // 清除掉取消的节点，踢出等待队列 unlinkCancelledWaiters(); //处理被中断的情况 if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; AQS#addConditionWaiter 添加节点到等待队列 123456789101112131415private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125; 这个方法应该比较好理解吧，就是添加一个节点，到等待队列。 ⚠️ 这里和把节点添加到同步队列还有点区别，不知道大家还有没有印象，在同步队列添加节点的时候，先判断tail是否为空，如果不是空，则直接添加；如果是空，则调用了enq(Node node)方法，先生成一个head节点，然后在把当前节点添加到后面，循环了两遍的。 这里是直接创建当前节点，然后将firstWaiter指针指向了node； AQS#fullyRelease 释放lock 123456789101112131415final int fullyRelease(Node node) &#123; boolean failed = true; try &#123; int savedState = getState(); if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 这个方法也不难，想一下，线程都已经调用await方法了，而且上一步就已经把节点添加到了等待队列中了，那么接下来要做什么呢？那肯定是释放锁lock了。对，这个方法就是做这个的。release方法之前已经介绍了，无非就是对state做一下减法，把对战线程清空一下，给新来的线程腾地方。 下面才是await的关键核心代码：‼️ 12345while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; isOnSyncQueue(node)判断当前节点是否在同步队列中，为什么要这个判断呢？原因很简单，当别的线程或者自己调用了signal方法之后，会把当前节点转移到同步队列中，在同步队列中说明什么呢，说明接下来这个线程要去竞争锁了，也就是被唤醒了，当竞争锁成功之后，这个线程就可以await后面的方法了。 (interruptMode = checkInterruptWhileWaiting(node)) != 0 如果当前线程被中断，则可以直接跳出循环，去竞争锁。 3.2 通知signal调用condition的signal或者signalAll方法可以将等待队列中等待时间最长的节点移动到同步队列中，使得该节点能够有机会获得lock。按照等待队列是先进先出（FIFO）的，所以等待队列的头节点必然会是等待时间最长的节点，也就是每次调用condition的signal方法是将头节点移动到同步队列中。signal方法源码为： 123456789public final void signal() &#123; //1. 先检测当前线程是否已经获取lock，如果没有获得锁，肯定是说不通的 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //2. 获取等待队列中第一个节点，之后的操作都是针对这个节点 Node first = firstWaiter; if (first != null) doSignal(first);&#125; signal方法首先会检测当前线程是否已经获取lock，如果没有获取lock会直接抛出异常，如果获取的话再得到等待队列的头指针引用的节点，之后的操作的doSignal方法也是基于该节点。下面我们来看看doSignal方法做了些什么事情。 AQS#doSignal 12345678910private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; //1. 将头结点从等待队列中移除 first.nextWaiter = null; //2. while中transferForSignal方法对头结点做真正的处理 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125; 具体逻辑请看注释，真正对头节点做处理的逻辑在transferForSignal放，该方法源码为： 12345678910111213141516171819202122final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ //1. 更新状态为0，加入同步队列的节点的初始状态是0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ //2.将该节点移入到同步队列中去 Node p = enq(node); int ws = p.waitStatus; // p节点是node的前置节点，需要将前驱节点的状态设置成Node.SIGNAL if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125; 关键逻辑请看注释，这段代码主要做了两件事情 1.将头结点的状态更改为CONDITION； 2.调用enq方法，将该节点尾插入到同步队列中，并且把前驱节点的状态设置成Node.SIGNAL 现在我们可以得出结论：调用condition的signal的前提条件是当前线程已经获取了lock，该方法会使得等待队列中的头节点即等待时间最长的那个节点移入到同步队列，而移入到同步队列后才有机会使得等待线程被唤醒，即从await方法中的LockSupport.park(this)方法中返回，从而才有机会使得调用await方法的线程成功退出。 signalAll方法通知所有等待线程 sigllAll与sigal方法的区别体现在doSignalAll方法上，前面我们已经知道doSignal方法只会对等待队列的头节点进行操作，而doSignalAll的源码为： 123456789private void doSignalAll(Node first) &#123; lastWaiter = firstWaiter = null; do &#123; Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; &#125; while (first != null);&#125; 该方法只不过时间等待队列中的每一个节点都移入到同步队列中，即“通知”当前调用condition.await()方法的每一个线程。 面试题 两个线程交替顺序打印1～10012345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.ibli.note;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class WaitNotifyDemo implements Runnable &#123; int count = 1; private Condition condition; private Lock lock; public WaitNotifyDemo(Condition condition, Lock lock) &#123; this.condition = condition; this.lock = lock; &#125; @Override public void run() &#123; while (true) &#123; lock.lock(); try &#123; condition.signal(); if (count &gt; 100) &#123; break; &#125; System.err.println(Thread.currentThread().getName() + &quot; =&gt; &quot; + count); count++; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;finally &#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; Lock lock = new ReentrantLock(); Condition condition = lock.newCondition(); WaitNotifyDemo waitNotifyDemo = new WaitNotifyDemo(condition, lock); new Thread(waitNotifyDemo).start(); new Thread(waitNotifyDemo).start(); &#125;&#125; 参考资料https://juejin.cn/post/6844903602419400718 https://juejin.cn/post/6844903654873382925","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之深入理解ReentrantLock","date":"2021-08-02T08:00:33.000Z","path":"wiki/Java并发编程之深入理解ReetrantLock/","text":"ReentrantLockReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞 加锁操作支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞 下面以非公平锁的lock方法为例，看一下ReentrantLock源码的实现 👇 首先是lock方法1、进入lock方法首先对调用compareAndSetState(0,1)去尝试获取锁，这一点正是体现了非公平锁 2、如果第一步没有获取到锁，然后执行第二步acquire(1) 1234567final void lock() &#123; // 非公平锁 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; lock方法首先会去cas修改AQS的state状态，独占锁模式下state增加1表示获取锁成功；state设置成功之后，需要将独占线程字段设置成当前线程：exclusiveOwnerThread = thread; AQS#acquire(1)如果没有抢占到锁，那么执行下面的acquire方法，这个方法定义在AQS类中 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; tryAcquire方法是在子类实现的，在这里我们看一下ReentrantLock的nonfairTryAcquire，也就是非公平锁的实现。 nonfairTryAcquire(int acquires)方法下面是ReentrantLock，非公平锁的lock实现代码： 1234567891011121314151617181920final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); // 获取锁 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; // 锁冲入 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; int c = getState() == 0 则表示没有线程占有锁，当前线程来加锁时，可以直接使用cas尝试获取锁。 current == getExclusiveOwnerThread() 表示当前线程已经持有线程锁了， int nextc = c + acquires;则表示支持锁重入，nextc的值则表示锁重入的次数； 以上如果没有加锁成功，则返回false，然后执行AQS的acquireQueue方法，首先将当前节点封装成addWaiter(Node.EXCLUSIVE), arg) 添加到同步队列，同时判断头节点是否获取锁成功，如果成功了，将当前节点添加到头上； AQS#addWaiter(Node mode)添加节点到队列中，Node.EXCLUSIVE独占锁，这里采用的是尾插法，在队列的队尾添加新的节点。 12345678910111213141516private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // 如果队列不是空的，则直接添加到队尾 if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 如果是空的，则调用enq方法，创建队列，并添加到队尾 enq(node); return node;&#125; AQS#enq(final Node node)第一个线程获取锁的时候，肯定是无锁的状态，根本走不到这一步，最早走到这里的是第二个去获取锁的线程。 当第二个线程执行到该方法是需要执行两次循环： 1、t == null时，需要初始化队列 2、执行下一次循环，将node添加到tail,由于这个方法还是处在并发环境下的，所以，设置队尾的时候还是需要cas操作。 123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; AQS#acquireQueued(final Node node, int arg)这个方法绝对是绝对的AQS核心方法 ‼️ 这个方法主要有3个重要操作： 1、判断前置节点是不是head，如果是的话，去尝试获取锁； 2、如果前置节点不是head，要把前置节点的waitState设置成SIGNAL，同时park当前线程，避免一直空转，因为这里是用的 for (;;) {} 3、如果获取锁和park都失败了，则把当前节点设置成cancel状态。 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; AQS.cancelAcquire(Node node)这个方法比较难理解，总结一下就干了下面几个事： 1、执行到这个方法的node肯定是要取消的，那个需要thread设置成null 2、查看当前节点之前的节点有没有是取消状态的，一起踢出队列 3、把当前节点设置成Node.CANCELLED状态 4、判断node在队列中的位置，如果是队尾的话，把tail指向node的前置节点，并且把前驱节点的next指向null 5、如果不是tail节点，那么判断是不是head，如果不是head，那么，将node的前驱节点的状态设置成Node.SIGNAL，并且把node的前驱节点node的next节点 6、如果node是head节点，那么直接unpark此线程去执行acquire 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private void cancelAcquire(Node node) &#123; // Ignore if node doesn&#x27;t exist if (node == null) return; node.thread = null; // Skip cancelled predecessors Node pred = node.prev; while (pred.waitStatus &gt; 0) //cancelled node.prev = pred = pred.prev; // predNext is the apparent node to unsplice. CASes below will // fail if not, in which case, we lost race vs another cancel // or signal, so no further action is necessary. Node predNext = pred.next; // Can use unconditional write instead of CAS here. // After this atomic step, other Nodes can skip past us. // Before, we are free of interference from other threads. node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // If successor needs signal, try to set pred&#x27;s next-link // so it will get one. Otherwise wake it up to propagate. int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; // if执行的逻辑是把前置节点设置成Node.SIGNAL Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) // 把node的前置前置节点的下一个节点指向node的下一个节点，因为上面node已经是Node.CANCELLED状态了，需要踢出队列 compareAndSetNext(pred, predNext, next); &#125; else &#123; // 前置节点是head，此时没有被人竞争锁资源，直接唤醒当前节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125; &#125; 上面是以ReentrantLock的非公平锁为例学习了一下ReentrantLock加锁的过程。那么思考一下公平锁和非公平锁的有什么区别呢？🤔 理解了上面的流程之后，下面直接比较源码遍很好理解两者之间的区别！ 公平锁和非公平锁如何制定ReentarntLock的公平锁和非公平锁？ 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 上面看了NonfairSync#lock的实现，下面看一下FairSync#lock的实现：👇 12345678910111213141516171819202122232425262728293031static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; final void lock() &#123; acquire(1); &#125; /** * Fair version of tryAcquire. Don&#x27;t grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125;&#125; FairSync和NonfairSync都是ReentrantLock的静态内部类，在FairSync的lock方法中，没有下面的代码： 12if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); 每一个线程都直接调用AQS#acquire(1)方法，而且在ReentrantLock#FairSync#FairSync(int acquires)的实现中，添加了一个判断 12345if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; 也就是hasQueuedPredecessors方法，这个方法的作用是判断队列中是否有节点在等待，如果有的话，ReentrantLock#FairSync#FairSync(int acquires)直接返回false，当前节点智能进入到队列中。这两点就是公平锁和非公平锁的明显区别体现。 释放锁操作 unlock()123public void unlock() &#123; sync.release(1);&#125; AQS#release(int arg)123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease的具体实现仍是有具体的子类来实现的。 ReentrantLock#tryRelease(int releases)方法1、释放锁的逻辑应该比较好理解，是将state做减法。 2、判断state == 0 , 则表示无锁状态，如果不是0，则表示还在线程重入的状态下，同时设置state 123456789101112protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; 这里注意一点，设置state的时候是直接赋值的，而没有使用cas，为什么？ 123protected final void setState(int newState) &#123; state = newState;&#125; 其实考虑到上下文就很简单了，此时设置state的时候，有两种状态，无锁和重入锁，肯定不会是多线程的场景。所以不需要cas操作。 接着分析上面的AQS#release方法: 当state设置成功之后，需要判断head节点，然后唤醒head的后驱节点的线程，如果存在的话。 12345678910111213141516171819202122232425262728private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; // if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 这里是共享锁，在ReentarntLock先跳过 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; tryLock(long time, TimeUnit unit)方法描述如下：在给定的等待时间内并且线程没有被中断以及锁可用的情况下，去获取锁。如果锁可用，方法会直接返回。如果锁不可用，则当前线程将会处于不可用状态以达到线程调度目的，并且休眠直到下面三个事件中的一个发生：①、当前线程获取到锁②、其他线程中断当前线程③、指定的等待时间已过假如当前线程：在该方法的条目上设置其中断状态或在获取锁时中断，并且支持锁获取中断时，将抛出中断异常，当前线程中断状态会被清除。假如给定的等待时间已过，将会返回false。 下面具体阅读源码实现,方法的入参指定了等待时间，和时间的单位，有NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS…等单位。 下面具体阅读源码实现,方法的入参指定了等待时间，和时间的单位，有NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS…等单位。 1234public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(timeout));&#125; 方法的内部调用了Sync的tryAcquireNanos，继续往下 123456789public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; //判断中断状态并决定是否抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //尝试获取锁，如果成功则返回true，失败则调用doAcquireNanos进行等待 return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout);&#125; tryAcqure和之前分析的是同一个方法，不再赘述。接下来是doAcquireNanos方法 12345678910111213141516171819202122232425262728293031323334353637383940private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; //如果给定的时间值小于等于0，则直接返回false if (nanosTimeout &lt;= 0L) return false; //根据给定参数计算截止时间 final long deadline = System.nanoTime() + nanosTimeout; //将当前线程添加到CLH等待队列 final Node node = addWaiter(Node.EXCLUSIVE); //初始失败标志 boolean failed = true; try &#123; //在给定时间内循环/自旋尝试获取锁 for (;;) &#123; //取出前置节点 final Node p = node.predecessor(); //如果前置节点为首节点，并且当前线程能够成功获取锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC 前首节点出队，帮助GC failed = false; return true; &#125; //判断是否等待超时，如果超时，则返回false nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) return false; //这里判断是否可以阻塞线程并做相应操作，跟之前分析的几个方法不一样的是，这里的阻塞多了一个判断，并且是在有限时间内阻塞，类似于sleep if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); //判断中断状态，并决定是否抛出异常 if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; doAcquireNanos的阻塞是有时间限制的，所以能在给定的时间内，返回获取锁的操作结果。 参考资料https://juejin.cn/post/6870099231361728525 https://www.processon.com/view/5f047c16f346fb1ae598b4dd?fromnew=1 https://www.imooc.com/article/51118","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程之AQS底层实现与原理","date":"2021-08-02T07:17:07.000Z","path":"wiki/Java并发编程之AQS底层实现与原理/","text":"AQS锁限时等待是如何实现的？ 公平锁与非公平锁流程是怎样的？ 独占锁&amp;共享锁独占锁即只允许一个线程获取同步状态，当这个线程还没有释放同步状态时，其他线程是获取不了的，只能加入到同步队列，进行等待。 公平锁&amp;非公平锁公平锁公平策略：在多个线程争用锁的情况下，公平策略倾向于将访问权授予等待时间最长的线程。也就是说，相当于有一个线程等待队列，先进入等待队列的线程后续会先获得锁，这样按照“先来后到”的原则，对于每一个等待线程都是公平的。 非公平锁在多个线程争用锁的情况下，能够最终获得锁的线程是随机的（由底层OS调度）。 注意：一般情况下，使用公平策略的程序在多线程访问时，总体吞吐量（即速度很慢，常常极其慢）比较低，因为此时在线程调度上面的开销比较大。 AQS是什么同步器是用来构建锁和其他同步组件的基础框架，它的实现主要依赖于一个int类型的成员变量来表示同步状态以及一个FIFO队列构建等待队列。它的子类必须重写AQS定义的几个protected修饰的用来改变同步状态的方法，其他方法主要是用来实现排队和阻塞机制的。 同步器是实现锁的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义，可以这样理解两者的关系： 锁是面向使用者的，它定义了使用者和锁交互的接口，隐藏了实现的细节，同步器是面向锁的实现者，它简化了锁的实现方式，屏蔽了同步状态的管理，线程的排队，等待和唤醒等底层操作。 AQS的设计是使用模版方法设计模式，它将一个方法开放给子类重写，而同步器给同步组件所提供的模版方法又会重新调用子类所重写的方法。 AQS核心思想如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。 如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 1、AQS使用一个int成员变量来表示同步状态 2、使用Node实现FIFO队列，可以用于构建锁或者其他同步装置 AQS资源共享方式：独占Exclusive（排它锁模式）和共享Share（共享锁模式） AQS它的所有子类中，要么实现并使用了它的独占功能的api，要么使用了共享锁的功能，而不会同时使用两套api，即便是最有名的子类ReentrantReadWriteLock也是通过两个内部类读锁和写锁分别实现了两套api来实现的 state状态state状态使用volatile int类型的变量，表示当前同步状态。state的访问方式有三种: getState() setState() compareAndSetState() Node内部类Node类是AQS的绝对核心类，AQS基于Node来构建同步队列和Condition队列； 源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /** waitStatus value to indicate successor&#x27;s thread needs unparking */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; volatile Node next; volatile Thread thread; Node nextWaiter; /** * Returns true if node is waiting in shared mode. */ final boolean isShared() &#123; return nextWaiter == SHARED; &#125; // 获取前置节点 final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125; &#125; CANCELLED waitStatus值为1时表示该线程节点已释放（超时、中断），已取消的节点不会再阻塞。 SIGNAL waitStatus为-1时表示该线程的后续线程需要阻塞，即只要前置节点释放锁，就会通知标识为 SIGNAL 状态的后续节点的线程 CONDITION waitStatus为-2时，表示该线程在condition队列中阻塞（Condition有使用） PROPAGATE waitStatus为-3时，表示该线程以及后续线程进行无条件传播（CountDownLatch中有使用）共享模式下， PROPAGATE 状态的线程处于可运行状态 AQS之独占+非公平获取锁acquire ReentrantLock是AQS独占模式的经典实现，ReentrantLock在构造实例是可以指定是否是fair lock。 123456789/** * Creates an instance of &#123;@code ReentrantLock&#125; with the * given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; acquire方法获取许可下面我们就从锁的获取入手开始解读AQS： 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; tryAcquire抽象方法tryAcquire是个protected方法，具体是实现在对应的子类中，这个方法的功能就是尝试去修改state的状态值 123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; nonfairTryAcquire非公平锁获取许可1234567// ReentrantLock 非公平锁进来就开始抢占锁，体现非公平性final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; 以ReentrantLock方法的实现为例，看一下源码： 123456789101112131415161718192021222324final boolean nonfairTryAcquire(int acquires) &#123; // 获取当前线程 final Thread current = Thread.currentThread(); // getState()返回的就是AQS类中的state字段的值 int c = getState(); // c == 0 说明当前锁没有被任何线程占有 if (c == 0) &#123; // 使用cas去修改state的值，独占模式下acquires = 1 if (compareAndSetState(0, acquires)) &#123; // 修改state成功之后，将独占线程设置成当前线程，并且返回true，表示抢占锁成功 setExclusiveOwnerThread(current); return true; &#125; &#125; // 如果state ！= 0 并且独占线程就是当前线程，表示当前线程持有对象的锁，此时，需要锁重入，state继续累加 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; AQS的acquire(int arg)方法中还有一部分就是 acquireQueued(addWaiter(Node.EXCLUSIVE), arg) addWaiter添加等待队列我们先看一下addWaiter方法，java.util.concurrent.locks.AbstractQueuedSynchronizer#addWaiter 如果tryAcquire返回FALSE（获取同步状态失败），则调用该方法将当前线程加入到CLH同步队列尾部。 1234567891011121314151617private Node addWaiter(Node mode) &#123; // 首先创建一个Node节点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // cas 将当前节点设置到同步队列的队尾 if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 如果上面cas设置没有成功，则通过enq方法将节点添加到队尾 enq(node); return node; &#125; java.util.concurrent.locks.AbstractQueuedSynchronizer#enq 12345678910111213141516private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize 初始化头节点 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125; 通过自旋，最终将node节点添加到同步队列中。 acquireQueued获取许可节点添加到同步队列之中，然后是一个非常重要的方法 ‼️ acquireQueued方法为一个自旋的过程，也就是说当前线程（Node）进入同步队列后，就会进入一个自旋的过程，每个节点都会自省地观察，当条件满足，获取到同步状态后，就可以从这个自旋过程中退出，否则会一直执行下去。 1234567891011121314151617181920212223242526final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 如果node节点是队列中第二个节点（因为第一个正在执行状态）肯定要队列中从第二个节点开始尝试获取锁 final Node p = node.predecessor(); // 第二个节点调用tryAcquire方法 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //把当前节点设置成队列头节点 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; //判断是否需要挂起队列中后续的节点 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; // 如果获取锁失败 if (failed) cancelAcquire(node); &#125; &#125; shouldParkAfterFailedAcquire方法shouldParkAfterFailedAcquire将队列后续节点挂起 12345678910111213141516171819private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 如果前一个节点的waitStatus == Node.SIGNAL 则直接返回true // 因为前一个节点状态是Node.SIGNAL时，才会通知后续节点进行park或者unpark return true; if (ws &gt; 0) &#123; // static final int CANCELLED = 1; // 取消状态的节点直接在等待队列中去除 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 将前一个节点的waitStatus设置成Node.SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false; &#125; 12345private final boolean parkAndCheckInterrupt() &#123; // 挂起当前线程，走到这肯定是没有拿到执行权的，线程需要挂起等待其他线程释放锁 LockSupport.park(this); return Thread.interrupted();&#125; 最后如果获取失败的话，会调用下面这个方法： cancelAcquire取消获取123456789101112131415161718192021222324252627282930313233343536373839private void cancelAcquire(Node node) &#123; // 如果节点为空，直接返回 if (node == null) return; // 由于线程要被取消了，所以将 thread 线程清掉 node.thread = null; // 下面这步表示将 node 的 pre 指向之前第一个非取消状态的结点（即跳过所有取消状态的结点）,waitStatus &gt; 0 表示当前结点状态为取消状态 Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 获取经过过滤后的 pre 的 next 结点，这一步主要用在后面的 CAS 设置 pre 的 next 节点上 Node predNext = pred.next; // 将当前结点设置为取消状态 node.waitStatus = Node.CANCELLED; // 如果当前取消结点为尾结点，使用 CAS 则将尾结点设置为其前驱节点，如果设置成功，则尾结点的 next 指针设置为空 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // 这一步看得有点绕，我们想想，如果当前节点取消了，那是不是要把当前节点的前驱节点指向当前节点的后继节点 // 但是我们之前也说了，要唤醒或阻塞结点，须在其前驱节点的状态为 SIGNAL 的条件才能操作， //所以在设置 pre 的 next 节点时要保证 pre 结点的状态为 SIGNAL，想通了这一点相信你不难理解以下代码。 int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; // 如果 pre 为 head，或者 pre 的状态设置 SIGNAL 失败，则直接唤醒后继结点去竞争锁，之前我们说过， SIGNAL 的结点取消（或释放锁）后可以唤醒后继结点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 释放锁release123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease方法也是由子类来实现的。 1234567891011121314protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; // 判断当前线程 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; // 将独占线程设置成null，下一个线程获取到锁时会设置成自己的 setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; 下面是执行unparkSuccessor(h);方法了，当前线程释放了锁之后，需要唤醒等待队列中的第二个节点对应的线程。这里注意一点的是，要执行的Node节点的waitStatus肯定是0；？？ 1234567891011121314151617private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) //置零当前线程所在的结点状态，允许失败 compareAndSetWaitStatus(node, ws, 0);// 从第二个节点开始往后找waitStatus&lt;=0的节点，然后执行unpark Node s = node.next; // 找到下一个需要唤醒的结点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; Condition在AQS中的实现上面已经介绍了AQS所提供的核心功能，当然它还有很多其他的特性，这里我们来继续说下Condition这个组件。 Condition是在java 1.5中才出现的，它用来替代传统的Object的wait()、notify()实现线程间的协作，相比使用Object的wait()、notify()，使用Condition中的await()、signal()这种方式实现线程间协作更加安全和高效。因此通常来说比较推荐使用`Condition 其中AbstractQueueSynchronizer中实现了Condition中的方法，主要对外提供awaite(Object.wait())和signal(Object.notify())调用。 Condition在java代码中的应用12345678910111213141516171819202122232425262728293031323334public class ReentrantLockDemo &#123; static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) &#123; Condition condition = lock.newCondition(); new Thread(() -&gt; &#123; lock.lock(); try &#123; System.out.println(&quot;线程一加锁成功&quot;); System.out.println(&quot;线程一执行await被挂起&quot;); condition.await(); System.out.println(&quot;线程一被唤醒成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); System.out.println(&quot;线程一释放锁成功&quot;); &#125; &#125;).start(); new Thread(() -&gt; &#123; lock.lock(); try &#123; System.out.println(&quot;线程二加锁成功&quot;); condition.signal(); System.out.println(&quot;线程二唤醒线程一&quot;); &#125; finally &#123; lock.unlock(); System.out.println(&quot;线程二释放锁成功&quot;); &#125; &#125;).start(); &#125;&#125; 线程一调用了condition.await();之后，线程二才可以获取到锁并且执行自己的任务，线程二调用 condition.signal();之后唤醒线程一，但是还没有执行权限，只有在线程二执行完成之后调用lock.unlock();之后，线程一重新回去到锁，然后执行线程一后续的流程。 await方法await使当前线程释放锁，也就是执行许可，然后进入Condition队列，等待在某个时刻被某个线程唤醒。 12345678910111213141516171819202122232425public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 将当前线程封装成Node节点添加的Condition队列中 Node node = addConditionWaiter(); // 添加到Condition队列中的线程需要释放锁资源 int savedState = fullyRelease(node); int interruptMode = 0; // 查看当前节点是不是在同步队列中 while (!isOnSyncQueue(node)) &#123; // 当前节点不在同步队列中，那么直接park挂起 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 表明已经有的线程调用了signal唤醒当前线程， // 并且节点已经存放到了同步等待队列中，所以可以调用如果acquireQueued请求许可了 // savedState是获取许可的个数 这个要和之前释放的许可个数一致 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); &#125; addConditionWaiter方法添加一个节点到Condition队列中 java.util.concurrent.locks.AbstractQueuedSynchronizer.ConditionObject#addConditionWaiter 1234567891011121314151617private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; // 先检查一遍有没有取消状态的节点，如果有的话，清除掉 unlinkCancelledWaiters(); t = lastWaiter; &#125; // 将当前线程封装成Node添加到Condition队列中 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; &#125; signal方法唤醒一个线程 1234567 public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125; doSignal方法123456789private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; // 将Condition队列中的节点状态设置成SIGNAL，并将节点添加到同步队列中 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); &#125; transferForSignal方法1234567891011121314151617181920212223final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ // 将节点状态改成0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ // 把当前添加到同步队列中，并返回前一个节点 Node p = enq(node); int ws = p.waitStatus; // 设置前一个节点的状态为SIGNAL if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 唤醒当前节点的线程 LockSupport.unpark(node.thread); return true; &#125; 参考资料https://mp.weixin.qq.com/s/hB5ncpe7_tVovQj1sNlDRA https://mp.weixin.qq.com/s/iNz6sTen2CSOdLE0j7qu9A https://github.com/AobingJava/JavaFamily https://segmentfault.com/a/1190000015804888/ https://juejin.cn/post/6844903997438951437 https://juejin.cn/post/6870099231361728525","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java并发编程系列合集","date":"2021-08-02T03:22:49.000Z","path":"wiki/Java并发编程系列合集/","text":"Lock框架继承关系图 知识清单 Java多线程与并发基础 Java内存模型 深入理解同步锁-synchronized关键字 synchronized实现原理概述 深入理解volatile关键字 深入理解final关键字 深入理解并发编程之Blocking Queue 深入理解并发编程之AQS 深入理解ReentrantLock应用及实现 深入Condition机制的底层原理 Java多线程之中断机制 深入理解并发容器之concurrentHashMap 深入理解并发编程之ThreadLocal 深入理解Atomic底层及原理 如何构建一个安全可用的线程池 JUC-CountDownLatch JUC-CyclicBarrier LockSupport工具 并发编程之Unsafe类 学习文档https://www.codercc.com/backend/basic/juc/ https://segmentfault.com/a/1190000015558984 https://www.pdai.tech/md/java/thread/java-thread-x-juc-overview.html https://github.com/AobingJava/JavaFamily https://dayarch.top/categories/Coding/Java-Concurrency/ http://tutorials.jenkov.com/java-concurrency/blocking-queues.html","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"JVM-垃圾收集器","date":"2021-07-31T06:13:17.000Z","path":"wiki/JVM-垃圾收集器/","text":"1、什么是垃圾收集器如果说垃圾收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 JVM规范对于垃圾收集器的应该如何实现没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器差别较大，这里只看HotSpot虚拟机。 就像没有最好的算法一样，垃圾收集器也没有最好，只有最合适。我们能做的就是根据具体的应用场景选择最合适的垃圾收集器。 ​ 上图展示了7种作用于不同分代的收集器，其中用于回收新生代的收集器包括Serial、PraNew、Parallel Scavenge，回收老年代的收集器包括Serial Old、Parallel Old、CMS，还有用于回收整个Java堆的G1收集器。不同收集器之间的连线表示它们可以搭配使用。 2、串行，并行和并发2.1 串行计算机中的串行是用 Serial 表示。A 和 B 两个任务运行在一个 CPU 线程上，在 A 任务执行完之前不可以执行 B。即，在整个程序的运行过程中，仅存在一个运行上下文，即一个调用栈一个堆。程序会按顺序执行每个指令。 2.2 并行并行性指两个或两个以上事件或活动在同一时刻发生。在多道程序环境下，并行性使多个程序同一时刻可在不同 CPU 上同时执行。比如，A 和 B 两个任务可以同时运行在不同的 CPU 线程上，效率较高，但受限于 CPU 线程数，如果任务数量超过了 CPU 线程数，那么每个线程上的任务仍然是顺序执行的。 2.3 并发并发指多个线程在宏观(相对于较长的时间区间而言)上表现为同时执行，而实际上是轮流穿插着执行，并发的实质是一个物理 CPU 在若干道程序之间多路复用，其目的是提高有限物理资源的运行效率。 并发与并行串行并不是互斥的概念，如果是在一个CPU线程上启用并发，那么自然就还是串行的，而如果在多个线程上启用并发，那么程序的执行就可以是既并发又并行的。 2.4 JVM 垃圾收集中的串行、并行和并发在 JVM 垃圾收集器中也涉及到如上的三个概念。 串行（Serial）：使用单线程进行垃圾回收的回收器。 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 在了解了这些概念之后，我们开始具体介绍常用的垃圾收集器。 3、主流的垃圾收集器3.1 Serial收集器Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了（新生代采用复制算法，老生代采用标志整理算法）。大家看名字就知道这个收集器是一个单线程收集器了。 它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ：将用户正常工作的线程全部暂停掉），直到它收集结束。 上图中： 新生代采用复制算法，Stop-The-World 老年代采用标记-整理算法，Stop-The-World 当它进行GC工作的时候，虽然会造成Stop-The-World，正如每种算法都有存在的原因，该串行收集器也有存在的原因：因为简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，没有线程交互的开销，专心做GC，自然可以获得最高的单线程效率。 所以Serial收集器对于运行在client模式下的应用是一个很好的选择（到目前为止，它依然是虚拟机运行在client模式下的默认新生代收集器） 串行收集器的缺点很明显，虚拟机的开发者当然也是知道这个缺点的，所以一直都在缩减Stop The World的时间。 在后续的垃圾收集器设计中停顿时间在不断缩短（但是仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续） 3.1.1 特点 针对新生代的收集器； 采用复制算法； 单线程收集； 进行垃圾收集时，必须暂停所有工作线程，直到完成； 即会”Stop The World”； 3.1.2 应用场景 依然是HotSpot在Client模式下默认的新生代收集器； 也有优于其他收集器的地方： 简单高效（与其他收集器的单线程相比）； 对于限定单个CPU的环境来说，Serial收集器没有线程交互（切换）开销，可以获得最高的单线程收集效率； 在用户的桌面应用场景中，可用内存一般不大（几十M至一两百M），可以在较短时间内完成垃圾收集（几十MS至一百多MS）,只要不频繁发生，这是可以接受的 3.1.3 参数设置添加该参数来显式的使用串行垃圾收集器: “-XX:+UseSerialGC” 3.2 ParNew收集器 Serial收集器的多线程版本-使用多条线程进行GC ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，目前只有它能与CMS收集器配合工作。 CMS收集器是一个被认为具有划时代意义的并发收集器，因此如果有一个垃圾收集器能和它一起搭配使用让其更加完美，那这个收集器必然也是一个不可或缺的部分了。 收集器的运行过程如下图所示： 3.2.1 应用场景：在Server模式下，ParNew收集器是一个非常重要的收集器，因为除Serial外，目前只有它能与CMS收集器配合工作； 但在单个CPU环境中，不会比Serail收集器有更好的效果，因为存在线程交互开销。 3.2.2 设置参数指定使用CMS后，会默认使用ParNew作为新生代收集: “-XX:+UseConcMarkSweepGC” 强制指定使用ParNew:“-XX:+UseParNewGC” 指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相: “-XX:ParallelGCThreads” 3.2.3 为什么只有ParNew能与CMS收集器配合 CMS是HotSpot在JDK1.5推出的第一款真正意义上的并发（Concurrent）收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作； CMS作为老年代收集器，但却无法与JDK1.4已经存在的新生代收集器Parallel Scavenge配合工作； 因为Parallel Scavenge（以及G1）都没有使用传统的GC收集器代码框架，而另外独立实现；而其余几种收集器则共用了部分的框架代码； 3.3 Parallel Scavenge收集器Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器。 Parallel Scavenge收集器关注点是吞吐量（如何高效率的利用CPU）。 CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。 所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。（吞吐量：CPU用于用户代码的时间/CPU总消耗时间的比值，即=运行用户代码的时间/(运行用户代码时间+垃圾收集时间)。比如，虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。） 运行示意图： 3.3.1 特点 新生代收集器； 采用复制算法； 多线程收集； CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间；而Parallel Scavenge收集器的目标则是达一个可控制的吞吐量（Throughput）； 3.3.2 应用场景 高吞吐量为目标，即减少垃圾收集时间，让用户代码获得更长的运行时间； 当应用程序运行在具有多个CPU上，对暂停时间没有特别高的要求时，即程序主要在后台进行计算，而不需要与用户进行太多交互； 例如，那些执行批量处理、订单处理（对账等）、工资支付、科学计算的应用程序； 3.3.3 设置参数Parallel Scavenge收集器提供两个参数用于精确控制吞吐量： 控制最大垃圾收集停顿时间 “-XX:MaxGCPauseMillis” 控制最大垃圾收集停顿时间，大于0的毫秒数； MaxGCPauseMillis设置得稍小，停顿时间可能会缩短，但也可能会使得吞吐量下降；因为可能导致垃圾收集发生得更频繁； 设置垃圾收集时间占总时间的比率 “-XX:GCTimeRatio” 设置垃圾收集时间占总时间的比率，0 &lt; n &lt; 100的整数； GCTimeRatio相当于设置吞吐量大小； 垃圾收集执行时间占应用程序执行时间的比例的计算方法是： 1 / (1 + n) 。 例如，选项-XX:GCTimeRatio=19，设置了垃圾收集时间占总时间的5% = 1/(1+19)；默认值是1% = 1/(1+99)，即n=99； 垃圾收集所花费的时间是年轻一代和老年代收集的总时间； 如果没有满足吞吐量目标，则增加代的内存大小以尽量增加用户程序运行的时间； 3.4 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。 它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案 3.4.1 特点 针对老年代； 采用”标记-整理-压缩”算法（Mark-Sweep-Compact）； 单线程收集； 3.4.2 应用场景 主要用于Client模式； 而在Server模式有两大用途：（A）、在JDK1.5及之前，与Parallel Scavenge收集器搭配使用（JDK1.6有Parallel Old收集器可搭配Parallel Scavenge收集器）；（B）、作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用； 3.5 Parallel Old收集器Parallel Scavenge收集器的老年代版本。 使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 在JDK1.6才有的。 3.5.1 特点 针对老年代； 采用”标记-整理-压缩”算法； 多线程收集； Parallel Scavenge/Parallel Old收集器运行示意图如下 3.5.2 应用场景 JDK1.6及之后用来代替老年代的Serial Old收集器； 特别是在Server模式，多CPU的情况下； 这样在注重吞吐量以及CPU资源敏感的场景，就有了Parallel Scavenge（新生代）加Parallel Old（老年代）收集器的”给力”应用组合； 3.5.3 设置参数指定使用Parallel Old收集器: “-XX:+UseParallelOldGC” 3.6 CMS（Concurrent Mark Sweep）收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常适合在注重用户体验的应用上使用。 3.6.1 特点 针对老年代 基于”标记-清除”算法(不进行压缩操作，会产生内存碎片) 以获取最短回收停顿时间为目标 并发收集、低停顿 需要更多的内存 CMS是HotSpot在JDK1.5推出的第一款真正意义上的并发（Concurrent）收集器； 第一次实现了让垃圾收集线程与用户线程（基本上）同时工作； 3.6.2 应用场景 与用户交互较多的场景；（如常见WEB、B/S-浏览器/服务器模式系统的服务器上的应用） 希望系统停顿时间最短，注重服务的响应速度； 以给用户带来较好的体验； 3.6.3 CMS收集器运作过程从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程可分为四个步骤： 初始标记： 暂停所有的其他线程，初始标记仅仅标记GC Roots能直接关联到的对象，速度很快； 并发标记 并发标记就是进行GC Roots Tracing的过程； 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方； 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录（采用多线程并行执行来提升效率）；需要”Stop The World”，且停顿时间比初始标记稍长，但远比并发标记短； 并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫，回收所有的垃圾对象； 由于整个过程耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作。 所以总体来说，CMS的内存回收是与用户线程一起“并发”执行的。 3.6.4参数设置指定使用CMS收集器 “-XX:+UseConcMarkSweepGC” 3.6.5 CMS收集器缺点3.6.5.1 对CPU资源敏感面向并发设计的程序都对CPU资源比较敏感（并发程序的特点）。在并发阶段，它虽然不会导致用户线程停顿，但会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。（在对账系统中，不适合使用CMS收集器）。 CMS的默认收集线程数量是=(CPU数量+3)/4； 当CPU数量越多，回收的线程占用CPU就少。 也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，对用户程序影响可能较大；不足4个时，影响更大，可能无法接受。（比如 CPU=2时，那么就启动一个线程回收，占了50%的CPU资源。） （一个回收线程会在回收期间一直占用CPU资源） 针对这种情况，曾出现了”增量式并发收集器”（Incremental Concurrent Mark Sweep/i-CMS）； 类似使用抢占式来模拟多任务机制的思想，让收集线程和用户线程交替运行，减少收集线程运行时间； 但效果并不理想，JDK1.6后就官方不再提倡用户使用。 3.6.5.2 无法处理浮动垃圾无法处理浮动垃圾,可能出现”Concurrent Mode Failure”失败 在并发清除时，用户线程新产生的垃圾，称为浮动垃圾； 解决办法： 这使得并发清除时需要预留一定的内存空间，不能像其他收集器在老年代几乎填满再进行收集； 也可以认为CMS所需要的空间比其他垃圾收集器大； 可以使用”-XX:CMSInitiatingOccupancyFraction”，设置CMS预留老年代内存空间； 3.6.5.3 产生大量内存碎片由于CMS是基于“标记+清除”算法来回收老年代对象的，因此长时间运行后会产生大量的空间碎片问题，可能导致新生代对象晋升到老生代失败。 由于碎片过多，将会给大对象的分配带来麻烦。因此会出现这样的情况，老年代还有很多剩余的空间，但是找不到连续的空间来分配当前对象，这样不得不提前触发一次Full GC。 解决办法 使用”-XX:+UseCMSCompactAtFullCollection”和”-XX:+CMSFullGCsBeforeCompaction”，需要结合使用。 UseCMSCompactAtFullCollection “-XX:+UseCMSCompactAtFullCollection” 为了解决空间碎片问题，CMS收集器提供−XX:+UseCMSCompactAlFullCollection标志，使得CMS出现上面这种情况时不进行Full GC，而开启内存碎片的合并整理过程； 但合并整理过程无法并发，停顿时间会变长； 默认开启（但不会进行，需要结合CMSFullGCsBeforeCompaction使用）； CMSFullGCsBeforeCompaction 由于合并整理是无法并发执行的，空间碎片问题没有了，但是有导致了连续的停顿。因此，可以使用另一个参数−XX:CMSFullGCsBeforeCompaction，表示在多少次不压缩的Full GC之后，对空间碎片进行压缩整理。 可以减少合并整理过程的停顿时间； 默认为0，也就是说每次都执行Full GC，不会进行压缩整理； 由于空间不再连续，CMS需要使用可用”空闲列表”内存分配方式，这比简单实用”碰撞指针”分配内存消耗大； 3.6.6 CMS&amp;Parallel Old总体来看，CMS与Parallel Old垃圾收集器相比，CMS减少了执行老年代垃圾收集时应用暂停的时间； 但却增加了新生代垃圾收集时应用暂停的时间、降低了吞吐量而且需要占用更大的堆空间； （原因：CMS不进行内存空间整理节省了时间，但是可用空间不再是连续的了，垃圾收集也不能简单的使用指针指向下一次可用来为对象分配内存的地址了。 相反，这种情况下，需要使用可用空间列表。即，会创建一个指向未分配区域的列表，每次为对象分配内存时，会从列表中找到一个合适大小的内存区域来为新对象分配内存。这样做的结果是，老年代上的内存的分配比简单实用碰撞指针分配内存消耗大。这也会增加年轻代垃圾收集的额外负担，因为老年代中的大部分对象是在新生代垃圾收集的时候从新生代提升为老年代的。） 当新生代对象无法分配过大对象，就会放到老年代进行分配。 3.7 G1收集器上一代的垃圾收集器(串行serial, 并行parallel, 以及CMS)都把堆内存划分为固定大小的三个部分: 年轻代(young generation), 年老代(old generation), 以及持久代(permanent generation)。 G1（Garbage-First）是JDK7-u4才推出商用的收集器；G1 (Garbage-First)是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器。以极高概率满足GC停顿时间要求的同时，还具备高吞吐量性能特征。被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。G1的使命是在未来替换CMS，并且在JDK1.9已经成为默认的收集器。 3.7.1 特点3.7.1.1 并行与并发G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 3.7.1.2 分代收集虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 能独立管理整个GC堆（新生代和老年代），而不需要与其他收集器搭配； 能够采用不同方式处理不同时期的对象； 虽然保留分代概念，但Java堆的内存布局有很大差别； 将整个堆划分为多个大小相等的独立区域（Region）； 新生代和老年代不再是物理隔离，它们都是一部分Region（不需要连续）的集合； 3.7.2 空间整合与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 从整体看，是基于标记-整理算法； 从局部（两个Region间）看，是基于复制算法；这是一种类似火车算法的实现；不会产生内存碎片，有利于长时间运行； （火车算法是分代收集器所用的算法，目的是在成熟对象空间中提供限定时间的渐进收集。在后面一篇中会专门介绍） 3.7.3 可预测的停顿这是G1相对于CMS的另一个大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型。可以明确指定M毫秒时间片内，垃圾收集消耗的时间不超过N毫秒。在低停顿的同时实现高吞吐量。 3.7.4 G1收集器延伸3.7.4.1 为什么G1可以实现可预测停顿 可以有计划地避免在Java堆的进行全区域的垃圾收集； G1收集器将内存分大小相等的独立区域（Region），新生代和老年代概念保留，但是已经不再物理隔离。 G1跟踪各个Region获得其收集价值大小，在后台维护一个优先列表； 每次根据允许的收集时间，优先回收价值最大的Region（名称Garbage-First的由来）； 这就保证了在有限的时间内可以获取尽可能高的收集效率； 3.7.4.2 一个对象被不同区域引用的问题一个Region不可能是孤立的，一个Region中的对象可能被其他任意Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题（而G1更突出）：回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率； 解决方法： 无论G1还是其他分代收集器，JVM都是使用Remembered Set来避免全局扫描： 每个Region都有一个对应的Remembered Set； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的Region（其他收集器：检查老年代对象是否引用了新生代对象）； 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在Region对应的Remembered Set中； 当进行垃圾收集时，在GC根节点的枚举范围加入Remembered Set； 就可以保证不进行全局扫描，也不会有遗漏。 3.7.5 应用场景 面向服务端应用，针对具有大内存、多处理器的机器； 最主要的应用是为需要低GC延迟，并具有大堆的应用程序提供解决方案； 如：在堆大小约6GB或更大时，可预测的暂停时间可以低于0.5秒； （实践：对账系统中将CMS垃圾收集器修改为G1，降低对账时间20秒以上） 具体什么情况下应用G1垃圾收集器比CMS好，可以参考以下几点（但不是绝对）： 超过50％的Java堆被活动数据占用； 对象分配频率或年代的提升频率变化很大； GC停顿时间过长（长于0.5至1秒）； 建议： 如果现在采用的收集器没有出现问题，不用急着去选择G1； 如果应用程序追求低停顿，可以尝试选择G1； 是否代替CMS只有需要实际场景测试才知道。（如果使用G1后发现性能还没有使用CMS好，那么还是选择CMS比较好） 3.7.6 设置参数可以通过下面的参数，来设置一些G1相关的配置。 指定使用G1收集器： “-XX:+UseG1GC” 当整个Java堆的占用率达到参数值时，开始并发标记阶段；默认为45： “-XX:InitiatingHeapOccupancyPercent” 为G1设置暂停时间目标，默认值为200毫秒： “-XX:MaxGCPauseMillis” 设置每个Region大小，范围1MB到32MB；目标是在最小Java堆时可以拥有约2048个Region: “-XX:G1HeapRegionSize” 新生代最小值，默认值5%: “-XX:G1NewSizePercent” 新生代最大值，默认值60%: “-XX:G1MaxNewSizePercent” 设置STW期间，并行GC线程数: “-XX:ParallelGCThreads” 设置并发标记阶段，并行执行的线程数: “-XX:ConcGCThreads” G1在标记过程中，每个区域的对象活性都被计算，在回收时候，就可以根据用户设置的停顿时间，选择活性较低的区域收集，这样既能保证垃圾回收，又能保证停顿时间，而且也不会降低太多的吞吐量。Remark（重新标记）阶段新算法的运用，以及收集过程中的压缩，都弥补了CMS不足。 引用Oracle官网的一句话：“G1 is planned as the long term replacement for the Concurrent Mark-Sweep Collector (CMS)”。 G1计划作为并发标记-清除收集器(CMS)的长期替代品 4、如何选择垃圾收集器垃圾收集器主要可以分为如下三大类： 串行收集器：Serial和Serial Old只能有一个垃圾回收线程执行，用户线程暂停。 适用于内存比较小的嵌入式设备 。 并行收集器[吞吐量优先]：Parallel Scanvenge和Parallel Old多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 适用于科学计算、后台处理等若交互场景 。 并发收集器[停顿时间优先]：CMS和G1。用户线程和垃圾收集线程同时执行(但并不一定是并行的，可能是交替执行的)，垃圾收集线程在执行的时候不会停顿用户线程的运行。 适用于对时间有要求的场景，比如Web应用。 参考资料https://zhuanlan.zhihu.com/p/142273073 https://juejin.cn/post/6844903877024677901 面试官：你对JVM垃圾收集器了解吗？13连问你是否抗的住！ https://juejin.cn/post/6874060477031579661#heading-32 https://juejin.cn/post/6844904041080684552 https://juejin.cn/post/6844904159817236494#heading-14 https://juejin.cn/post/6844903892774289421#heading-20","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-垃圾回收机制","date":"2021-07-30T11:49:33.000Z","path":"wiki/JVM-垃圾回收机制/","text":"垃圾回收总体思路： 1、什么是垃圾回收，为什么需要垃圾回收； 2、回收的到底是什么？由谁来回收谁？ 3、回收的判断标准是什么 4、什么时候回收，回收的种类和流程是怎样的 5、在哪些地方进行回收 1. 什么是垃圾回收任何语言在运行过程中都会创建对象，也就意味着需要在内存中为这些对象在内存中分配空间，在这些对象失去使用的意义的时候，需要释放掉这些内容，保证内存能够提供给新的对象使用。对于对象内存的释放就是垃圾回收机制，也叫做gc。 对于java开发者来说gc是一个双刃剑，一方面，java程序员在开发程序的时候不需要像开发C++那样手动分配对象的内存，还要在合适的时机手动释放，一定程度地降低了java程序员的开发难度。另一方面，虚拟机可以帮助程序开发人员管理内存，如果程序员不了解虚拟机垃圾回收的原理，很容易引起OOM，造成服务的崩溃和系统的宕机； 2、对象如何判活对象存活表示的是当前对象是否还在被使用，没有被使用的对象我们可以称其为已经“死亡”，如果对象依然在被使用，我们称其为“存活”状态，对象是否被使用则是通过对象的引用进行判断的。而垃圾回收机制就是负责将已经死亡的对象进行清理。 程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭，栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈的操作。每一个栈帧中分配多少内存基本上是在类结构确定下来时就已知的，因此这几个区域的内存分配和回收都具备确定性，在这几个区域不需要过多的考虑回收的问题，当方法结束或者线程结束的时候，内存自然就跟着回收了。 Java堆则和上述三种区域不同，Java中一个接口的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也不一样，而**只有当Java程序运行时我们才能知道哪些对象会被创建**，所以堆中的内存分配和回收都是动态进行的，因此垃圾收集器所关注的也是这部分的内存。 垃圾回收器在对堆进行回收前，第一件事情就是要判断堆中的对象哪些是依旧在使用的，哪些已经不可能再被使用了。这里的判断主要有两种方式，第一种是引用计数算法，第二种是可达性分析算法。 2.1 引用计数算法引用计数算法给对象添加一个引用计数器，每当有一个地方引用它时，计数器就加1，当引用失效时，计数器就减1，任何时刻计数器为0的对象就是不可能再被使用的。这种算法实现简单，判定效率也很高，但是它难以解决对象之间循环引用的问题，例如对象A和对象B相互引用了对方，而A和B都没有在被使用了，但这两个对象却也不会被垃圾回收器回收。 2.2 可达性分析主流的判断方法则是使用可达性分析算法来判断对象是否存活。这个算法需要选择一些对象作为“GC Roots”，每次都通过这些roots节点向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots不存在引用链的时候，则证明这个对象是不可用的。 在Java语言中，可作为GC Roots的对象包括下面几种： 1、虚拟机栈中引用的对象2、方法区中类静态属性引用的对象3、方法区中常量引用的对象4、本地方法栈中JNI（即Native方法）引用的对象 可达性分析算法中根据GC Roots找引用链，存在两个主要的问题。 一个是可作为GC Roots的节点主要在全局性的引用（例如常量或者类静态属性）于上下文(例如栈帧中的本地变量表）中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，将会消耗很多的时间。 还有一个问题是GC停顿，可达性分析必须确保在整个的分析过程中，执行系统就像被冻结在某个时间节点，整个分析过程中对象的引用关系不能发生变化，这样才能保证分析结果的准确性，因此在进行GC时，需要停顿所有的Java线程。（Stop The World） 3.3 对象两次标记判活即使在可达性分析算法中不可达的对象，也并非是”非死不可“的，这时候它们暂时处于”缓刑“阶段，要宣告一个对象死亡，至少要经历两次标记过程： 如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法(当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为”没有必要执行“)。 如果一个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里的执行是指由虚拟机去触发这个方法，但并不一定会等待该方法执行完毕（为了避免finalize方法中出现类似死循环都操作，导致内存无法被回收，同时导致F-Queue队列中的其他对象一直处于等待状态）。 当执行完finalze()方法后，GC将会对F-Queue中的对象进行第二次小规模的标记，如果对象在finalize()方法中又重新获得了引用，对象将会被移出对列并且继续存活，如果对象依旧存在于队列中并且被进行第二次标记，对象将被GC回收。 需要注意的是任何一个对象的finalize()方法只会执行一次，如果第一次通过finalize()方法救活了对象，那么第二次相同的方法就会失效。同时由于finalize()方法的运行代价高昂，不确定性大，无法保证各个对象的调用顺序，因此应当尽量避免使用finalize()方法。 3. 对象引用分类JDK1.2以前，Java中引用的定义很传统，如果reference类型的数据中存储的数值代表的是另一块内存的起始地址，就称这块内存代表着一个引用。这种定义下的对象只存在两种状态，被引用和未被引用状态。但有些对象我们希望当内存存够的时候能够保留这些对象，当内存不足的时候则能够对这些对象进行清理，这一类对象则无法使用这种传统的定义来表示。 JDK1.2之后，Java对引用进行了扩充，将引用分为强引用、软引用、弱引用和虚引用四种，这四种引用的强度依次逐渐减弱。 3.1 强引用就是指在程序代码中普遍存在的，类似 Object obj = new Object() 这类的引用，只要强引用还存在，垃圾收集器永远不会回收被引用的对象。即使内存不足时，垃圾回收器也不会回收强引用的对象，而是会直接抛出OutOfMemoryError异常。如果想让强引用对象被回收，可以手动设置obj = null;来实现。 3.2 软引用用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在内存足够的时候，是不会回收软引用的对象的，而在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果软引用回收后依然内存不足，则会抛出OutOfMemoryError异常。在JDK1.2之后，提供了SoftReference类来实现软引用。软引用可以用来实现缓存技术。 3.3 弱引用弱引用和软引用一样用来描述非必须的对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉被弱引用关联的对象。在JDK1.2之后，提供了WeakReference类来实现弱引用。 3.4 虚引用虚引用也被称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK1.2之后，提供了PhantomReference类来实现虚引用。 4.垃圾回收算法垃圾收集算法的目的是在已经明确了哪些内存块需要回收以后，如何高效的回收这些内存空间。 4.1 标记清除算法标记-清除算法采用从根集合进行扫描，对存活的对象对象标记，标记完毕后，再扫描整个空间中未被标记的对象，进行回收，如图所示。标记-清除算法不需要进行对象的移动，并且仅对不存活的对象进行处理，在存活对象比较多的情况下极为高效。 标记清除算法主要有两个不足之处：一个是效率问题，标记和清除两个过程的效率都不高；另一个问题是空间问题，标记清除之后会造成内存空间中存在大量的内存碎片，空间碎片太多时，当要分配一片大内存空间时可能会找不到合适的连续内存空间进行分配，从而触发另一次垃圾收集动作。 4.2 标记复制算法该算法的提出是为了克服句柄的开销和解决堆碎片的垃圾回收。建立在存活对象少，垃圾对象多的前提下。此算法每次只处理正在使用中的对象，因此复制成本比较小，同时复制过去后还能进行相应的内存整理，不会出现碎片问题。但缺点也是很明显，就是需要两倍内存空间。 它开始时把堆分成 一个对象面和多个空闲面， 程序从对象面为对象分配空间，当对象满了，基于copying算法的垃圾 收集就从根集中扫描活动对象，并将每个活动对象复制到空闲面(使得活动对象所占的内存之间没有空闲洞)，这样空闲面变成了对象面，原来的对象面变成了空闲面，程序会在新的对象面中分配内存。一种典型的基于coping算法的垃圾回收是stop-and-copy算法，它将堆分成对象面和空闲区域面，在对象面与空闲区域面的切换过程中，程序暂停执行。 现在的商业虚拟机都会采用这种算法来回收新生代，根据统计新生代中98%的对象都是“朝夕生死”的，因此对于新生代的回收不用按照1:1的比例来进行内存划分，可以将内存划分为一块Eden区域和两块Survivor空间，每次使用时都选择Eden区域和一块Survivor区域进行内存分配。当回收时，将Eden区域和Survivor区域中还存活的对象全部移动到另一块Survivor区域，然后清理掉Eden区域和刚刚使用的Survivor区域。 HotSpot虚拟机中Eden和Survivor的比例是1:8，即每次都有90%的内存空间在进行使用，只有10%的内存空间被浪费了。当然，如果每次内存都有98%被回收，那么每次被移动到另一块Survivor区域的内存只有2%，这样是没有任何问题的，但是如果移动到另一块Survivor区域的内存超过了10%，就需要依赖其他的内存（这里指老年代）进行分配担保了（将多出的对象分配到老年代）。 4.3 标记整理算法 此算法是结合了“标记-清除”和“复制算法”两个算法的优点。避免了“标记-清除”的碎片问题，同时也避免了“复制”算法的空间问题。 标记-整理算法的标记过程和标记-清除算法的标记过程一致，但是在标记完以后，标记-整理算法会将所有存活的对象都移动到一端，然后再进行清除。这种算法适用于老年代，因为老年代的对象存活率都会比较高，如果像之前一样进行复制移动，将会产生大量的复制操作导致效率变低，同时每次都会存活下大量对象导致需要很多的内存空间来进行分配担保。 4.4 分代收集算法 当前商业虚拟机的垃圾收集都采用“分代收集”算法，这种算法根据对象存活周期的不同将内存划分为几块，一般是把Java堆划分位新生代和老年代。新生代中每次都会有大批对象死去，只有少量对象存活，因此可以选用复制算法。 老年代每次都会有大量对象存活，因此选择标记-清理或者标记-整理算法来进行。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-对象创建","date":"2021-07-30T06:42:55.000Z","path":"wiki/JVM-对象创建/","text":"1.对象创建方式Java中有一下几种方式创建对象： 方式 实质 使用new关键 调用无参或有参构造器函数创建 使用Class的newInstance方法 调用无参或有参构造器函数创建，且需要是publi的构造函数 使用Constructor类的newInstance方法 调用有参和私有private构造器函数创建，实用性更广 使用Clone方法 不调用任何参构造器函数，且对象需要实现Cloneable接口并实现其定义的clone方法，且默认为浅复制 第三方库Objenesis 利用了asm字节码技术，动态生成Constructor对象 2、对象创建过程 2.1 类的加载虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 2.2 分配内存类的加载检查通过后，接下来是为新生对象分配内存。但类加载完成后所需的内存大小就已经完全确定，为对象分配空间的任务等同于把一块确定大小的内存从java堆中划分出来。分配内存有两种方式： 指针碰撞（Bump the Pointer）：假设java堆中内存是绝对规整的，所有用过得内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪到一段与对象大小相等的距离 空闲列表（Free List）：如果java堆中的内存并不是完整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 选择哪种分配方式由java堆是否完整决定，而java堆是否规整又由所采用的垃圾收集器是否带有压缩整理（标记-整理）功能决定。因此，在使用Serial、ParNew等带有Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。 还有一个问题需要考虑，在虚拟机中对象频繁的创建（即使是修改一个指针所指的位置），在并发情况下会带来线程安全的问题。作为虚拟机来说，必须保证线程安全，所有虚拟机采用两种方式保证线程安全： CAS+失败重试：CAS是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用CAS配上失败重试的方式保证更新操作的原子性 本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）：为每一个线程预先在Eden区分配一块内存，JVM在给线程中的对象分配内存时，首先在TLAB分配，当对象大于TLAB中剩余内存或TLAB的内存已用尽时，在采用上述的CAS进行内存分配 2.3 初始化零值内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这⼀步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使⽤，程序能访问到这些字段的数据类型所对应的零值。 2.4 设置对象头初始化零值完成之后，虚拟机要对对象进⾏必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运⾏状态的不同，如是否启⽤偏向锁等，对象头会有不同的设置⽅式。 2.5 执行Init方法在上⾯⼯作都完成之后，从虚拟机的视⻆来看，⼀个新的对象已经产⽣了，但从 Java 程序的视⻆来看，对象创建才刚开始， &lt;init&gt; ⽅法还没有执⾏，所有的字段都还为零。所以⼀般来说，执⾏ new 指令之后会接着执⾏ &lt;init&gt;⽅法，把对象按照程序员的意愿进⾏初始化，这样⼀个真正可⽤的对象才算完全产⽣出来。 3.对象在内存布局 对象头(Header)：包含两部分，第一部分用于存储对象自身的运行时数据，如哈希码、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等，32 位虚拟机占 32 bit，64 位虚拟机占 64 bit。官方称为 ‘Mark Word’。第二部分是类型指针，即对象指向它的类的元数据指针，虚拟机通过这个指针确定这个对象是哪个类的实例。另外，如果是 Java 数组，对象头中还必须有一块用于记录数组长度的数据，因为普通对象可以通过 Java 对象元数据确定大小，而数组对象不可以。 实例数据(Instance Data)：程序代码中所定义的各种类型的字段内容(包含父类继承下来的和子类中定义的)。 对齐填充(Padding)：不是必然需要，主要是占位，保证对象大小是某个字节的整数倍。 4. 对象访问建⽴对象就是为了使⽤对象，我们的Java程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问⽅式有虚拟机实现⽽定，⽬前主流的访问⽅式有使⽤句柄和直接指针两种： 4.1 句柄访问 如果使⽤句柄的话，那么Java堆中将会划分出⼀块内存来作为句柄池，reference 中存储的就是对象的句柄地址，⽽句柄中包含了对象实例数据与类型数据各⾃的具体地址信息； 4 .2 直接指针 如果使⽤直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，⽽reference 中存储的直接就是对象的地址。 这两种对象访问⽅式各有优势。使⽤句柄来访问的最⼤好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，⽽ reference 本身不需要修改。使⽤直接指针访问⽅式最⼤的好处就是速度快，它节省了⼀次指针定位的时间开销。 5、对象内存分配 Java内存体系中所提倡的自动内存管理最终可以归结为自动化地解决两个问题：给对象分配内存和回收分配给对象的内存。 对象的内存分配，往大方向讲，就是在堆上分配，对象主要分配在新生代的Eden区域，如果启动了本地线程分配缓冲，将按线程优先分配在TLAB上。少数情况下也可能直接分配在老年代中。具体的分配规则取决于垃圾收集器的类型以及虚拟机中参数的配置。但是有几条最普遍的内存分配规则如下： 5.1 对象优先在Eden分配大多数情况下，对象在新生代Eden区进行分配。当Eden区没有足够内存进行分配时，虚拟机将会发起一次Minor GC。 5.2 大对象直接进入老年代所谓大对象，是指需要大量连续存储空间的Java对象，最典型的大对象就是那种很长的字符串或者数组。大对象对虚拟机分配来说是一个坏消息，经常出现大对象会导致虚拟机需要经常调用GC来为这些大对象整理出足够的连续空间。 5.3长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应该放在新生代，哪些对象应该放在老年代。为了做到这一点，虚拟机给每一个对象定义了一个对象年龄计数器。如果对象在Eden出生并且经过了第一次Minor GC后仍然存活，并且能够被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设置为1。对象在Survivor区域中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁）对象将会被晋身到老年代中。 5.4 动态对象年龄判定为了能够更好的适应不同程序的内存状况，虚拟机并不是每次都要等到对象的年龄到达阈值才将对象移动到老年代。如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需等待年龄增长。 5.6 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代中最大可用的连续空间是否大于新生代所有对象空间综合，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次GC是有风险的；如果小于，或者设置不允许，那这时将改为进行一次Full GC。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-内存结构","date":"2021-07-30T03:53:42.000Z","path":"wiki/JVM-内存结构/","text":"作为程序员，最常接触到Java虚拟机的部分应该是内存结构这一部分了，同样这一部分的内容很多，面试也是最常被问到的。虽然JDK已经发布了16版本，但是国内大部分企业都还在使用JDK8。 今天学习一下虚拟机的运行时数据区的组成和各个组件的功能。 JDK8官方网站文档链接 – 》 JDK 运行时数据区Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些数据区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户的启动和结束而建立和销毁。 程序计数器程序计数器（Program Counter Register），它是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的模型概念中，字节码解释器的工作就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 如果线程正在执行一个 Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是 Native 方法，这个计数器的值则为 (Undefined)。 由于Java虚拟机的多线程是通过线程轮流切换处理器执行时间的方式实现的，在任何一个确定的时刻，一个处理器的一个核只会执行一条线程中的指令，因此，为了线程切换后能够恢复到正确的执行位置，每一条线程都需要拥有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，这类内存区域称为“线程私有”的内存，即如上图所示，每一个线程都会拥有自己的一块内存区域。 程序计数器在执行本地方法时（例如调用C语言代码）计数器值为空，其他时候则是指向正在执行的虚拟机字节码指令的地址。 程序计数器是在Java虚拟机规范中唯一一个没有规定任何OutOfMemoryError情况的区域，因为Java程序计数器它所需要存储的内容仅仅就是下一个需要待执行的命令的地址，其所需内存是创建时即可只晓的，不需要后期进行扩容等其他的操作。 Java虚拟机栈Java虚拟机栈（Java Virtual Machine Stacks），Java虚拟机栈也是线程私有的,它的生命周期与线程相同。Java每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至方法执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 虚拟机栈中局部变量表部分与Java对象内存分配关系密切，局部变量表存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，该类型可能是一个指向对象起始地址的引用指针，也可能是一个代表对象的句柄或其他于此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 局部变量表中，64位长度的long和double类型的数据会占用2个局部变量空间，其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况： 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态拓展，如果拓展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈作用类似，它们之间的区别是虚拟机栈为虚拟机执行Java方法，而本地方法栈则为虚拟机执行Native方法服务。有些虚拟机会将本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈也会抛出StackOverflowErro和OutOfMemoryError异常。 Java堆Java堆（Java Heap），对于大多数的应用来说，Java堆是虚拟机所管理的最大的一块内存。Java堆是被所有的线程所共享的，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都是在这里分配内存的（Java虚拟机规范中描述为所有的对象实例和数组都要在堆上分配内存）。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称为GC堆。从内存回收的角度来看，由于现在收集器基本都是采用分代算法收集器，所以Java堆中还可以细分为：新生代和老年代；再细致一点可以分为Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB）。 根据Java虚拟机规范，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，在实现时既可以是固定大小的，也可以是可拓展的，当前主流的虚拟机都是按照可拓展来实现的。如果在堆中没有内存完成实例分配，并且堆也无法再拓展时，将会抛出OutOfMemoryError异常。 在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。新生代 ( Young ) 又被划分为三个区域：Eden、**From Survivor(S0)、To Survivor(S1)**。如图所示： 这样划分的目的是为了使JVM能够更好的管理内存中的对象，包括内存的分配以及回收。 而新生代按eden和两个survivor的分法，是为了 有效空间增大，eden+1个survivor； 有利于对象代的计算，当一个对象在S0/S1中达到设置的XX:MaxTenuringThreshold值后，会将其挪到老年代中，即只需扫描其中一个survivor。如果没有S0/S1,直接分成两个区，该如何计算对象经过了多少次GC还没被释放。 两个Survivor区可解决内存碎片化 堆栈相关参数 参数 描述 -Xms 堆内存初始大小，单位m、g -Xmx 堆内存最大允许大小，一般不要大于物理内存的80% -Xmn 年轻代内存初始大小 -Xss 每个线程的堆栈大小，即JVM栈的大小 -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值 -XX:NewSzie(-Xns) 年轻代内存初始大小,可以缩写-Xns -XX:MaxNewSize(-Xmx) 年轻代内存最大允许大小，可以缩写-Xmx -XX:SurvivorRatio 年轻代中Eden区与Survivor区的容量比例值，默认为8，即8:1 -XX:MinHeapFreeRatio GC后，如果发现空闲堆内存占到整个预估堆内存的40%，则放大堆内存的预估最大值，但不超过固定最大值。 -XX:MaxHeapFreeRatio 预估堆内存是堆大小动态调控的重要选项之一。堆内存预估最大值一定小于或等于固定最大值(-Xmx指定的数值)。前者会根据使用情况动态调大或缩小，以提高GC回收的效率，默认70% -XX:MaxTenuringThreshold 垃圾最大年龄，设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代。对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率 -XX:InitialTenuringThreshold 可以设定老年代阀值的初始值 -XX:+PrintTenuringDistribution 查看每次minor GC后新的存活周期的阈值 Note： 每次GC 后会调整堆的大小，为了防止动态调整带来的性能损耗，一般设置-Xms、-Xmx 相等。 新生代的三个设置参数：-Xmn，-XX:NewSize，-XX:NewRatio的优先级： （1）.最高优先级： -XX:NewSize=1024m和-XX:MaxNewSize=1024m （2）.次高优先级： -Xmn1024m （默认等效效果是：-XX:NewSize==-XX:MaxNewSize==1024m） （3）.最低优先级：-XX:NewRatio=2 推荐使用的是-Xmn参数，原因是这个参数很简洁，相当于一次性设定NewSize和MaxNewSIze，而且两者相等。 方法区方法区（Method Area）与Java堆一样，是线程共享的，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。类加载的信息和数据就放在方法区。 Java虚拟机规范堆方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可拓展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说，这个区域的内存回收成绩比较令人难以满意，尤其时类型卸载，条件相当苛刻，但是这个区域的内存回收也是必要的。 根据Java虚拟机规范规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分，Class文件中除了类的版本、字段、方法、接口等描述信息以外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译器才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量池放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 当常量池无法再申请到内存时会抛出OutOfMemoryError异常。 直接内存直接内存（Direct Memory)并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范所定义的内存区域，但是这部分内存也被频繁的使用，而且也可能导致OutOfMemoryError异常出现。 在JDK1.4中新加入的NIO（New Input/Output）类，引入了一种基于通道与缓冲区的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，这样能在一些场景中显著提高性能，因为避免了在Java堆中和Native堆中来回复制数据。 直接内存虽然不会受到Java堆大小的限制，但是受到本机总内存大小以及处理器寻址空间的限制，如果忽略了直接内存，当各个区域内存总和大于服务器内存时，将会导致动态拓展时出现OutOfMemoryError异常。 参考资料https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-2.html#jvms-2.5 https://blog.csdn.net/qq_21122519/article/details/94408118 https://www.processon.com/view/5ec5d7c60791290fe0768668?fromnew=1","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"JVM-深入理解Java虚拟机","date":"2021-07-30T02:35:12.000Z","path":"wiki/JVM-深入理解Java虚拟机/","text":"JVM结构图 知识清单 Java内存结构 Java对象创建 Java内存模型 类加载机制 垃圾回收机制 垃圾收集器 虚拟机调优 学习资料JAVA虚拟机概述Java虚拟机（JVM）工作原理推荐收藏系列：一文理解JVM虚拟机（内存、垃圾回收、性能优化）解决面试中遇到问题Java虚拟机内存管理和性能调优重读 JVM【2021最新版】JVM面试题总结（87道题含答案解析）JVM 基础 - Java 类加载机制JVM知识点整理https://www.processon.com/view/5ec5d7c60791290fe0768668?fromnew=1","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[]},{"title":"Java并发编程-深入理解volatile","date":"2021-07-29T08:25:05.000Z","path":"wiki/Java并发编程-深入理解volatile/","text":"volatile特性正确理解volatile 多级cache结构 -&gt; 缓存一致性协议（MESI）-&gt; store buffer和invalidate queue -&gt; 内存屏障 可见性volatile的可见性依赖于Java内存模型。 可以参见之前的文章 👉 Java内存模型 Java内存模型(JavaMemoryModel)描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量，存储到内存和从内存中读取变量这样的底层细节。 所有的共享变量都存储于主内存，这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。 每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。 线程对变量的所有的操作(读，取)都必须在工作内存中完成，而不能直接读写主内存中的变量。 不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。 volatile实现可见性 每个线程操作数据的时候会把数据从主内存读取到自己的工作内存，如果他操作了数据并且写会了，他其他已经读取的线程的变量副本就会失效了，需要都数据进行操作又要再次去主内存中读取了。 volatile保证不同线程对共享变量操作的可见性，也就是说一个线程修改了volatile修饰的变量，当修改写回主内存时，另外一个线程立即看到最新的值。 至于其他线程是如何更新缓存行中的数据以及其他线程的缓存行是如何失效的，可以参见之前的文章。 Java内存模型 嗅探机制在现代计算机中，CPU 的速度是极高的，如果 CPU 需要存取数据时都直接与内存打交道，在存取过程中，CPU 将一直空闲，这是一种极大的浪费，所以，为了提高处理速度，CPU 不直接和内存进行通信，而是在 CPU 与内存之间加入很多寄存器，多级缓存，它们比内存的存取速度高得多，这样就解决了 CPU 运算速度和内存读取速度不一致问题。 由于 CPU 与内存之间加入了缓存，在进行数据操作时，先将数据从内存拷贝到缓存中，CPU 直接操作的是缓存中的数据。但在多处理器下，将可能导致各自的缓存数据不一致（这也是可见性问题的由来），为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，而嗅探是实现缓存一致性的常见机制。 嗅探机制工作原理：每个处理器通过监听在总线上传播的数据来检查自己的缓存值是不是过期了，如果处理器发现自己缓存行对应的内存地址修改，就会将当前处理器的缓存行设置无效状态，当处理器对这个数据进行修改操作的时候，会重新从主内存中把数据读到处理器缓存中。 注意： 基于 CPU 缓存一致性协议，JVM 实现了 volatile 的可见性，但由于总线嗅探机制，会不断的监听总线，如果大量使用 volatile 会引起总线风暴。所以，volatile 的使用要适合具体场景。 重排序什么是指令重排序? 为了提高性能，编译器和处理器常常会对既定的代码执行顺序进行指令重排序。 【源代码】 -&gt; 【编译器优化重排序】-&gt; 【指令集并行重排序】-&gt; 【内存系统重排序】-&gt; 【最终执行指令序列】 一般重排序可以分为如下三种： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序; 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序; 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行的。 as-if-serial语义不管怎么重排序，单线程下的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 java编译器会在生成指令系列时在适当的位置会插入内存屏障指令来禁止特定类型的处理器重排序。 为了实现volatile的内存语义，JMM会限制特定类型的编译器和处理器重排序，JMM会针对编译器制定volatile重排序规则表： 内存屏障 说明 StoreStore 屏障 禁止上面的普通写和下面的 volatile 写重排序。 StoreLoad 屏障 防止上面的 volatile 写与下面可能有的 volatile 读/写重排序。 LoadLoad 屏障 禁止下面所有的普通读操作和上面的 volatile 读重排序。 LoadStore 屏障 禁止下面所有的普通写操作和上面的 volatile 读重排序。 happens-before规则如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。 volatile域规则：对一个volatile域的写操作，happens-before于任意线程后续对这个volatile域的读。 volatile在DCL的应用1234567891011121314151617public class Singleton &#123; public static volatile Singleton singleton; /** * 构造函数私有，禁止外部实例化 */ private Singleton() &#123;&#125;; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 现在我们分析一下为什么要在变量singleton之间加上volatile关键字。要理解这个问题，先要了解对象的构造过程，实例化一个对象其实可以分为三个步骤： 分配内存空间。 初始化对象。 将内存空间的地址赋值给对应的引用。 但是由于操作系统可以对指令进行重排序，所以上面的过程也可能会变成如下过程： 分配内存空间。 将内存空间的地址赋值给对应的引用。 初始化对象 如果是这个流程，多线程环境下就可能将一个未初始化的对象引用暴露出来，从而导致不可预料的结果。因此，为了防止这个过程的重排序，我们需要将变量设置为volatile类型的变量。 一次性安全发布12345678910111213141516171819public class BackgroundFloobleLoader &#123; public volatile Flooble theFlooble; public void initInBackground() &#123; // do lots of stuff theFlooble = new Flooble(); // this is the only write to theFlooble &#125;&#125;public class SomeOtherClass &#123; public void doWork() &#123; while (true) &#123; // do some stuff... // use the Flooble, but only if it is ready if (floobleLoader.theFlooble != null) doSomething(floobleLoader.theFlooble); &#125; &#125;&#125; 如果 theFlooble 引用不是 volatile 类型，doWork() 中的代码在解除对 theFlooble 的引用时，将会得到一个不完全构造的 Flooble。 该模式的一个必要条件是：被发布的对象必须是线程安全的，或者是有效的不可变对象（有效不可变意味着对象的状态在发布之后永远不会被修改）。volatile 类型的引用可以确保对象的发布形式的可见性，但是如果对象的状态在发布后将发生更改，那么就需要额外的同步。 无法保证原子性所谓的原子性是指在一次操作或者多次操作中，要么所有的操作全部都得到了执行并且不会受到任何因素的干扰而中断，要么所有的操作都不执行。 在多线程环境下，volatile 关键字可以保证共享数据的可见性，但是并不能保证对数据操作的原子性。也就是说，多线程环境下，使用 volatile 修饰的变量是线程不安全的。 要解决这个问题，我们可以使用锁机制，或者使用原子类（如 AtomicInteger）。 这里特别说一下，对任意单个使用 volatile 修饰的变量的读 / 写是具有原子性，但类似于 flag = !flag 这种复合操作不具有原子性。简单地说就是，单纯的赋值操作是原子性的。 volatile 和 synchronizedvolatile只能修饰实例变量和类变量，而synchronized可以修饰方法，以及代码块。 volatile保证数据的可见性，但是不保证原子性(多线程进行写操作，不保证线程安全);而synchronized是一种排他(互斥)的机制。 volatile用于禁止指令重排序：可以解决单例双重检查对象初始化代码执行乱序问题。 volatile可以看做是轻量版的synchronized，volatile不保证原子性，但是如果是对一个共享变量进行多个线程的赋值，而没有其他的操作，那么就可以用volatile来代替synchronized，因为赋值本身是有原子性的，而volatile又保证了可见性，所以就可以保证线程安全了。 总结1、volatile修饰符适用于以下场景：某个属性被多个线程共享，其中有一个线程修改了此属性，其他线程可以立即得到修改后的值，比如booleanflag;或者作为触发器，实现轻量级同步。 2、volatile属性的读写操作都是无锁的，它不能替代synchronized，因为它没有提供原子性和互斥性。因为无锁，不需要花费时间在获取锁和释放锁_上，所以说它是低成本的。 3、volatile只能作用于属性，我们用volatile修饰属性，这样compilers就不会对这个属性做指令重排序。 4、volatile提供了可见性，任何一个线程对其的修改将立马对其他线程可见，volatile属性不会被线程缓存，始终从主 存中读取。 5、volatile提供了happens-before保证，对volatile变量v的写入happens-before所有其他线程后续对v的读操作。 6、volatile可以使得long和double的赋值是原子的。 7、volatile可以在单例双重检查中实现可见性和禁止指令重排序，从而保证安全性。 参考资料 面试官想到，一个Volatile，敖丙都能吹半小时 面试官最爱的volatile关键字 一文吃透Volatile，征服面试官 java语言的线程安全volatile用法","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"Java-synchronized关键字剖析","date":"2021-07-29T05:37:27.000Z","path":"wiki/Java-synchronized关键字剖析/","text":"之前写过一篇关于synchronized关键字的文章，是当时听马士兵老师的公开课时记录的一些关键笔记📒 链接🔗 下面我们还是要学习和总结一下synchronized synchronized 特性 有序性as-if-serial 不管编译器和CPU如何重排序，必须保证在单线程情况下程序的结果是正确的，还有就是有数据依赖的也是不能重排序的。 12int a = 1;int b = a; 这两段是怎么都不能重排序的，b的值依赖a的值，a如果不先赋值，那就为空了。 可见性主要依靠Java内存模型实现 原子性通过汇编指令控制 可重入synchronized锁对象的时候有个计数器，他会记录下线程获取锁的次数，在执行完对应的代码块之后，计数器就会-1，直到计数器清零，就释放锁了 不可中断不可中断就是指，一个线程获取锁之后，另外一个线程处于阻塞或者等待状态，前一个不释放，后一个也一直会阻塞或者等待，不可以被中断。值得一提的是，Lock的tryLock方法是可以被中断的。 了解对象头Mark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。 在64位的虚拟机中： 32位虚拟机中： 可以参见之前的文章 👉 Java对象头 synchronized实现之前的文章已经在 Java代码、字节码、JVM级别和汇编指令四个级别介绍了synchronzied的实现。JDK对synchronzied不断的优化，大家熟悉的锁升级过程，其实就是在源码里面，调用了不同的实现去获取获取锁，失败就调用更高级的实现，最后升级完成。 升级方向：【 无锁 】 -&gt; 【 偏向锁 】-&gt; 【 轻量级锁 】-&gt; 【 重量级锁 】 Tip：切记这个升级过程是不可逆的； 无锁无锁是指没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 无锁的特点是修改操作会在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。 偏向锁对象头是由 Mark Word 和 Class pointer 组成，锁争夺也就是对象头指向的Monitor对象的争夺，一旦有线程持有了这个对象，标志位修改为1，就进入偏向模式，同时会把这个线程的ID记录在对象的Mark Word中。 这个过程是采用了CAS乐观锁操作的，每次同一线程进入，虚拟机就不进行任何同步的操作了，对标志位+1就好了，不同线程过来，CAS会失败，也就意味着获取锁失败。 偏向锁在1.6之后是默认开启的，1.5中是关闭的，需要手动开启参数是xx:-UseBiasedLocking=false。 初次执行到synchronized代码块的时候，锁对象变成偏向锁（通过CAS修改对象头里的锁标志位），字面意思是“偏向于第一个获得它的线程”的锁。执行完同步代码块后，线程并不会主动释放偏向锁。 当第二次到达同步代码块时，线程会判断此时持有锁的线程是否就是自己（持有锁的线程ID也在对象头里），如果是则正常往下执行。由于之前没有释放锁，这里也就不需要重新加锁。如果自始至终使用锁的线程只有一个，很明显偏向锁几乎没有额外开销，性能极高。 偏向锁是指当一段同步代码一直被同一个线程所访问时，即不存在多个线程的竞争时，那么该线程在后续访问时便会自动获得锁，从而降低获取锁带来的消耗，即提高性能。 当一个线程访问同步代码块并获取锁时，会在 Mark Word 里存储锁偏向的线程 ID。在线程进入和退出同步块时不再通过 CAS 操作来加锁和解锁，而是检测 Mark Word 里是否存储着指向当前线程的偏向锁。轻量级锁的获取及释放依赖多次 CAS 原子指令，而偏向锁只需要在置换 ThreadID 的时候依赖一次 CAS 原子指令即可。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程是不会主动释放偏向锁的。 关于偏向锁的撤销，需要等待全局安全点，即在某个时间点上没有字节码正在执行时，它会先暂停拥有偏向锁的线程，然后判断锁对象是否处于被锁定状态。如果线程不处于活动状态，则将对象头设置成无锁状态，并撤销偏向锁，恢复到无锁（标志位为01）或轻量级锁（标志位为00）的状态。 偏向锁关闭，或者多个线程竞争偏向锁怎么办呢？ 轻量级锁还是跟Mark Work 相关，如果这个对象是无锁的，jvm就会在当前线程的栈帧中建立一个叫锁记录（Lock Record）的空间，用来存储锁对象的Mark Word 拷贝，然后把Lock Record中的owner指向当前对象。 JVM接下来会利用CAS尝试把对象原本的Mark Word 更新会Lock Record的指针，成功就说明加锁成功，改变锁标志位，执行相关同步操作。 如果失败了，就会判断当前对象的Mark Word是否指向了当前线程的栈帧，是则表示当前的线程已经持有了这个对象的锁，否则说明被其他线程持有了，继续锁升级，修改锁的状态，之后等待的线程也阻塞。 轻量级锁是指当锁是偏向锁的时候，却被另外的线程所访问，此时偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，线程不会阻塞，从而提高性能。 轻量级锁的获取主要由两种情况：① 当关闭偏向锁功能时；② 由于多个线程竞争偏向锁导致偏向锁升级为轻量级锁。 一旦有第二个线程加入锁竞争，偏向锁就升级为轻量级锁（自旋锁）。这里要明确一下什么是锁竞争： 如果多个线程轮流获取一个锁，但是每次获取锁的时候都很顺利，没有发生阻塞，那么就不存在锁竞争。只有当某线程尝试获取锁的时候，发现该锁已经被占用，只能等待其释放，这才发生了锁竞争。 在轻量级锁状态下继续锁竞争，没有抢到锁的线程将自旋，即不停地循环判断锁是否能够被成功获取。获取锁的操作，其实就是通过CAS修改对象头里的锁标志位。先比较当前锁标志位是否为“释放”，如果是则将其设置为“锁定”，比较并设置是原子性发生的。这就算抢到锁了，然后线程将当前锁的持有者信息修改为自己。 长时间的自旋操作是非常消耗资源的，一个线程持有锁，其他线程就只能在原地空耗CPU，执行不了任何有效的任务，这种现象叫做忙等（busy-waiting）。如果多个线程用一个锁，但是没有发生锁竞争，或者发生了很轻微的锁竞争，那么synchronized就用轻量级锁，允许短时间的忙等现象。这是一种折衷的想法，短时间的忙等，换取线程在用户态和内核态之间切换的开销。 自旋锁我不是在上面提到了Linux系统的用户态和内核态的切换很耗资源，其实就是线程的等待唤起过程，那怎么才能减少这种消耗呢？ 自旋，过来的现在就不断自旋，防止线程被挂起，一旦可以获取资源，就直接尝试成功，直到超出阈值，自旋锁的默认大小是10次，-XX：PreBlockSpin可以修改。 自旋都失败了，那就升级为重量级的锁，像1.5的一样，等待唤起咯。 自适应自旋锁自适应意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定： 如果在同一个锁对象上，自旋等待之前成功获得过的锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，因此允许自旋等待持续相对更长的时间。 相反的，如果对于某个锁，自旋很少成功获得过，那么以后要获取这个锁时将可能减少自旋时间甚至省略自旋过程，以避免浪费处理器资源。 自适应自旋解决的是“锁竞争时间不确定”的问题。JVM很难感知确切的锁竞争时间，而交给用户分析就违反了JVM的设计初衷。自适应自旋假定不同线程持有同一个锁对象的时间基本相当，竞争程度趋于稳定。因此，可以根据上一次自旋的时间与结果调整下一次自旋的时间。 重量级锁如果锁竞争情况严重，某个达到最大自旋次数的线程，会将轻量级锁升级为重量级锁（依然是CAS修改锁标志位，但不修改持有锁的线程ID）。当后续线程尝试获取锁时，发现被占用的锁是重量级锁，则直接将自己挂起（而不是忙等），等待将来被唤醒。 重量级锁是指当有一个线程获取锁之后，其余所有等待获取该锁的线程都会处于阻塞状态。 简言之，就是所有的控制权都交给了操作系统，由操作系统来负责线程间的调度和线程的状态变更。而这样会出现频繁地对线程运行状态的切换，线程的挂起和唤醒，从而消耗大量的系统资 大家在看ObjectMonitor源码的时候，会发现Atomic::cmpxchg_ptr，Atomic::inc_ptr等内核函数，对应的线程就是park()和upark()。这个操作涉及用户态和内核态的转换了，这种切换是很耗资源的，所以知道为啥有自旋锁这样的操作了吧，按道理类似死循环的操作更费资源才是对吧？其实不是，大家了解一下就知道了。 那用户态和内核态又是啥呢？ Linux系统的体系结构分为用户空间（应用程序的活动空间）和内核。我们所有的程序都在用户空间运行，进入用户运行状态也就是（用户态），但是很多操作可能涉及内核运行，比如涉及到I/O，我们就会进入内核运行状态（内核态）。 这个过程是很复杂的，也涉及很多值的传递，我简单概括下流程： 用户态把一些数据放到寄存器，或者创建对应的堆栈，表明需要操作系统提供的服务。 用户态执行系统调用（系统调用是操作系统的最小功能单位）。 CPU切换到内核态，跳到对应的内存指定的位置执行指令。 系统调用处理器去读取我们先前放到内存的数据参数，执行程序的请求。 调用完成，操作系统重置CPU为用户态返回结果，并执行下个指令。 所以大家一直说，1.6之前是重量级锁，没错，但是他重量的本质，是ObjectMonitor调用的过程，以及Linux内核的复杂运行机制决定的，大量的系统资源消耗，所以效率才低。还有两种情况也会发生内核态和用户态的切换：异常事件和外围设备的中断 大家也可以了解下。 普通的IO读写也会涉及到用户态和内核的切换，但是为了提升IO 的性能，操作系统可以通过 零拷贝 来实现，Redis和Kafka,还有netty的底层IO模型都存在零拷贝。 锁对比 synchronized和Lock对比我们先看看他们的区别： synchronized是关键字，是JVM层面的底层啥都帮我们做了，而Lock是一个接口，是JDK层面的有丰富的API。 synchronized会自动释放锁，而Lock必须手动释放锁。 synchronized是不可中断的，Lock可以中断也可以不中断。 通过Lock可以知道线程有没有拿到锁，而synchronized不能。 synchronized能锁住方法和代码块，而Lock只能锁住代码块。 Lock可以使用读锁提高多线程读效率。 synchronized是非公平锁，ReentrantLock可以控制是否是公平锁。 两者一个是JDK层面的一个是JVM层面的，我觉得最大的区别其实在，我们是否需要丰富的api，还有一个我们的场景。 比如我现在是滴滴，我早上有打车高峰，我代码使用了大量的synchronized，有什么问题？锁升级过程是不可逆的，过了高峰我们还是重量级的锁，那效率是不是大打折扣了？这个时候你用Lock是不是很好？ synchronized使用注意事项synchronized是通过软件(JVM)实现的，简单易用，即使在JDK5之后有了Lock，仍然被广泛地使用。 使用Synchronized有哪些要注意的？ 锁对象不能为空，因为锁的信息都保存在对象头里 作用域不宜过大，影响程序执行的速度，控制范围过大，编写代码也容易出错 避免死锁 在能选择的情况下，既不要用Lock也不要用synchronized关键字，用java.util.concurrent包中的各种各样的类，如果不用该包下的类，在满足业务的情况下，可以使用synchronized关键，因为代码量少，避免出错 synchronized是公平锁吗？ synchronized实际上是非公平的，新来的线程有可能立即获得监视器，而在等待区中等候已久的线程可能再次等待，不过这种抢占的方式可以预防饥饿。 参考文档Linux探秘之用户态与内核态 傻瓜三歪让我教他「零拷贝」 偏向锁、轻量级锁、重量级锁、自旋锁、自适应自旋锁 关于 锁的四种状态与锁升级过程 图文详解 死磕Synchronized底层实现","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[]},{"title":"分布式-秒杀系统设计","date":"2021-07-29T03:03:58.000Z","path":"wiki/分布式-秒杀系统设计/","text":"秒杀系统的挑战高并发秒杀活动的特点就是短时间内聚集大量请求瞬时到达服务端，此时数据库已经无法支撑如此大数据量的请求了。单个的数据库QPS仅有几百，你可能会想，那我是不是可以部署一个数据库集群呢，在数据库集群前使用Nginx分发，将负载平均分摊到每一个数据库不就可以了吗。这种方式一定程度上是可以的，但是国内真正的互联网大厂肯定不是这样做的，在高并发的情况下完全可以使用内存操作来代替访问数据库，比如Redis集群。在网络带宽允许的情况下，Redis的集群的系统吞吐量肯定和数据库集群不是一个量级的，这里要查几十倍几百倍。 超卖超卖最直接的后果就是可能会对公司造成直接的经济损失。防止超卖是秒杀系统必须要保证的一点。超卖的解决办法就是加锁，保证在高并发的情况下库存可以正常的正确的扣减。 恶意请求产品层策略秒杀器一般下单和购买及其迅速，根据购买记录可以甄别出一部分。可以通过校验码达到一定的方法，这就要求校验码足够安全，不被破解，采用的方式有：秒杀专用验证码，电视公布验证码，秒杀答题。 前端控制除此之外前段可以添加点击次数限制，点击一定次数之后，将按钮置灰色，或者在js层级进行控制，用户看到的是每次都点击成功了，但是仅仅发起一次服务端请求； 后端控制 添加消息队列，消息执行一定数量时，队列后续的消息不在执行 后端架构按照模块拆分，用户不同的请求分散转发到各个模块的服务器，负载均衡 数据库分库分表（分片策略）和redis集群 升级服务器带宽，压力测试，保证系统吞吐量 过载保护，限流，请求拒绝和服务降级 链接加盐链接加盐一定程度上可以保护恶意攻击，比如下单接口，如果暴露之后，就会存在恶意攻击，一个用户下了几百个单的情况或者一个IP下了很多单子，类似与黄牛抢票之后再去售卖。 秒杀系统架构图（参考） 参考资料 敖丙带你设计【秒杀系统】《吊打面试官》系列-秒杀系统设计这是我读过写得最好的【秒杀系统架构】分析与实战！这一次，彻底弄懂“秒杀系统”","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[]},{"title":"JVM-Xms,Xmx和Xss","date":"2021-07-28T13:04:14.000Z","path":"wiki/JVM-Xms-Xmx和Xss/","text":"性能调优参数Xms，Xmx，Xss的含义-Xss规定了每个线程虚拟机栈及堆栈的大小，一般情况下，256k是足够的，此配置将会影响此进程中并发线程数的大小。 -Xms表示初始化JAVA堆的大小及该进程刚创建出来的时候，他的专属JAVA堆的大小，一旦对象容量超过了JAVA堆的初始容量，JAVA堆将会自动扩容到-Xmx大小。 -Xmx表示java堆可以扩展到的最大值，在很多情况下，通常将-Xms和-Xmx设置成一样的，因为当堆不够用而发生扩容时，会发生内存抖动影响程序运行时的稳定性。 堆内存分配：JVM初始分配的内存由-Xms指定，默认是物理内存的1/64JVM最大分配的内存由-Xmx指定，默认是物理内存的1/4默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。对象的堆内存由称为垃圾回收器的自动内存管理系统回收。非堆内存分配：JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。-Xmn2G：设置年轻代大小为2G。-XX:SurvivorRatio，设置年轻代中Eden区与Survivor区的比值。 参考资料1、类似-Xms、-Xmn这些参数的含义：2、JVM三大性能调优参数Xms，Xmx，Xss的含义，你又知道多少呢","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-自定义类加载器","date":"2021-07-28T13:02:32.000Z","path":"wiki/JVM-自定义类加载器/","text":"如何自定义类加载器为什么要自定义加载器 原因：1、存放在自定义路径上的类，需要通过自定义类加载器去加载。【注意：AppClassLoader加载classpath下的类】2、类不一定从文件中加载，也可能从网络中的流中加载，这就需要自定义加载器去实现加密解密。3、可以定义类的实现机制，实现类的热部署,如OSGi中的bundle模块就是通过实现自己的ClassLoader实现的，如tomcat实现的自定义类加载模型。 如何实现自定义加载器 实现自定义类加载有以下两步：1、继承ClassLoader2、重写findClass，在findClass里获取类的字节码，并调用ClassLoader中的defineClass方法来加载类，获取class对象。注意：如果要打破双亲委派机制，需要重写loadClass方法。如下：是一个自定义 的类加载器 1234567891011121314151617181920public static class MyClassLoader extends ClassLoader&#123; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] data=null; try &#123; data= loadByte(name); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return this.defineClass(data,0,data.length); &#125; private byte[] loadByte(String name) throws IOException &#123; File file = new File(&quot;/Users/admin/test/&quot;+name); FileInputStream fi = new FileInputStream(file); int len = fi.available(); byte[] b = new byte[len]; fi.read(b); return b; &#125; &#125; 下面是要加载的类： 12345public class Demo&#123;public void say()&#123;System.out.println(&quot;hello&quot;);&#125;&#125; 该类编译后的class 文件放置在/Users/admin/test/下,然后执行如下代码去加载： 1234567MyClassLoader classLoader = new MyClassLoader(); Class clazz = classLoader.loadClass(&quot;Demo.class&quot;); Object o=clazz.newInstance(); Method method = clazz.getMethod(&quot;say&quot;); method.invoke(o);输出:hello 能不能自己写一个java.lang.String 1、代码书写后可以编译不会报错2、在另一个类中加载java.lang.String，通过反射调用自己写的String类里的方法，得到结果NoSuchMethod，说明加载的还是原来的String，因为通过双亲委派机制，会把java.lang.String一直提交给启动类加载器去加载，通过他加载，加载到的永远是/lib下面的java.lang.String3、在这个自己写的类中写上main方法public static void main(String[] args)执行main方法报错，因为这个String并不是系统的java.lang.String，所以JVM找不到main方法的签名 参考资料JVM:如何实现一个自定义类加载器？原文链接：https://blog.csdn.net/qq_28605513/article/details/85014451","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"分布式-一致性哈希算法","date":"2021-07-28T13:01:05.000Z","path":"wiki/分布式-一致性哈希算法/","text":"一致性哈希算法一致性哈希算法(Consistent Hashing Algorithm)是一种分布式算法，常用于负载均衡。Memcached client也选择这种算法，解决将key-value均匀分配到众多Memcached server上的问题。它可以取代传统的取模操作，解决了取模操作无法应对增删Memcached Server的问题(增删server会导致同一个key,在get操作时分配不到数据真正存储的server，命中率会急剧下降)。 哈希指标 评估一个哈希算法的优劣，有如下指标，而一致性哈希全部满足： 均衡性(Balance)：将关键字的哈希地址均匀地分布在地址空间中，使地址空间得到充分利用，这是设计哈希的一个基本特性。 单调性(Monotonicity): 单调性是指当地址空间增大时，通过哈希函数所得到的关键字的哈希地址也能映射的新的地址空间，而不是仅限于原先的地址空间。或等地址空间减少时，也是只能映射到有效的地址空间中。简单的哈希函数往往不能满足此性质。 分散性(Spread): 哈希经常用在分布式环境中，终端用户通过哈希函数将自己的内容存到不同的缓冲区。此时，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load): 负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷 一致性哈希 将节点通过hash映射到hash环上，理想的情况是多个节点直接分布均匀 当我们的对象通过hash算法分配在hash环上的时候，它是固定分配到一个节点的空间上的，当我们在BC之间插入一个节点时，仅仅会影响到BC这一段空间上的数据，而不是整个环上的数据都要跟着变化； 现实情况下，节点之间可能分配不均匀 这和传统的hash取模一样，同样会数据倾斜的问题！ 虚拟节点 这个时候虚拟节点就此诞生，下面让我们来看一下虚拟节点在一致性Hash中的作用。当我们在Hash环上新增若干个点，那么每个点之间的距离就会接近相等。按照这个思路我们可以新增若干个片/表，但是成本有限，我们通过复制多个A、B、C的副本({A1-An},{B1-Bn},{C1-Cn})一起参与计算，按照顺时针的方向进行数据分布，按照下图示意: 此时A=[A,C1)&amp;[A1,C2)&amp;[A2,B4)&amp;[A3,A4)&amp;[A4,B1)；B=[B,A1)&amp;[B2,C)&amp;[B3,C3)&amp;[B4,C4)&amp;[B1,A)；C=[C1,B)&amp;[C2,B2)&amp;[C,B3)&amp;[B3,C3)&amp;[C4,A3)；由图可以看出分布点越密集，平衡性约好。 算法实现一致性哈希算法有多种具体的实现，包括 Chord 算法，KAD 算法等，都比较复杂。 参考资料1 、一致性哈希算法的原理与实现2、浅谈一致性Hash原理及应用","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"}]},{"title":"分布式-CAP理论","date":"2021-07-28T10:15:10.000Z","path":"wiki/分布式-CAP理论/","text":"CAP 原则CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP原则的精髓就是要么AP，要么CP，要么AC，但是不存在CAP。 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值，即写操作之后的读操作，必须返回该值。（分为弱一致性、强一致性和最终一致性） 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 取舍策略 CA without P 如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库RDBMS：Oracle、MySQL就是CA。 CP without A 如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。 AP wihtout C 要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。 解决方案——BASEBASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果。 核心思想：即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 基本可用Basically Available 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态Soft state 软状态也称弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性Eventually consistent 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 参考资料原文链接：https://blog.csdn.net/lixinkuan328/article/details/95535691","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"}]},{"title":"Mysql-事务特性与实现原理","date":"2021-07-28T10:13:07.000Z","path":"wiki/Mysql-事务特性与实现原理/","text":"MySQL事务特性与实现原理1. 事务特性 原子性(Atomicity) 事务中的所有操作作为一个整体像原子一样不可分割，要么全部成功,要么全部失败。 一致性(Consistency) 事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态。一致性状态是指:1.系统的状态满足数据的完整性约束(主码,参照完整性,check约束等) 2.系统的状态反应数据库本应描述的现实世界的真实状态,比如转账前后两个账户的金额总和应该保持不变。 隔离性(Isolation) 并发执行的事务不会相互影响,其对数据库的影响和它们串行执行时一样。比如多个用户同时往一个账户转账,最后账户的结果应该和他们按先后次序转账的结果一样。 持久性(Durability) 事务一旦提交,其对数据库的更新就是持久的。任何事务或系统故障都不会导致数据丢失。 在事务的ACID特性中,C即一致性是事务的根本追求,而对数据一致性的破坏主要来自两个方面1.事务的并发执行2.事务故障或系统故障 2. 事务实现原理 并发控制技术保证了事务的隔离性,使数据库的一致性状态不会因为并发执行的操作被破坏。 日志恢复技术保证了事务的原子性,使一致性状态不会因事务或系统故障被破坏。同时使已提交的对数据库的修改不会因系统崩溃而丢失,保证了事务的持久性。 2.1 回滚日志（undo）undo log属于 「 逻辑日志 」，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。 undo log用于存放数据被修改前的值，如果修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。另外InnoDB MVCC事务特性也是基于undo日志实现的。 因此，undo log有两个作用：提供回滚和多个行版本控制(MVCC)。 2.2 重做日志（redo）redo log重做日志记录的是新数据的备份，属于物理日志。在事务提交前，只要将redo log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是redo log已经持久化。系统可以根据redo log的内容，将所有数据恢复到最新的状态。 redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。 MySQL中redo log刷新规则采用一种称为Checkpoint的机制（利用LSN实现），为了确保安全性，又引入double write机制。 事务基本操作开启事务：start transaction回滚事务：rollback提交事务：commit 数据库隔离级别SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。另外，这篇分布式事务不理解？一次给你讲清楚！推荐大家阅读。 Read Uncommitted（读取未提交内容）在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容）这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。 Repeatable Read（可重读）这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化）这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 事务隔离级别产生的问题 脏读(Drity Read)某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 12345事务A第一次读取到price=100同时事务B更新update price=120，但是此时的事务B还未commit事务A读取的price=120事务B-&gt;rollback操作事务A读取到的是脏数据 不可重复读(Non-repeatable read)在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 1234事务A第一次读取到price=100同时事务B更新update price=120，并commit事务A读取的price=120事务A多次读取的结果不一致 幻读(Phantom Read)幻读和不可重复读的区别在于，幻读主要表现在数据的删除和插入，而不可重复读表现在数据的更新。 1234事务A第一次读取到price=100同时事务B更新delete price=100 这条记录，并commit事务A读取的price=100price这条记录已经不存在，但是事务A还是可以读取到 1、在可重复读隔离级别下，普通查询是快照读，是不会看到别的事务插入的数据的，幻读只在当前读下才会出现。2、幻读专指新插入的行，读到原本存在行的更新结果不算。因为当前读的作用就是能读到所有已经提交记录的最新值。 参考资料详细分析MySQL事务日志(redo log和undo log)数据库事务的概念及其实现原理数据库事务实现原理mysql数据库的隔离级别MYSQL数据库的四种隔离级别MySQL幻读MySQL 事务&amp;&amp;锁机制&amp;&amp;MVCC","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Mysql-MVCC多版本并发控制","date":"2021-07-28T10:12:28.000Z","path":"wiki/Mysql-MVCC多版本并发控制/","text":"MVCC多版本并发控制MVCC，全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。 MVCC在MySQL InnoDB中的实现主要是为了 「 提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读 」 当前读和快照读 当前读 像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 快照读 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本 说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现 MVCC模型在MySQL中的具体实现则是由 3个隐式字段，undo日志 ，Read View 等去完成的，具体可以看下面的MVCC实现原理 MVCC有什么好处，解决了什么问题多版本并发控制（MVCC）是一种用来「 解决读-写冲突的无锁并发控制 」，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题 MVCC的实现原理MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个point的概念 隐式字段每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段 DB_TRX_ID 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID DB_ROLL_PTR 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） DB_ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了 undo日志undo log主要分为两种： insert undo log 代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log 事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 Read View(读视图)什么是Read View，说白了Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大) 所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 Read View遵循一个可见性算法，主要是将要被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本; 参考资料1、MVCC多版本并发控制2、MVCC浅析3、乐观锁、悲观锁和MVCC，今天让你一次搞懂4、面试官：谈谈你对Mysql的MVCC的理解？5、Mysql中MVCC的使用及原理详解","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Java对象头","date":"2021-07-28T09:59:59.000Z","path":"wiki/Java对象头/","text":"Java对象头JOL查看对象头信息在项目中引入以下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 写一个main方法，创建一个Object，然后打印对象信息： 1234 public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable());&#125; 打印结果如下： 12345678java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 由此可知，new Object()在内存中占16个字节，组成部分8字节的markword+4字节的class point+4字节的对齐； Java对象在内存中的布局 markword 存储sync锁标志，分代年龄等一些关键信息 8字节 class pointer 指向当前对象所属类类型 4字节 查看java命令默认带的参数命令： java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=134217728-XX:MaxHeapSize=2147483648-XX:+PrintCommandLineFlags-XX:+UseCompressedClassPointers 压缩类指针 4字节-XX:+UseCompressedOops 普通对象指针压缩 4字节-XX:+UseParallelGC instance data 寸尺当前对象的实例数据 padding 对齐填充，当对象所占字节数不能被8整除之后，进行填充对齐。 目前的操作系统基本上都是64位的； 顺丰面试题，new Object()在内存中占多少个字节1、如果创建的是空对象，没有实例数据 默认开启了class pointer指针压缩 8字节markword + 4字节class pointer + 4字节 padding 如果关闭了类指针压缩 8字节markword + 8字节class pointer 2、如果创建的对象有实力数据，如下对象： 1Person（int age , String name） 默认开启了class pointer指针压缩 8字节markword + 4字节class pointer + 4字节int + 4字节String + 4字节padding对齐","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-CAS原理和底层实现","date":"2021-07-28T09:59:17.000Z","path":"wiki/Java-CAS原理和底层实现/","text":"CAS原理和底层实现 什么是CASCAS是（compare and swap） 的缩写，它能在不加锁的情况下，在多线程的环境下，保证多线程一致性的改动某一值； ABA问题ABA问题是一个线程在CAS比较值和原来是否相等的过程中，别的线程修改过这个值，但是又改回去了，倒置当前线程比较的时候，发现是相等的，但是，中间是被修改过的； 添加版本号，比较值的时候同时比较版本号 CAS底层原理AtomicInteger:123456789101112public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; Unsafe:1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 运用： 12345678910111213141516171819202122232425262728package com.mashibing.jol;import sun.misc.Unsafe;import java.lang.reflect.Field;public class T02_TestUnsafe &#123; int i = 0; private static T02_TestUnsafe t = new T02_TestUnsafe(); public static void main(String[] args) throws Exception &#123; //Unsafe unsafe = Unsafe.getUnsafe(); Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); Field f = T02_TestUnsafe.class.getDeclaredField(&quot;i&quot;); long offset = unsafe.objectFieldOffset(f); System.out.println(offset); boolean success = unsafe.compareAndSwapInt(t, offset, 0, 1); System.out.println(success); System.out.println(t.i); //unsafe.compareAndSwapInt() &#125;&#125; jdk8u: unsafe.cpp: cmpxchg = compare and exchange 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(&quot;Unsafe_CompareAndSwapInt&quot;); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END jdk8u: atomic_linux_x86.inline.hpp 93行is_MP = Multi Processor 12345678inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) &quot;cmpxchgl %1,(%3)&quot; : &quot;=a&quot; (exchange_value) : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest), &quot;r&quot; (mp) : &quot;cc&quot;, &quot;memory&quot;); return exchange_value;&#125; jdk8u: os.hpp is_MP() 12345678910static inline bool is_MP() &#123; // During bootstrap if _processor_count is not yet initialized // we claim to be MP as that is safest. If any platform has a // stub generator that might be triggered in this phase and for // which being declared MP when in fact not, is a problem - then // the bootstrap routine for the stub generator needs to check // the processor count directly and leave the bootstrap routine // in place until called after initialization has ocurred. return (_processor_count != 1) || AssumeMP;&#125; jdk8u: atomic_linux_x86.inline.hpp 1#define LOCK_IF_MP(mp) &quot;cmp $0, &quot; #mp &quot;; je 1f; lock; 1: &quot; 最终实现：底层对应一个汇编指令「lock comxchg」，但是comxchg这条指令不是原子性的，他不能保证在比较的时候，别的线程会不会改变值；而保证线程安全的则是lock这条指令，lock这条指令在执行后面执行的时候锁定一个「北桥信号」，而不是采用纵线锁的方式； CAS在JDK中的实现1、AtomitInteger2、ConcurrentHashMap","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-FutureTask原理","date":"2021-07-28T09:58:47.000Z","path":"wiki/Java-FutureTask原理/","text":"FutureTaskFuture方法介绍123456789101112131415161718public interface Future&lt;V&gt; &#123; // 取消任务 可中断的方式取消 boolean cancel(boolean mayInterruptIfRunning); // 判断任务是否处于取消状态 boolean isCancelled(); // 判断异步任务是否执行完成 ==这里使用轮训的方式监听== boolean isDone(); // 获取异步线程的执行结果，如果没有执行完成，则一直阻塞到有结果返回； V get() throws InterruptedException, ExecutionException; // 获取异步线程的执行结果，如果没有执行完成，则一直阻塞到设置的时间，有结果返回，没有结果则抛出异常； V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 简单示范Callable&amp;Future（1）向线程池中提交任务的submit方法不是阻塞方法，而Future.get方法是一个阻塞方法（2）submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果，所以一般需要使用future.isDone先判断任务是否全部执行完成，完成后再使用future.get得到结果。（也可以用get (long timeout, TimeUnit unit)方法可以设置超时时间，防止无限时间的等待） 1234567891011121314151617181920212223242526272829303132333435public class FutureTest implements Callable&lt;Integer&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ @Override public Integer call() throws Exception &#123; System.err.println(&quot;start call method...&quot;); Thread.sleep(3000); return 1111; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; System.err.println(&quot;main method start....&quot;); FutureTest futureTest = new FutureTest(); Future1Test future1Test = new Future1Test(); long time = System.currentTimeMillis(); ExecutorService executorService = Executors.newFixedThreadPool(2); Future&lt;String&gt; future = executorService.submit(future1Test); if (!future.isDone()) &#123; System.err.println(&quot;future not done !&quot;); &#125; Future&lt;Integer&gt; future1 = executorService.submit(futureTest); // submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果 // 这里先提交了future1Test，休眠了4s, futureTest休眠了3s，但是等我们get到结果的时候，是消耗的4s时间的； System.err.println(&quot;cost time: &quot; + (System.currentTimeMillis() - time)); System.err.println(&quot;future: &quot; + future.get()); System.err.println(&quot;future1: &quot; + future1.get()); System.err.println(&quot;main method end....&quot;); executorService.shutdown(); &#125;&#125; 执行结果12345678main method start....future not done !// 说明了第一 get()方法是阻塞，第二线程池任务都执行完成之后，按提交任务顺序get结果返回值cost time: 4start call method...future: future 2 testfuture1: 1111main method end.... 注意点 线程池执行任务有两种方式execute和submit，execute是不带返回值的，submit是有返回值的; main方法中可以不使用线程池，可以直接创建线程，调用start方法就可以，切记只有在演示代码的时候后。手动直接创建线程的方式还是不要用，因为一旦请求变多，则会创建无数的线程，线程数大于CPU核数，进而导致CPU频繁切换上下分进行调度，性能严重下降。 而且线程的数据是存放在内存中的，会占用大量的内存，增加垃圾回收的压力。严重的会发生OOM; 异常main方法中我们使用的是Future future接收异步任务执行的放回结果，但实际上Future其实是一个interface，并不能接收返回结果的，那实际我们调用future.get()是，是实例了一个FutureTask对象来接受的； FutureTask讲解下面主要针对Future的实现类FutureTask的几个重要方法展开 FutureTask继承关系1234567891011public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123;...&#125;// 下面是RunnableFuture接口的继承关系public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; /** * Sets this Future to the result of its computation * unless it has been cancelled. */ void run();&#125; FutureTask 重要的成员变量 12345678910111213141516171819202122/** The underlying callable; nulled out after running */private Callable&lt;V&gt; callable;/** The result to return or exception to throw from get() *///任务执行结果或者任务异常private Object outcome; // non-volatile, protected by state reads/writes/** The thread running the callable; CASed during run() *///执行任务的线程private volatile Thread runner;/** Treiber stack of waiting threads *///等待节点，关联等待线程private volatile WaitNode waiters;private static final sun.misc.Unsafe UNSAFE;//state字段的内存偏移量 这个在线程池执行任务的时候进行状态判断的时候会用到private static final long stateOffset;//runner字段的内存偏移量private static final long runnerOffset;//waiters字段的内存偏移量private static final long waitersOffset; 定义任务的生命周期 12345678private volatile int state;private static final int NEW = 0;private static final int COMPLETING = 1;private static final int NORMAL = 2;private static final int EXCEPTIONAL = 3;private static final int CANCELLED = 4;private static final int INTERRUPTING = 5;private static final int INTERRUPTED = 6; NORMAL:指的是任务能够正常执行状态 EXCEPTIONAL：表示任务执行异常 CANCELLED：取消状态，之后的状态都表示任务取消或终端 下面看一下FutureTask中几个重要的方法 执行结果 | report方法 Returns result or throws exception for completed task.主要是上报异步任务执行的结果或返回任务执行发生的异常 1234567891011121314/** * Returns result or throws exception for completed task. * * @param s completed state value */ @SuppressWarnings(&quot;unchecked&quot;) private V report(int s) throws ExecutionException &#123; Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); &#125; 判断逻辑就是根据参数，也是是任务状态，根据不同的状态处理相应的逻辑。比如NORNAL状态，表示任务正常执行，直接返回结果就可以。如果状态大于CANCELLED，说明任务被取消或终端，会抛出CancellationException()；如果不是异常状态，则抛出ExecutionException； 任务执行 | run() 执行异步任务 1234567891011121314151617181920212223242526272829303132333435363738394041public void run() &#123; // 如果状态 state 不是 NEW，或者设置 runner 值失败 // 表示有别的线程在此之前调用 run 方法，并成功设置了 runner 值 // 保证了只有一个线程可以运行 try 代码块中的代码。 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; //以上state值变更的由CAS操作保证原子性 try &#123; Callable&lt;V&gt; c = callable; //只有c不为null且状态state为NEW的情况 if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //调用callable的call方法，并获得返回结果 result = c.call(); //运行成功 ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; setException(ex); &#125; if (ran) //设置结果 set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125; 核心逻辑就是调用Callable的call方法，==result=c.call();== 并且对任务执行的结果或异常信息进行处理； 获取结果 | get() throws InterruptedException, ExecutionException 获取异步任务执行的结果或异常信息 123456public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125; get方法执行两个操作： 判断任务的状态,如果没有执行完成，调用awaitDone方法 任务完成，调用我们上面说的report方法，返回任务执行结果 任务阻塞 | awaitDone(boolean timed, long nanos) 等到任务执行完成 也是get方法阻塞特性的关键所在 123456789101112131415161718192021222324252627282930313233343536373839404142434445private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; // CPU轮转 for (;;) &#123; // 如果线程中断了，将线程移除等待队列，抛出中断异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; // 如果任务状态大于完成，则直接返回； if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; // 如果任务完成，但是返回值outcome还没有设置，可以先让出线程执行权，让其他线程执行 else if (s == COMPLETING) // cannot time out yet Thread.yield(); // 下面是任务还没有执行完成的状态，将线程添加到等待队列 else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // 判断get方法是否设置了超时时间 else if (timed) &#123; nanos = deadline - System.nanoTime(); // 如果超出设置的时间，线程移除等到队列 if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; // 没有设置超时时间，线程直接阻塞，直到任务完成 else LockSupport.park(this); &#125; &#125; 主要执行步骤： 判断线程是否被中断，如果被中断了，就从等待的线程栈中移除该等待节点，然后抛出中断异常 读取state,判断任务是否已经完成，如果已经完成或者任务已经取消，此时调用get方法的线程不会阻塞，会直接获取到结果或者拿到异常信息； 如果s == COMPLETING，说明任务已经结束，但是结果还没有保存到outcome中，==此时线程让出执行权，给其他线程先执行；== 如果任务没有执行完成，则需要创建等待节点，等待插入到阻塞队列 判断queued，这里是将c中创建节点q加入队列头。使用Unsafe的CAS方法，对waiters进行赋值，waiters也是一个WaitNode节点，相当于队列头，或者理解为队列的头指针。通过WaitNode可以遍历整个阻塞队列 然后判断超时时间，时间是在调用get方法的时候传输进来的，如果有超时时间，则设置超时时间，如果超出时间，则将线程移除等待队列；如果没有设置时间，则直接阻塞线程； 取消任务 | cancel(boolean mayInterruptIfRunning)123456789101112131415161718192021222324252627@Param mayInterruptIfRunning 是否中断public boolean cancel(boolean mayInterruptIfRunning) &#123; /* * 在状态还为NEW的时候，根据参数中的是否允许传递， * 将状态流转到INTERRUPTING或者CANCELLED。 */ if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // in case call to interrupt throws exception if (mayInterruptIfRunning) &#123; try &#123; Thread t = runner; if (t != null) t.interrupt(); &#125; finally &#123; // final state UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true; &#125; 123456789101112131415161718192021222324252627282930313233343536private void finishCompletion() &#123; for (WaitNode q; (q = waiters) != null;) &#123; // 必须将栈顶CAS为null，否则重读栈顶并重试。 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; // 遍历并唤醒栈中节点对应的线程。 for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; WaitNode next = q.next; if (next == null) break; // 将next域置为null，这样对GC友好。 q.next = null; q = next; &#125; break; &#125; &#125; /* * done方法是暴露给子类的一个钩子方法。 * * 这个方法在ExecutorCompletionService.QueueingFuture中的override实现是把结果加到阻塞队列里。 * CompletionService谁用谁知道，奥秘全在这。 */ done(); /* * callable置为null主要为了减少内存开销, * 更多可以了解JVM memory footprint相关资料。 */ callable = null;&#125; Callable&amp;Future使用场景 异步任务需要拿到返回值 多线程并发调用，顺序组装返回值，一些并发框架中会看到相应体现 还有一些分布式任务调度的场景，远程调用需要回填执行结果 还有很多通信框架中都有体现 参考资料 (1) future.get方法阻塞问题的解决，实现按照任务完成的先后顺序获取任务的结果(2) Java多线程引发的性能问题以及调优策略(3) 可取消的异步任务——FutureTask用法及解析(4) FutureTask源码解读","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-join方法原理解析","date":"2021-07-28T09:58:22.000Z","path":"wiki/Java-join方法原理解析/","text":"join方法12345join重载方法1 join()2 join(long millis) //参数为毫秒3 join(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 功能演示123456789101112131415public class JoinDemo implements Runnable&#123; public void run() &#123; System.err.println(&quot;join thread demo &quot;); &#125; public static void main(String[] args) throws Exception &#123; System.err.println(&quot;main thread start... &quot;); Runnable r = new JoinDemo(); Thread t = new Thread(r); t.setName(&quot;ibli joinTest ...&quot;); t.start();// t.join(); System.err.println(&quot;main thread end... &quot;); &#125;&#125; 以上将t.join();注释掉，执行的一种可能结果如下： 12345678main thread start... main thread end... join thread demo还有可能是这种结果：main thread start... join thread demomain thread end... 但是把注释去掉，结果如下： 123main thread start... join thread demo main thread end... 这是一个非常简单的demo,效果是显而易见的。当main线程去调用t.join()是，会将自己当前线程阻塞，等到t线程执行完成到达完结状态，main线程才可以继续执行。 我们看一下join()设置超时时间的方法： 123456789101112131415161718192021222324public class JoinDemo implements Runnable&#123; public void run() &#123; System.err.println(&quot;join thread demo &quot;); try &#123; // 线程睡眠4s Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; List&lt;String&gt; strings = null; System.err.println(strings.get(0)); &#125; public static void main(String[] args) throws Exception &#123; System.err.println(&quot;main thread start... &quot;); Runnable r = new JoinDemo(); Thread t = new Thread(r); t.setName(&quot;ibli joinTest ...&quot;); t.start(); // 但是主线程join的超时时间是1s t.join(1000); System.err.println(&quot;main thread end... &quot;); &#125;&#125; 执行效果： 123456main thread start... join thread demo main thread end... Exception in thread &quot;ibli joinTest ...&quot; java.lang.NullPointerException at com.ibli.threadTest.api.JoinDemo.run(JoinDemo.java:14) at java.lang.Thread.run(Thread.java:748) 上面的执行结果可以看到，子线程设置了4s的超时时间，但是主线程在1秒超时后，并没有等待子线程执行完毕，就被唤醒执行后续操作了；这样的预期是否符合你的预期呢？下面我们按照join的源码去分析吧！ join方法原理下面是join的原理图 join()源码 首先会调用join(0)方法，其实是join的重载方法； 123public final void join() throws InterruptedException &#123; join(0);&#125; 下面是join的核心实现： 1234567891011121314151617181920212223242526public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; // 首先校验参数是否合法 if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; // 如果join方法没有参数，则相当于直接调用wait方法 if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 下面是isAlive方法的源码 1public final native boolean isAlive(); 这是一个本地方法，作用是判断当前的线程是否处于活动状态。什么是活动状态呢？活动状态就是线程已经启动且尚未终止。线程处于正在运行或准备开始运行的状态，就认为线程是“存活”的。 这里有一个点要注意，join为什么阻塞的是主线程，而不是子线程呢？ 不理解的原因是阻塞主线程的方法是放在previousThread这个实例作用，让大家误以为应该阻塞previousThread线程。实际上主线程会持有previousThread这个对象的锁，然后调用wait方法去阻塞，而这个方法的调用者是在主线程中的。所以造成主线程阻塞。 其实join()方法的核心在于wait(),在主线程中调用t.join()相当于在main方法中添加 new JoinDemo().wait();是一样的效果；在这里只不过是wait方法写在了子线程的方法中。 再次重申一遍，join方法的作用是在主线程阻塞，等在子线程执行完之后，由子线程唤醒主线程，再继续执行主线程调用t.join()方法之后的逻辑。 那么主线程是在什么情况下知道要继续执行呢？就是上面说的，主线程其实是由join的子线程在执行完成之后调用的notifyAll()方法，来唤醒等待的线程。怎么证明呢？ 其实大家可以去翻看JVM的源码实现，Thread.cpp文件中，有一段代码： 123456void JavaThread::exit(bool destroy_vm, ExitType exit_type) &#123; // Notify waiters on thread object. This has to be done after exit() is called // on the thread (if the thread is the last thread in a daemon ThreadGroup the // group should have the destroyed bit set before waiters are notified). ensure_join(this);&#125; 其中调用ensure_join方法 123456789101112131415161718static void ensure_join(JavaThread* thread) &#123; // We do not need to grap the Threads_lock, since we are operating on ourself. Handle threadObj(thread, thread-&gt;threadObj()); assert(threadObj.not_null(), &quot;java thread object must exist&quot;); ObjectLocker lock(threadObj, thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception(); // Thread is exiting. So set thread_status field in java.lang.Thread class to TERMINATED. java_lang_Thread::set_thread_status(threadObj(), java_lang_Thread::TERMINATED); // Clear the native thread instance - this makes isAlive return false and allows the join() // to complete once we&#x27;ve done the notify_all below //这里是清除native线程，这个操作会导致isAlive()方法返回false java_lang_Thread::set_thread(threadObj(), NULL); // 在这里唤醒等待的线程 lock.notify_all(thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception();&#125; 在JVM的代码中，线程执行结束的最终调用了lock.notify_all(thread)方法来唤醒所有处于等到的线程 使用场景 比如我们使用Callable执行异步任务，需要在主线程处理任务的返回值时，可以调用join方法； 还有一些场景希望线程之间顺序执行的； join()方法与sleep()的比较我们先说一下sleep方法： 让当前线程休眠指定时间。 休眠时间的准确性依赖于系统时钟和CPU调度机制。 不释放已获取的锁资源，如果sleep方法在同步上下文中调用，那么其他线程是无法进- 入到当前同步块或者同步方法中的。 可通过调用interrupt()方法来唤醒休眠线程。 sleep是静态方法，可以在任何地方调用 相比与sleep方法sleep是静态方法，而且sleep的线程不是放锁资源，而join方法是对象方法，并且在等待的过程中会释放掉对象锁； 关于join方法会释放对象锁，那到底是释放的那个对象的锁呢，可以参照 关于join() 是否会释放锁的一些思考 参考资料 1、Java多线程中join方法的理解2、Thread.join的作用和原理3、Thread.join的作用和原理 d 山脚太拥挤 我们更高处见。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java内存模型","date":"2021-07-28T09:57:51.000Z","path":"wiki/Java内存模型/","text":"Java内存模型 什么是JMM?Java Memory Model简称JMM, 是一系列的Java虚拟机平台对开发者提供的多线程环境下的内存可见性、是否可以重排序等问题的无关具体平台的统一的保证。(可能在术语上与Java运行时内存分布有歧义，后者指堆、方法区、线程栈等内存区域)。 JMM规范的内容 所有变量存储在主内存 主内存是虚拟机内存的一部分 每条线程有自己的工作内存 线程的工作内存保存变量的主内存副本 线程对变量的操作必须在工作内存中进行 不同线程之间无法直接访问对方工作内存中的变量 线程间变量值的传递均需要通过主内存来完成 JMM并不是一个客观存在的东西，它实际是为了规范Java虚拟机制定到一套标准。那为什么需要这套标准呢？ 其实我们都知道JVM是运行在操作系统之上的。而目前的操作系统都是基于冯诺伊曼设置的计算机系统体系来的。CPU是计算机中用来执行控制和计算的核心组件。 所有的计算任务全部在CPU中完成，但是我们的所有变量的数据全部存储在主内存中。CPU在执行计算时，需要去主内存加载数据，CPU执行运算的速度极快，这就设计一个CPU执行速度和数据加载速度不一致的问题。 在操作系统级别解决这个问题的办法是引入了CPU缓存。每个CPU都有自己私有的L1缓存和L2缓存，当执行计算时，会优先去CPU自己的缓存中寻找数据，没有的话才会重新加载内存数据。这种方式一定程度上解决了CPU计算和数据加载不一致的问题。 但是也会引入一个新的问题，就是数据一致性问题。 缓存一致性与MESI协议 首先看一下什么是MESI协议 缓存一致性协议给缓存行（通常为64字节）定义了个状态：独占（exclusive）、共享（share）、修改（modified）、失效（invalid），用来描述该缓存行是否被多处理器共享、是否修改。所以缓存一致性协议也称MESI协议。 独占（exclusive）：仅当前处理器拥有该缓存行，并且没有修改过，是最新的值。 共享（share）：有多个处理器拥有该缓存行，每个处理器都没有修改过缓存，是最新的值。 修改（modified）：仅当前处理器拥有该缓存行，并且缓存行被修改过了，一定时间内会写回主存，会写成功状态会变为S。 失效（invalid）：缓存行被其他处理器修改过，该值不是最新的值，需要读取主存上最新的值。 如何解决缓存一致性问题呢？ 如上图所示，共享变量是存储在主内存Memory中，在CPU计算时，每一个CPU都有改变量的独立拷贝，每个CPU可以去读取甚至修改共享变量的值，但是为了保证数据的一致性，一个CPU modify了变量的值，需要通知其他的CPU这个变量的最新值是什么。那么可以怎么做呢。 1、在初始状态，每个CPU还没有加载共享变量，所有每一个CPU的缓存行的状态都是invalid； 2、当CPU0去使用这个共享变量的时候，首先去自己的缓存中查找，肯定是缓存不命中的，也就是cache miss,这个时候去主内存Memory中去加载，当共享变量的值加载到CPU0的缓存后，CPU缓存行状态变成shared, 也就是共享状态； 3、如果这个时候有其他的CPU也读取了共享变量的值，它们的cache line 的状态同样也是shared共享状态；此时一个CPU如果修改共享变量的值，而没有通知其他的CPU,就会造成缓存一致性问题； 4、当CPU0尝试去修改共享变量的值时，它会发出一个read invalidate命令，同时CPU0的缓存行状态设置为exclusive(独占),同时将其他加载了这个共享变量的cacheline的状态设置为invalid。通俗一点就是CPU0独占的这个变量的缓存行，其他的CPU缓存的共享变量都失效了； 5、CPU0接下来修改共享变量的值，它会将cacheline的状态修改为modified,其实也是独占共享变量的cacheline，只不过是此时缓存行的数据和主内存Memory的数据不一致的，而exclusive虽然也是独占状态，但是共享变量的值是一样的，modified的值需要write back到Memory中去的，而exclusive是不需要的； 6、在cacheline没有替换出CPU0的cache之前，当有其他CPU来读取共享变量，此时肯定是cache miss ,因为CPU0的modify操作已经将它的缓存失效了。如果CPU0的状态是modified状态，它必须响应其他CPU的读操作，会告知其他CPU主内存的数据是dirty data。所以其他的CPU的状态可能会变成shared。如果CPU0还没有write back操作，其他的CPU状态还是invalid状态。 Store Buffer正如上面所描述的，在CPU0进行共享变量的修改，会同步修改其他CPU的cacheline状态为invalid，这个操作是和共享变量的写操作同步进行的，因此共享变量的写操作的性能是非常差的。在修改其他的CPU cacheline状态时，CPU0其实是处于阻塞状态的。所以为了优化这个问题，提出了Store Buffer的解决方案。 这样的话，写操作不必等到cacheline被加载，而是直接写到store buffer中，然后去执行后续的操作。由于是store buffer相当于是异步处理，在这里可能会出现因为并发执行导致的执行执行交叉问题，具体解决方法是依赖于内存屏障。 具体可以参考这篇文章：Linux内核同步机制之（三）：memory barrier Invalidate Queue处理失效的缓存也不是简单的，需要读取主存。并且存储缓存也不是无限大的，那么当存储缓存满的时候，处理器还是要等待失效响应的。为了解决上面两个问题，引进了失效队列（invalidate queue）。 处理失效的工作如下： 收到失效消息时，放到失效队列中去。 为了不让处理器久等失效响应，收到失效消息需要马上回复失效响应。 为了不频繁阻塞处理器，不会马上读主存以及设置缓存为invlid，合适的时候再一块处理失效队列。 happens- before原则 虽然指令重排提高了并发的性能，但是Java虚拟机会对指令重排做出一些规则限制，并不能让所有的指令都随意的改变执行位置，主要有以下几点： 1、单线程每个操作，happen-before于该线程中任意后续操作；2、volatile写happen-before与后续对这个变量的读；3、synchronized解锁happen-before后续对这个锁的加锁；4、final变量的写happen-before于final域对象的读，happen-before后续对final变量的读；5、传递性规则，A先于B，B先于C，那么A一定先于C发生； https://www.processon.com/view/5c8b0978e4b0c996d363dcbc?fromnew=1 深入理解Java内存模型-3y","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-synchronzied底层原理","date":"2021-07-28T09:57:19.000Z","path":"wiki/Java-synchronzied底层原理/","text":"synchronzied底层原理synchronzied四个层级实现 Java代码 通过添加synchronzied给对象或者方法或者代码块 字节码层级通过一组 MONITORENTER/MONITOREXIT指令 JVM层级：锁升级过程 汇编执行通过 lock comxchg指令保证原子操作 JDK早期，synchronized 叫做重量级锁， 因为申请锁资源必须通过kernel, 系统调用 1234567891011121314151617181920;hello.asm;write(int fd, const void *buffer, size_t nbytes)section data msg db &quot;Hello&quot;, 0xA len equ $ - msgsection .textglobal _start_start: mov edx, len mov ecx, msg mov ebx, 1 ;文件描述符1 std_out mov eax, 4 ;write函数系统调用号 4 int 0x80 mov ebx, 0 mov eax, 1 ;exit函数系统调用号 int 0x80 优化后的synchronized如下👇： Java层级12345678public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable()); synchronized (object)&#123; System.out.println(ClassLayout.parseInstance(object).toPrintable()); &#125; &#125; 字节码层级12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// access flags 0x9 public static main([Ljava/lang/String;)V // parameter args TRYCATCHBLOCK L0 L1 L2 null TRYCATCHBLOCK L2 L3 L2 null L4 LINENUMBER 13 L4 NEW java/lang/Object DUP INVOKESPECIAL java/lang/Object.&lt;init&gt; ()V ASTORE 1 L5 LINENUMBER 14 L5 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 1 INVOKESTATIC org/openjdk/jol/info/ClassLayout.parseInstance (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout; INVOKEVIRTUAL org/openjdk/jol/info/ClassLayout.toPrintable ()Ljava/lang/String; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L6 LINENUMBER 16 L6 ALOAD 1 DUP ASTORE 2 MONITORENTER L0 LINENUMBER 17 L0 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 1 INVOKESTATIC org/openjdk/jol/info/ClassLayout.parseInstance (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout; INVOKEVIRTUAL org/openjdk/jol/info/ClassLayout.toPrintable ()Ljava/lang/String; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L7 LINENUMBER 18 L7 ALOAD 2 MONITOREXIT L1 GOTO L8 L2 FRAME FULL [[Ljava/lang/String; java/lang/Object java/lang/Object] [java/lang/Throwable] ASTORE 3 ALOAD 2 MONITOREXIT L3 ALOAD 3 ATHROW L8 LINENUMBER 19 L8 FRAME CHOP 1 RETURN L9 LOCALVARIABLE args [Ljava/lang/String; L4 L9 0 LOCALVARIABLE object Ljava/lang/Object; L5 L9 1 MAXSTACK = 2 MAXLOCALS = 4&#125; 主要通过MONITORENTER 和 MONITOREXIT 两个字节码指令控制加锁过程 JVM层级通过锁升级过程实现加锁；无锁 -&gt; 偏向锁 -&gt; 自旋锁（轻量级锁 自适应锁）-&gt; 重量级锁锁升级过程可以查看 锁升级过程 复制理解 汇编指令级别linux操作系统安装hsdis插件，查看java代码的汇编指令： 1234567891011121314public class T &#123; static volatile int i = 0; public static void n() &#123; i++; &#125; public static synchronized void m() &#123;&#125; publics static void main(String[] args) &#123; for(int j=0; j&lt;1000_000; j++) &#123; m(); n(); &#125; &#125;&#125; 执行以下命令： 1java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly T C1 Compile Level 1 (一级优化) C2 Compile Level 2 (二级优化) 找到m() n()方法的汇编码，会看到 lock comxchg …..指令","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-锁消除和锁膨胀","date":"2021-07-28T09:57:00.000Z","path":"wiki/Java-锁消除和锁膨胀/","text":"锁消除和锁粗化锁消除 （lock eliminate）1234public void add(String str1,String str2)&#123; StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2);&#125; 我们都知道 StringBuffer 是线程安全的，因为它的关键方法都是被 synchronized 修饰过的，但我们看上面这段代码，我们会发现，sb 这个引用只会在 add 方法中使用，不可能被其它线程引用（因为是局部变量，栈私有），因此 sb 是不可能共享的资源，JVM 会自动消除 StringBuffer 对象内部的锁。 锁粗化 （lock coarsening）123456789public String test(String str)&#123; int i = 0; StringBuffer sb = new StringBuffer(): while(i &lt; 100)&#123; sb.append(str); i++; &#125; return sb.toString():&#125; JVM 会检测到这样一连串的操作都对同一个对象加锁（while 循环内 100 次执行 append，没有锁粗化的就要进行 100 次加锁/解锁），此时 JVM 就会将加锁的范围粗化到这一连串的操作的外部（比如 while 虚幻体外），使得这一连串操作只需要加一次锁即可。 锁降级https://www.zhihu.com/question/63859501 其实，只被VMThread访问，降级也就没啥意义了。所以可以简单认为锁降级不存在！","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-锁升级过程","date":"2021-07-28T09:56:39.000Z","path":"wiki/Java-锁升级过程/","text":"锁升级使用JOL查看加锁之后的对象信息12345678public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable()); synchronized (object)&#123; System.out.println(ClassLayout.parseInstance(object).toPrintable()); &#125; &#125; 查看打印结果： 1234567891011121314151617java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totaljava.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) f0 b8 d0 0f (11110000 10111000 11010000 00001111) (265337072) 4 4 (object header) 00 70 00 00 (00000000 01110000 00000000 00000000) (28672) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 锁升级过程 锁升级过程： new - 偏向锁 - 轻量级锁 （无锁, 自旋锁，自适应自旋）- 重量级锁 自旋锁什么时候升级为重量级锁？ 超过自选次数仍没有获得锁 为什么有自旋锁还需要重量级锁？ 自旋是消耗CPU资源的，如果锁的时间长，或者自旋线程多，CPU会被大量消耗重量级锁有等待队列，所有拿不到锁的进入等待队列，不需要消耗CPU资源 偏向锁是否一定比自旋锁效率高？ 不一定，在明确知道会有多线程竞争的情况下，偏向锁肯定会涉及锁撤销，这时候直接使用自旋锁JVM启动过程，会有很多线程竞争（明确），所以默认情况启动时不打开偏向锁，过一段儿时间再打开 synchronized优化的过程和markword息息相关用markword中最低的三位代表锁状态 其中1位是偏向锁位 两位是普通锁位 Object o = new Object()锁 = 0 01 无锁态注意：如果偏向锁打开，默认是匿名偏向状态 o.hashCode()001 + hashcode 1200000001 10101101 00110100 0011011001011001 00000000 00000000 00000000 little endian big endian 00000000 00000000 00000000 01011001 00110110 00110100 10101101 00000000 默认synchronized(o)00 -&gt; 轻量级锁默认情况 偏向锁有个时延，默认是4秒why? 因为JVM虚拟机自己有一些默认启动的线程，里面有好多sync代码，这些sync代码启动时就知道肯定会有竞争，如果使用偏向锁，就会造成偏向锁不断的进行锁撤销和锁升级的操作，效率较低。 1-XX:BiasedLockingStartupDelay=0 如果设定上述参数new Object () - &gt; 101 偏向锁 -&gt;线程ID为0 -&gt; Anonymous BiasedLock打开偏向锁，new出来的对象，默认就是一个可偏向匿名对象101 如果有线程上锁上偏向锁，指的就是，把markword的线程ID改为自己线程ID的过程偏向锁不可重偏向 批量偏向 批量撤销 如果有线程竞争撤销偏向锁，升级轻量级锁线程在自己的线程栈生成LockRecord ，用CAS操作将markword设置为指向自己这个线程的LR的指针，设置成功者得到锁 如果竞争加剧竞争加剧：有线程超过10次自旋， -XX:PreBlockSpin， 或者自旋线程数超过CPU核数的一半， 1.6之后，加入自适应自旋 Adapative Self Spinning ， JVM自己控制升级重量级锁：-&gt; 向操作系统申请资源，linux mutex , CPU从3级-0级系统调用，线程挂起，进入等待队列，等待操作系统的调度，然后再映射回用户空间 (以上实验环境是JDK11，打开就是偏向锁，而JDK8默认对象头是无锁)偏向锁默认是打开的，但是有一个时延，如果要观察到偏向锁，应该设定参数 如果计算过对象的hashCode，则对象无法进入偏向状态！ 轻量级锁重量级锁的hashCode存在与什么地方？答案：线程栈中，轻量级锁的LR中，或是代表重量级锁的ObjectMonitor的成员中 关于epoch: (不重要) 批量重偏向与批量撤销渊源：从偏向锁的加锁解锁过程中可看出，当只有一个线程反复进入同步块时，偏向锁带来的性能开销基本可以忽略，但是当有其他线程尝试获得锁时，就需要等到safe point时，再将偏向锁撤销为无锁状态或升级为轻量级，会消耗一定的性能，所以在多线程竞争频繁的情况下，偏向锁不仅不能提高性能，还会导致性能下降。于是，就有了批量重偏向与批量撤销的机制。 原理以class为单位，为每个class维护解决场景批量重偏向（bulk rebias）机制是为了解决：一个线程创建了大量对象并执行了初始的同步操作，后来另一个线程也来将这些对象作为锁对象进行操作，这样会导致大量的偏向锁撤销操作。批量撤销（bulk revoke）机制是为了解决：在明显多线程竞争剧烈的场景下使用偏向锁是不合适的。 一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向。每个class对象会有一个对应的epoch字段，每个处于偏向锁状态对象的Mark Word中也有该字段，其初始值为创建该对象时class中的epoch的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其epoch字段改为新值。下次获得锁时，发现当前对象的epoch值和class的epoch不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其Mark Word的Thread Id 改成当前线程Id。当达到重偏向阈值后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑。 没错，我就是厕所所长 加锁，指的是锁定对象 锁升级的过程 JDK较早的版本 OS的资源 互斥量 用户态 -&gt; 内核态的转换 重量级 效率比较低 现代版本进行了优化 无锁 - 偏向锁 -轻量级锁（自旋锁）-重量级锁 1、偏向锁 - markword 上记录当前线程指针，下次同一个线程加锁的时候，不需要争用，只需要判断线程指针是否同一个，所以，偏向锁，偏向加锁的第一个线程 。hashCode备份在线程栈上 线程销毁，锁降级为无锁 2、有争用 - 锁升级为轻量级锁 - 每个线程有自己的LockRecord在自己的线程栈上，用CAS去争用markword的LR的指针，指针指向哪个线程的LR，哪个线程就拥有锁 3、自旋超过10次，升级为重量级锁 - 如果太多线程自旋 CPU消耗过大，不如升级为重量级锁，进入等待队列（不消耗CPU）-XX:PreBlockSpin 自旋锁在 JDK1.4.2 中引入，使用 -XX:+UseSpinning 来开启。JDK 6 中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。 4、自适应自旋锁意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 5、偏向锁由于有锁撤销的过程revoke，会消耗系统资源，所以，在锁争用特别激烈的时候，用偏向锁未必效率高。还不如直接使用轻量级锁。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"JVM-strace","date":"2021-07-28T09:52:23.000Z","path":"wiki/JVM-strace/","text":"strace 命令查看操作系统日志strace -ff -o out java ***.class -ff : 跟踪进程下所有线程用到的系统命令-o : 将跟踪的操作系统日志输出 下面查看JDK1.8下，BIO模式都有哪些系统命令的执行 123456789101112131415161718192021222324252627282930313233package com.ibli.javaBase.io.bio;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.net.ServerSocket;import java.net.Socket;/** * @Author gaolei * @Date 2021/4/3 2:55 下午 * @Version 1.0 */public class SockerIo &#123; public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(9090); // 阻塞 Socket client = serverSocket.accept(); InputStream inputStream = client.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 读阻塞 System.err.println(bufferedReader.readLine()); while (true)&#123; &#125; &#125;&#125; 服务端 1、javac SockerIo.java 得到SockerIo.class然后，使用strace启动java程序👇：2、strace -ff -0 out java SockerIo得到如下日志： 客户端使用nc连接9090端口，然后请求数据 nc 127.0.0.1 9090 发送如下数据 strace查看日志 查看主线程日志：如上图，👆文件最大的是主线程日志： 根据上面👆strace命令跟踪的日志可以看到，JDK1.8下的BIO的多路复用器是使用的「poll」","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"Java-NIO核心组件--channel","date":"2021-07-28T09:51:42.000Z","path":"wiki/Java-NIO核心组件-channel/","text":"NIO核心组件 - ChannelSocketChannel 和 ServerSocketChannel学习此部分可以对比Socket和ServerSocket 服务端代码 123456789101112131415161718192021222324252627282930313233343536373839public class NioSocketServer01 &#123; public static void main(String[] args) &#123; try &#123; // ServerSocketChannel 支持阻塞/非阻塞 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 设置成非阻塞。默认阻塞true serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(8080)); // 循环监听客户端连接 while (true) &#123; // 如果有客户端连接，则返回一个socketChannel实例，否则socketChannel=null SocketChannel socketChannel = serverSocketChannel.accept(); // 代码执行到此处，说明有客户端链接 if (socketChannel != null) &#123; // 读取客户端发送的数据，并输出 ByteBuffer buffer = ByteBuffer.allocate(1024); socketChannel.read(buffer); System.err.println(new String(buffer.array())); // 将数据在写会客户端 buffer.flip(); socketChannel.write(buffer); //验证客户端 socketChannel设置成false时，从服务端read数据的操作变成非阻塞的 //ByteBuffer buffer = ByteBuffer.allocate(1024); //buffer.put(&quot;this is server!&quot;); //buffer.flip(); //socketChannel.write(buffer); &#125; else &#123; Thread.sleep(1000L); System.err.println(&quot;no client&quot;); &#125; &#125; &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 客户端代码 12345678910111213141516171819202122232425262728293031public class NioSocketClient1 &#123; public static void main(String[] args) &#123; try &#123; SocketChannel socketChannel = SocketChannel.open(); // 默认阻塞IO true socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(&quot;localhost&quot;, 8080)); // finishConnect的主要作用就是确认通道连接已建立，方便后续IO操作（读写）不会因连接没建立而导致NotYetConnectedException异常。 if (socketChannel.isConnectionPending()) &#123; // finishConnect一直阻塞到connect建立完成 socketChannel.finishConnect(); &#125; ByteBuffer byteBuffer = ByteBuffer.allocate(1024); byteBuffer.put(&quot;hello world&quot;.getBytes()); byteBuffer.flip(); socketChannel.write(byteBuffer); byteBuffer.clear(); int r = socketChannel.read(byteBuffer); // 非阻塞方法 byteBuffer的数据还是上面put的 if (r &gt; 0) &#123; System.out.println(&quot;get msg:&#123;&#125;&quot; + new String(byteBuffer.array())); &#125; else &#123; System.out.println(&quot;server no back&quot;); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO核心组件--selector","date":"2021-07-28T09:51:30.000Z","path":"wiki/Java-NIO核心组件-selector/","text":"多路复用器select1、select选择器会告诉客户端哪些连接有数据要读取，但是读取的操作还是用户自己触发的，这种叫做「同步」 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.ibli.javaBase.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.*;import java.util.Iterator;import java.util.Set;/** * @Author gaolei * @Date 2021/4/3 4:09 下午 * @Version 1.0 */public class SelectMultiple &#123; private ServerSocketChannel server = null; private Selector selector = null; int port = 9090; public void initServer() throws IOException &#123; server = ServerSocketChannel.open(); server.configureBlocking(false); server.bind(new InetSocketAddress(port)); server.register(selector, SelectionKey.OP_ACCEPT); &#125; public void start() throws IOException &#123; initServer(); System.err.println(&quot;server started ....&quot;); while (true) &#123; // selector.select() 调用系统内核的select while (selector.select() &gt; 0) &#123; // 从多路复用器中选择有效的key Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectionKeys.iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); if (key.isAcceptable()) &#123; acceptHandle(key); &#125; else if (key.isReadable()) &#123; readHandle(key); &#125; &#125; &#125; &#125; &#125; public void acceptHandle(SelectionKey key) throws IOException &#123; ServerSocketChannel ssc = (ServerSocketChannel) key.channel(); SocketChannel client = ssc.accept(); client.configureBlocking(false); ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1024); client.register(selector, SelectionKey.OP_READ, byteBuffer); System.err.println(&quot;client arrived &quot; + client.getRemoteAddress()); &#125; public void readHandle(SelectionKey key) throws IOException &#123; SocketChannel client = (SocketChannel) key.channel(); ByteBuffer buffer = (ByteBuffer) key.attachment(); buffer.clear(); int read = 0; while (true) &#123; read = client.read(buffer); if (read &gt; 0) &#123; // 服务端读到的数据，再写一遍给到客户端 buffer.flip(); while (buffer.hasRemaining()) &#123; client.write(buffer); &#125; buffer.clear(); &#125; else if (read == 0) &#123; break; &#125; else &#123; // client 发生错误 或者断开 read == -1 // 导致空转 最终CPU达到100% client.close(); break; &#125; &#125; &#125;&#125; 上面的写法是一个selector既担任boss又担任worker","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO核心组件--buffer","date":"2021-07-28T09:51:10.000Z","path":"wiki/Java-NIO核心组件-buffer/","text":"Buffer 读写NIO之BufferBuffer作为NIO三大核心组件之一，本质上是一块可以写入数据，以及从中读取数据的内存，实际上也是一个byte[]数据,只是在NIO中被封装成了NIO Buffer对象并提供了一组方法来访问这个内存块。 下面是一个简单的Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// 读取一个text.txt文件，生成一个新的text1.txt文件public class FirstNioDemo &#123; public static void main(String[] args) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(&quot;/Users/gaolei/Desktop/text.txt&quot;); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/text1.txt&quot;); FileChannel inChannel = fileInputStream.getChannel(); FileChannel outChannel = fileOutputStream.getChannel(); // 声明缓冲区大小为1024字节 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // 从通道中读取数据 inChannel.read(byteBuffer); // 读模式切换为写模式 byteBuffer.flip(); //把缓冲区的数据写到通道 outChannel.write(byteBuffer); // 数据写完之后清空全部缓冲区 byteBuffer.clear(); //关闭文件流 fileInputStream.close(); fileOutputStream.close(); &#125;&#125;``` &gt; 执行结果：生成/Users/gaolei/Desktop/text1.txt文件 **Buffer进行数据读写操作的一般步骤** 1、写入数据到Buffer 2、调用flip()方法 3、从Buffer中读取数据 4、调用clear()方法或者compact()方法 &gt; clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 ### Buffer三个核心的属性 - capacity 容量 与buffer处在什么模式无关- position 游标位置 指向下一个存放/读取数据的位置 范围（0 ～ capacity–1）- limit ### 读写操作中Buffer三大属性的变化初始状态 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71f90dfd671f80eb9f6142f135b7c2dfc92.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 第一次读取数据 position处于起始位置，limit和capacity都处于结尾 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-41b47d9e54d58c7b39caf9e514fc9b5261f.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 第二次读取数据 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-07f3d1aa1f886b592b386cd4d846810911d.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 当写数据的时候，需要调用flip方法： 当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position） &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b9323701bbb34a6c12f61d5ac2652ab7eeb.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; Clear方法 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71f90dfd671f80eb9f6142f135b7c2dfc92.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; ### JAVA NIO下的Buffer分类- ByteBuffer- MappedByteBuffer- CharBuffer- DoubleBuffer- FloatBuffer- IntBuffer- LongBuffer- ShortBuffer&gt; Java基本类型除了布尔类型，都有其对应的Buffer ### ByteBuffer使用&gt; 下面以ByteBuffer为例子看一下Buffer如何使用```java// 创建一个byteBuffer，设置容量为1024字节ByteBuffer byteBuffer = ByteBuffer.allocate(1024); 1、如下代码，其实调用了new HeapByteBuffer(capacity, capacity)来创建一个buffer 12345public static ByteBuffer allocate(int capacity) &#123; if (capacity &lt; 0) throw new IllegalArgumentException(); return new HeapByteBuffer(capacity, capacity); &#125; 2、创建了buffer之后要往里面写数据，除了上面从channel中读取数据之外，还可以调用put方法,如下 12ByteBuffer byteBuffer = ByteBuffer.allocate(1024);byteBuffer.put(&quot;hello world&quot;.getBytes()); 3、如果写将buffer中的数据写出去，必须先调用flap方法 flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。 4、将数据写到通道中 inChannel.write(buf); 5、数据写出到通道之后，要将缓存清空，一般调用clear方法clear方法 12345678public final Buffer clear() &#123; //position将被设回0 position = 0; //limit被设置成 capacity的值 limit = capacity; mark = -1; return this; &#125; Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。compact方法如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。 1234567891011public ByteBuffer compact() &#123; //compact()方法将所有未读的数据拷贝到Buffer起始处。 System.arraycopy(hb, ix(position()), hb, ix(0), remaining()); //position设到最后一个未读元素正后面 position(remaining()); //limit属性设置成capacity limit(capacity()); discardMark(); return this; &#125;现在Buffer准备好写数据了，但是不会覆盖未读的数据 零拷贝原理– 零拷贝，第一次接触零拷贝是在kafka的数据存储部分–IO流程：内存映射缓冲区比普通IO操作文件快很多，甚至比channel还要快很多。因为避免了很多系统调用（System.read System.write）。减少了内核缓冲区的数据拷贝到用户缓冲区。 举个栗子： 123456789101112public static void main(String[] args) throws IOException &#123; FileChannel in = FileChannel.open(Paths.get(&quot;/Users/gaolei/Desktop/text.txt&quot;), StandardOpenOption.READ); FileChannel out = FileChannel.open(Paths.get(&quot;/Users/gaolei/Desktop/text1.txt&quot;), StandardOpenOption.READ, StandardOpenOption.CREATE, StandardOpenOption.WRITE); MappedByteBuffer inBuffer = in.map(FileChannel.MapMode.READ_ONLY, 0, in.size()); MappedByteBuffer outBuffer = out.map(FileChannel.MapMode.READ_WRITE, 0, in.size()); byte[] bytes = new byte[inBuffer.limit()]; inBuffer.get(bytes); outBuffer.put(bytes); in.close(); out.close(); &#125; 普通的网络IO拷贝流程1、首先系统从磁盘上拷贝文件到内核空间缓冲区2、然后在内核空间拷贝数据到用户空间3、第三次，用户缓冲区再将数据拷贝到内核部分的socket缓冲4、内核在将存储在socket缓冲区的数据拷贝并发送到网卡缓冲区以上一个常规的网络IO经历了4次数据拷贝； 设置缓冲区的意义在于提升性能，当用户空间仅仅需要一小部分数据的时候，操作系统会在磁盘上读取一块数据方法内核缓冲区，这个叫做局部性原理。 零拷贝减去了内核空间数据到用户空间数据的拷贝，从而提升IO性能。假设读取的文件很大，操作系统需要读取磁盘大量数据到内核空间，这时候内核缓冲区的作用是很难体现的。因为如果用户空间需要少量数据的时候是可以直接在内核空间获取的（局部性原理）。正式因为有了零拷贝，操作系统在磁盘读取数据之后，可以直接发送到网卡缓冲区，从而大大提升IO性能。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO","date":"2021-07-28T09:50:38.000Z","path":"wiki/Java-NIO/","text":"Java NIO Java NIO 对于Java BIO的优化 Java 非阻塞IO 及时不使用线程池，也可以处理多个客户端请求 12345678910111213141516171819202122232425262728293031323334353637383940public static void main(String[] args) throws IOException, InterruptedException &#123; LinkedList&lt;SocketChannel&gt; clients = new LinkedList&lt;&gt;(); ServerSocketChannel ss = ServerSocketChannel.open(); ss.bind(new InetSocketAddress(9090)); ss.configureBlocking(false); while (true) &#123; Thread.sleep(1000L); // 非阻塞 SocketChannel client = ss.accept(); if (client == null) &#123; System.err.println(&quot;client is null&quot;); &#125; else &#123; client.configureBlocking(false); int port = client.socket().getPort(); System.err.println(&quot;client port &quot; + port); clients.add(client); &#125; ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4096); // 串型话 // 真实场景下 每一个client一个独自的buffer for (SocketChannel c : clients) &#123; // -1 出现空轮训 int num = c.read(byteBuffer); if (num &gt; 0) &#123; byteBuffer.flip(); byte[] aaa = new byte[byteBuffer.limit()]; byteBuffer.get(aaa); String b = new String(aaa); System.err.println(c.socket().getPort() + &quot; : &quot; + b); // 清空 循环下一次client在使用 byteBuffer.clear(); &#125; &#125; &#125;&#125; 以上可以实现，一个线程可以处理多个客户端链接，服务端非阻塞接收，接收之后，读取数据也是非阻塞的； NIO的非阻塞是操作系统内部实现的，底层调用了linux内核的accept函数 d Java的NIO有什么弊端 服务端还是会进行空转 不管有没有客户端连接建立，服务端都要不断执行accept方法 不管客户端连接有没有传输数据，都会执行一遍read操作 资源浪费问题 还是会存在C10k的问题","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-传统的BIO","date":"2021-07-28T09:50:21.000Z","path":"wiki/Java-传统的BIO/","text":"传统的BIOSocket 和 ServerSocket1234567891011121314151617public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(9090); // 阻塞 Socket client = serverSocket.accept(); InputStream inputStream = client.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 读阻塞 System.err.println(bufferedReader.readLine()); while (true)&#123; &#125; &#125; new ServerSocket(9090); 这个java程序创建new ServerSocket(9090);会调用操作系统内核，也就是系统调用，比如linux操作系统，应用进程也就是我们的java进程，会调用linux的内核方法，创建一个socket，在linux系统中就是一个文件描述符fd，最终对得到： 123socket() = XXfdbind(XXfd,9090)listen(XXfd) socket 的read方法 ，读取客户端发送的数据，如果没有，则一直阻塞 serverSocket的accept方法，等待客户端的链接，如果没有链接，则一直阻塞等待 serverSocket 一次只能处理一个客户端请求 BIO程序有哪些弊端？ 服务端一次处理一个请求，并发非常低 没有客户端请求，服务端一直阻塞，占用资源 如果在bio的基础上，利用多线程处理客户端请求？ d C10K问题 来一个链接，服务端创建一个线程 ，去处理请求，服务端继续监听客户端，是不是可以增加并发？有什么问题？ 线程消耗内存资源 如果一下子过来10万个请求呢？服务器要创建10万个线程，内存就崩了。 如果搞一个线程池呢？ 并发度最大为最大线程数？ 并发度已经定死了？","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"JVM-jstat","date":"2021-07-28T09:46:25.000Z","path":"wiki/JVM-jstat/","text":"jstatjstat是一个简单的实用工具，在JDK中存在，用于提供与JVM性能相关的统计信息，例如垃圾收集，编译活动。 jstat的主要优势在于，它可以在运行JVM且无需任何先决条件的情况下动态捕获这些指标。 这是什么意思？ 例如，如果要捕获与垃圾回收相关的统计信息，则需要在启动JVM之前传递以下参数： -Xlog:gc*:file={file-path} 此参数将启用GC日志并将其打印在指定的文件路径中。 假设您尚未传递此参数，那么将不会生成与GC相关的统计信息。 这是jstat可以派上用场的地方。 您可以动态地连接到JVM并捕获GC，编译相关的统计信息如下所示。 jstat操作执行命令： 1jstat -gc -t 11656 10000 30 -gc ：将显示与垃圾收集相关的统计信息 自JVM启动以来的-t时间戳将被打印 11656：目标JVM进程ID 10000：每10,000毫秒（即10秒）将打印一次统计信息。 30 ：将打印30次迭代的统计信息。 因此，以上选项将导致JVM打印指标300秒（即10秒x 30次迭代）。 （请注意，除了-gc之外，您还可以传递其他各种选项来生成不同的数据集。有关不同选项的更多详细信息，请参见此处 。）打印结果： 1234Timestamp S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 34486.1 1536.0 1536.0 0.0 878.8 226816.0 132809.2 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 34496.3 1536.0 1536.0 0.0 878.8 226816.0 138030.9 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 34506.3 1536.0 1536.0 0.0 878.8 226816.0 195648.1 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 字段解读S0C –幸存者0区域的容量，以KB为单位 S1C –幸存者1区域的容量，以KB为单位 S0U –幸存者0区域使用的空间以KB为单位 S1U –幸存者1区域以KB为单位使用空间 EC –伊甸园地区容量（KB） 欧盟–伊甸园地区的已利用空间（以KB为单位） OC –旧区域容量（KB） OU –旧区域的已利用空间，以KB为单位 MC –元空间区域容量，以KB为单位 MU –元空间区域使用的空间以KB为单位 CCSC –压缩类空间区域的容量，以KB为单位 CCSU –压缩类空间区域以KB为单位使用空间 YGC –迄今为止发生的年轻GC事件的数量 YGCT –到目前为止，年轻GC花费的时间 FGC –迄今为止已发生的完全GC事件的数量 FGCT –到目前为止已花费的完整GC时间 GCT –到目前为止所花费的GC时间总量（基本上是YGCT + FGCT） 参考资料jstat分析_jstat –分析","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-jstack","date":"2021-07-28T09:46:12.000Z","path":"wiki/JVM-jstack/","text":"jstackjstack 功能主要分为两个功能： a． 针对活着的进程做本地的或远程的线程dump； b． 针对core文件做线程dump。 jstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack 操作方式 jps -l | grep keyword -&gt; pidjstack pid jstack结果如下； 123456789101112131415161718&quot;lettuce-nioEventLoop-4-1&quot; #639 daemon prio=5 os_prio=0 tid=0x00007ff27025d800 nid=0x258f runnable [0x00007ff262af7000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked &lt;0x000000008988d6a8&gt; (a io.netty.channel.nio.SelectedSelectionKeySet) - locked &lt;0x000000008988d770&gt; (a java.util.Collections$UnmodifiableSet) - locked &lt;0x000000008988d600&gt; (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101) at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68) at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:803) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:748) 参考资料原文链接：https://blog.csdn.net/weixin_30013175/article/details/113901522","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-类加载机制","date":"2021-07-28T09:45:58.000Z","path":"wiki/JVM-类加载机制/","text":"类加载机制1. 类加载机制具体流程 Java 的类加载过程可以分为 5 个阶段：载入、验证、准备、解析和初始化。这 5 个阶段一般是顺序发生的，但在动态绑定的情况下，解析阶段发生在初始化阶段之后。 1.1 Loading（载入） JVM 在该阶段的主要目的是将字节码从不同的数据源（可能是 class 文件、也可能是 jar 包，甚至网络）转化为二进制字节流加载到内存中，并生成一个代表该类的 java.lang.Class 对象。 通过一个类的全限定名来获取定义次类的二进制流(ZIP 包、网络、运算生成、JSP 生成、数据库读取)。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法去这个类的各种数据的访问入口。 数组类的特殊性：数组类本身不通过类加载器创建，它是由 Java 虚拟机直接创建的。但数组类与类加载器仍然有很密切的关系，因为数组类的元素类型最终是要靠类加载器去创建的，数组创建过程如下： 如果数组的组件类型是引用类型，那就递归采用类加载加载。 如果数组的组件类型不是引用类型，Java 虚拟机会把数组标记为引导类加载器关联。 数组类的可见性与他的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为 public。 内存中实例的 java.lang.Class 对象存在方法区中。作为程序访问方法区中这些类型数据的外部接口。加载阶段与连接阶段的部分内容是交叉进行的，但是开始时间保持先后顺序。 1.2 Verification（验证）JVM 会在该阶段对二进制字节流进行校验，只有符合 JVM 字节码规范的才能被 JVM 正确执行。该阶段是保证 JVM 安全的重要屏障，下面是一些主要的检查。 1.2.1 文件格式验证 是否以魔数 0xCAFEBABE 开头 主、次版本号是否在当前虚拟机处理范围之内 常量池的常量是否有不被支持常量的类型（检查常量 tag 标志） 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量 CONSTANT_Utf8_info 型的常量中是否有不符合 UTF8 编码的数据 Class 文件中各个部分集文件本身是否有被删除的附加的其他信息 …… 只有通过这个阶段的验证后，字节流才会进入内存的方法区进行存储，所以后面 3 个验证阶段全部是基于方法区的存储结构进行的，不再直接操作字节流。 1.2.2 元数据验证 这个类是否有父类（除 java.lang.Object 之外） 这个类的父类是否继承了不允许被继承的类（final 修饰的类） 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法 类中的字段、方法是否与父类产生矛盾（覆盖父类 final 字段、出现不符合规范的重载） 这一阶段主要是对类的元数据信息进行语义校验，保证不存在不符合 Java 语言规范的元数据信息。 1.2.3 字节码验证 保证任意时刻操作数栈的数据类型与指令代码序列都鞥配合工作（不会出现按照 long 类型读一个 int 型数据） 保证跳转指令不会跳转到方法体以外的字节码指令上 保证方法体中的类型转换是有效的（子类对象赋值给父类数据类型是安全的，反过来不合法的） …… 这是整个验证过程中最复杂的一个阶段，主要目的是通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。这个阶段对类的方法体进行校验分析，保证校验类的方法在运行时不会做出危害虚拟机安全的事件。 1.2.4 符号引用验证 符号引用中通过字符创描述的全限定名是否能找到对应的类 在指定类中是否存在符方法的字段描述符以及简单名称所描述的方法和字段 符号引用中的类、字段、方法的访问性（private、protected、public、default）是否可被当前类访问 …… 最后一个阶段的校验发生在迅疾将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生。符号引用验证可以看做是对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验，还有以上提及的内容。符号引用的目的是确保解析动作能正常执行，如果无法通过符号引用验证将抛出一个 java.lang.IncompatibleClass.ChangeError 异常的子类。如 java.lang.IllegalAccessError、java.lang.NoSuchFieldError、java.lang.NoSuchMethodError 等。 1.3 Preparation（准备）1.3.1 设置默认值JVM 会在该阶段对类变量（也称为静态变量，static 关键字修饰的）分配内存并初始化（对应数据类型的默认初始值，如 0、0L、null、false 等）。 也就是说，假如有这样一段代码： 123public String chenmo = &quot;沉默&quot;;public static String wanger = &quot;王二&quot;;public static final String cmower = &quot;沉默王二&quot;; chenmo 不会被分配内存，而 wanger 会；但 wanger 的初始值不是“王二”而是 null。 需要注意的是，static final 修饰的变量被称作为常量，和类变量不同。常量一旦赋值就不会改变了，所以 cmower 在准备阶段的值为“沉默王二”而不是 null。 1.3.2 各种类型默认值 1.4 Resolution（解析）该阶段将常量池中的符号引用转化为直接引用。 **符号引用** : 以一组符号（任何形式的字面量，只要在使用时能够无歧义的定位到目标即可）来描述所引用的目标。 **直接引用** : 直接引用可以使直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用和迅疾的内存布局实现有关 在编译时，Java 类并不知道所引用的类的实际地址，因此只能使用符号引用来代替。比如 com.Wanger 类引用了 com.Chenmo 类，编译时 Wanger 类并不知道 Chenmo 类的实际内存地址，因此只能使用符号 com.Chenmo。 直接引用通过对符号引用进行解析，找到引用的实际内存地址。 1.5 Initialization（初始化）该阶段是类加载过程的最后一步。在准备阶段，类变量已经被赋过默认初始值，而在初始化阶段，类变量将被赋值为代码期望赋的值。换句话说，初始化阶段是执行类构造器方法的过程。 oh，no，上面这段话说得很抽象，不好理解，对不对，我来举个例子。 String cmower = new String(&quot;沉默王二&quot;);上面这段代码使用了 new 关键字来实例化一个字符串对象，那么这时候，就会调用 String 类的构造方法对 cmower 进行实例化。 2. 双亲委派机制聊完类加载过程，就不得不聊聊类加载器。 2.1 什么是类加载 一般来说，Java 程序员并不需要直接同类加载器进行交互。JVM 默认的行为就已经足够满足大多数情况的需求了。不过，如果遇到了需要和类加载器进行交互的情况，而对类加载器的机制又不是很了解的话，就不得不花大量的时间去调试ClassNotFoundException 和 NoClassDefFoundError 等异常。 对于任意一个类，都需要由它的类加载器和这个类本身一同确定其在 JVM 中的唯一性。也就是说，如果两个类的加载器不同，即使两个类来源于同一个字节码文件，那这两个类就必定不相等（比如两个类的 Class 对象不 equals）。 站在程序员的角度来看，Java 类加载器可以分为三种。 1）启动类加载器（Bootstrap Class-Loader），加载 jre/lib 包下面的 jar 文件，比如说常见的 rt.jar。 2）扩展类加载器（Extension or Ext Class-Loader），加载 jre/lib/ext 包下面的 jar 文件。 3）应用类加载器（Application or App Clas-Loader），根据程序的类路径（classpath）来加载 Java 类。 来来来，通过一段简单的代码了解下。 1234567891011public class Test &#123; public static void main(String[] args) &#123; ClassLoader loader = Test.class.getClassLoader(); while (loader != null) &#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; &#125;&#125; 每个 Java 类都维护着一个指向定义它的类加载器的引用，通过 类名.class.getClassLoader() 可以获取到此引用；然后通过 loader.getParent() 可以获取类加载器的上层类加载器。 这段代码的输出结果如下： sun.misc.Launcher$AppClassLoader@73d16e93sun.misc.Launcher$ExtClassLoader@15db9742第一行输出为 Test 的类加载器，即应用类加载器，它是 sun.misc.Launcher$AppClassLoader 类的实例；第二行输出为扩展类加载器，是 sun.misc.Launcher$ExtClassLoader 类的实例。那启动类加载器呢？ 按理说，扩展类加载器的上层类加载器是启动类加载器，但在我这个版本的 JDK 中， 扩展类加载器的 getParent() 返回 null。所以没有输出。 2.2 双亲委派的意义使用双亲委派模型，Java类随着它的加载器一起具备了一种带有优先级的层次关系，通过这种层次模型，可以避免类的重复加载，也可以避免核心类被不同的类加载器加载到内存中造成冲突和混乱，从而保证了Java核心库的安全。 3. 类加载的应用https://juejin.cn/post/6931972267609948167 类加载 热部署 加密保护 依赖冲突 4. 双亲委派如何破坏线程上下文加载器 https://enfangzhong.github.io/2019/12/17/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E7%A0%B4%E5%9D%8F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/ https://www.cnblogs.com/joemsu/p/9310226.html#_caption_2","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-jmap","date":"2021-07-28T09:45:41.000Z","path":"wiki/JVM-jmap/","text":"jmap命令jmap是一个多功能的命令。它可以生成 java 程序的 dump 文件， 也可以查看堆内对象示例的统计信息、查看 ClassLoader 的信息以及 finalizer 队列。 jmap -heap pid1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Attaching to process ID 7183, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.242-b08using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2051014656 (1956.0MB) NewSize = 42991616 (41.0MB) MaxNewSize = 683671552 (652.0MB) OldSize = 87031808 (83.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 233308160 (222.5MB) used = 161611280 (154.12452697753906MB) free = 71696880 (68.37547302246094MB) 69.26945032698384% usedFrom Space: capacity = 1572864 (1.5MB) used = 899896 (0.8582077026367188MB) free = 672968 (0.6417922973632812MB) 57.213846842447914% usedTo Space: capacity = 1572864 (1.5MB) used = 0 (0.0MB) free = 1572864 (1.5MB) 0.0% usedPS Old Generation capacity = 223346688 (213.0MB) used = 115841432 (110.4749984741211MB) free = 107505256 (102.5250015258789MB) 51.866196466723515% used41772 interned Strings occupying 4324472 bytes. 参考资料jvm 性能调优工具之 jmapJVM调试工具-jmap通过jstack与jmap分析一次线上故障","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"SpringBean循环依赖","date":"2021-07-28T09:34:20.000Z","path":"wiki/SpringBean循环依赖/","text":"Spring Bean 循环依赖为什么会存在循环依赖 如上图👆所示，A对象的一个属性是B,B对象的一个属性是A,而Spring中的bean默认情况下都是单例的\b，所以这两个Bean就产生了循环依赖的问题！ 那么循环依赖的问题出现在什么情况呢 想一下属性赋值的方式有几种呢？ 构造器赋值 这种形式循环依赖问题无法解决 GET/SET方法 调用SET方法进行赋值的时候，可以通过三级缓存的策略来解决循环依赖的问题 所以，三级缓存的策略是针对于使用SET方法对属性赋值的场景下的！ 循环依赖如何解决 在实例化的过程中，将处于半成品的对象全部放到缓存中，方便后续来进行调用；只要有了当前对象的引用地址，那么后续来进行赋值即可； d 能不能将创建好的对象也放到缓存中呢？ 不能，如果放在一起将无法区分对象是成品对象还是半成品对象了所以再次引出多级缓存的概念，可以创建两个缓存对象，一个用来存放已经实例化的半成品对象，另一个存放完成实例化并且完成初始化的成品对象，这个应该比较好理解吧！ 思考一下以上的设计有没有问题呢？ 为什么需要三级缓存？Spring在解决对象Bean循环依赖的问题的解决方案是使用了「三级缓存」；为什么需要三级缓存，也就是三个Map对象； org.springframework.beans.factory.support.DefaultSingletonBeanRegistry 123456// 一级缓存private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap(256);// 二级缓存private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap(16);// 三级缓存private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap(16); 三级缓存中分别保存的是什么内容 一级缓存： 成品对象 二级缓存： 半成品对象 三级缓存； lambda表达式 如果只有二级缓存可不可行 在Spring源码中，只有addSingleton方法和doCreateBean方法中向三级缓存中添加东西的； org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#addSingletonFactory 123456789protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized(this.singletonObjects) &#123; this.singletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125; org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#getSingleton(java.lang.String, boolean) 1234567891011121314151617181920@Nullable protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; this.isSingletonCurrentlyInCreation(beanName)) &#123; synchronized(this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, &quot;Bean name must not be null&quot;); synchronized(this.singletonObjects) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; if (this.singletonsCurrentlyInDestruction) &#123; throw new BeanCreationNotAllowedException(beanName, &quot;Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)&quot;); &#125; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(&quot;Creating shared instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; this.beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = this.suppressedExceptions == null; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = new LinkedHashSet(); &#125; try &#123; singletonObject = singletonFactory.getObject(); newSingleton = true; &#125; catch (IllegalStateException var16) &#123; singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; throw var16; &#125; &#125; catch (BeanCreationException var17) &#123; BeanCreationException ex = var17; if (recordSuppressedExceptions) &#123; Iterator var8 = this.suppressedExceptions.iterator(); while(var8.hasNext()) &#123; Exception suppressedException = (Exception)var8.next(); ex.addRelatedCause(suppressedException); &#125; &#125; throw ex; &#125; finally &#123; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = null; &#125; this.afterSingletonCreation(beanName); &#125; if (newSingleton) &#123; this.addSingleton(beanName, singletonObject); &#125; &#125; return singletonObject; &#125; &#125;","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"Spring加载配置文件原理","date":"2021-07-28T09:33:48.000Z","path":"wiki/Spring加载配置文件原理/","text":"Spring如何加载配置文件到应用程序加载Xml文件配置，获取对象 xml文件 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;user&quot; class=&quot;com.ibli.javaBase.reflection.User&quot;&gt; &lt;property name=&quot;age&quot; value=&quot;12&quot;/&gt; &lt;property name=&quot;name&quot; value=&quot;gaolei&quot;/&gt; &lt;property name=&quot;sex&quot; value=&quot;male&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 测试类 1234567public class IocDemo &#123; public static void main(String[] args) &#123; ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;spring-ioc.xml&quot;); User user = (User) ac.getBean(&quot;user&quot;); System.out.println(user); &#125;&#125; Spring 加载Xml文件流程 首先猜想一下宏观的流程 我们可以大体猜想流程是什么样的，如下👇 接下来debug源码看一下具体流程： ClassPathXmlApplicationContext调用refresh方法 12345678public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); this.setConfigLocations(configLocations); if (refresh) &#123; // Spring 启动入口 this.refresh(); &#125; &#125; Spring 启动入口 this.refresh(); 👆 调用AbstractRefreshableApplicationContext下的refreshBeanFactory org.springframework.context.support.AbstractRefreshableApplicationContext#refreshBeanFactory 1234567891011121314151617181920protected final void refreshBeanFactory() throws BeansException &#123; if (this.hasBeanFactory()) &#123; this.destroyBeans(); this.closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = this.createBeanFactory(); beanFactory.setSerializationId(this.getId()); this.customizeBeanFactory(beanFactory); // 从这里进入下一步 👇 this.loadBeanDefinitions(beanFactory); synchronized(this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException var5) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + this.getDisplayName(), var5); &#125; &#125; 关键方法是this.loadBeanDefinitions(beanFactory); 找到XmlBeanDefinitionReader 这是读取配置的关键所在 关键对象 XmlBeanDefinitionReader 这个在 「梳理Spring启动脉络」中提到了，Spring提供的抽象接口！ 12345678910protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化beanDefinitionReader对象 this.initBeanDefinitionReader(beanDefinitionReader); // 加载配置文件 获得BeanDefinitions this.loadBeanDefinitions(beanDefinitionReader); &#125; 继续调用 loadBeanDefinitions 这个有很多重载方法，一直点下去就行！ 12345678910111213protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = this.getConfigResources(); if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = this.getConfigLocations(); //spring-ioc.xml if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125; &#125; configLocations 就是我们Xml配置文件的路径 接下来一直调用loadBeanDefinitions方法 直到这一步 👇 org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.support.EncodedResource) 1234567891011121314151617181920212223242526272829try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); Throwable var4 = null; try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; var6 = this.doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; catch (Throwable var24) &#123; var4 = var24; throw var24; &#125; finally &#123; if (inputStream != null) &#123; if (var4 != null) &#123; try &#123; inputStream.close(); &#125; catch (Throwable var23) &#123; var4.addSuppressed(var23); &#125; &#125; else &#123; inputStream.close(); &#125; &#125; &#125; &#125; 这里看到 nputStream 很明显，这里是通过IO流读取制定位置的文件的 ! 获取到文件输入流之后，将输入流转换成Document文件去解析 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException 12// 转换成Document的关键方法Document doc = this.doLoadDocument(inputSource, resource); 调用doRegisterBeanDefinitions方法 org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#doRegisterBeanDefinitions调用parseBeanDefinitions方法去解析数据 调用DefaultBeanDefinitionDocumentReader的parseBeanDefinitions方法 来解析Element org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#parseBeanDefinitions 1234567891011121314151617181920protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for(int i = 0; i &lt; nl.getLength(); ++i) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element)node; if (delegate.isDefaultNamespace(ele)) &#123; this.parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125; &#125; 调用parseDefaultElement方法 org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#parseDefaultElement 123456789101112private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, &quot;import&quot;)) &#123; this.importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, &quot;alias&quot;)) &#123; this.processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, &quot;bean&quot;)) &#123; this.processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, &quot;beans&quot;)) &#123; this.doRegisterBeanDefinitions(ele); &#125; &#125; 这里看到if (delegate.nodeNameEquals(ele, &quot;bean&quot;)) 会不会很兴奋呢，接下来就是解析的方法了👇 跳转到 processBeanDefinition(ele, delegate); 12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 是的 就是这个方法了 👉 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, this.getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException var5) &#123; this.getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, var5); &#125; this.getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; parseBeanDefinitionElement 将元素数据解析到beanDefinition org.springframework.beans.factory.xml.BeanDefinitionParserDelegate#parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Nullable public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(&quot;id&quot;); String nameAttr = ele.getAttribute(&quot;name&quot;); List&lt;String&gt; aliases = new ArrayList(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, &quot;,; &quot;); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(id) &amp;&amp; !aliases.isEmpty()) &#123; beanName = (String)aliases.remove(0); if (this.logger.isTraceEnabled()) &#123; this.logger.trace(&quot;No XML &#x27;id&#x27; specified - using &#x27;&quot; + beanName + &quot;&#x27; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; this.checkNameUniqueness(beanName, aliases, ele); &#125; // 将element数据最终转换成一个beanDefinition对象 是不是很惊奇 哈哈哈 👉 AbstractBeanDefinition beanDefinition = this.parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName(beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (this.logger.isTraceEnabled()) &#123; this.logger.trace(&quot;Neither XML &#x27;id&#x27; nor &#x27;name&#x27; specified - using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception var9) &#123; this.error(var9.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; else &#123; return null; &#125; &#125;","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"Spring梳理启动脉络","date":"2021-07-28T09:33:20.000Z","path":"wiki/Spring梳理启动脉络/","text":"Spring是如何启动的Spring最大的核心就是Bean容器；容器： 从对象创建，使用和销毁全部由容器帮我们控制，用户仅仅使用就可以。 两大核心 IOC 控制反转 AOP 面向切面编程 思考：我们是如何使用Spring的呢？ 加入从配置文件中加载bean 我们猜想一下大致流程是怎样的 123&lt;bean id=getPerson class=com.ibli.Person&gt;&lt;property name=id value=1&gt;&lt;property name=age value=20&gt; 配置文件如上👆，这里是伪代码！ 先猜想大致流程： 通过上面猜想创建的对象流程，创建出对象，对象已经好了，就是使用了，那么如何使用呢？ 一般情况下我们会可以这样使用，写一下伪代码吧👇： 12创建一个ApplicationContext对象Object obj = applicationContext.getBean(&quot;bean name); 思考，创建的对象如何存储？ 或者容器到底是什么呢？ 应该可以猜到是Map结构，具体是什么Map,先不管； 1、首先容器是创建好的，容器创建好之后，才可以加载配置文件 也就是我们猜想的Map 2、加载配置文件 配置文件可能会有多种方式，比如XML格式，property格式，yaml格式，注解格式，这个格式各不相同，又是如何加载的呢？Spring提供了一个接口，BeanDefinitionReader,它有一个抽象实现类AbstractBeanDefinitionReader，不同配置文件的Reader来继承这个抽象类，实现它们自己的逻辑； 123public class PropertiesBeanDefinitionReader extends AbstractBeanDefinitionReaderpublic class GroovyBeanDefinitionReader extends AbstractBeanDefinitionReader implements GroovyObjectpublic class XmlBeanDefinitionReader extends AbstractBeanDefinitionReader 3、读取的配置文件会转换成Spring定义的格式，也就是BeanDefinition； BeanDefinition定义了类的所有相关的数据； 此时得到的BeanDefinition的属性值只是「符号类型」,并不是真正的属性值； 我们可能会见过这中加载数据源的方式👇 12345&lt;bean id=dataSource class=com.alibab.durid.pool.DruidDataSource&gt;&lt;property name=url value=$&#123;jdbc.url&#125;&gt;&lt;property name=username value=$&#123;jdbc.username&#125;&gt;&lt;property name=password value=$&#123;jdbc.password&#125;&gt;&lt;/bean&gt; 数据源的具体配置是放在配置文件中的，当通过XmlBeanDefinitionReader读取并解析到的BeanDefinition，仅仅是将Xml中的文件数据存放到BeanDefinition中，属性的值是${jdbc.url}而不是真正的我们数据源的地址； 4、得到所有的BeanDefinition之后，通过BeanFactoryPostProcessor来处理上一步骤中，属性value不是真实数据的问题 比如PlaceHolderConfigurerSupport(占位符处理) 经过工厂后置处理器处理之后，BeanDefinition的属性值就是真实需要的数据了； 5、BeanDefinition数据准备完成之后，由BeanFactory来完成Bean的创建 实例化 对象中分配堆内存等操作 反射调用无参构造函数 创建对象 但是属性是空的 初始化 6、初始化之前需要准备的工作 1、准备BeanPostProcessors2、观察者模式，准备监听器 事件 广播器 7、初始化环节有很多步骤 对象的填充 其实就是调用get/set方法对属性赋值 调用aware方法 如果我们的对象中的属性是BeanFactory 我们不用自己去完成setBeanFactory方法，只需要当前类实现BeanFactoryAware方法即可 123public interface BeanFactoryAware extends Aware &#123; void setBeanFactory(BeanFactory var1) throws BeansException;&#125; 处理before操作 调用init方法 执行after方法 before和after此处是调用的BeanPostProcessor的方法 1234// 前置方法postProcessBeforeInitialization// 后置方法postProcessAfterInitialization 8、执行到此，完成对象的创建，得到一个可以使用的对象","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"","date":"2021-07-28T09:32:41.602Z","path":"wiki/Spring-Overview/","text":"title: Spring-Overviewtoc: truedate: 2021-07-28 17:32:41tags: Springcategories: [Spring Family , Spring Framework] SpringThe Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. A key element of Spring is infrastructural support at the application level: Spring focuses on the “plumbing” of enterprise applications so that teams can focus on application-level business logic, without unnecessary ties to specific deployment environments. 学习方法 先梳理脉络，先宏观，再细节 Spring源码注释很重要 见名知意 学习spring的命名规范 猜测和验证 坚持看 不要三分钟热度 学习资料 Spring框架官方网站 【官网】可以下载Spirng源码在本地查看更舒服！ 源码请戳 👉👉 【源码】 👈👈 Spring5最新完整教程IDEA版通俗易懂 视频教程 狂神说 （这个大佬在B站很火的）原链接请点击👉 【传送】 24集彻底搞懂aop ioc mvc底层原理 视频目录比较好 👉 【2020年史上最新Spring源码合集，24集彻底搞懂aop ioc mvc底层原理。】 手撕SpringIOC源码 马士兵教育 【400分钟学完Spring源码设计及原理，手撕SpringIOC源码，从我做起】 图灵学院公开课 课程目录还不错 5个小时 21年录制比较新 【2021年新版Java-Spring底层原理，阿里P8大佬全套讲解】 Mybatis + Spring 源码解读 VIP公开课【终于有字节跳动技术大牛把【mybatis底层原理：spring整理mybatis】讲明白了】","tags":[],"categories":[]},{"title":"mysql乐观锁实现分布式锁","date":"2021-07-28T09:21:46.000Z","path":"wiki/mysql乐观锁实现分布式锁/","text":"基于数据表乐观锁实现分布式锁整体的实际思路要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。当我们要锁住某个方法或资源的时候，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。 基于数据表实现分布式锁的几个要点1、这把锁依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。2、这把锁没有失效时间，一旦解决操作失败，就会导致记录一直在数据库中，其他线程无法在获得锁。3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁的操作。4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据库表中数据已经存在了。 当然，我们也可以有其它方式解决上面的问题： 1、数据库是单点？那就搞两个数据库，数据库之前双向同步，一旦挂掉快速切换到备库上。2、没有失效时间？可以做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。3、非阻塞？可以写一个while循环，直到insert成功再返回成功。4、非重入？可以在数据库表中加一个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库中可以查到的话，就直接把锁分配给它即可。 乐观锁&amp;悲观锁乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。 乐观锁的实现实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。 1、使用版本号实现乐观锁使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。 12345671.查询出商品信息select (status,status,version) from t_goods where id=#&#123;id&#125;2.根据商品信息生成订单3.修改商品status为2update t_goodsset status=2,version=version+1where id=#&#123;id&#125; and version=#&#123;version&#125;; 需要注意的是，乐观锁机制往往基于系统中数据存储逻辑，因此也具备一定的局限性。由于乐观锁机制是在我们的系统中实现的，对于来自外部系统的用户数据更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。在系统设计阶段，我们应该充分考虑到这些情况，并进行相应的调整（如将乐观锁策略在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。 这一点其实在微服务架构中只要做好数据隔离就可以避免，比如user这张数据表，按照边界划分应该属于用户中心服务的，其他服务比如仓储，物流等需要用户的信息，应该有用户中心暴露出接口，而不是仓储去数据库查询user这张表的数据，甚至update user的数据。 2、使用时间戳一般都是使用update_time字段，并且这个字段肯定是跟随数据库时间配置的，即 update on current_timestamp ； 乐观锁的优点与不足乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。能够提升数据库的吞吐量；但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。 参考资料基于数据库的分布式锁实现分布式锁方式（一、基于数据库的分布式锁）分布式锁看这篇就够了乐观锁与悲观锁深入学习","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"","date":"2021-07-28T09:05:46.241Z","path":"wiki/mybatis配置文件解析/","text":"title: mybatis配置文件解析toc: truedate: 2021-07-28 17:05:46tags: mybatiscategories: [Spring Family] Mybatis配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- 引入数据库属性文件 --&gt; &lt;properties resource=&quot;database.properties&quot;&gt; &lt;!-- &lt;property name=&quot;username&quot; value=&quot;sa&quot;&gt;&lt;/property&gt; --&gt; &lt;/properties&gt; &lt;!-- mybatis配置文件 --&gt; &lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;false&quot;/&gt; &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt; &lt;/settings&gt; &lt;!-- 别名的配置 --&gt; &lt;typeAliases&gt; &lt;!-- &lt;typeAlias type=&quot;com.xit.pojo.User&quot; alias=&quot;user&quot;/&gt; --&gt; &lt;package name=&quot;com.xit.pojo&quot;/&gt; &lt;/typeAliases&gt; &lt;!-- 配置运行环境 --&gt; &lt;environments default=&quot;default&quot;&gt; &lt;environment id=&quot;default&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;!-- 由JDBC管理事务 --&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!-- 配置数据源：连接池 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 引入Mapper映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;com/xit/pojo/UserMapper.xml&quot;/&gt; &lt;!-- URL方式 --&gt; &lt;!-- &lt;mapper url=&quot;file:///C:/eclipse-workspace/mybatis-01/src/com/xit/pojo/UserMapper.xml&quot;/&gt; --&gt; &lt;/mappers&gt;&lt;/configuration&gt; Mybatis有几部分全局配置properties=&gt;ettings=&gt;typeAliases=&gt;typeHandlers=&gt;objectFactory=&gt;plugins=&gt;environment=&gt;databaseIdProvider=&gt;mappers Mybatis 加载Mapper文件有几种方式？以上是Mybatis官方文档介绍的样例👆，原文链接请点击这 有4种方式；按照优先级从高到底依次是： package resource url class 下面是Mybatis加载mybatis-config.xml文件配置的源码，从代码中也可以看到加载的4中方式和优先级！👇org.apache.ibatis.builder.xml.XMLConfigBuilder#typeHandlerElement 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private void mapperElement(XNode parent) throws Exception &#123; if (parent != null) &#123; Iterator var2 = parent.getChildren().iterator(); while(true) &#123; while(var2.hasNext()) &#123; XNode child = (XNode)var2.next(); String resource; if (&quot;package&quot;.equals(child.getName())) &#123; resource = child.getStringAttribute(&quot;name&quot;); this.configuration.addMappers(resource); &#125; else &#123; resource = child.getStringAttribute(&quot;resource&quot;); String url = child.getStringAttribute(&quot;url&quot;); String mapperClass = child.getStringAttribute(&quot;class&quot;); XMLMapperBuilder mapperParser; InputStream inputStream; if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(resource); inputStream = Resources.getResourceAsStream(resource); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, resource, this.configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(url); inputStream = Resources.getUrlAsStream(url); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, url, this.configuration.getSqlFragments()); mapperParser.parse(); &#125; else &#123; if (resource != null || url != null || mapperClass == null) &#123; throw new BuilderException(&quot;A mapper element may only specify a url, resource or class, but not more than one.&quot;); &#125; Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass); this.configuration.addMapper(mapperInterface); &#125; &#125; &#125; return; &#125; &#125; &#125;``` ## Mybatis有几种执行器mybatis有3中执行器； ```textpackage org.apache.ibatis.session;public enum ExecutorType &#123; SIMPLE, // 默认 REUSE, BATCH; private ExecutorType() &#123; &#125;&#125;","tags":[],"categories":[]},{"title":"mybatis-工作原理","date":"2021-07-28T08:52:38.000Z","path":"wiki/mybatis-工作原理/","text":"Mybatis工作原理Mybatis整体框架 工作原理解析 1）读取MyBatis配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。 读取配置文件将mybatis-config.xml转换为org.apache.ibatis.session.Configuration类，这里mybatis包含9个全局配置； 2）加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。 扫描Mapping目录下的***Mapper.xml文件； 3）构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。 1234567@Bean(name = &quot;sqlSessionFactory&quot;)public SqlSessionFactory sqlSessionFactory(HikariDataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:com/****/mapping/**/*.xml&quot;)); return bean.getObject();&#125; 生成工厂实例： 123456789101112131415161718192021222324252627public class SqlSessionFactoryBean implements FactoryBean&lt;SqlSessionFactory&gt;, InitializingBean, ApplicationListener&lt;ApplicationEvent&gt; &#123; private static final Log LOGGER = LogFactory.getLog(SqlSessionFactoryBean.class); private Resource configLocation; private Configuration configuration; private Resource[] mapperLocations; private DataSource dataSource; private TransactionFactory transactionFactory; private Properties configurationProperties; private SqlSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); private SqlSessionFactory sqlSessionFactory; private String environment = SqlSessionFactoryBean.class.getSimpleName(); private boolean failFast; private Interceptor[] plugins; private TypeHandler&lt;?&gt;[] typeHandlers; private String typeHandlersPackage; private Class&lt;?&gt;[] typeAliases; private String typeAliasesPackage; private Class&lt;?&gt; typeAliasesSuperType; private DatabaseIdProvider databaseIdProvider; private Class&lt;? extends VFS&gt; vfs; private Cache cache; private ObjectFactory objectFactory; private ObjectWrapperFactory objectWrapperFactory; public SqlSessionFactoryBean() &#123; &#125;&#125; 4）创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。 SqlSession对象完成和数据库的交互： 5）Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。 3种执行期类型（Simple Pre Batch） Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。 6）MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。 借助MappedStatement中的结果映射关系，将返回结果转化成HashMap、JavaBean等存储结构并返回。 7）输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。8）输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。 参考资料【1】MyBatis的工作原理 C语言网","tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://example.com/tags/mybatis/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"mybatis","slug":"Spring-Family/mybatis","permalink":"http://example.com/categories/Spring-Family/mybatis/"}]},{"title":"Redis-缓存穿透、击穿和雪崩","date":"2021-07-28T08:51:22.000Z","path":"wiki/Redis-缓存穿透、击穿和雪崩/","text":"缓存击穿，穿透和雪崩背景首先说一下为什么会写这片文章，因为这个对我来说是印象非常深刻的，那是还在实习的时候，当时接了一个任务（其实就是练手的），大致需求是写一个白名单，然后有一个功能对白名单开放。因为是新功能，需要在部分地区试点，如果没有问题才会放开到全国城市运行。就是这么一个小的功能，让当时的我，呵呵。我记得那是第一次使用Redis,根本不知道什么是缓存雪崩啊，缓存击穿啊，穿透呀这些，还有更可怕的缓存一致性问题（TODO 下期再说）。 当时团队十几位大佬review我的代码，哼。这就是为什么印象会深刻一些吧，关键那是我第一个review代码，还是第一次使用redis，整个review就像是十几个大佬在面试我。真是怀疑人生了。。我是废物，别笑。 12345678910// 大致伪代码public boolean isPermit(String cityCode)&#123; // 先查询缓存中是否存在 String tmpCity = getCache(cityCode); if(StringUtil.isNotBlank(tmpCity))&#123; reture Boolean.TRUE; &#125; // 缓存中没有在去查数据库 reture judgeForDb(cityCode);&#125; 我上面的例子其实就是典型的缓存穿透的问题。因为仅仅是开放了十几个城市来试点功能，所以大部分的查询都是缓存不命中的。 下面引出今天我们的三个关键词： 缓存穿透 要查询的数据，缓存中基本上没有，所以大概率情况下缓存是不命中的，而是去数据库中去查询数据，导致缓存相当于是一个摆设。如果Key不是热点访问还可以，如果是热点Key，而且并发量也会很大的情况下，绝大多数的请求都会打到数据库上，很容易造成数据库宕机。 解决方案1、布隆过滤器进行校验，bloom filter典型应用场景（用户名是否存在，黑名单机制，单词错误检测）2、缓存空值方法，这个网上也是说的比较多的，这种方式也是可以的 在此我说一下我的解决方案：1、校验一个城市是否是城市白名单的城市时，直接查询缓存，存储的数据结构是使用的hash；2、如果没有查到这个城市，则认为这个城市不在白名单中，因为我是先查的hash，然后在查询hash中的具体城市。如果hash的值都没有查到，那说明缓存失效了；3、针对缓存失效的情况，可以再查DB来更新缓存，这个时候有人要较真了，会不会出线缓存击穿的情况呀，这个看具体场景，因为我的业务是不需要的，所以这里就直接查库更新缓存了，如果需要的话，那可以上缓存击穿的解决方案，或者看看上面两种解决方案有没有合适的，哈哈哈；4、因为城市是在后台进行配置的，所以我是在增删改的操作时，保证了缓存数据的一致性的前提下，才选择相信缓存的。这是我当时的解决方案。 缓存击穿 某个热点Key在一段时间内失效了，此时有大量请求瞬时抵达，会严重增加数据库的压力。因为我们的缓存数据一般都是要设置过期时间的，当缓存失效时，会去查询数据库同时更新缓存数据。 解决方案1、可以加锁，来保证只有第一个请求进来时达到数据库上，然后更新缓存，第二个请求进来是就会命中缓存，当然如果是分布式服务，那就需要使用分布式锁了。2、合理设置缓存时间，可以将热点Key时间设置长一些，或者根据业务将失效时间设置在业务量比较小的波段，都是可以的。有的解决方案会不设置Key的过期时间，这个，看情况吧，不建议这样。 如果大量的key不设置过期时间，则长期占用内存也是不好的。 缓存雪崩 缓存雪崩顾名思义，就是大量Key在一段时间或者瞬时失效，或者Redis服务重启（所有Key失效，因为是存在内存中），从而导致大量请求打到数据库上，增加数据库压力 解决方案1、将热点key设置不同波段的过期时间，把过期时间散列开。2、也可以使用分布式锁来限制高并发的请求，和缓存击穿的解决方案同理。3、对于Redis重启或宕机的问题，可以考虑集群部署，并保证数据的同步和一致性； 生活远不止眼前的苟且","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-字符串底层原理","date":"2021-07-28T08:50:26.000Z","path":"wiki/Redis-字符串底层原理/","text":"Redis底层实现及原理 关键词 SDS embstr 二进制安全 空间预分配 String类型不同的编码方式 使用整数存储： 只对长度小于或等于 21 字节，并且可以被解释为整数的字符串进行编码 使用EMBSTR 编码： 尝试将 RAW 编码的字符串编码为 EMBSTR 编码， 使用SDS编码: 这个对象没办法进行编码，尝试从 SDS 中移除所有空余空间 下面举个例子看一下👇 embstr与动态字符串 embstr的创建只需分配一次内存，而raw为两次（一次为sds分配对象，另一次为redisObject分配对象，embstr省去了第一次）。 相对地，释放内存的次数也由两次变为一次。 embstr的redisObject和sds放在一起，更好地利用缓存带来的优势 但是redis并未提供任何修改embstr的方式，即embstr是只读的形式。对embstr的修改实际上是先转换为raw再进行修改。 SDS(simple dynamic string)SDS定义123456789struct sdshdr&#123; //记录buf数组中已使用字节的数量 //等于 SDS 保存字符串的长度 int len; //记录 buf 数组中未使用字节的数量 int free; //字节数组，用于保存字符串 char buf[];&#125; SDS有什么优点1、常数复杂度获取字符串长度sdshdr 中由于 len 属性的存在，获取 SDS 字符串的长度只需要读取 len 属性，时间复杂度为 O(1)，而对于 C 语言来说， 获取字符串的长度通常是遍历字符串计数来实现的，时间复杂度为 O(n)。 2、杜绝缓冲区溢出我们知道在 C 语言中使用 strcat 函数来进行两个字符串的拼接，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。而对于 SDS 数据类型，在进行字符修改的时候， 会首先根据记录的 len属性检查内存空间是否满足需求，如果不满足，会进行相应的空间扩展，然后在进行修改操作，所以不会出现缓冲区溢出。 3、减少修改字符串时带来的内存重分配次数C语言由于不记录字符串的长度，所以如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。而对于SDS，由于len属性和free属性的存在，对于修改字符串SDS实现了空间预分配和惰性空间释放两种策略： 3.1 字符串长度增加操作时，进行空间预分配 对字符串进行空间扩展的时候，扩展的内存比实际需要的多，这样可以减少连续执行字符串增长操作所需的内存重分配次数。 3.2 字符串长度减少操作时，惰性空间释放 对字符串进行缩短操作时，程序不立即使用内存重新分配来回收缩短后多余的字节，而是使用 free 属性将这些字节的数量记录下来，等待后续使用。（当然SDS也提供了相应的API，当我们有需要时，也可以手动释放这些未使用的空间。 4、二进制安全因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取； 而所有 SDS 的API 都是以处理二进制的方式来处理 buf 里面的元素，并且 SDS不是以空字符串来判断是否结束，而是以 len 属性表示的长度来判断字符串是否结束。 5、兼容部分C字符串函数虽然 SDS 是二进制安全的，但是一样遵从每个字符串都是以空字符串结尾的惯例，这样可以重用 C 语言库&lt;string.h&gt; 中的一部分函数。 为什么字符串长度大于44就是用raw方式编码这个是因为C语言函数库分配内存的长度只能是2/4/8/16/32/64；最大分配64位的长度；但是redisObj的长度加上字符串对象头的长度，占用20位，所以字符串长度最多是44位，超过这个长度，就是用raw方式进行编码； – 《Redis深度历险-String数据结构》 参考资料1、《闲扯Redis二》String数据类型之底层解析2、每个程序员都应该知道的Redis知识 - String底层原理3、Redis详解（四）—— redis的底层数据结构4、redis string底层数据结构","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-overview","date":"2021-07-28T08:49:42.000Z","path":"wiki/Redis-overview/","text":"Redis （Remote Dictionary Server）Redis简介Redis 本质上是一个 Key-Value 类型的内存数据库，很像 memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据 flush 到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value DB。Redis 的出色之处不仅仅是性能，Redis 最大的魅力是支持保存多种数据结构，此外单个 value 的最大限制是 1GB，不像 memcached 只能保存 1MB 的数据，因此 Redis 可以用来实现很多有用的功能。比方说用他的 List 来做 FIFO 双向链表，实现一个轻量级的高性 能消息队列服务，用他的 Set 可以做高性能的 tag 系统等等。另外 Redis 也可以对存入的 Key-Value 设置 expire 时间，因此也可以被当作一 个功能加强版的 memcached 来用。Redis 的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此 Redis 适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 如何设置密码及验证密码？设置密码：config set requirepass 123456授权密码：auth 123456 Redis 有哪几种数据淘汰策略？noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但 DEL 和几个例外）allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。allkeys-random: 回收随机的键使得新添加的数据有空间存放。volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 Redis 有哪些适合的场景？（1）会话缓存（Session Cache）最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会话比其他存储（如 Memcached）的优势在于：Redis 提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 （2）全页缓存（FPC）除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似 PHP 本地 FPC。再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）队列Redis 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本地程序语言（如 Python）对 list 的 push/pop操作。如果你快速的在 Google 中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用 Redis 作为 broker，你可以从这里去查看。 （4）排行榜/计数器Redis 在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis 只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可：当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行：ZRANGE user_scores 0 10 WITHSCORESAgora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数据的，你可以在这里看到。###（5）发布/订阅最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！ Redis 常见的性能问题和解决方案1、master 最好不要做持久化工作，如 RDB 内存快照和 AOF 日志文件2、如果数据比较重要，某个 slave 开启 AOF 备份，策略设置成每秒同步一次3、为了主从复制的速度和连接的稳定性，master 和 Slave 最好在一个局域网内4、尽量避免在压力大得主库上增加从库5、主从复制不要采用网状结构，尽量是线性结构，Master&lt;–Slave1&lt;—-Slave2 ….","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-哈希表实现","date":"2021-07-28T08:46:45.000Z","path":"wiki/Redis-哈希表实现/","text":"Redis之Hash表底层实现 关键词 字段dict | 渐进式哈希 | ziplist | 哈希表 字典底层结构 dict 字典结构体12345678typedf struct dict&#123; dictType *type;//类型特定函数，包括一些自定义函数，这些函数使得key和 //value能够存储 void *private;//私有数据 dictht ht[2];//两张hash表 int rehashidx;//rehash索引，字典没有进行rehash时，此值为-1 unsigned long iterators; //正在迭代的迭代器数量&#125;dict; type和private这两个属性是为了实现字典多态而设置的，当字典中存放着不同类型的值，对应的一些复制，比较函数也不一样，这两个属性配合起来可以实现多态的方法调用； ht[2]，两个hash表 rehashidx，这是一个辅助变量，用于记录rehash过程的进度，以及是否正在进行rehash等信息，当此值为-1时，表示该dict此时没有rehash过程 iterators，记录此时dict有几个迭代器正在进行遍历过程 dictht 哈希表结构体1234567typedf struct dictht&#123; dictEntry **table;//存储数据的数组 二维 unsigned long size;//数组的大小 unsigned long sizemask;//哈希表的大小的掩码，用于计算索引值，总是等于 //size-1 unsigned long used;//// 哈希表中中元素个数&#125;dictht; dictEntry 哈希数组结构12345678910typedf struct dictEntry&#123; void *key;//键 union&#123; void val; unit64_t u64; int64_t s64; double d; &#125;v;//值 struct dictEntry *next；//指向下一个节点的指针&#125;dictEntry; 注意这里还有一个指向下一个哈希表节点的指针，我们知道哈希表最大的问题是存在哈希冲突，如何解决哈希冲突，有开放地址法和链地址法。这里采用的便是链地址法，通过next这个指针可以将多个哈希值相同的键值对连接在一起，用来解决哈希冲突。 扩容与缩容当哈希表保存的键值对太多或者太少时，就要通过 rerehash(重新散列）来对哈希表进行相应的扩展或者收缩。具体步骤： 1、如果执行扩展操作，会基于原哈希表创建一个大小等于 ht[0].used*2n 的哈希表（也就是每次扩展都是根据原哈希表已使用的空间扩大一倍创建另一个哈希表）。相反如果执行的是收缩操作，每次收缩是根据已使用空间缩小一倍创建一个新的哈希表。2、重新利用上面的哈希算法，计算索引值，然后将键值对放到新的哈希表位置上。3、所有键值对都迁徙完毕后，释放原哈希表的内存空间。 触发扩容的条件： 1、服务器目前没有执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。 2、服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。ps：负载因子 = 哈希表已保存节点数量 / 哈希表大小。 为什么扩容的时候要考虑BIGSAVE的影响，而缩容时不需要？ BGSAVE时，dict要是进行扩容，则此时就需要为dictht[1]分配内存，若是dictht[0]的数据量很大时，就会占用更多系统内存，造成内存页过多分离，所以为了避免系统耗费更多的开销去回收内存，此时最好不要进行扩容； 缩容时，结合缩容的条件，此时负载因子&lt;0.1，说明此时dict中数据很少，就算为dictht[1]分配内存，也消耗不了多少资源； 渐进式哈希什么叫渐进式 rehash？也就是说扩容和收缩操作不是一次性、集中式完成的，而是分多次、渐进式完成的。如果保存在Redis中的键值对只有几个几十个，那么 rehash 操作可以瞬间完成，但是如果键值对有几百万，几千万甚至几亿，那么要一次性的进行 rehash，势必会造成Redis一段时间内不能进行别的操作。所以Redis采用渐进式 rehash,这样在进行渐进式rehash期间，字典的删除查找更新等操作可能会在两个哈希表上进行，第一个哈希表没有找到，就会去第二个哈希表上进行查找。但是进行增加操作，一定是在新的哈希表上进行的。 渐进式哈希其实就是慢慢的，一步一步的将hash表的数据迁移到另一个hash表中 redis会有一个定时任务去检测是否需要进行rehash rehash的过程中会在字典dict中维护一个rehashidx的标志 在rehash的过程中，两个hash表中都会有数据，此时如果有数据新增，将会存在ht[1]也就是第二个哈希表上； 在rehash的过程中，如果有删改查，则优先选择第一张表，如果第一张表没有查到数据，则查找第二章哈希表； 参考资料1、Redis详解（四）—— redis的底层数据结构2、Redis底层数据结构之hash3、Redis Hash数据结构的底层实现4、图解redis五种数据结构底层实现(动图哦)5、redis hash底层数据结构6、Redis底层数据结构之hash","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-list底层实现","date":"2021-07-28T08:45:33.000Z","path":"wiki/Redis-list底层实现/","text":"Redis List 底层实现 关键字 连锁更新问题 | quicklist | ziplist | linkedlist List底层数据结构在 3.0 版本的 Redis 中，List 类型有两种实现方式：数据结构底层采用压缩列表ziplist或linkedlist两种数据结构进行存储，首先以ziplist进行存储，在不满足ziplist的存储要求后转换为linkedlist列表。当列表对象同时满足以下两个条件时，列表对象使用ziplist进行存储，否则用linkedlist存储。 ziplist转换成linkedlist的条件1、触发一下任意一条即进行转换： 列表对象保存的所有字符串元素的长度小于64字节 列表对象保存的元素数量小于512个。 2、redis.conf配置文件 12list-max-ziplist-value 64 list-max-ziplist-entries 512 3、ziplist和linkedlist底层实现1、使用压缩列表（ziplist）实现的列表对象。2、使用双端链表（linkedlist）实现的列表对象。 在 3.2 版本后新增了 quicklist 数据结构实现了 list，现在就来分析下 quicklist 的结构。 quicklistziplist会引入频繁的内存申请和释放，而linkedlist由于指针也会造成内存的浪费，而且每个节点是单独存在的，会造成很多内存碎片，所以结合两个结构的特点，设计了quickList。quickList 是一个 ziplist 组成的双向链表。每个节点使用 ziplist 来保存数据。本质上来说， quicklist 里面保存着一个一个小的 ziplist。 quicklist表头结构12345678910111213141516171819typedef struct quicklist &#123; //指向头部(最左边)quicklist节点的指针 quicklistNode *head; //指向尾部(最右边)quicklist节点的指针 quicklistNode *tail; //ziplist中的entry节点计数器 unsigned long count; /* total count of all entries in all ziplists */ //quicklist的quicklistNode节点计数器 unsigned int len; /* number of quicklistNodes */ //保存ziplist的大小，配置文件设定，占16bits int fill : 16; /* fill factor for individual nodes */ //保存压缩程度值，配置文件设定，占16bits，0表示不压缩 unsigned int compress : 16; /* depth of end nodes not to compress;0=off */&#125; quicklist; head 和 tail 分别指向这个双端链表的表头和表尾, quicklist 存储的节点是一个叫做 quicklistNode 的结构, 如果这个 quicklist 是空的,那么 head 和 tail 会同时成为空指针, 如果这个双端链表的大小为 1, 那么 head 和 tail 会同时指向一个相同的节点 count 是一个计数器, 表示当前这个 list 结构一共存储了多少个元素, 它的类型是 unsigned long, 所以一个 list 能存储的最多的元素在 字长为 64 bit 的机器上是 (1 &lt;&lt; 64) - 1, 字长为 32 bit 的机器上是 (1 &lt;&lt; 32) - 1 len 表示了这个双端链表的长度(quicklistNodes 的数量) fill 表示了单个节点(quicklistNode)的负载比例(fill factor), 这是什么意思呢 Lists 结构使用了一种特殊的编码方式来节省空间, Lists 中每一个节点所能存储的东西可以通过最大长度或者一个最大存储的空间大小来限制,对于想限制每个节点最大存储空间的用户, 用 -5 到 -1 来表示这个限制值 5: 最大存储空间: 64 Kb &lt;– 通常情况下不要设置这个值 4: 最大存储空间: 32 Kb &lt;– 非常不推荐 3: 最大存储空间: 16 Kb &lt;– 不推荐 2: 最大存储空间: 8 Kb &lt;– 推荐 1: 最大存储空间: 4 Kb &lt;– 推荐 对于正整数则表示最多能存储到你设置的那个值, 当前的节点就装满了通常在 -2 (8 Kb size) 或 -1 (4 Kb size) 时, 性能表现最好但是如果你的使用场景非常独特的话, 调整到适合你的场景的值！！！！ redis.conf, 其中有一个可配置的参数叫做 list-max-ziplist-size, 默认值为 -2, 它控制了 quicklist 中的 fill 字段的值, 负数限制 quicklistNode 中的 ziplist 的字节长度, 正数限制 quicklistNode 中的 ziplist 的最大长度 compress 则表示 quicklist 中的节点 quicklistNode, 除开最两端的 compress 个节点之后, 中间的节点都会被压缩 Lists 在某些情况下是会被压缩的, 压缩深度是表示除开 list 两侧的这么多个节点不会被压缩, 剩下的节点都会被尝试进行压缩, 头尾两个节点一定不会被进行压缩,因为要保证 push/pop 操作的性能, 有以下的值可以设置:0: 关闭压缩功能 1: 深度 1 表示至少在 1 个节点以后才会开始尝试压缩, 方向为从头到尾或者从尾到头 12[head]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[tail][head], [tail] 永远都是不会被压缩的状态; 中间的节点则会被压缩 2 不会尝试压缩 head 或者 head-&gt;next 或者 tail-&gt;prev 或者 tail 但是会压缩这中间的所有节点 1[head]-&gt;[next]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[prev]-&gt;[tail] 3: 以此类推，最大为2的16次方。 quicklistNode 节点123456789101112131415161718192021222324252627282930typedef struct quicklistNode &#123; struct quicklistNode *prev; //前驱节点指针 struct quicklistNode *next; //后继节点指针 //不设置压缩数据参数recompress时指向一个ziplist结构 //设置压缩数据参数recompress指向quicklistLZF结构 unsigned char *zl; //压缩列表ziplist的总长度 unsigned int sz; /* ziplist size in bytes */ //ziplist中包的节点数，占16 bits长度 unsigned int count : 16; /* count of items in ziplist */ //表示是否采用了LZF压缩算法压缩quicklist节点，1表示压缩过，2表示没压缩，占2 bits长度 unsigned int encoding : 2; /* RAW==1 or LZF==2 */ //表示一个quicklistNode节点是否采用ziplist结构保存数据，2表示压缩了，1表示没压缩，默认是2，占2bits长度 unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ //标记quicklist节点的ziplist之前是否被解压缩过，占1bit长度 //如果recompress为1，则等待被再次压缩 unsigned int recompress : 1; /* was this node previous compressed? */ //测试时使用 unsigned int attempted_compress : 1; /* node can&#x27;t compress; too small */ //额外扩展位，占10bits长度 unsigned int extra : 10; /* more bits to steal for future usage */&#125; quicklistNode; prev 和 next 分别指向当前 quicklistNode 的前一个和后一个节点 zl 指向实际的 ziplist sz 存储了当前这个 ziplist 的占用空间的大小, 单位是字节 count 表示当前有多少个元素存储在这个节点的 ziplist 中, 它是一个 16 bit 大小的字段, 所以一个 quicklistNode 最多也只能存储 65536 个元素 encoding 表示当前节点中的 ziplist 的编码方式, 1(RAW) 表示默认的方式存储, 2(LZF) 表示用 LZF 算法压缩后进行的存储 container 表示 quicklistNode 当前使用哪种数据结构进行存储的, 目前支持的也是默认的值为 2(ZIPLIST), 未来也许会引入更多其他的结构 recompress 是一个 1 bit 大小的布尔值, 它表示当前的 quicklistNode 是不是已经被解压出来作临时使用 attempted_compress 只在测试的时候使用 extra 是剩下多出来的 bit, 可以留作未来使用 quicklistLZF 结构定义1234Copytypedef struct quicklistLZF &#123; unsigned int sz; //压缩后的ziplist大小 char compressed[];//柔性数组，存放压缩后的ziplist字节数组&#125; quicklistLZF; 当指定使用lzf压缩算法压缩ziplist的entry节点时，quicklistNode结构的zl成员指向quicklistLZF结构; 参考资料1、Redis列表list 底层原理2、Redis中string、list的底层数据结构原理3、《闲扯Redis五》List数据类型底层之quicklist4、Redis源码剖析和注释（七）— 快速列表(quicklist)5、redis 列表结构 底层实现(quicklist)","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"kafka安装与初体验","date":"2021-07-28T08:43:24.000Z","path":"wiki/kafka安装与初体验/","text":"Kafka的安装安装zookeeper1brew install zookeeper 默认端口：2181默认安装位置：/usr/local/Cellar/zookeeper配置文件位置：/usr/local/etc/zookeeper日志文件位置：/usr/local/var/log/zookeeper/zookeeper.log 启动zookeeper1nohup zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; 安装Kafka1brew install kafka 默认端口：9092默认安装位置：/usr/local/Cellar/kafka配置文件位置：/usr/local/etc/kafka日志文件位置：/usr/local/var/lib/kafka-logs 启动kafkanohup kafka-server-start /usr/local/etc/kafka/server.properties &amp; 订阅发布Demo创建一个Topic1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看创建的Topic1kafka-topics --list --zookeeper localhost:2181 生产者生产消息1kafka-console-producer --broker-list localhost:9092 --topic test 消费者消费消息1kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic test1 --from-beginning –from-beginning: 将从第一个消息开始接收 SpringBoot集成Kafka源码地址：https://gitee.com/IBLiplus/kafka-demo.git项目启动前按照上述安装启动步骤，在本地启动kafka.创建Maven项目，引入一下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;pring-kafka&lt;/artifactId&gt; &lt;version&gt;2.5.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 添加如下配置，端口号可以自己定配置文件： 123456789101112131415161718192021222324252627282930313233server.port=9010spring.kafka.bootstrap-servers= 127.0.0.1:9092# 发生错误后，消息重发的次数。spring.kafka.producer.retries= 0#当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。spring.kafka.producer.batch-size= 16384# 设置生产者内存缓冲区的大小。spring.kafka.producer.buffer-memory= 33554432# 键的序列化方式spring.kafka.producer.key-serializer= org.apache.kafka.common.serialization.StringSerializer# 值的序列化方式spring.kafka.producer.value-serializer = org.apache.kafka.common.serialization.StringSerializer# acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。# acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。# acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。spring.kafka.producer.acks= 1# 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5Dspring.kafka.consumer.auto-commit-interval= 1S# 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：# latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）# earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录spring.kafka.consumer.auto-offset-reset= earliest# 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量spring.kafka.consumer.enable-auto-commit= false# 键的反序列化方式spring.kafka.consumer.key-deserializer= org.apache.kafka.common.serialization.StringDeserializer# 值的反序列化方式spring.kafka.consumer.value-deserializer= org.apache.kafka.common.serialization.StringDeserializer# 在侦听器容器中运行的线程数。spring.kafka.listener.concurrency= 5#listner负责ack，每调用一次，就立即commitspring.kafka.listener.ack-mode= manual_immediatespring.kafka.listener.missing-topics-fatal= false 生产者生产消息： 123456789101112131415161718192021222324252627282930@Componentpublic class ProductDemo &#123; Logger log = LoggerFactory.getLogger(ProductDemo.class); @Resource private KafkaTemplate&lt;String, Object&gt; kafkaTemplate; //自定义topic public static final String TOPIC_TEST = &quot;topic.test&quot;; public static final String TOPIC_GROUP1 = &quot;topic.group1&quot;; public static final String TOPIC_GROUP2 = &quot;topic.group2&quot;; public void send(String obj) &#123; log.info(&quot;准备发送消息为：&#123;&#125;&quot;, obj); //发送消息 ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(TOPIC_TEST, obj); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() &#123; @Override public void onFailure(Throwable throwable) &#123; //发送失败的处理 log.info(TOPIC_TEST + &quot; - 生产者 发送消息失败：&quot; + throwable.getMessage()); &#125; @Override public void onSuccess(SendResult&lt;String, Object&gt; stringObjectSendResult) &#123; //成功的处理 log.info(TOPIC_TEST + &quot; - 生产者 发送消息成功：&quot; + stringObjectSendResult.toString()); &#125; &#125;); &#125;&#125; 消费消息 1234567891011121314151617181920212223242526@Componentpublic class ConsumerDemo &#123; Logger log = LoggerFactory.getLogger(ConsumerDemo.class); @KafkaListener(topics = ProductDemo.TOPIC_TEST, groupId = ProductDemo.TOPIC_GROUP1) public void topic_test(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) &#123; Optional message = Optional.ofNullable(record.value()); if (message.isPresent()) &#123; Object msg = message.get(); log.info(&quot;topic_test 消费了： Topic:&quot; + topic + &quot;,Message:&quot; + msg); ack.acknowledge(); &#125; &#125; @KafkaListener(topics = ProductDemo.TOPIC_TEST, groupId = ProductDemo.TOPIC_GROUP2) public void topic_test1(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) &#123; Optional message = Optional.ofNullable(record.value()); if (message.isPresent()) &#123; Object msg = message.get(); log.info(&quot;topic_test1 消费了： Topic:&quot; + topic + &quot;,Message:&quot; + msg); ack.acknowledge(); &#125; &#125;&#125; 测试接口： 123456789@Resourceprivate ProductDemo productDemo;@GetMapping(&quot;/kafka/test&quot;)public void testKafka()&#123; logger.info(&quot;start test&quot;); productDemo.send(&quot;hello kafka&quot;); logger.info(&quot;end test&quot;);&#125; 山脚太拥挤 我们更高处见。","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"}]},{"title":"docker整体架构图","date":"2021-07-28T08:20:03.000Z","path":"wiki/docker整体架构图/","text":"Docker的总架构图 docker是一个C/S模式的架构，后端是一个松耦合架构，模块各司其职。 1、用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。2、Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；3、Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。4、Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；5、当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；6、当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。7、libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"}]},{"title":"docker常用手册","date":"2021-07-28T07:54:20.000Z","path":"wiki/docker常用手册/","text":"docker中文文档 http://www.dockerinfo.net/documentdocker doc https://docs.docker.com/engine/reference/commandline/docker/docker 中文社区 https://www.docker.org.cn/ 搜索可用镜像docker search tutorial 检查运行的镜像docker inspect efe 发布镜像docker push 下载镜像docker pull 在容器中安装新的程序apt-get updateapt-get install vim 保存对容器的修改docker commit pid","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"}]},{"title":"docker-compost安装mongodb","date":"2021-07-28T07:24:01.000Z","path":"wiki/docker-compost安装mongodb/","text":"mongo 配置文件 -&gt; https://www.cnblogs.com/xibuhaohao/p/12580331.html docker-compose 配置文件123456789mongo: image: mongo:4.4.7 #根据需要选择自己的镜像 ports: - 27017:27017 #对外暴露停供服务的端口，正式生产的时候理论不用暴露。 volumes: - ./mongodb/data/db:/data/db # 挂载数据目录 - ./mongodb/data/log:/var/log/mongodb # 挂载日志目录 - ./mongodb/data/config:/etc/mongo # 挂载配置目录 # command: --config /docker/mongodb/mongod.conf # 配置文件 按照上面👆配置文件设置目录/data/db/mongodb/datals -lconfig db log mongo 配置文件1234567891011121314151617181920212223242526272829303132333435363738# Where and how to store data.storage: dbPath: /data/db/mongodb/data/db journal: enabled: true# engine:# mmapv1:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /data/db/mongodb/data/log# network interfacesnet: port: 27017 bindIp: 0.0.0.0# how the process runsprocessManagement: timeZoneInfo: /usr/share/zoneinfo#security:#operationProfiling:#replication:#sharding:## Enterprise-Only Options:#auditLog:#snmp: bindIp: 0.0.0.0 允许远程访问 docker-compose启动mongodocker-compose up -ddocker ps 进入dockerdocker psdocker exec -it xxxxxxxxxx bash mongo创建数据库登录mongo 查看数据库show dbs 创建数据库use wechat_spider 然后 db 查看 创建用户1234567db.createUser( &#123; user:&quot;wechat&quot;, pwd:&quot;123456&quot;, roles:[&#123;role:&quot;readWrite&quot;,db:&quot;wechat_spider&quot;&#125;] &#125; ) Java客户端链接配置mvn12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;pring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt;&lt;/dependency&gt; 配置文件12345678910111213spring: data: mongodb: username: &#x27;wechat&#x27; password: &#x27;123456&#x27;# port: 3333 port: 27017 database: wechat_spider# host: 123.56.77.177 host: 39.107.117.232 repositories: type: auto domain1234567891011121314151617181920212223242526272829import org.springframework.data.mongodb.core.mapping.Document;/** * @Author gaolei * @Date 2021/7/28 上午10:02 * @Version 1.0 */@Document(collection = &quot;passenger&quot;)public class Passenger &#123; private String name; private String password; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; controller12345678910111213141516171819@RestControllerpublic class TestContoller &#123; @Autowired private MongoTemplate mongoTemplate; @RequestMapping(&quot;/insert&quot;) public String insert() &#123; Passenger passenger = new Passenger(); passenger.setName(&quot;hello&quot;); passenger.setPassword(&quot;world1&quot;); passenger = mongoTemplate.insert(passenger); if (passenger != null) &#123; return &quot;success&quot;; &#125; else &#123; return &quot;false&quot;; &#125; &#125;&#125;","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://example.com/tags/mongodb/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MongoDB","slug":"DataBase/MongoDB","permalink":"http://example.com/categories/DataBase/MongoDB/"}]},{"title":"基于BIO实现简易tomcat","date":"2021-07-28T07:11:55.000Z","path":"wiki/基于BIO实现简易tomcat/","text":"基于传统的BIO手写一个简易Tomcat本文主要基于传统的BIO来实现一个简单的Http请求处理过程；1、Servlet请求无非就是doGet/doPost，所以我们定义抽象Servlet记忆GET/POST方法；2、基于Socket和ServerSocket实现CS通信；3、模拟Spring加载配置文件，注册请求以及控制器； GlRequest 封装一个请求 当然是一个很简单的请求，这里只处理请求的URL和请求方法；获取请求，也就是输入流，解析数据Url和Method，并做相应的处理； 12345678910111213141516171819202122232425262728293031public class GlRequest &#123; private String url; private String method; public GlRequest(InputStream is) &#123; try &#123; // 解析http请求的具体内容； String content = &quot;&quot;; byte[] buff = new byte[1024]; int len = 0; if ((len = is.read(buff)) &gt; 0) &#123; content = new String(buff, 0, len); &#125; String line = content.split(&quot;\\\\n&quot;)[0]; String [] arr = line.split(&quot;\\\\s&quot;); this.method = arr[0]; this.url = arr[1].split(&quot;\\\\?&quot;)[0]; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public String getUrl() &#123; return this.url; &#125; public String getMethod() &#123; return this.method; &#125;&#125; GlResponse 定义返回值response 处理请求返回值，将业务处理的结果通过输出流输出；输出大致分为两部分，第一是返回的数据，第二是返回数据的Header; 1234567891011121314151617public class GlResponse &#123; private OutputStream outputStream; public GlResponse(OutputStream os) &#123; this.outputStream = os; &#125; public void write(String string) throws Exception &#123; StringBuffer sb = new StringBuffer(); sb.append(&quot;HTTP/1.1 200 OK\\n&quot;) .append(&quot;Content-Type: text/html;\\n&quot;) .append(&quot;\\r\\n&quot;) .append(string); outputStream.write(sb.toString().getBytes()); &#125;&#125; GlServlet 定义抽象servlet，定义GET方法和POST方法 定义抽象的Servlet和doGet方法和doPost方法，具体的业务去实现自己的方法和逻辑； 12345678910111213141516public abstract class GlServlet &#123; private final static String GET = &quot;GET&quot;; public void service(GlRequest request, GlResponse response) throws Exception &#123; if (GET.equals(request.getMethod())) &#123; doGet(request, response); &#125; else &#123; doPost(request, response); &#125; &#125; public abstract void doGet(GlRequest request, GlResponse response) throws Exception; public abstract void doPost(GlRequest request, GlResponse response) throws Exception; &#125; FirstServlet 具体的业务Servlet实现抽象Servlet的方法12345678910111213public class FirstServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; // 具体的逻辑 this.doPost(request, response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This is first servlet from BIO&quot;); &#125;&#125; SecondServlet 具体的业务Servlet实现抽象Servlet方法123456789101112public class SecondServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; doPost(request,response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This second request form BIO&quot;); &#125;&#125; web-bio.properties 配置文件 配置请求和处理器，Spring中是通过Controller下的@XXXMapping注解去扫描并加载到工厂的； 12345servlet.one.className=com.ibli.netty.tomcat.bio.servlet.FirstServletservlet.one.url=/firstServlet.doservlet.two.className=com.ibli.netty.tomcat.bio.servlet.SecondServletservlet.two.url=/secondServlet.do GlTomcat测试类 启动服务端，在网页中访问本地8080端口，输入配置文件中定义的url进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class GlTomcat &#123; private ServerSocket server; private final Integer PORT = 8080; private Properties webXml = new Properties(); private Map&lt;String, GlServlet&gt; servletMapping = new HashMap&lt;String, GlServlet&gt;(); /** * 模拟项目main方法，启动加载配置 * * @param args 启动参数 */ public static void main(String[] args) &#123; new GlTomcat().start(); &#125; /** * Tomcat的启动入口 */ private void start() &#123; //1、加载web配置文件，解析配置 init(); //2、启动服务器socket，等待用户请求 try &#123; server = new ServerSocket(this.PORT); System.err.println(&quot;Gl tomcat started in port &quot; + this.PORT); while (true) &#123; Socket client = server.accept(); // 3、获得请求信息，解析HTTP协议的内容 process(client); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 加载配置文件 */ private void init() &#123; try &#123; String WEB_INF = this.getClass().getResource(&quot;/&quot;).getPath(); FileInputStream fis = new FileInputStream(WEB_INF + &quot;web-bio.properties&quot;); webXml.load(fis); for (Object k : webXml.keySet()) &#123; String key = k.toString(); if (key.endsWith(&quot;.url&quot;)) &#123; //servlet.two.url String servletName = key.replaceAll(&quot;\\\\.url&quot;, &quot;&quot;); String url = webXml.getProperty(key); //servlet.two.className String className = webXml.getProperty(servletName + &quot;.className&quot;); //反射创建servlet实例 // load-on-startup &gt;=1 :web启动的时候初始化 0：用户请求的时候才启动 GlServlet obj = (GlServlet) Class.forName(className).newInstance(); // 将url和servlet建立映射关系 servletMapping.put(url, obj); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 解析客户端请求 * * @param client 客户端 */ private void process(Socket client) throws Exception &#123; InputStream is = null; OutputStream os = null; try &#123; //请求 is = client.getInputStream(); //封装返回值 os = client.getOutputStream(); GlRequest request = new GlRequest(is); GlResponse response = new GlResponse(os); String url = request.getUrl(); if (servletMapping.containsKey(url)) &#123; servletMapping.get(url).service(request, response); &#125; else &#123; response.write(&quot;404 Not found!&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (os != null) &#123; os.flush(); os.close(); &#125; if (is != null) &#123; is.close(); client.close(); &#125; &#125; &#125;&#125; 打印请求信息123456789101112Request content : GET /fitstServlet.do HTTP/1.1Host: localhost:8080Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 OPR/74.0.3911.160Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9Sec-Fetch-Site: noneSec-Fetch-Mode: navigateSec-Fetch-User: ?1Sec-Fetch-Dest: documentAccept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9 客户端发送请求及结果展示 请求： http://localhost:8080/firstServlet.do","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"netty实现简易RPC调用","date":"2021-07-27T12:01:08.000Z","path":"wiki/netty实现简易RPC调用/","text":"基于Netty手写一个RPC简易远程调用 抽象协议12345678910111213141516171819202122@Datapublic class InvokerProtocol implements Serializable &#123; // 基于二进制流调用协议 /** * 类名 */ private String className; /** * 方法名 */ private String methodName; /** * 形参 */ private Class&lt;?&gt;[] params; /** * 实参 */ private Object[] values;&#125; 注册中心RpcRegistry 基于Netty实现的RPC注册中心 1、 ServerBootstrap 启动8080端口，等待客户端链接；2、 RegisterHandler用来处理RPC接口的发现和注册； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class RpcRegistry &#123; private Integer post; public RpcRegistry(Integer post) &#123; this.post = post; &#125; private void start() &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap(); server.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; // 接受客户端请求的处理 ChannelPipeline pipeline = ch.pipeline(); //配置通用解码器 int fieldLength = 4; pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, fieldLength, 0, fieldLength)); pipeline.addLast(new LengthFieldPrepender(fieldLength)); //对象编码器 pipeline.addLast(&quot;encoder&quot;, new ObjectEncoder()); pipeline.addLast(&quot;decoder&quot;, new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(new RegisterHandler()); &#125; &#125;) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); try &#123; ChannelFuture future = server.bind(this.post).sync(); System.out.println(&quot;Rpc registry started in port &quot; + this.post); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; new RpcRegistry(8080).start(); &#125; &#125; RegisterHandler 执行RPC的发现和注册 1、扫描固定包下或者路径下的类;2、接口为key，具体实例作为value； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class RegisterHandler extends ChannelInboundHandlerAdapter &#123; /** * 注册中心容器 */ private static final ConcurrentHashMap&lt;String, Object&gt; REGISTRY_MAP = new ConcurrentHashMap&lt;String, Object&gt;(); private List&lt;String&gt; classNameList = new ArrayList&lt;String&gt;(); public RegisterHandler() &#123; // 1、扫描所有需要注册的类 scannerClass(&quot;com.ibli.netty.rpc.provider&quot;); // 执行注册 doRegistry(); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Object result; InvokerProtocol request = (InvokerProtocol) msg; if (REGISTRY_MAP.containsKey(request.getClassName())) &#123; Object provider = REGISTRY_MAP.get(request.getClassName()); Method method = provider.getClass().getMethod(request.getMethodName(), request.getParams()); result = method.invoke(provider, request.getValues()); ctx.write(result); ctx.flush(); ctx.close(); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; private void doRegistry() &#123; if (classNameList.isEmpty()) &#123; return; &#125; for (String className : classNameList) &#123; try &#123; Class&lt;?&gt; clazz = Class.forName(className); Class&lt;?&gt; i = clazz.getInterfaces()[0]; REGISTRY_MAP.put(i.getName(), clazz.newInstance()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void scannerClass(String packageName) &#123; URL url = this.getClass().getClassLoader().getResource(packageName.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File dir = new File(url.getFile()); for (File file : dir.listFiles()) &#123; if (file.isDirectory()) &#123; scannerClass(packageName + &quot;.&quot; + file.getName()); &#125; else &#123; classNameList.add(packageName + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;).trim()); &#125; &#125; &#125;&#125; API以及实现RPC接口 定义一个简单的服务接口 作为一个微服务对外暴露的API; 123456789public interface IRpcService &#123; int add(int a, int b); int mul(int a, int b); int sub(int a, int b); int div(int a, int b);&#125; RPC接口实现 provider实现具体的接口，提供具体的服务； 1234567891011121314151617public class RpcServiceImpl implements IRpcService &#123; public int add(int a, int b) &#123; return a + b; &#125; public int mul(int a, int b) &#123; return a * b; &#125; public int sub(int a, int b) &#123; return a - b; &#125; public int div(int a, int b) &#123; return a / b; &#125;&#125; RPC调用方调用RPC12345678public class RpcConsumer &#123; public static void main(String[] args) &#123; IRpcService rpc = RpcProxy.create(IRpcService.class); System.err.println(rpc.add(1,3)); System.err.println(rpc.mul(3,3)); System.err.println(rpc.sub(14,3)); &#125;&#125; RpcProxy 动态代理对象请求RPC 通过Netty Bootstrap访问8080端口； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class RpcProxy &#123; public static &lt;T&gt; T create(Class&lt;?&gt; clazz) &#123; MethodProxy proxy = new MethodProxy(clazz); Class&lt;?&gt;[] interfaces = clazz.isInterface() ? new Class[]&#123;clazz&#125; : clazz.getInterfaces(); T result = (T) Proxy.newProxyInstance(clazz.getClassLoader(), interfaces, proxy); return result; &#125; public static class MethodProxy implements InvocationHandler &#123; private Class&lt;?&gt; clazz; public MethodProxy(Class&lt;?&gt; clazz) &#123; this.clazz = clazz; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); &#125; else &#123; return rpcInvoke(proxy, method, args); &#125; &#125; private Object rpcInvoke(Object proxy, Method method, Object[] args) &#123; //封装请求的内容 InvokerProtocol msg = new InvokerProtocol(); msg.setClassName(this.clazz.getName()); msg.setMethodName(method.getName()); msg.setParams(method.getParameterTypes()); msg.setValues(args); final RpcProxyHandler consumerHandler = new RpcProxyHandler(); EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap client = new Bootstrap(); client.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; //接收课客户端请求的处理流程 ChannelPipeline pipeline = ch.pipeline(); int fieldLength = 4; //通用解码器设置 pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, fieldLength, 0, fieldLength)); //通用编码器 pipeline.addLast(new LengthFieldPrepender(fieldLength)); //对象编码器 pipeline.addLast(&quot;encoder&quot;, new ObjectEncoder()); //对象解码器 pipeline.addLast(&quot;decoder&quot;, new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(&quot;handler&quot;, consumerHandler); &#125; &#125;) .option(ChannelOption.TCP_NODELAY, true); ChannelFuture future = client.connect(&quot;localhost&quot;, 8080).sync(); future.channel().writeAndFlush(msg).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; group.shutdownGracefully(); &#125; return consumerHandler.getResponse(); &#125; &#125;&#125; RPC调用方接受并处理调用结果123456789101112131415161718public class RpcProxyHandler extends ChannelInboundHandlerAdapter &#123; private Object response; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; this.response = msg; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); &#125; public Object getResponse() &#123; return this.response; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"netty实现简易tomcat","date":"2021-07-27T11:54:15.000Z","path":"wiki/netty实现简易tomcat/","text":"基于Netty手写一个简易的Tomcat容器本文主要基于传统的BIO来实现一个简单的Http请求处理过程；1、Servlet请求无非就是doGet/doPost，所以我们定义抽象Servlet记忆GET/POST方法；2、基于Netty API实现CS通信；3、模拟Spring加载配置文件，注册请求以及控制器； Netty版本12345&lt;dependency&gt; &lt;groupId&gt;o.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.6.Final&lt;/version&gt;&lt;/dependency&gt; GlRequest 基于Netty&amp;HttpRequest的API操作，非常简单1234567891011121314151617181920212223242526272829303132public class GlRequest &#123; private ChannelHandlerContext ctx; private HttpRequest req; public GlRequest(ChannelHandlerContext ctx, HttpRequest req) &#123; this.ctx = ctx; this.req = req; &#125; public String getUrl() &#123; return this.req.uri(); &#125; public String getMethod() &#123; return this.req.method().name(); &#125; public Map&lt;String, List&lt;String&gt;&gt; getParams() &#123; QueryStringDecoder decoder = new QueryStringDecoder(req.uri()); return decoder.parameters(); &#125; public String getParam(String name) &#123; Map&lt;String, List&lt;String&gt;&gt; params = getParams(); List&lt;String&gt; strings = params.get(name); if (strings == null) &#123; return null; &#125; return strings.get(0); &#125;&#125; GlResponse 基于Netty&amp;FullHttpResponse的API操作 FullHttpResponse作为返回请求的主体； 123456789101112131415161718192021222324252627282930313233public class GlResponse &#123; private ChannelHandlerContext ctx; private HttpRequest req; public GlResponse(ChannelHandlerContext ctx, HttpRequest req) &#123; this.req = req; this.ctx = ctx; &#125; public void write(String string) throws Exception &#123; if (string == null || string.length() == 0) &#123; return; &#125; try &#123; FullHttpResponse response = new DefaultFullHttpResponse( HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(string.getBytes(&quot;UTF-8&quot;)) ); response.headers().set(&quot;Content-Type&quot;, &quot;text/html&quot;); ctx.write(response); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; ctx.flush(); ctx.close(); &#125; &#125;&#125; GlServlet 定义抽象servlet，定义GET方法和POST方法 定义抽象的Servlet和doGet方法和doPost方法，具体的业务去实现自己的方法和逻辑； 12345678910111213141516public abstract class GlServlet &#123; private final static String GET = &quot;GET&quot;; public void service(GlRequest request, GlResponse response) throws Exception &#123; if (GET.equals(request.getMethod())) &#123; doGet(request, response); &#125; else &#123; doPost(request, response); &#125; &#125; public abstract void doGet(GlRequest request, GlResponse response) throws Exception; public abstract void doPost(GlRequest request, GlResponse response) throws Exception; &#125; FirstServlet 具体的业务Servlet实现抽象Servlet的方法12345678910111213public class FirstServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; // 具体的逻辑 this.doPost(request, response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This is first servlet from NIO&quot;); &#125;&#125; SecondServlet 具体的业务Servlet实现抽象Servlet方法123456789101112public class SecondServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; doPost(request,response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This second request form NIO&quot;); &#125;&#125; web-nio.properties 配置文件 配置请求和处理器，Spring中是通过Controller下的@XXXMapping注解去扫描并加载到工厂的； 12345servlet.one.className=com.ibli.netty.tomcat.nio.servlet.FirstServletservlet.one.url=/firstServlet.doservlet.two.className=com.ibli.netty.tomcat.nio.servlet.SecondServletservlet.two.url=/secondServlet.do GlTomcat 启动服务端，在网页中访问本地8080端口，输入配置文件中定义的url进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class GlTomcat &#123; private final Integer PORT = 8080; private Properties webXml = new Properties(); private Map&lt;String, GlServlet&gt; servletMapping = new HashMap&lt;String, GlServlet&gt;(); public static void main(String[] args) &#123; new GlTomcat().start(); &#125; /** * Tomcat的启动入口 */ private void start() &#123; //1、加载web配置文件，解析配置 init(); // Boss线程 EventLoopGroup bossGroup = new NioEventLoopGroup(); // Worker线程 EventLoopGroup workGroup = new NioEventLoopGroup(); //2、创建Netty服务端对象 ServerBootstrap server = new ServerBootstrap(); //3、 配置服务端参数 server.group(bossGroup, workGroup) // 配置主线程的处理逻辑 .channel(NioServerSocketChannel.class) // 子线程的回调逻辑 .childHandler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel client) &#123; // 处理具体的回调逻辑 // 责任链模式 //返回-编码 client.pipeline().addLast(new HttpResponseEncoder()); //请求-解码 client.pipeline().addLast(new HttpRequestDecoder()); //用户自己的逻辑处理 client.pipeline().addLast(new GlTomcatHandler()); &#125; &#125;) // 配置主线程可分配的最大线程数 .option(ChannelOption.SO_BACKLOG, 128) //保持长链接 .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = null; try &#123; future = server.bind(this.PORT).sync(); System.err.println(&quot;Gl tomcat started in pory &quot; + this.PORT); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; /** * 加载配置文件 * 这其实使用了策略模式 */ private void init() &#123; try &#123; String WEB_INF = this.getClass().getResource(&quot;/&quot;).getPath(); FileInputStream fis = new FileInputStream(WEB_INF + &quot;web-nio.properties&quot;); webXml.load(fis); for (Object k : webXml.keySet()) &#123; String key = k.toString(); if (key.endsWith(&quot;.url&quot;)) &#123; //servlet.two.url String servletName = key.replaceAll(&quot;\\\\.url&quot;, &quot;&quot;); String url = webXml.getProperty(key); //servlet.two.className String className = webXml.getProperty(servletName + &quot;.className&quot;); //反射创建servlet实例 // load-on-startup &gt;=1 :web启动的时候初始化 0：用户请求的时候才启动 GlServlet obj = (GlServlet) Class.forName(className).newInstance(); // 将url和servlet建立映射关系 servletMapping.put(url, obj); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 处理用户请求 */ public class GlTomcatHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof HttpRequest) &#123; HttpRequest req = (HttpRequest) msg; GlRequest request = new GlRequest(ctx,req); GlResponse response = new GlResponse(ctx,req); String url = request.getUrl(); if (servletMapping.containsKey(url))&#123; servletMapping.get(url).service(request,response); &#125; else &#123; response.write(&quot;404 Not Fount&quot;); &#125; &#125; &#125; &#125;&#125; 测试结果 请求 : http://localhost:8080/secoundServlet.do 这的地址写错误 ⚠️ 请求 : http://localhost:8080/secondServlet.do","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java位运算","date":"2021-07-27T11:47:25.000Z","path":"wiki/Java位运算/","text":"在计算机中所有数据都是以二进制的形式储存的。位运算其实就是直接对在内存中的二进制数据进行操作，因此处理数据的速度非常快。在实际编程中，如果能巧妙运用位操作，完全可以达到四两拨千斤的效果，正因为位操作的这些优点，所以位操作在各大IT公司的笔试面试中一直是个热点问题。 位操作基础基本的位操作符有与、或、异或、取反、左移、右移这6种，它们的运算规则如下所示： 在这6种操作符，只有~取反是单目操作符，其它5种都是双目操作符。 位操作只能用于整形数据，对float和double类型进行位操作会被编译器报错。 位操作符的运算优先级比较低，因为尽量使用括号来确保运算顺序，否则很可能会得到莫明其妙的结果。比如要得到像1，3， 5，9这些2^i+1的数字。写成int a = 1 « i + 1;是不对的，程序会先执行i + 1，再执行左移操作。应该写成int a = (1 « i) + 1; 另外位操作还有一些复合操作符，如&amp;=、|=、 ^=、«=、»=。 12345678package com.king.bit;public class BitMain &#123; public static void main(String [] args) &#123; int a = -15, b = 15; System.out.println(a &gt;&gt; 2); // -4：-15 = 1111 0001(二进制)，右移二位，最高位由符号位填充将得到1111 1100即-4 System.out.println(b &gt;&gt; 2); // 3：15=0000 1111(二进制)，右移二位，最高位由符号位填充将得到0000 0011即3 &#125;&#125; 常用位操作小技巧下面对位操作的一些常见应用作个总结，有判断奇偶、交换两数、变换符号及求绝对值。这些小技巧应用易记，应当熟练掌握。 判断奇偶只要根据最未位是0还是1来决定，为0就是偶数，为1就是奇数。因此可以用if ((a &amp; 1) == 0)代替if (a % 2 == 0)来判断a是不是偶数。下面程序将输出0到100之间的所有偶数： 12345for (int i = 0; i &lt; 100; i ++) &#123; if ((i &amp; 1) == 0) &#123; // 偶数 System.out.println(i); &#125;&#125; 交换两数123456int c = 1, d = 2;c ^= d;d ^= c;c ^= d;System.out.println(&quot;c=&quot; + c);System.out.println(&quot;d=&quot; + d); 可以这样理解： 第一步 a=b 即a=(ab)；第二步 b=a 即b=b(ab)，由于运算满足交换律，b(ab)=bba。由于一个数和自己异或的结果为0并且任何数与0异或都会不变的，所以此时b被赋上了a的值；第三步 a=b 就是a=ab，由于前面二步可知a=(ab)，b=a，所以a=ab即a=(ab)a。故a会被赋上b的值； 变换符号变换符号就是正数变成负数，负数变成正数。如对于-11和11，可以通过下面的变换方法将-11变成11 11111 0101(二进制) –取反-&gt; 0000 1010(二进制) –加1-&gt; 0000 1011(二进制) 同样可以这样的将11变成-11 10000 1011(二进制) –取反-&gt; 0000 0100(二进制) –加1-&gt; 1111 0101(二进制) 因此变换符号只需要取反后加1即可。完整代码如下： 123int a = -15, b = 15;System.out.println(~a + 1);System.out.println(~b + 1); 求绝对值位操作也可以用来求绝对值，对于负数可以通过对其取反后加1来得到正数。对-6可以这样： 11111 1010(二进制) –取反-&gt;0000 0101(二进制) -加1-&gt; 0000 0110(二进制) 来得到6。 因此先移位来取符号位，int i = a » 31;要注意如果a为正数，i等于0，为负数，i等于-1。然后对i进行判断——如果i等于0，直接返回。否之，返回~a+1。完整代码如下： 12int i = a &gt;&gt; 31;System.out.println(i == 0 ? a : (~a + 1)); 现在再分析下。对于任何数，与0异或都会保持不变，与-1即0xFFFFFFFF异或就相当于取反。因此，a与i异或后再减i（因为i为0或-1，所以减i即是要么加0要么加1）也可以得到绝对值。所以可以对上面代码优化下： 12int j = a &gt;&gt; 31;System.out.println((a ^ j) - j); 注意这种方法没用任何判断表达式，而且有些笔面试题就要求这样做，因此建议读者记住该方法（_讲解过后应该是比较好记了）。 位操作与空间压缩筛素数法在这里不就详细介绍了，本文着重对筛素数法所使用的素数表进行优化来减小其空间占用。要压缩素数表的空间占用，可以使用位操作。下面是用筛素数法计算100以内的素数示例代码（注2）： 1234567891011121314151617// 打印100以内素数：// （1）对每个素数，它的倍数必定不是素数；// （2）有很多重复访问如flag[10]会在访问flag[2]和flag[5]时各访问一次；int max = 100;boolean[] flags = new boolean[max];int [] primes = new int[max / 3 + 1];int pi = 0;for (int m = 2; m &lt; max ; m ++) &#123; if (!flags[m]) &#123; primes[pi++] = m; for(int n = m; n &lt; max; n += m) &#123; flags[n] = true; &#125; &#125;&#125;System.out.println(Arrays.toString(primes)); 运行结果如下： 1[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0] 在上面程序是用bool数组来作标记的，bool型数据占1个字节（8位），因此用位操作来压缩下空间占用将会使空间的占用减少八分之七。 下面考虑下如何在数组中对指定位置置1，先考虑如何对一个整数在指定位置上置1。对于一个整数可以通过将1向左移位后与其相或来达到在指定位上置1的效果，代码如下所示： 1234// 在一个数指定位上置1int e = 0;e |= 1 &lt;&lt; 10;System.out.println(e); 同样，可以1向左移位后与原数相与来判断指定位上是0还是1（也可以将原数右移若干位再与1相与）。 12345//判断指定位上是0还是1if ((e &amp; (1 &lt;&lt; 10)) != 0) System.out.println(&quot;指定位上为1&quot;);else System.out.println(&quot;指定位上为0&quot;); 扩展到数组上，我们可以采用这种方法，因为数组在内存上也是连续分配的一段空间，完全可以“认为”是一个很长的整数。先写一份测试代码，看看如何在数组中使用位操作： 1234567891011int[] bits = new int[40];for (int m = 0; m &lt; 40; m += 3) &#123; bits[m / 32] |= (1 &lt;&lt; (m % 32));&#125;// 输出整个bitsfor (int m = 0; m &lt; 40; m++) &#123; if (((bits[m / 32] &gt;&gt; (m % 32)) &amp; 1) != 0) System.out.print(&#x27;1&#x27;); else System.out.print(&#x27;0&#x27;);&#125; 运行结果如下： 11001001001001001001001001001001001001001 可以看出该数组每3个就置成了1，证明我们上面对数组进行位操作的方法是正确的。因此可以将上面筛素数方法改成使用位操作压缩后的筛素数方法： 12345678910111213int[] flags2 = new int[max / 32 + 1];pi = 0;for (int m = 2; m &lt; max ; m ++) &#123; if ((((flags2[m / 32] &gt;&gt; (m % 32)) &amp; 1) == 0)) &#123; primes[pi++] = m; for(int n = m; n &lt; max; n += m) &#123; flags2[n / 32] |= (1 &lt;&lt; (n % 32)); &#125; &#125;&#125; System.out.println();System.out.println(Arrays.toString(primes)); 运行结果如下： 1[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0] 位操作工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.king.bit; /** * Java 位运算的常用方法封装 */public class BitUtils &#123; /** * 获取运算数指定位置的值 * 例如： 0000 1011 获取其第 0 位的值为 1, 第 2 位 的值为 0 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @return 指定位置的值(0 or 1) */ public static byte getBitValue(byte source, int pos) &#123; return (byte) ((source &gt;&gt; pos) &amp; 1); &#125; /** * 将运算数指定位置的值置为指定值 * 例: 0000 1011 需要更新为 0000 1111, 即第 2 位的值需要置为 1 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @param value * 只能取值为 0, 或 1, 所有大于0的值作为1处理, 所有小于0的值作为0处理 * * @return 运算后的结果数 */ public static byte setBitValue(byte source, int pos, byte value) &#123; byte mask = (byte) (1 &lt;&lt; pos); if (value &gt; 0) &#123; source |= mask; &#125; else &#123; source &amp;= (~mask); &#125; return source; &#125; /** * 将运算数指定位置取反值 * 例： 0000 1011 指定第 3 位取反, 结果为 0000 0011; 指定第2位取反, 结果为 0000 1111 * * @param source * * @param pos * 指定位置 (0&lt;=pos&lt;=7) * * @return 运算后的结果数 */ public static byte reverseBitValue(byte source, int pos) &#123; byte mask = (byte) (1 &lt;&lt; pos); return (byte) (source ^ mask); &#125; /** * 检查运算数的指定位置是否为1 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @return true 表示指定位置值为1, false 表示指定位置值为 0 */ public static boolean checkBitValue(byte source, int pos) &#123; source = (byte) (source &gt;&gt;&gt; pos); return (source &amp; 1) == 1; &#125; /** * 入口函数做测试 * * @param args */ public static void main(String[] args) &#123; // 取十进制 11 (二级制 0000 1011) 为例子 byte source = 11; // 取第2位值并输出, 结果应为 0000 1011 for (byte i = 7; i &gt;= 0; i--) &#123; System.out.printf(&quot;%d &quot;, getBitValue(source, i)); &#125; // 将第6位置为1并输出 , 结果为 75 (0100 1011) System.out.println(&quot;\\n&quot; + setBitValue(source, 6, (byte) 1)); // 将第6位取反并输出, 结果应为75(0100 1011) System.out.println(reverseBitValue(source, 6)); // 检查第6位是否为1，结果应为false System.out.println(checkBitValue(source, 6)); // 输出为1的位, 结果应为 0 1 3 for (byte i = 0; i &lt; 8; i++) &#123; if (checkBitValue(source, i)) &#123; System.out.printf(&quot;%d &quot;, i); &#125; &#125; &#125;&#125; BitSet类BitSet类：大小可动态改变, 取值为true或false的位集合。用于表示一组布尔标志。 此类实现了一个按需增长的位向量。位 set 的每个组件都有一个 boolean 值。用非负的整数将 BitSet 的位编入索引。可以对每个编入索引的位进行测试、设置或者清除。通过逻辑与、逻辑或和逻辑异或操作，可以使用一个 BitSet 修改另一个 BitSet 的内容。默认情况下，set 中所有位的初始值都是 false。 每个位 set 都有一个当前大小，也就是该位 set 当前所用空间的位数。注意，这个大小与位 set 的实现有关，所以它可能随实现的不同而更改。位 set 的长度与位 set 的逻辑长度有关，并且是与实现无关而定义的。 除非另行说明，否则将 null 参数传递给 BitSet 中的任何方法都将导致 NullPointerException。 在没有外部同步的情况下，多个线程操作一个 BitSet 是不安全的。 构造函数: BitSet() or BitSet(int nbits)，默认初始大小为64。 123456789101112131415161718192021222324252627282930public void set(int pos): 位置pos的字位设置为true。public void set(int bitIndex, boolean value): 将指定索引处的位设置为指定的值。public void clear(int pos): 位置pos的字位设置为false。public void clear(): 将此 BitSet 中的所有位设置为 false。public int cardinality(): 返回此 BitSet 中设置为 true 的位数。public boolean get(int pos): 返回位置是pos的字位值。public void and(BitSet other): other同该字位集进行与操作，结果作为该字位集的新值。public void or(BitSet other): other同该字位集进行或操作，结果作为该字位集的新值。public void xor(BitSet other): other同该字位集进行异或操作，结果作为该字位集的新值。public void andNot(BitSet set): 清除此 BitSet 中所有的位,set – 用来屏蔽此 BitSet 的 BitSetpublic int size(): 返回此 BitSet 表示位值时实际使用空间的位数。public int length(): 返回此 BitSet 的“逻辑大小”：BitSet 中最高设置位的索引加 1。public int hashCode(): 返回该集合Hash 码， 这个码同集合中的字位值有关。public boolean equals(Object other): 如果other中的字位同集合中的字位相同，返回true。public Object clone(): 克隆此 BitSet，生成一个与之相等的新 BitSet。public String toString(): 返回此位 set 的字符串表示形式。 例1：标明一个字符串中用了哪些字符 1234567891011121314151617181920212223242526package com.king.bit;import java.util.BitSet;public class WhichChars &#123; private BitSet used = new BitSet(); public WhichChars(String str) &#123; for (int i = 0; i &lt; str.length(); i++) used.set(str.charAt(i)); // set bit for char &#125; public String toString() &#123; String desc = &quot;[&quot;; int size = used.size(); for (int i = 0; i &lt; size; i++) &#123; if (used.get(i)) desc += (char) i; &#125; return desc + &quot;]&quot;; &#125; public static void main(String args[]) &#123; WhichChars w = new WhichChars(&quot;How do you do&quot;); System.out.println(w); &#125;&#125; 例2： 1234567891011121314151617181920package com.king.bit;import java.util.BitSet;public class MainTestThree &#123; /** * @param args */ public static void main(String[] args) &#123; BitSet bm = new BitSet(); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(0); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(1); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); System.out.println(bm.get(65)); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(65); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); &#125;&#125; 例3： 12345678910111213141516171819202122package com.king.bit;import java.util.BitSet;public class MainTestFour &#123; /** * @param args */ public static void main(String[] args) &#123; BitSet bm1 = new BitSet(7); System.out.println(bm1.isEmpty() + &quot;--&quot; + bm1.size()); BitSet bm2 = new BitSet(63); System.out.println(bm2.isEmpty() + &quot;--&quot; + bm2.size()); BitSet bm3 = new BitSet(65); System.out.println(bm3.isEmpty() + &quot;--&quot; + bm3.size()); BitSet bm4 = new BitSet(111); System.out.println(bm4.isEmpty() + &quot;--&quot; + bm4.size()); &#125; &#125; 位操作技巧123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 1. 获得int型最大值System.out.println((1 &lt;&lt; 31) - 1);// 2147483647， 由于优先级关系，括号不可省略System.out.println(~(1 &lt;&lt; 31));// 2147483647 // 2. 获得int型最小值System.out.println(1 &lt;&lt; 31);System.out.println(1 &lt;&lt; -1); // 3. 获得long类型的最大值System.out.println(((long)1 &lt;&lt; 127) - 1); // 4. 乘以2运算System.out.println(10&lt;&lt;1); // 5. 除以2运算(负奇数的运算不可用)System.out.println(10&gt;&gt;1); // 6. 乘以2的m次方System.out.println(10&lt;&lt;2); // 7. 除以2的m次方System.out.println(16&gt;&gt;2); // 8. 判断一个数的奇偶性System.out.println((10 &amp; 1) == 1);System.out.println((9 &amp; 1) == 1); // 9. 不用临时变量交换两个数（面试常考）a ^= b;b ^= a;a ^= b; // 10. 取绝对值（某些机器上，效率比n&gt;0 ? n:-n 高）int n = -1;System.out.println((n ^ (n &gt;&gt; 31)) - (n &gt;&gt; 31));/* n&gt;&gt;31 取得n的符号，若n为正数，n&gt;&gt;31等于0，若n为负数，n&gt;&gt;31等于-1若n为正数 n^0-0数不变，若n为负数n^-1 需要计算n和-1的补码，异或后再取补码，结果n变号并且绝对值减1，再减去-1就是绝对值 */ // 11. 取两个数的最大值（某些机器上，效率比a&gt;b ? a:b高）System.out.println(b&amp;((a-b)&gt;&gt;31) | a&amp;(~(a-b)&gt;&gt;31)); // 12. 取两个数的最小值（某些机器上，效率比a&gt;b ? b:a高）System.out.println(a&amp;((a-b)&gt;&gt;31) | b&amp;(~(a-b)&gt;&gt;31)); // 13. 判断符号是否相同(true 表示 x和y有相同的符号， false表示x，y有相反的符号。)System.out.println((a ^ b) &gt; 0); // 14. 计算2的n次方 n &gt; 0System.out.println(2&lt;&lt;(n-1)); // 15. 判断一个数n是不是2的幂System.out.println((n &amp; (n - 1)) == 0);/*如果是2的幂，n一定是100... n-1就是1111....所以做与运算结果为0*/ // 16. 求两个整数的平均值System.out.println((a+b) &gt;&gt; 1); // 17. 从低位到高位,取n的第m位int m = 2;System.out.println((n &gt;&gt; (m-1)) &amp; 1); // 18. 从低位到高位.将n的第m位置为1System.out.println(n | (1&lt;&lt;(m-1)));/*将1左移m-1位找到第m位，得到000...1...000n在和这个数做或运算*/ // 19. 从低位到高位,将n的第m位置为0System.out.println(n &amp; ~(0&lt;&lt;(m-1)));/* 将1左移m-1位找到第m位，取反后变成111...0...1111n再和这个数做与运算*/","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java注解","date":"2021-07-27T11:45:12.000Z","path":"wiki/Java注解/","text":"Java基础之注解机制详解 注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它是框架学习和设计者必须掌握的基础。 注解基础注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它主要的作用有以下四方面：— 生成文档，通过代码里标识的元数据生成javadoc文档。— 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。— 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理， 例如使用反射注入实例。这么来说是比较抽象的，我们具体看下注解的常见分类： Java自带的标准注解， 包括@Override、@Deprecated和@SuppressWarnings，分别用于标明重写某个方法、标明某个类或方法过时、标明要忽略的警告，用这些注解标明后编译器就会进行检查。 元注解，元注解是用于定义注解的注解，包括@Retention、@Target、@Inherited、@Documented，@Retention用于标明注解被保留的阶段，@Target用于标明注解使用的范围，@Inherited用于标明注解可继承，@Documented用于标明是否生成javadoc文档。 自定义注解，可以根据自己的需求定义注解，并可用元注解对自定义注解进行注解。接下来我们通过这个分类角度来理解注解。 Java内置注解我们从最为常见的Java内置的注解开始说起，先看下下面的代码： 123456789101112131415161718192021222324252627282930313233class A&#123; public void test() &#123; &#125;&#125;class B extends A&#123; /** * 重载父类的test方法 */ @Override public void test() &#123; &#125; /** * 被弃用的方法 */ @Deprecated public void oldMethod() &#123; &#125; /** * 忽略告警 * * @return */ @SuppressWarnings(&quot;rawtypes&quot;) public List processList() &#123; List list = new ArrayList(); return list; &#125;&#125; Java 1.5开始自带的标准注解，包括@Override、@Deprecated和@SuppressWarnings： @Override：表示当前的方法定义将覆盖父类中的方法 @Deprecated：表示代码被弃用，如果使用了被@Deprecated注解的代码则编译器将发出警告 @SuppressWarnings：表示关闭编译器警告信息 我们再具体看下这几个内置注解，同时通过这几个内置注解中的元注解的定义来引出元注解。 内置注解 - @Override我们先来看一下这个注解类型的定义： 1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 从它的定义我们可以看到，这个注解可以被用来修饰方法，并且它只在编译时有效，在编译后的class文件中便不再存在。这个注解的作用我们大家都不陌生，那就是告诉编译器被修饰的方法是重写的父类的中的相同签名的方法，编译器会对此做出检查，若发现父类中不存在这个方法或是存在的方法签名不同，则会报错。 内置注解 - @Deprecated这个注解的定义如下： 12345 @Documented@Retention(RetentionPolicy.RUNTIME)@Target(value=&#123;CONSTRUCTOR, FIELD, LOCAL_VARIABLE, METHOD, PACKAGE, PARAMETER, TYPE&#125;)public @interface Deprecated &#123;&#125; 从它的定义我们可以知道，它会被文档化，能够保留到运行时，能够修饰构造方法、属性、局部变量、方法、包、参数、类型。这个注解的作用是告诉编译器被修饰的程序元素已被“废弃”，不再建议用户使用。 内置注解 - @SuppressWarnings这个注解我们也比较常用到，先来看下它的定义： 12345@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.SOURCE)public @interface SuppressWarnings &#123;String[] value();&#125; 它能够修饰的程序元素包括类型、属性、方法、参数、构造器、局部变量，只能存活在源码时，取值为String[]。它的作用是告诉编译器忽略指定的警告信息，它可以取的值如下所示： // TODO 参考资料https://www.pdai.tech/md/java/basic/java-basic-x-annotation.html","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java泛型","date":"2021-07-27T11:39:32.000Z","path":"wiki/Java泛型/","text":"Java泛型1、泛型定义 使用泛型机制编写的程序代码要比那些杂乱地使用Object变量，然后在进行强制类型转换的代码具有更好的安全性和可读性。 –《Java核心技术》 泛型是在编译时期作用的； 泛型变量使用大写形式，在Java库中，一般使用变量E表示集合的元素类型，K和V表示表的关键字与值的类型。 2、通配符2.1 无边界通配符无边界通配符又成为非限定通配符 1234567891011121314public static void main(String[] args) &#123; List&lt;String&gt; list1 = new ArrayList&lt;&gt;(); list1.add(&quot;1&quot;); list1.add(&quot;2&quot;); list1.add(&quot;3&quot;); list1.add(&quot;4&quot;); loop(list1); &#125; public static void loop(List&lt;?&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; 2.2 上边界通配符上边界通配符和下边界通配符都属于限定通配符 12345678910111213141516public static void main(String[] args) &#123; //List中的类型必须是Number的子类，不然会报编译错误 List&lt;Integer&gt; list1 = new ArrayList&lt;&gt;(); list1.add(1); list1.add(2); list1.add(3); list1.add(4); loop(list1); &#125; // 传进来的list的类型必须是Number或Number的子类才可以 public static void loop(List&lt;? extends Number&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; ? extends Number如果限定的类型有多个，之间使用 &amp; 进行分割 2.3 下边界通配符1234567891011121314151617181920public static void main(String[] args) &#123; //List的泛型是Number 添加的元素只要是Number下的类型就可以 List&lt;Number&gt; list1 = new ArrayList&lt;&gt;(); list1.add(1); list1.add(2L); list1.add(new BigDecimal(22)); list1.add(4); loop(list1); &#125; /** * 通用类型必须是Number到Object之间的类型 * * @param list */ public static void loop(List&lt;? super Number&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; 3、泛型的使用 泛型必须先声明，再使用，不然会有编译错误；泛型的声明是用过一对&lt;&gt;来完成，约定使用一个大写的字母来表示;通配符不能用作返回值; 123456public &lt;T&gt; T testA(T t, Test1&lt;T&gt; test1) &#123; System.out.println(&quot;这是传入的T:&quot; + t); t = test1.t; System.out.println(&quot;这是赋值后的T:&quot; + t); return t;&#125; 要从泛型类取数据时，用extends； 要往泛型类写数据时，用super； 既要取又要写，就不用通配符（即extends与super都不用）。 3.1 泛型类12345public class Demo&lt;K, V&gt; &#123; public &lt;K&gt; K test(V v) &#123; return null; &#125;&#125; 3.2 泛型方法1234567891011121314151617181920212223242526272829303132333435public class DemoTest4&lt;K, V&gt; &#123; /** * &lt;T&gt; 代表泛型的声明 * * @param t 本方法声明的泛型类型 * @param &lt;T&gt; 本方法声明的泛型类型 * @return */ public &lt;T&gt; T test(T t) &#123; return null; &#125; /** * 普通的泛型方法 * * @param k 类中定义的泛型类型 * @param &lt;X&gt; 本方法中声明的泛型类型 * @return */ public &lt;X&gt; X aa(K k) &#123; return (X) null; &#125; /** * 静态方法中是无法使用类中声明的泛型类型的 * 可以使用在本方法中声明的泛型类型 * * @return */ public static &lt;X&gt; X bb() &#123; return null; &#125;&#125; 3.3 泛型接口首先看一下不使用泛型接口的Demo 12345678910111213141516171819202122先定义接口，声明两个方法public interface IGeneric &#123; Integer aa(Integer a); Integer bb(Integer b);&#125;//然后创建一个类来实现方法：public class IntegerDemo implements IGeneric&#123; @Override public Integer aa(Integer a) &#123; return null; &#125; @Override public Integer bb(Integer b) &#123; return null; &#125;&#125; 上面是没有使用泛型的接口设计，但是aa方法的操作类型相当于在接口中写死了，如果此时我们需要一个String类型的aa方法，那是不是还要在声明一个String类型的接口，然后再去实现呢，这样是不是显得代码很臃肿，代码重复；所以我们可以看一下使用泛型之后是怎么样的。 1234567891011121314151617181920212223242526272829303132定义泛型接口public interface IGenericInte&lt;T&gt; &#123; T aa(T a); T bb(T b);&#125;下面是根据不同类型的实现类泛型传如Integer类型public class IGenericInteger implements IGenericInte&lt;Integer&gt; &#123; @Override public Integer aa(Integer a) &#123; return null; &#125; @Override public Integer bb(Integer b) &#123; return null; &#125;&#125;泛型传入String类型public class IGenericString implements IGenericInte&lt;String&gt; &#123; @Override public String aa(String a) &#123; return null; &#125; @Override public String bb(String b) &#123; return null; &#125;&#125; 4、泛型擦除在虚拟机上没有泛型类型对象，所有的对象都属于普通类。Java在处理泛型类型的时候，会处理成一个相应的原始类型。 擦除类型变量，并替换为限定类型，如果没有限定类型，默认使用Object替代。如果有限定类型，并且是多个，会使用第一个限定的类型来替换。 1234public interface IGenericInte&lt;T&gt; &#123; T aa(T a); T bb(T b);&#125; 像上面这个T是一个无限定的变量，泛型擦除之后会直接使用Object替换。当然调用泛型方法时，如果擦除返回类型，编译器插入强制类型转换 12Pair&lt;Employee&gt; buddies = ....Employee buddy = buddies.getFirst(); 擦除getFirst的返回类型后将返回Object类型。编译器自动插入Employee的强制类型转换，也就是说，编译器调用方法是其实是执行了一下两个虚拟机指令： 对原始方法Pair.getFirst()方法的调用 将返回的Object类型强制转换为Employee类型 1public static &lt;T extends Comparable&gt; T foo(T [] args) 在擦除类型之后变成： 1public static Comparable T foo(Comparable [] args) 参数类型T已经被擦除，只留下限定类型Comparable; 总之有关Java泛型转换的事实： 虚拟机没有泛型，只有普通的类和方法 所有的类型参数都用它们的限定类型替换 ==桥方法被合成来保证多态== 为了保持类型安全型，必要时插入强制类型转换 第一条应该很好理解，这也是为什么会有泛型擦除这个概念，是因为JVM不能操作泛型；第二条就是解释泛型如何进行类型的擦除；第三条是泛型方法可能与多态的理念矛盾，所以使用桥方法来过渡或兼容；第四条上面也有提到，会出现强制类型转换的情况； 5、泛型的约束与局限性当然泛型的设计在java中并没有那么完美，它确实可以解决代码结构重用等问题，但是也是有一些局限性，下面是我根据《Java核心技术》进行的总结： 5.1 不能使用基础数据类型实例化类型参数原因是类型擦除之后，如果使用Object原始类型，Object是无法存储基本数据类型的值。所以只能通过其包装类型声明； 5.2 运行时查询类型只适用与原始类型1234567public class DemoTest5&lt;T&gt; &#123; public static void main(String[] args) &#123; DemoTest5&lt;String&gt; demoTest5 = new DemoTest5&lt;&gt;(); DemoTest5&lt;Integer&gt; demoTest4 = new DemoTest5&lt;&gt;(); System.err.println(demoTest4.getClass().equals(demoTest5.getClass())); &#125;&#125; demoTest4.getClass().equals(demoTest5.getClass())其实比较的是DemoTest5这个类类型，我们输出一下demoTest4.getClass()的结果看一下： 1class com.ibli.javaBase.generics.DemoTest5 所以这里有一道非常经典的面试题，如何判断一个泛型他的具体类型是什么，这里我们可以使用反射去拿到泛型的具体类型； 5.3 不能创造参数化类型的数组对于参数化类型的数组，在类型擦除之后，会变成Object[]类型，如果此时试图存储一个String类型的元素，就会抛出一个Array-StoreException异常；主要目的还是处于到数组安全的保护，可以参考几篇文章: 1、如果Java不支持参数化类型数组，那么Arrays.asList()如何处理它们？2、java不能创建参数化类型的泛型数组3、java.lang.ArrayStoreException 5.4 Varargs警告向参数个数可变的方法传递一个泛型类型的实例的场景，编译器会发出警告！抑制这种警告的方式有两种： 在调用方法上增加注解@SuppressWarnings(“unchecked”) 还可以使用@SafeVarargs注解直接标注方法 参考 java不能创建参数化类型的泛型数组 5.5 不能实例化类型变量不能使用new T(..) 或则new T[…]和T.class这样的表达式的类型变量；因为类型擦除后，T变成Object，显然我们在这里并不是想要创建一个Object实例。解决办法是在调用者提供一个构造器表达式，下面是用Supplier函数实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Pair&lt;T&gt; &#123; private T first; private T second; public T getFirst() &#123; return first; &#125; public void setFirst(T first) &#123; this.first = first; &#125; public T getSecond() &#123; return second; &#125; public void setSecond(T second) &#123; this.second = second; &#125; public Pair(T first, T second) &#123; this.first = first; this.second = second; &#125; public static &lt;T&gt; Pair&lt;T&gt; build(Supplier&lt;T&gt; constr) &#123; return new Pair&lt;&gt;(constr.get(), constr.get()); &#125; /** * Cannot infer type arguments for Pair2&lt;&gt; * 当函数头返回值为Pair时,无法推断,改为Pair2后可以推断. * @param c1 * @return */ public static &lt;T&gt; Pair&lt;T&gt; build(Class&lt;T&gt; c1)&#123; try &#123; return new Pair&lt;&gt;(c1.newInstance(),c1.newInstance()); &#125; catch (InstantiationException | IllegalAccessException e) &#123; return null; &#125; &#125;&#125; Supplier是一个函数接口，返回一个无参数并且返回类型为T的函数： 12345678910@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; /** * Gets a result. * * @return a result */ T get();&#125; 12345678910111213141516171819202122232425262728293031323334public class TestMakePair &#123; public static void main(String[] args) &#123; /** * 1.接受Supplier&lt;T&gt;--它是一个函数式接口。表示无参数且返回类型为T的函数。 * 因为不能实例化类型变量，如： * public Pair() &#123;first = new T();second = new T();&#125; * 所以最好的方式是让调用者提供一个构造器表达式.形式如下: * @param constr * @return */ Pair&lt;String&gt; pair = Pair.build(String::new); System.out.println(pair.getFirst().length()); /** * public void buildT()&#123; 2.传统的方式是通过Class.newInstance方法来构造泛型对象. 但由于细节过于复杂,T.class是不合法的.它会被擦除为Object.class.如下: Illegal class literal for the type parameter T T.class.newInstance(); &#125; * 3. * T.class是不合法的,但若API涉及如下 * reason:因为String.class是Class&lt;String&gt;的一个实例. */ Pair&lt;String&gt; pair1 = Pair.build(String.class); System.out.println(pair1.getFirst().length()); &#125;&#125;执行结果：00 5.6 不能构造泛型数组就像不能实例化一个泛型实例一样，也不能实例化数组。数组本身也有类型，用来监控存储在JVM中的数组，这个类型会被擦除，例如： 1234public static &lt;T extends Comparable&gt; T[] foo(T[] a)&#123; T[] mm = new T[2]; ...&#125; 类型擦除，会让这个方法永远构造Comparabel[2]数组； 5.7 泛型类的静态上下文中类型变量无效这个应该是比较好理解的，上文也提到过了，泛型类型是作用在泛型类上的，一些静态的方法或这静态的属性不能够使用泛型类的变量类型，编译器会直接报错； 5.8 不能抛出或者捕获泛型类的实例Java既不能抛出也不能捕获泛型类对象，实际上，甚至泛型类扩展Throwable都是不合法的。 12345678public static &lt;T extends Throwable&gt; void doWork(Class&lt;T&gt; t)&#123; try&#123; ... &#125;catch (T ex)&#123; 此处无法捕获 catch必须捕获具体的异常 .... &#125;&#125; 在异常规范中使用类型变量是允许的，如下： 123456789public static &lt;T extends Throwable&gt; void doWork(Class&lt;T&gt; t) throws T &#123; try&#123; ... &#125;catch (Throwable ex)&#123; t.initCause(ex); throw t; &#125;&#125; 5.9 可以消除对受查异常的检查Java异常处理要求必须为所有的受查异常提供一个处理器，但是使用泛型，可以规避这一点； 1234@SuppressWarnings(&quot;unchecked&quot;)public static &lt;T extends Throwable&gt; void throwAs(Throwable e) throws T&#123; throw (T)e;&#125; 调用上面的方法，编译器会认为t是一个非受查异常; 5.10 注意擦除后的冲突比如一个泛型类的equals方法，擦除之后，和Object的equals冲突；解决办法是重新命名引发错误的方法； 6、泛型的继承关系如果Manage extends Employee,那么Pair&lt; Manage &gt;是Pair&lt; Employee &gt;的子类吗？ 不是的！但是泛型类可以扩展或实现其他的泛型类，很典型的一个例子ArrayList: 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;&#125; ArrayList[E]继承了AbstractList[E]; 对于Java泛型的一些思考 编译器如何推断出具体的类型？ 参考资料：深入理解 Java 泛型 ------------------- 他日若遂凌云志 敢笑黄巢不丈夫 -------------------","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java反射","date":"2021-07-27T11:39:08.000Z","path":"wiki/Java反射/","text":"Java反射 反向探知，在程序运行是动态的获取类的相关属性这种动态获取类的内容以及动态调用对象的方法和获取属性的机制，叫做java反射机制； 反射的优缺点 优点增加了程序的灵活性，避免的固有逻辑写死到程序中代码简介，提高程序的复用性 缺点相比于直接调用，反射有比较大的性能消耗内部暴露和安全隐患 （因为反射可以操作private成员变量和调用private成员方法） 反射的基本操作获取类对象的4种方式123456789101112// 调用forName方法得到一个对象，这也是最容易想到的方式Class&lt;?&gt; object = Class.forName(&quot;com.ibli.javaBase.reflection.User&quot;);// 通过实例对象调用getClass方法Teacher teacher = new Teacher();Class&lt;?&gt; objectT = teacher.getClass();// 通过类加载器的方式Class&lt;?&gt; loader = ClassLoader.getSystemClassLoader().loadClass(&quot;com.ibli.javaBase.reflection.User&quot;);//通过一个类.classClass&lt;?&gt; tt = Teacher.class; 基本信息操作 类修饰符 PUBLIC PRIVATE PROTECTED STATIC FINAL SYNCHRONIZED VOLATILE TRANSIENT NATIVE INTERFACE ABSTRACT modifiers 1 2 4 8 16 32 64 128 256 512 1024 12345678910111213141516// 类的修饰符 具体的值可以参考JDK API文档中的定义 返回值是int类型 public：1System.err.println(tt.getModifiers());// 包名System.err.println(tt.getPackage());// 类的名称System.err.println(tt.getName());// 父类System.err.println(tt.getSuperclass());// 类加载器System.err.println(tt.getClassLoader());// 简称System.err.println(tt.getSimpleName());// 类实现的所有的接口System.err.println(tt.getInterfaces().length);// 所有的注解类型System.err.println(tt.getAnnotations().length); 执行结果： 123456781package com.ibli.javaBase.reflectioncom.ibli.javaBase.reflection.Teacherclass java.lang.Objectsun.misc.Launcher$AppClassLoader@18b4aac2Teacher00 查看类的变量12345678910111213141516// User extend Person(aa,bb)Class&lt;User&gt; obj = User.class;User user = obj.newInstance();// 能够拿到类的所有的变量Field[] fields = obj.getDeclaredFields();for (Field field : fields)&#123; System.out.println(field.getModifiers() + &quot; &quot; + field.getName());&#125;System.out.println(&quot; &quot;);// 只能够拿到类的public的变量Field[] fields1 = obj.getFields();for (Field field : fields1)&#123; System.out.println(field.getModifiers() + &quot; &quot; + field.getName());&#125;System.out.println(&quot; &quot;); 执行结果： 123456782 age2 name1 sex10 height 1 sex1 aa1 bb 结论： getDeclaredFields（1）getDeclaredFields能够获取本类的所有成员变量，无论是public还是private;（2）但是不能获取父类的任何属性；（3）可以获取static类型的属性； getFields（1）只能够获取本类的public属性；（2）能够获取父类的public属性；（3）可以获取static类型的属性； 修改属性1234567891011// 设置Person中的变量aaField aaField = obj.getField(&quot;aa&quot;);aaField.setInt(user,111);System.err.println(user.getAa());// 设置User私有成员变量Field ageField = obj.getDeclaredField(&quot;age&quot;);// 设置访问权限ageField.setAccessible(true);ageField.set(user,333);System.err.println(user.getAge()); 执行结果： 12111333 查看方法1234567891011121314151617Class&lt;User&gt; obj = User.class;User user = obj.newInstance();// 可以获取父类的方法Method[] methods = obj.getMethods();for (Method method : methods) &#123; System.out.println(method.getModifiers() + &quot; &quot; + method.getName());&#125;System.err.println(&quot; ----- &quot;);// 获取本类中的所有方法Method[] methods1 = obj.getDeclaredMethods();for (Method method : methods1) &#123; System.out.println(method.getModifiers() + &quot; &quot; + method.getName());&#125;System.err.println(&quot; 。。。。。。 &quot;);// 执行结果就不展示了 结论： getDeclaredMethods（1）可以获取本类中的所有方法；（2）可以获取本类的静态方法 getMethods（1）可以获取本类中的所有==公有==方法；（2）可以获取父类中的所有==公有==方法；（3）可以获取本类和父类的公有静态方法； 调用方法123456789// 访问私有方法Method sleep = obj.getDeclaredMethod(&quot;sleep&quot;);sleep.setAccessible(true);sleep.invoke(user);// 如果是静态方法，invoke第一个参数传null即可Method say = obj.getDeclaredMethod(&quot;say&quot;,String.class);say.setAccessible(true);say.invoke(null,&quot;hello java&quot;); 执行结果： 12Im sleeping!say hello java 构造器的使用123456789101112Class&lt;User&gt; obj = User.class;// 查询共有的构造器Constructor&lt;?&gt;[] constructors = obj.getConstructors();for (Constructor&lt;?&gt; constructor : constructors)&#123; System.out.println(constructor.getModifiers() + &quot; &quot; + constructor.getName());&#125;// 可以获取私有的构造器Constructor&lt;?&gt;[] constructors1 = obj.getDeclaredConstructors();for (Constructor&lt;?&gt; constructor : constructors1)&#123; System.err.println(constructor.getModifiers() + &quot; &quot; + constructor.getName());&#125; 执行结果： 1234561 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User2 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User 结论： getConstructors（1）获得本类所有的公有构造器 getDeclaredConstructors（1）获得本类所有的构造器（public&amp;private） 实例化对象1234567// 使用newInstance创建对象 调用无参构造器User user = obj.newInstance();// 获取构造器来实例化对象Constructor&lt;User&gt; constructor = obj.getDeclaredConstructor(Integer.class, String.class);constructor.setAccessible(true);User temp = constructor.newInstance(22, &quot;java&quot;);System.err.println(temp.getAge() + &quot; &quot; + temp.getName()); 执行结果： 22 java 反射性能为什么差 可以从两方面考虑，第一个是反射生成Class对象时性能差，第二是通过反射调用对象方式是的性能差； （1） 调用forName 本地方法（2）每次newInstance 都会进行一次安全检查（3）在默认情况下，方法的反射调用为委派实现，委派给本地实现来进行方法调用。在调用超过 15次之后，委派实现便会将委派对象切换至动态实现。这个动态实现的字节码是自动生成的，它将直接使用 invoke 指令来调用目标方法。 方法的反射调用会带来不少性能开销，原因主要有三个： 变长参数方法导致的Object数组 基本类型的自动装箱、拆箱 (参考资料2) 还有最重要的方法内联。 参考资料(1)反射为什么慢(2)关于装箱拆箱为什么会影响效率(3)jvm之方法内联优化 反射使用的场景 JDBC封装 Spring IOC jdbcTemplate Mybatis使用大量反射 使用反射注意点 在获取Field,method,construtor的时候，应尽量避免是用getDelcaredXXX(),应该传进参数获取指定的字段，方法和构造器； 使用缓存机制缓存反射操作相关元数据的原因是因为反射操作相关元数据的实时获取是比较耗时的 --------------------- 前途浩浩荡荡 万事尽可期待。----------------------- 反射在IOC中的应用","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java并发编程之同步锁","date":"2021-07-26T14:27:05.000Z","path":"wiki/Java并发编程之同步锁/","text":"","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java多线程之ThreadLocal","date":"2021-07-26T08:28:01.000Z","path":"wiki/Java多线程之ThreadLocal/","text":"ThreadLocalMap结构 ThreadLocal底层实际上是依赖ThreadLocalMap来实现数据存储的，而ThreadLocalMap并不是真正的Map结构，它是基于ThreadLocalMap类中的内部类Entry类型的数组来实现。 123456789 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 它的key其实就是当前threadlocal变量，继承了WeakReference。然后Object value;实际存储的值。 Thread类中有ThreadLocal类型的变量，如下👇 12345678910 /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */// 子线程可以获取到inheritableThreadLocals中的值 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; 为什么ThreadLocalMap使用ThreadLocal当作key而不是Thread呢？ 1、因为一个线程可能会出现多个ThreadLocal变量，所以一个线程一个ThreadLocalMap（实质上是Entry数组）来存放多个ThreadLocal变量。 2、倘若是Thread作为key，就会变成多个线程共同访问一个ThreadLocalMap，就会变成线程公用的变量，那个每个线程中可能存储多个ThreadLocal变量的情况下，Entry可能真的用到map结果才可以实现呀 3、多个线程共同访问ThreadLocalMap，那么可能会出现ThreadLocalMap提及很大从而降低性能，而且何时销毁这个变量是无法确定的 ThreadLocal set流程 下面是set方法的源码 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; set的操作就是向Entry数组中添加当前变量和值👇 用数组是因为，我们开发过程中可以一个线程可以有多个TreadLocal来存放不同类型的对象的，但是他们都将放到你当前线程的ThreadLocalMap里，所以肯定要数组来存。 至于Hash冲突，我们先看一下源码： 1234567891011121314151617181920212223private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); &#125; 我从源码里面看到ThreadLocalMap在存储的时候会给每一个ThreadLocal对象一个threadLocalHashCode，在插入过程中，根据ThreadLocal对象的hash值，定位到table中的位置i，**int i = key.threadLocalHashCode &amp; (len-1)**。 然后会判断一下：如果当前位置是空的，就初始化一个Entry对象放在位置i上； 1234if (k == null) &#123; replaceStaleEntry(key, value, i); return;&#125; 如果位置i不为空，如果这个Entry对象的key正好是即将设置的key，那么就刷新Entry中的value； 1234if (k == key) &#123; e.value = value; return;&#125; 如果位置i的不为空，而且key不等于entry，那就找下一个空位置，直到为空为止。 ThreadLocal是如何引起内存泄漏的？如上面Entry的源码大家也看到了，static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; 一个ThreadLocal变量存在两条引用： 1、ThreadLocalRef（栈）-》ThreadLocal（key）和 ThreadLocalMap -&gt; ThreadLocal（key） 2、ThreadRef -&gt; Thread -&gt; ThreadLocalMap -&gt; Entry -&gt; Value 内存泄漏值的是ThreadLocal被回收了，ThreadLocalMap -&gt; Entry -&gt; key没有了指向，但是Entry的value的指向还在，长期占用内存，就可能会导致内存泄漏。 如何避免内存泄漏ThreadLocal使用完之后及时remove 为什么建议ThreadLocal变量为static类型的ThreadLocal能够实现线程隔离的关键在于Thread持有自己的一个ThreadLocalMap变量，需要每一个ThreadLocal变量占用一个Entry就可以了，没有必要作为成员变量频繁创建，浪费内存空间。 参考资料 Java面试必问：ThreadLocal终极篇 【对线面试官】ThreadLocal","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"基础面试题目","date":"2021-07-26T02:05:07.000Z","path":"wiki/基础面试题目/","text":"1.String不可变String 对象的不可变性 了解了 String 对象的实现后，你有没有发现在实现代码中 String 类被 final 关键字修饰了，而且变量 char 数组也被 final 修饰了。 我们知道类被 final 修饰代表该类不可继承，而 char[] 被 final+private 修饰，代表了 String 对象不可被更改。Java 实现的这个特性叫作 String 对象的不可变性，即 String 对象一旦创建成功，就不能再对它进行改变。 Java 这样做的好处在哪里呢？ 第一，保证 String 对象的安全性。假设 String 对象是可变的，那么 String 对象将可能被恶意修改。 第二，保证 hash 属性值不会频繁变更，确保了唯一性，使得类似 HashMap 容器才能实现相应的 key-value 缓存功能。 第三，可以实现字符串常量池。在 Java 中，通常有两种创建字符串对象的方式，一种是通过字符串常量的方式创建，如 String str=“abc”；另一种是字符串变量通过 new 形式的创建，如 String str = new String(“abc”)。 当代码中使用第一种方式创建字符串对象时，JVM 首先会检查该对象是否在字符串常量池中，如果在，就返回该对象引用，否则新的字符串将在常量池中被创建。这种方式可以减少同一个值的字符串对象的重复创建，节约内存。 2.String 和 StringBuilder、StringBuffer 的区别？https://www.cnblogs.com/weibanggang/p/9455926.html 3.描述一下 JVM 加载 class 文件的原理机制？https://www.cnblogs.com/williamjie/p/11167920.html 4.char 型变量中能不能存贮一个中文汉字，为什么？正确答案： char型变量是用来存储Unicode编码的字符的，unicode编码字符集中包含了汉字， 所以，char型变量中当然可以存储汉字啦。不过，如果某个特殊的汉字没有被包含在unicode编码字符集中， 那么，这个char型变量中就不能存储这个特殊汉字。 补充说明：unicode编码占用两个字节，所以，char类型的变量也是占用两个字节 5.抽象类（abstract class）和接口（interface）有什么异同？https://blog.csdn.net/aptentity/article/details/68942916 6.静态嵌套类(Static Nested Class)和内部类（Inner Class）的不同？https://blog.csdn.net/machinecat0898/article/details/80071242 7.抽象的（abstract）方法是否可同时是静态的（static）,（native）， synchronized 修饰？ 答：都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。本地方法是由本地代码（如C代码）实现的方法，而抽象方法是没有实现的，也是矛盾的。synchronized和方法的实现细节有关，抽象方法不涉及实现细节，因此也是相互矛盾的。 8.如何实现对象克隆https://www.cnblogs.com/fnlingnzb-learner/p/10649509.html 9.内部类可以引用它的包含类（外部类）的成员吗？有没有什么限制https://www.cnblogs.com/aademeng/articles/11084885.htmlhttps://www.cnblogs.com/dolphin0520/p/3811445.html 静态内部类：它是用static修饰的，在访问限制上它只能访问外部类中的static所修饰的成员变量或者是方法成员内部类：成员内部类是最普通的内部类，它可以无条件访问外部类的所有成员属性和成员方法（包括private成员和静态成员）。【注意】当成员内部类拥有和外部类同名的成员变量或者方法时，会发生隐藏现象，即默认情况下访问的是成员内部类的成员。如果要访问外部类的同名成员，需要以下面的形式进行访问：局部内部类：局部内部类是定义在外围类的方法中的，在访问的时候它可以直接访问外围类的所有成员！但是不能随便访问局部变量，除非这个局部变量被final修饰。匿名内部类： 10.Java 中的 final 关键字有哪些用法？https://www.cnblogs.com/dotgua/p/6357951.html多线程下的final语义 👇https://www.codercc.com/backend/basic/juc/concurrent-keywords/final.html#_1-final%E7%9A%84%E7%AE%80%E4%BB%8B 修饰变量基本类型的变量，值是不可以变化的引用类型的变量，引用是不可以变化的，但是可以修改引用的值方法参数： 保证这个变量在这个方法中的值不会发生变化 修饰方法它表示该方法不能被覆盖。这种使用方式主要是从设计的角度考虑，即明确告诉其他可能会继承该类的程序员，不希望他们去覆盖这个方法。这种方式我们很容易理解，然而，关于private和final关键字还有一点联系，这就是类中所有的private方法都隐式地指定为是final的，由于无法在类外使用private方法，所以也就无法覆盖它 修饰类用final修饰的类是无法被继承的 11.Thread 类的 sleep()方法和对象的 wait()方法都可以让线程暂停执行，它们有什么区别?sleep()方法是Thread类 sleep是Thread的静态native方法,可随时调用,会使当前线程休眠,并释放CPU资源,但不会释放对象锁; wait()方法是Object类 wait()方法是Object的native方法,只能在同步方法或同步代码块中使用,调用会进入休眠状态,并释放CPU资源与对象锁,需要我们调用notify/notifyAll方法唤醒指定或全部的休眠线程,再次竞争CPU资源. 注意:sleep(long millis)存在睡眠时间,不算特点因为wait()方法存在重载wait(long timeout),即设置了等待超时时间它们两个都需要再次抢夺CPU资源 12.线程的 sleep()方法和 yield()方法有什么区别？sleep()方法在给其他线程运行机会时不考虑线程的优先级。因此会给低优先级的线程运行的机会，而yield()方法只会给相同优先级或更高优先级的线程运行的机会。线程执行sleep()方法后会转入阻塞状态，所以执行sleep()方法的线程在指定的时间内肯定不会被执行，而yield()方法只是使当前线程重新回到就绪状态，所以执行yield()方法的线程有可能在进入到就绪状态后又立马被执行。 13.线程的基本状态以及状态之间的关系 https://blog.csdn.net/zhangdongnihao/article/details/104029972 https://juejin.cn/post/6885159254764814349 14.访问修饰符 public,private,protected,以及不写（默认）时的区别？ 15.请说出与线程同步以及线程调度相关的方法。（1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；（2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常；（3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关；（4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； 补充：Java 5 通过 Lock 接口提供了显式的锁机制（explicit lock），增强了灵活性以及对线程的协调。Lock 接口中定义了加锁（lock()）和解锁（unlock()）的方法，同时还提供了 newCondition()方法来产生用于线程之间通信的 Condition 对象；此外，Java 5 还提供了信号量机制（semaphore），信号量可以用来限制对某个共享资源进行访问的线程的数量。在对资源进行访问之前，线程必须得到信号量的许可（调用 Semaphore 对象的 acquire()方法）；在完成对资源的访问后，线程必须向信号量归还许可（调用 Semaphore 对象的 release()方法）。 16.synchronized 关键字的用法？12345678910111213141516171819202122public class SyncDemo &#123; final Object lock = new Object(); public synchronized void m1()&#123;&#125; public void m2()&#123; synchronized (SyncDemo.class)&#123; // TODO &#125; synchronized (lock)&#123; &#125; synchronized (this)&#123; &#125; &#125; public synchronized static void m3()&#123; &#125;&#125; 17.Java 中如何实现序列化，有什么意义？序列化就是一种用来处理对象流的机制，所谓对象流也就是将对象的内容进行流化。可以对流化后的对象进行读写操作，也可将流化后的对象传输于网络之间。序列化是为了解决对象流读写操作时可能引发的问题（如果不进行序列化可能会存在数据乱序的问题）。要实现序列化，需要让一个类实现 Serializable 接口，该接口是一个标识性接口，标注该类对象是可被序列化的，然后使用一个输出流来构造一个对象输出流并通过 writeObject(Object)方法就可以将实现对象写出（即保存其状态）；如果需要反序列化则可以用一个输入流建立对象输入流，然后通过 readObject 方法从流中读取对象。序列化除了能够实现对象的持久化之外，还能够用于对象的深度克隆 18.阐述 JDBC 操作数据库的步骤下面的代码以连接本机的 Oracle 数据库为例，演示 JDBC 操作数据库的步骤。（1） 加载驱动。Class.forName(&quot;oracle.jdbc.driver.OracleDriver&quot;);（2） 创建连接。Connection con = DriverManager.getConnection(&quot;jdbc:oracle:thin:@localhost:1521:orcl&quot;,&quot;scott&quot;, &quot;tiger&quot;);（3） 创建语句。 123PreparedStatement ps = con.prepareStatement(&quot;select * from emp where sal between ? and ?&quot;);ps.setint(1, 1000);ps.setint(2, 3000); （4）执行语句。ResultSet rs = ps.executeQuery();（5）处理结果。 1234while(rs.next()) &#123; System.out.println(rs.getint(&quot;empno&quot;) + &quot; - &quot; + rs.getString(&quot;ename&quot;));&#125; （6） 关闭资源。 12345678910finally &#123; if(con != null) &#123; try &#123; con.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 提示：关闭外部资源的顺序应该和打开的顺序相反，也就是说先关闭 ResultSet、再关闭 Statement、在关闭 Connection。上面的代码只关闭了 Connection（连接），虽然通常情况下在关闭连接时，连接上创建的语句和打开的游标也会关闭，但不能保证总是如此，因此应该按照刚才说的顺序分别关闭。此外，第一步加载驱动在 JDBC 4.0 中是可以省略的（自动从类路径中加载驱动），但是我们建议保留。 19.Statement 和 PreparedStatement 有什么区别？哪个性能更好？与 Statement 相比，①PreparedStatement 接口代表预编译的语句，它主要的优势在于可以减少 SQL 的编译错误并增加 SQL 的安全性（减少 SQL 注射攻击的可能性）；②PreparedStatement 中的 SQL 语句是可以带参数的，避免了用字符串连接拼接 SQL 语句的麻烦和不安全；③当批量处理 SQL 或频繁执行相同的查询时，PreparedStatement 有明显的性能上的优势，由于数据库可以将编译优化后的SQL语句缓存起来，下次执行相同结构的语句时就会很快（不用再次编译和生成执行计划）。 补充：为了提供对存储过程的调用，JDBC API 中还提供了 CallableStatement 接口。存储过程（Stored Procedure）是数据库中一组为了完成特定功能的 SQL 语句的集合，经编译后存储在数据库中，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。虽然调用存储过程会在网络开销、安全性、性能上获得很多好处，但是存在如果底层数据库发生迁移时就会有很多麻烦，因为每种数据库的存储过程在书写上存在不少的差别。 20.在进行数据库编程时，连接池有什么作用？由于创建连接和释放连接都有很大的开销（尤其是数据库服务器不在本地时，每次建立连接都需要进行 TCP 的三次握手，释放连接需要进行 TCP 四次握手，造成的开销是不可忽视的），为了提升系统访问数据库的性能，可以事先创建若干连接置于连接池中，需要时直接从连接池获取，使用结束时归还连接池而不必关闭连接，从而避免频繁创建和释放连接所造成的开销，这是典型的用空间换取时间的策略（浪费了空间存储连接，但节省了创建和释放连接的时间）。池化技术在Java 开发中是很常见的，在使用线程时创建线程池的道理与此相同。基于 Java 的开源数据库连接池主要有：C3P0、Proxool、DBCP、BoneCP、Druid 等。 补充：在计算机系统中时间和空间是不可调和的矛盾，理解这一点对设计满足性能要求的算法是至关重要的。大型网站性能优化的一个关键就是使用缓存，而缓存跟上面讲的连接池道理非常类似，也是使用空间换时间的策略。可以将热点数据置于缓存中，当用户查询这些数据时可以直接从缓存中得到，这无论如何也快过去数据库中查询。当然，缓存的置换策略等也会对系统性能产生重要影响，对于这个问题的讨论已经超出了这里要阐述的范围。 21.什么是 DAO 模式?DAO（Data Access Object）顾名思义是一个为数据库或其他持久化机制提供了抽象接口的对象，在不暴露底层持久化方案实现细节的前提下提供了各种数据访问操作。在实际的开发中，应该将所有对数据源的访问操作进行抽象化后封装在一个公共API中。用程序设计语言来说，就是建立一个接口，接口中定义了此应用程序中将会用到的所有事务方法。在这个应用程序中，当需要和数据源进行交互的时候则使用这个接口，并且编写一个单独的类来实现这个接口，在逻辑上该类对应一个特定的数据存储。DAO 模式实际上包含了两个模式，一是 DataAccessor（数据访问器），二是 Data Object（数据对象），前者要解决如何访问数据的问题，而后者要解决的是如何用对象封装数据。 22.Java 中是如何支持正则表达式操作的？Java 中的 String 类提供了支持正则表达式操作的方法，包括：matches()、replaceAll()、replaceFirst()、split()。此外，Java 中可以用 Pattern 类表示正则表达式对象，它提供了丰富的 API 进行各种正则表达式操作。面试题： - 如果要从字符串中截取第一个英文左括号之前的字符串，例如：北京市(朝阳区)(西城区)(海淀区)，截取结果为：北京市，那么正则表达式怎么写？ 123456789101112import java.util.regex.Matcher;import java.util.regex.Pattern;class RegExpTest &#123; public static void main(String[] args) &#123; String str = &quot;北京市(朝阳区)(西城区)(海淀区)&quot;; Pattern p = Pattern.compile(&quot;.*?(?=\\()&quot;); Matcher m = p.matcher(str); if(m.find()) &#123; System.out.println(m.group()); &#125; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"面向对象特征","date":"2021-07-26T01:59:21.000Z","path":"wiki/面向对象特征/","text":"封装封装是保证软件部件具有优良的模块性的基础，封装的目标就是实现软件内部的“高内聚、低耦合”。防止程序相互依赖而带来的变动影响。在面向对象的编程语言中，对象是封装的最基本单位，面向对象的封装比传统语言的封装更为清晰、也更为有力，面向对象的封装就是把描述一个对象的属性和行为的代码封装在一个“模块”中，或者说是一个类中，属性用变量定义，行为用方法定义，方法可以直接访问同一个对象中的属性。 将一个类中的成员变量全部定义为私有的，只有这个类自己的方法才可以访问到这些成员变量，这就基本上实现对象的封装，把握一个原则：把对同一事物进行操作的方法和相关的方法放在同一个类中，把方法和它操作的数据放在同一个类中。 抽象抽象就是找出一些事物的相似和共性之处，然后将这些事物归为一类，这个类只考虑这些事物的相似和共性之处，并且会忽略与当前主题和目标无关的那些方面，将注意力集中在与当前目标有关的方面。 继承在定义和实现一个类的时候，可以在一个已经存在的类的基础上来进行，把这个已经存在的类所定义的内容作为自己的内容，并可以加入若干新的内容，或修改原来的方法使之更适合特殊的需要，这就是继承。 继承是子类自动共享父类资源（数据或者方法）的机制，这是类之间的一种关系，提高了软件的可重用性和可扩展性。 多态多态是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在该编程时不确定，而是在程序运行期间才确定，即一个引用变量倒底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。因为在程序运行时才确定具体的类，这样，不用修改源程序代码，就可以让引用变量绑定到各种不同的类实现上，从而导致该引用调用的具体方法随之改变，即不修改程序代码就可以改变程序运行时所绑定的具体代码，让程序可以选择多个运行状态，这就是多态性。 多态性增强了软件的灵活性和扩展性，简单一句话理解多态的话就是，编译看左边，运行看右边 编译看左边 – 是指 想要成功的保存,就要使用左边也就是 只能使用父类提供的功能!!如果父类中没有，那么会编译报错运行看右边 – 是指 想要得到结果,就要看右边也就是 使用子类的方法体!!!","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"kafka-overview","date":"2021-07-22T03:00:12.000Z","path":"wiki/kafka-overview/","text":"参考资料 kafka详细教程 Kafka 集群管理 OrcHome kafka中文教程 面试官：说说Kafka处理请求的全流程 蘑菇街千亿级消息Kafka上云实践 kafka 集群搭建 kafka-2-11集群部署","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"}]},{"title":"Java-为什么禁止把SimpleDateFormat定义成static变量?","date":"2021-07-21T12:06:46.000Z","path":"wiki/Java-为什么禁止把SimpleDateFormat定义成static变量/","text":"本文参照 《Java技术灵魂15问》 简介在日常开发中，我们经常会用到时间，我们有很多办法在 Java 代码中获取时 间。但是不同的方法获取到的时间的格式都不尽相同，这时候就需要一种格式化工 具，把时间显示成我们需要的格式。 最常用的方法就是使用 SimpleDateFormat 类。这是一个看上去功能比较简单 的类，但是，一旦使用不当也有可能导致很大的问题。在 Java 开发手册中，有如下明确规定: 那么，本文就围绕 SimpleDateFormat 的用法、原理等来深入分析下如何以正 确的姿势使用它。 SimpleDateFormat 是 Java 提供的一个格式化和解析日期的工具类。它允许进 行格式化(日期 -&gt; 文本)、解析(文本 -&gt; 日期)和规范化。SimpleDateFormat 使 得可以选择任何用户定义的日期 - 时间格式的模式。 在 Java 中，可以使用 SimpleDateFormat 的 format 方法，将一个 Date 类型 转化成 String 类型，并且可以指定输出格式。 SimpleDateFormat 用法12345 // Date转StringDate data = new Date();SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);String dataStr = sdf.format(data);System.out.println(dataStr); 以上代码，转换的结果是:2018-11-25 13:00:00，日期和时间格式由”日期 和时间模式”字符串指定。如果你想要转换成其他格式，只要指定不同的时间模式就 行了。 在 Java 中，可以使用 SimpleDateFormat 的 parse 方法，将一个 String 类型 转化成 Date 类型。 12// String转Data System.out.println(sdf.parse(dataStr)); 日期和时间模式表达方法在使用 SimpleDateFormat 的时候，需要通过字母来描述时间元素，并组装成 想要的日期和时间模式。常用的时间元素和字母的对应表如下: 模式字母通常是重复的，其数量确定其精确表示。如下表是常用的输出格式的表 示方法。 输出不同时区的时间时区是地球上的区域使用同一个时间定义。以前，人们通过观察太阳的位置(时 角)决定时间，这就使得不同经度的地方的时间有所不同(地方时)。1863 年，首次 使用时区的概念。时区通过设立一个区域的标准时间部分地解决了这个问题。 世界各个国家位于地球不同位置上，因此不同国家，特别是东西跨度大的国家日 出、日落时间必定有所偏差。这些偏差就是所谓的时差。 现今全球共分为 24 个时区。由于实用上常常 1 个国家，或 1 个省份同时跨着 2 个或更多时区，为了照顾到行政上的方便，常将 1 个国家或 1 个省份划在一起。所以 时区并不严格按南北直线来划分，而是按自然条件来划分。例如，中国幅员宽广，差 不多跨 5 个时区，但为了使用方便简单，实际上在只用东八时区的标准时即北京时间 为准。 由于不同的时区的时间是不一样的，甚至同一个国家的不同城市时间都可能不一 样，所以，在 Java 中想要获取时间的时候，要重点关注一下时区问题。默认情况下，如果不指明，在创建日期的时候，会使用当前计算机所在的时区作为默认时区，这也是为什么我们通过只要使用new Date()就可以获取中国的当前 时间的原因。 那么，如何在 Java 代码中获取不同时区的时间呢? SimpleDateFormat 可以 实现这个功能。 123SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); sdf.setTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;)); System.out.println(sdf.format(Calendar.getInstance().getTime())); 以上代码，转换的结果是:2018-11-24 21:00:00 。既中国的时间是 11 月 25 日的 13 点，而美国洛杉矶时间比中国北京时间慢了 16 个小时(这还和冬夏令时有关 系，就不详细展开了)。 如果你感兴趣，你还可以尝试打印一下美国纽约时间(America/New_York)。 纽约时间是 2018-11-25 00:00:00。纽约时间比中国北京时间早了 13 个小时。 当然，这不是显示其他时区的唯一方法，不过本文主要为了介绍 SimpleDate-Format，其他方法暂不介绍了。 SimpleDateFormat 线程安全性由于 SimpleDateFormat 比较常用，而且在一般情况下，一个应用中的时间显 示模式都是一样的，所以很多人愿意使用如下方式定义 SimpleDateFormat: 123456789public class Main &#123; private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static void main(String[] args) &#123; simpleDateFormat.setTimeZone(TimeZone.getTimeZone(&quot;America/New_York&quot;)); System.out.println(simpleDateFormat.format(Calendar.getInstance(). getTime())); &#125; &#125; ⚠️ 这种定义方式，存在很大的安全隐患。 我们来看一段代码，以下代码使用线程池来执行时间输出。 123456789101112131415161718192021222324252627282930313233343536373839public class Main &#123; /** * 定义一个全局的SimpleDateFormat */ private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); /** * 使用ThreadFactoryBuilder定义一个线程池 */ private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); /** * 定义一个CountDownLatch，保证所有子线程执行完之后主线程再执行 */ private static CountDownLatch countDownLatch = new CountDownLatch(100); public static void main(String[] args) &#123; // 定义一个线程安全的 HashSet Set&lt;String&gt; dates = Collections.synchronizedSet(new HashSet&lt;String&gt;()); for (int i = 0; i &lt; 100; i++) &#123; // 获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // 时间增加 calendar.add(Calendar.DATE, finalI); // 通过 simpleDateFormat 把时间转换成字符串 String dateString = simpleDateFormat.format(calendar. getTime()); // 把字符串放入 Set 中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;); &#125; // 阻塞，直到 countDown 数量为 0 countDownLatch.await(); // 输出去重后的时间个数 System.out.println(dates.size()); &#125; &#125; 以上代码，其实比较简单，很容易理解。就是循环一百次，每次循环的时候都在 当前时间基础上增加一个天数(这个天数随着循环次数而变化)，然后把所有日期放入 一个线程安全的、带有去重功能的 Set 中，然后输出 Set 中元素个数。 正常情况下，以上代码输出结果应该是 100。但是实际执行结果是一个小于 100 的数字。 原因就是因为 SimpleDateFormat 作为一个非线程安全的类，被当做了共享变 量在多个线程中进行使用，这就出现了线程安全问题。 线程不安全原因通过以上代码，我们发现了在并发场景中使用 SimpleDateFormat 会有线程安 全问题。其实，JDK 文档中已经明确表明了 SimpleDateFormat 不应该用在多线程 场景中: Date formats are not synchronized. It is recommended to create separate format instances for each thread. If multiple threads access a format concurrently, it must be synchronized externally. 那么接下来分析下为什么会出现这种问题，SimpleDateFormat 底层到底是怎 么实现的?我们跟一下 SimpleDateFormat 类中 format 方法的实现其实就能发现端倪。 123456789101112131415161718192021222324252627282930313233// Called from Format after creating a FieldDelegate private StringBuffer format(Date date, StringBuffer toAppendTo, FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo; &#125; SimpleDateFormat 中的 format 方法在执行过程中，会使用一个成员变量 calendar 来保存时间。这其实就是问题的关键。 由于我们在声明 SimpleDateFormat 的时候，使用的是 static 定义的。那么 这 个 SimpleDateFormat就是一个共享变量， 随 之，SimpleDateFormat 中 的 calendar 也就可以被多个线程访问到。 假设线程 1 刚刚执行完 calendar.setTime 把时间设置成 2018-11-11，还 没等执行完，线程 2 又执行了 calendar.setTime 把时间改成了 2018-12-12。 这时候线程 1 继续往下执行，拿到的 calendar.getTime 得到的时间就是线程 2 改 过之后的。 除了 format 方法以外，SimpleDateFormat 的 parse 方法也有同样的问题。 所以，不要把 SimpleDateFormat 作为一个共享变量使用。 如何解决线程安全问题 使用局部变量 不要使用static 加同步锁12345678910111213141516for (int i = 0; i &lt; 100; i++) &#123; // 获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // 加锁 synchronized (simpleDateFormat) &#123; // 时间增加 calendar.add(Calendar.DATE, finalI); // 通过 simpleDateFormat 把时间转换成字符串 String dateString = simpleDateFormat.format(calendar.getTime()); // 把字符串放入 Set 中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125; &#125;); &#125; 其实以上代码还有可以改进的地方，就是可以把锁的粒度再设置的小一点，可以 只对 simpleDateFormat.format 这一行加锁，这样效率更高一些。 使用 ThreadLocal 第三种方式，就是使用 ThreadLocal。 ThreadLocal 可以确保每个线程都可以 得到单独的一个 SimpleDateFormat 的对象，那么自然也就不存在竞争问题了。 12345678910/** * 使用ThreadLocal定义一个全局的SimpleDateFormat */ private static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;() &#123; @Override protected SimpleDateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; // 用法 String dateString = simpleDateFormatThreadLocal.get().format(calendar.getTime()); 用 ThreadLocal 来实现其实是有点类似于缓存的思路，每个线程都有一个独享 的对象，避免了频繁创建对象，也避免了多线程的竞争。 当然，以上代码也有改进空间，就是，其实 SimpleDateFormat 的创建过程可 以改为延迟加载。这里就不详细介绍了。 使用 DateTimeFormatter如果是 Java8 应用，可以使用 DateTimeFormatter 代替 SimpleDateFormat， 这是一个线程安全的格式化工具类。就像官方文档中说的，这个类 simple beautiful strong immutable thread-safe。 123456789// 解析日期String dateStr = &quot;2016年10月25日&quot;;DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日&quot;);LocalDate date = LocalDate.parse(dateStr, formatter);// 日期转换为字符串LocalDateTime now = LocalDateTime.now();DateTimeFormatter format = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日 hh:mm a&quot;);String nowStr = now.format(format);System.out.println(nowStr); 总结本 文 介 绍 了 SimpleDateFormat 的 用 法，SimpleDateFormat 主 要 可 以 在 String 和 Date 之间做转换，还可以将时间转换成不同时区输出。同时提到在并发场 景中 SimpleDateFormat 是不能保证线程安全的，需要开发者自己来保证其安全性。 主要的几个手段有改为局部变量、使用 synchronized 加锁、使用 Threadlocal 为每一个线程单独创建一个等。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java并发编程-多线程的发展和意义","date":"2021-07-21T11:45:01.000Z","path":"wiki/Java并发编程-多线程的发展和意义/","text":"线程基础概念什么是线程线程是CPU执行任务的基本单位，一个进程中包含一个或者多个线程，一个进程内的多个线程共享进程的资源，每一个线程有自己的独立内存，是线程不共享的。 并行与并发 并行 同一时刻，横向有多少个线程可以运行 并发 系统和服务器同一时刻能够承受的并发线程 线程的特征 异步（不需要等待） 比如说注册之后发送验证码，验证码的过程可以异步去做不需要客户去在注册接口等待这个时间； 并行（CPU核数） Java中线程的使用 继承Thread 实现Runnalbe 实现Callable/Future 线程原理1234567public class ThreadDemo extend Thread&#123; int a = 0; public void run()&#123; int b = 0; b = a + 1; &#125;&#125; 执行start方法，其实是调用JVM相关的指令， thread.cpp java thread.start() -&gt; cpp thread.start() -&gt; os指令:create.thread start.thread操作系统层面会创建线程，线程创建之后，线程可以启动，（线程启动之后并不一定马上执行）这些线程统一有CPU调度算法来处理；决定那个线程分配给那个执行CPU；CPU执行线程任务的时候，会调用run方法 -&gt; cpp run方法 -&gt; java thread.run() ⚠️ CompletableFuture 异步回调通知，基于Future的优化 线程的生命周期线程创建，当线程中的指令执行完成之后，run（）结束 线程销毁其他线程状态 等待状态 （sleep join wait） 锁阻塞状态 （blocked 竞争锁失败 park） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ThreadStatusDemo &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;time waitting&quot;).start(); new Thread(()-&gt;&#123; while (true)&#123; synchronized (ThreadStatusDemo.class)&#123; try &#123; ThreadStatusDemo.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;,&quot;waitting&quot;).start(); new Thread(new BlockDemo(),&quot;block demo 1&quot;).start(); new Thread(new BlockDemo(),&quot;block demo 2&quot;).start(); &#125; static class BlockDemo extends Thread&#123; @Override public void run() &#123; synchronized (BlockDemo.class)&#123; while (true)&#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 查看线程状态 12jps -ljstack pid 线程如何停止interrupt() 停止线程主动停止方式 -&gt; run方法执行结束被动停止方式 一般中断线程是在无法控制线程的情况下，比如线程wait ， 线程sleep ， 线程while(true)Thread.currnetThread().isInterrupted() stop方法停止线程 禁止使用 相当于kill线程 不友好 interrupt 功能 唤醒阻塞状态的线程 修改中断标志，false -&gt; true 问题排查","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"为什么禁止开发人员修改 serialVersionUID 字段的值?","date":"2021-07-21T09:47:46.000Z","path":"wiki/为什么禁止开发人员修改-serialVersionUID-字段的值/","text":"序列化是一种对象持久化的手段。普遍应用在网络传输、RMI 等场景中。类通 过实现 java.io.Serializable 接口以启用其序列化功能。Java 对象的序列化与反序列化、深入分析 Java 的序列化与反序列化、单例与 序列化的那些事儿 在这几篇文章中，分别介绍过了序列化涉及到的类和接口、如何自定义序列化 策略、transient 关键字和序列化的关系等，还通过学习 ArrayList 对序列化的实现源 码深入学习了序列化。并且还拓展分析了一下序列化对单例的影响等。但是，还有一个知识点并未展开介绍，那就是关于 serialVersionUID 。这个 字段到底有什么用?如果不设置会怎么样?为什么《Java 开发手册》中有以下规定: 背景知识Serializable 和 Externalizable类通过实现 java.io.Serializable 接口以启用其序列化功能。未实现此接 口的类将无法进行序列化或反序列化。可序列化类的所有子类型本身都是可序列 化的。如果读者看过 Serializable 的源码，就会发现，他只是一个空的接口，里 面什么东西都没有。Serializable 接口没有方法或字段，仅用于标识可序列化的 语义。但是，如果一个类没有实现这个接口，想要被序列化的话，就会抛出 java. io.NotSerializableException 异常。 它是怎么保证只有实现了该接口的方法才能进行序列化与反序列化的呢?原因是在执行序列化的过程中，会执行到以下代码: 12345678910111213141516if (obj instanceof String) &#123; writeString((String) obj, unshared); &#125; else if (cl.isArray()) &#123; writeArray(obj, desc, unshared); &#125; else if (obj instanceof Enum) &#123; writeEnum((Enum&lt;?&gt;) obj, desc, unshared); &#125; else if (obj instanceof Serializable) &#123; writeOrdinaryObject(obj, desc, unshared); &#125; else &#123; if (extendedDebugInfo) &#123; throw new NotSerializableException( cl.getName() + &quot;\\n&quot; + debugInfoStack.toString()); &#125; else &#123; throw new NotSerializableException(cl.getName()); &#125; &#125; 在进行序列化操作时，会判断要被序列化的类是否是 Enum、Array 和 Serializable 类型，如果都不是则直接抛出 NotSerializableException。Java 中还提供了 Externalizable 接口，也可以实现它来提供序列化能力。 Externalizable 继承自 Serializable，该接口中定义了两个抽象方法: writeExternal() 与 readExternal()。当使用 Externalizable 接口来进行序列化与反序列化的时候需要开发人员重 写 writeExternal() 与 readExternal() 方法。否则所有变量的值都会变成默认值。 transienttransient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可 以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。 自定义序列化策略在序列化过程中，如果被序列化的类中定义了 writeObject 和 readObject 方法， 虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的 序列化和反序列化。 如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteOb- ject 方法以及 ObjectInputStream 的 defaultReadObject 方法。用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程， 比如可以在序列化的过程中动态改变序列化的数值。 所以，对于一些特殊字段需要定义序列化的策略的时候，可以考虑使用 tran- sient 修 饰， 并 自 己 重 写 writeObject 和 readObject 方 法， 如 java.util. ArrayList 中就有这样的实现。 我们随便找几个 Java 中实现了序列化接口的类，如 String、Integer 等，我们 可以发现一个细节，那就是这些类除了实现了 Serializable 外，还定义了一个 serialVersionUID 那么，到底什么是 serialVersionUID 呢?为什么要设置这样一个字段呢? 什么是 serialVersionUID序列化是将对象的状态信息转换为可存储或传输的形式的过程。我们都知道， Java 对象是保存在 JVM 的堆内存中的，也就是说，如果 JVM 堆不存在了，那么对 象也就跟着消失了。 而序列化提供了一种方案，可以让你在即使 JVM 停机的情况下也能把对象保存 下来的方案。就像我们平时用的 U 盘一样。把 Java 对象序列化成可存储或传输的形 式(如二进制流)，比如保存在文件中。这样，当再次需要这个对象的时候，从文件中 读取出二进制流，再从二进制流中反序列化出对象。 虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重 要的一点是两个类的序列化 ID 是否一致，这个所谓的序列化 ID，就是我们在代码中 定义的 serialVersionUID。 如果 serialVersionUID 变了会怎样我们举个例子吧，看看如果 serialVersionUID 被修改了会发生什么? 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SerializableDemo1 &#123; public static void main(String[] args) &#123;//Initializes The Object User1 user = new User1(); user.setName(&quot;hollis&quot;); //Write Obj to File ObjectOutputStream oos = null; try &#123; oos = new ObjectOutputStream(new FileOutputStream(&quot;tempFile&quot;)); oos.writeObject(user); &#125; catch( IOException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(oos); &#125; &#125; &#125; class User1 implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125;``` 我们先执行以上代码，把一个 User1 对象写入到文件中。然后我们修改一下 User1 类，把 serialVersionUID 的值改为 2L。```javaclass User1 implements Serializable &#123; private static final long serialVersionUID = 2L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 然后执行以下代码，把文件中的对象反序列化出来: 1234567891011121314151617181920212223242526272829 public class SerializableDemo2 &#123; public static void main(String[] args) &#123;//Read Obj from File File file = new File(&quot;tempFile&quot;); ObjectInputStream ois = null; try &#123; ois = new ObjectInputStream(new FileInputStream(file)); User1 newUser = (User1) ois.readObject(); System.out.println(newUser); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(ois); try &#123; FileUtils.forceDelete(file); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;``` 执行结果如下```javajava.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdescserialVersionUID = 1, local class serialVersionUID = 2 可以发现，以上代码抛出了一个 java.io.InvalidClassException，并且 指出 serialVersionUID 不一致。 这是因为，在进行反序列化时，JVM 会把传来的字节流中的 serialVersio- nUID 与本地相应实体类的 serialVersionUID 进行比较，如果相同就认为是一致 的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即是 Invalid- CastException。 这也是《Java 开发手册》中规定，在兼容性升级中，在修改类的时候，不要 修改 serialVersionUID 的原因。除非是完全不兼容的两个版本。所以，serialVersionUID 其实是验证版本一致性的。 如果读者感兴趣，可以把各个版本的 JDK 代码都拿出来看一下，那些向下兼容 的类的 serialVersionUID 是没有变化过的。比如 String 类的 serialVersionUID一直都是 -6849794470754667710L。 但是，作者认为，这个规范其实还可以再严格一些，那就是规定:如果一个类实现了 Serializable 接口，就必须手动添加一个 private static final long serialVersionUID变量，并且设置初始值。 为什么要明确定一个 serialVersionUID如果我们没有在类中明确的定义一个 serialVersionUID 的话，看看会发生什么。 尝试修改上面的 demo 代码，先使用以下类定义一个对象，该类中不定义 serialVersionUID，将其写入文件。 1234567891011class User1 implements Serializable &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 然后我们修改 User1 类，向其中增加一个属性。在尝试将其从文件中读取出来， 并进行反序列化。 12345678910111213141516class User1 implements Serializable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125;&#125; 执 行 结 果:java.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdesc serialVersionUID = -2986778152837257883, local class serialVersionUID = 7961728318907695402 同样，抛出了 InvalidClassException，并且指出两个 serialVersio- nUID 不同，分别是 -2986778152837257883 和 7961728318907695402。从这里可以看出，系统自己添加了一个 serialVersionUID。 所以，一旦类实现了 Serializable，就建议明确的定义一个 serialVersionUID。不然在修改类的时候，就会发生异常。 serialVersionUID 有两种显示的生成方式: 一是默认的1L，比如:private static final long serialVersionUID = 1L;二是根据类名、接口名、成员方法及属性等来生成一个 64 位的哈希字段，比如:private static final long serialVersionUID = xxxxL; 小结serialVersionUID 是用来验证版本一致性的。所以在做兼容性升级的时候， 不要改变类中 serialVersionUID 的值。 如果一个类实现了 Serializable 接口，一定要记得定义 serialVersionUID，否则会发生异常。可以在 IDE 中通过设置，让他帮忙提示，并且可以一键快速生成一 个 serialVersionUID。 之所以会发生异常，是因为反序列化过程中做了校验，并且如果没有明确定义的 话，会根据类的属性自动生成一个。 参考资料 Java技术灵魂15问","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"极尽HashMap底层原理","date":"2021-07-21T06:26:25.000Z","path":"wiki/极尽HashMap底层原理/","text":"HashMap 中的容量与扩容实现，细致入微，值的一品 Java 8系列之重新认识HashMap 美团技术团队 HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(&quot;美团&quot;,&quot;小美&quot;); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下: int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考[http://blog.csdn.net/liuqiyao_01/article/details/14475159]，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考[http://blog.csdn.net/v_july_v/article/details/6105630]。 实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125; &#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;，&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， &quot;C&quot;); new Thread(&quot;Thread1&quot;) &#123; public void run() &#123; map.put(7, &quot;B&quot;); System.out.println(map); &#125;; &#125;.start(); new Thread(&quot;Thread2&quot;) &#123; public void run() &#123; map.put(3, &quot;A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。(5) 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"elasticsearch字符串查询汇总","date":"2021-07-20T14:08:29.000Z","path":"wiki/elasticsearch字符串查询汇总/","text":"filterexistsfuzzyidsprefixregexptermtermsterms_setwildcardtext搜索 intervalmatchmatch_bool_prefixmatch_phrasematch_phrase_prefixmulti_matchcommonquery_stringsimple_query_string 参考资料 查询是否包含字符串_十九种Elasticsearch字符串搜索方式终极介绍","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch 查询值前缀不包含某个字符串","date":"2021-07-20T13:48:31.000Z","path":"wiki/elasticsearch-查询值前缀不包含某个字符串/","text":"需求 查询IP不是以11.开头的所有文档，然后获取文档访问量前100条 curl -X GET &quot;localhost:9200/yj_visit_data2,yj_visit_data3/_search?pretty&quot; -u elastic:elastic -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;11.&quot; &#125; &#125; &#125;, &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;1.&quot; &#125; &#125; &#125; ] &#125; &#125; ], &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;visitTime&quot;: &#123; &quot;gte&quot;: 1577808000000, &quot;lte&quot;: 1609430399000 &#125; &#125; &#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;term_article&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;ip&quot;, &quot;min_doc_count&quot;: 20, &quot;size&quot;: 10000 &#125; &#125; &#125;&#125;&#x27;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"https协议","date":"2021-07-20T08:30:45.000Z","path":"wiki/https协议/","text":"参考资料 《 HTTPS 升级指南 》 深入理解 HTTPS 原理、过程与实践 HTTPS实现原理 深入理解HTTPS原理、过程与实践","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"elasticsearch-reindex","date":"2021-07-20T03:59:18.000Z","path":"wiki/elasticsearch-reindex/","text":"reindex 常规使用 Reindex要求为源索引中的所有文档启用_source。Reindex不尝试设置目标索引，它不复制源索引的设置，你应该在运行_reindex操作之前设置目标索引，包括设置映射、碎片计数、副本等。 如下示例将把文档从twitter索引复制到new_twitter索引： 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 下面是返回值： 12345678910111213141516171819&#123; &quot;took&quot; : 299, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 2, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 就像_update_by_query一样，_reindex获取源索引的快照，但它的目标必须是不同的索引，因此不太可能发生版本冲突。可以像index API那样配置dest元素来控制乐观并发控制。仅仅省略version_type(如上所述)或将其设置为internal，都会导致Elasticsearch盲目地将文档转储到目标中，覆盖任何碰巧具有相同类型和id的文档 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot;, &quot;version_type&quot;: &quot;internal&quot; &#125;&#125;&#x27; 将version_type设置为external将导致Elasticsearch保存源文件的版本，创建任何缺失的文档，并更新目标索引中比源索引中版本更旧的文档： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;&#125;&#x27; 设置op_type=create将导致_reindex只在目标索引中创建缺失的文档。所有现有文件将导致版本冲突： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 默认情况下，版本冲突将中止_reindex进程，“conflicts”请求体参数可用于指示_reindex处理关于版本冲突的下一个文档，需要注意的是，其他错误类型的处理不受“conflicts”参数的影响，当在请求体中设置“conflicts”:“proceed”时，_reindex进程将继续处理版本冲突，并返回所遇到的版本冲突计数： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 返回值如下： 12345678910111213141516171819&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 2, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 可以通过向源添加查询来限制文档。这将只复制由kimchy发出的tweet到new_twitter： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; source中的index可以是一个列表，允许你在一个请求中从多个源复制。这将从twitter和blog索引复制文档： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: [&quot;twitter&quot;,&quot;blog&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; 注意：Reindex API不处理ID冲突，因此最后编写的文档将“胜出”，但顺序通常是不可预测的，因此依赖这种行为不是一个好主意，相反，可以使用脚本确保id是惟一的。还可以通过设置大小来限制处理文档的数量，示例将只复制一个单一的文件从twitter到new_twitter： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 1, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 如果你想从twitter索引中获得一组特定的文档，你需要使用sort。排序会降低滚动的效率，但在某些上下文中，这样做是值得的。如果可能的话，选择一个比大小和排序更具选择性的查询。这将把10000个文档从twitter复制到new_twitter： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10000, &quot;source&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;sort&quot;: &#123; &quot;age&quot;: &quot;desc&quot; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; source部分支持搜索请求中支持的所有元素。例如，只有原始文档中的一部分字段可以使用源过滤重新索引，如下所示： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;_source&quot;: [&quot;user&quot;, &quot;_doc&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 与_update_by_query一样，_reindex支持修改文档的脚本。与_update_by_query不同，脚本允许修改文档的元数据。这个例子改变了源文档的版本： 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;if (ctx._source.foo == \\&quot;bar\\&quot;) &#123;ctx._version++; ctx._source.remove(\\&quot;foo\\&quot;)&#125;&quot;, &quot;lang&quot;: &quot;painless&quot; &#125;&#125;&#x27; 就像在_update_by_query中一样，你可以设置ctx.op更改在目标索引上执行的操作，值为noop，delete。设置ctx.op到任何其他字段都会返回一个错误，在ctx中设置任何其他字段也是如此。可以修改以下值：_id、_index、_version、_routing。将_version设置为null或将它从ctx映射中清除，就像没有在索引请求中发送版本一样;它将导致在目标索引中覆盖文档，而不管目标上的版本或在_reindex请求中使用的版本类型。默认情况下，如果_reindex看到一个带有路由的文档，那么该路由将被保留，除非脚本更改了它，你可以设置路由对dest的请求，以改变这一点：keep：将为每个匹配发送的批量请求上的路由设置为匹配上的路由。这是默认值。discard：将为每个匹配发送的批量请求上的路由设置为null。=：将为每个匹配发送的批量请求上的路由设置为=之后的所有文本。例如，你可以使用以下请求将所有文档从具有公司名称cat的源索引复制到路由设置为cat的dest索引中： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;company&quot;: &quot;cat&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; 默认情况下，_reindex使用滚动批次为1000，可以使用源元素中的size字段更改批大小： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 100 &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; _reindex还可以通过像这样指定管道来使用Ingest节点特性： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;pipeline&quot;: &quot;some_ingest_pipeline&quot; &#125;&#125;&#x27; 远程reindex12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;username&quot;: &quot;user&quot;, &quot;password&quot;: &quot;pass&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; host参数必须包含scheme, host, port（如：http://otherhost:9200）,也可以加路径（如：http://otherhost:9200/proxy），username和password是可选的，如果远程集群开启了安全认证，那么是必选的，如果需要使用username和password，需要使用https。远程主机需要设置白名单，可以通过elasticsearch.yml文件里的reindex.remote.whitelist属性进行设置，如果设置多个值可以使用逗号来进行分隔（如：otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*），这里的配置可以忽略scheme，如： reindex.remote.whitelist: &quot;otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*&quot; 必须让每个处理reindex的节点上添加白名单的配置。这个特性应该适用于可能找到的任何版本的Elasticsearch的远程集群，这应该允许通过从旧版本的集群reindex，将Elasticsearch的任何版本升级到当前版本。要启用发送到旧版本Elasticsearch的查询，无需验证或修改即可将查询参数直接发送到远程主机，注意：远程reindex不支持手动或者自动slicing。从远程服务器reindex使用堆上缓冲区，默认最大大小为100mb，如果远程索引包含非常大的文档，则需要使用更小的批处理大小，下面的示例将批处理大小设置为10。 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 还可以使用socket_timeout字段设置远程连接上的套接字读取超时，使用connect_timeout字段设置连接超时，他们的默认值为30秒，下面示例例将套接字读取超时设置为1分钟，连接超时设置为10秒： 12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;socket_timeout&quot;: &quot;1m&quot;, &quot;connect_timeout&quot;: &quot;10s&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 配置SSL参数远程reindex支持配置SSL参数，除了在Elasticsearch秘钥库中添加安全设置之外，还需要在elasticsearch.yml文件中进行配置，不可能在_reindex请求体中配置。支持以下设置：reindex.ssl.certificate_authorities应受信任的PEM编码证书文件的路径列表，不同同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.path包含要信任的证书的Java密钥存储文件的路径，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.secure_password一起使用。reindex.ssl.truststore.secure_passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.password一起使用。reindex.ssl.truststore.typereindex.ssl.truststore.path信任存储库的类型，必须是jks或PKCS12，如果reindex.ssl.truststore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.verification_mode指示用于防止中间人攻击和伪造证书的验证类型。可以设置为full（验证主机名和证书路径）、certificate（验证证书路径，但不验证主机名）、none（不执行验证——这在生产环境中是强烈不鼓励的），默认是full。reindex.ssl.certificate指定PEM编码证书的路径或者证书链用于HTTP客户端身份认证，这个配置还需要设置reindex.ssl.key值，不能同时设置reindex.ssl.certificate和reindex.ssl.keystore.path。reindex.ssl.key指定与用于客户端身份验证的证书相关联的PEM编码私钥的路径，不能同时设置reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.secure_key_passphrase一起使用。reindex.ssl.secure_key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.key_passphrase一起使用。reindex.ssl.keystore.path指定密钥存储库的路径，其中包含用于HTTP客户机身份验证的私钥和证书(如果远程集群需要)，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.keystore.type密钥存储库的类型(reindex.ssl.keystore.path)，必须是jks或者PKCS12，如果reindex.ssl.keystore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.keystore.password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.secure_password一起使用。reindex.ssl.keystore.secure_password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.password一起使用。reindex.ssl.keystore.key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.secure_key_password一起使用。reindex.ssl.keystore.secure_key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.key_password一起使用。 URL参数除了标准的pretty参数外， reindex还支持refresh, wait_for_completion, wait_for_active_shards, timeout, scroll和requests_per_second。发送refresh url参数将导致对所写请求的所有索引进行刷新，这与Index API的refresh参数不同，后者只会刷新接收新数据的碎片，与index API不同的是，它不支持wait_for。如果请求包含wait_for_completion=false，则Elasticsearch将执行一些执行前检查，启动请求，然后返回一个任务，该任务可与Tasks api一起用于取消或获取任务状态，Elasticsearch还将创建此任务的记录，作为.tasks/task/${taskId}的文档，你可以自己决定是保留或者删除他，当你已经完成了，删除他，这样es会回收他使用的空间。wait_for_active_shards控制在进行重新索引之前必须激活多少个shard副本，超时控制每个写请求等待不可用碎片变为可用的时间，两者在批量API中的工作方式完全相同，由于_reindex使用滚动搜索，你还可以指定滚动参数来控制“搜索上下文”存活的时间(例如?scroll=10m)，默认值是5分钟。requests_per_second可以设置为任何正数(1.4、6、1000等)，并通过在每个批中填充等待时间来控制_reindex发出批索引操作的速率，可以通过将requests_per_second设置为-1来禁用。节流是通过在批之间等待来完成的，这样就可以给_reindex内部使用的滚动设置一个考虑填充的超时，填充时间是批大小除以requests_per_second和写入时间之间的差额，默认情况下批处理大小为1000，所以如果requests_per_second被设置为500： target_time = 1000 / 500 per second = 2 seconds padding time = target_time - write_time = 2 seconds - 0.5 seconds = 1.5 seconds 由于批处理是作为单个_bulk请求发出的，因此较大的批处理大小将导致Elasticsearch创建许多请求，然后等待一段时间再启动下一个请求集，这是“bursty”而不是“smooth”，默认值是-1。 响应体 12345678910111213141516171819&#123; &quot;took&quot;: 639, &quot;timed_out&quot;: false, &quot;total&quot;: 5, &quot;updated&quot;: 0, &quot;created&quot;: 5, &quot;deleted&quot;: 0, &quot;batches&quot;: 1, &quot;noops&quot;: 0, &quot;version_conflicts&quot;: 2, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: 1, &quot;throttled_until_millis&quot;: 0, &quot;failures&quot;: []&#125; took整个操作花费的总毫秒数。timed_out如果在reindex期间执行的任何请求超时，则将此标志设置为true。total成功处理的文档数量。updated成功更新的文档数量。created成功创建的文档的数量。deleted成功删除的文档数量。batches由reindex回拉的滚动响应的数量。noops由于用于reindex的脚本返回了ctx.op的noop值而被忽略的文档数量。version_conflictsreindex命中的版本冲突数。retriesreindex尝试重试的次数,bulk是重试的批量操作的数量，search是重试的搜索操作的数量。throttled_millis请求休眠以符合requests_per_second的毫秒数。requests_per_second在reindex期间每秒有效执行的请求数。throttled_until_millis在_reindex响应中，该字段应该始终等于零，它只有在使用任务API时才有意义，在任务API中，它指示下一次(毫秒)再次执行节流请求，以符合requests_per_second。failures出现多个错误以数组返回。 使用task api获取task 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&amp;wait_for_completion=false&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 返回值为 123&#123; &quot;task&quot; : &quot;8uQK-B00RiWq03awtJok1Q:18&quot;&#125; 你可以用任务API获取所有正在运行的reindex请求的状态：curl -XGET &quot;http://127.0.0.1:9200/_tasks?detailed=true&amp;actions=*reindex&amp;pretty&quot;或者根据task id获取curl -XGET &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48?pretty&quot;返回值为： 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;completed&quot; : true, &quot;task&quot; : &#123; &quot;node&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;id&quot; : 48, &quot;type&quot; : &quot;transport&quot;, &quot;action&quot; : &quot;indices:data/write/reindex&quot;, &quot;status&quot; : &#123; &quot;total&quot; : 0, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 0, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : 0.0, &quot;throttled_until_millis&quot; : 0 &#125;, &quot;description&quot; : &quot;reindex from [twitter] to [new_twitter][_doc]&quot;, &quot;start_time_in_millis&quot; : 1566216815832, &quot;running_time_in_nanos&quot; : 86829, &quot;cancellable&quot; : true, &quot;headers&quot; : &#123; &#125; &#125;, &quot;error&quot; : &#123; &quot;type&quot; : &quot;index_not_found_exception&quot;, &quot;reason&quot; : &quot;no such index [new_twitter] and [action.auto_create_index] ([twitter,index10,-index1*,+ind*,-myIndex]) doesn&#x27;t match&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;new_twitter&quot; &#125;&#125; 取消task任何reindex接口都可以使用task cancel api取消： 123456789101112131415 curl -XPOST &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48/_cancel?pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:48] doesn&#x27;t support cancellation&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; 取消应该很快发生，但可能需要几秒钟，Tasks API将继续列出任务，直到它醒来取消自己。 rethrottle可以在url中使用_rethrottle，并使用requests_per_second参数来设置节流： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex/8uQK-B00RiWq03awtJok1Q:250/_rethrottle?requests_per_second=-1&amp;pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:250] is missing&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; reindex改变属性名称_reindex可以重命名属性名，假设你创建了一个包含如下文档的索引： 12345curl -XPOST &quot;http://127.0.0.1:9200/test/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;text&quot;: &quot;words words&quot;, &quot;flag&quot;: &quot;foo&quot;&#125;&#x27; 在reindex的时候想把flag修改为tag，示例如： 12345678910111213curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.tag = ctx._source.remove(\\&quot;flag\\&quot;)&quot; &#125;&#125;&#x27; 查看order2的数据： 123456789101112131415curl -XGET &quot;http://127.0.0.1:9200/order2/_doc/1?pretty&quot;&#123; &quot;_index&quot; : &quot;order2&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;words words&quot;, &quot;tag&quot; : &quot;foo&quot; &#125;&#125; 切片Reindex支持切片滚动，以并行化重新索引过程。这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分 手动切片通过为每个请求提供一个片id和片的总数，手工切片一个重索引请求： 123456789101112131415161718192021222324252627curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 0, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27;curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 1, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 你可以通过以下方法来验证： curl -XGET &quot;http://127.0.0.1:9200/_refresh?pretty&quot; curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot; 返回值为： 123456789&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; 自动切面你还可以让_reindex使用切片滚动自动并行化_uid上的切片，使用slices指定要使用的片数: 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?slices=5&amp;refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 通过下面请求进行验证： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot;&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; reindex多个索引如果有许多索引需要reindex，通常最好一次reindex一个索引，而不是使用一个glob模式来获取许多索引。这样，如果有任何错误，可以删除部分完成的索引并从该索引重新开始，从而恢复该过程。它还使并行化过程变得相当简单：将索引列表拆分为reindex并并行运行每个列表。可以使用一次性脚本： 1234567891011for index in i1 i2 i3 i4 i5; do curl -HContent-Type:application/json -XPOST localhost:9200/_reindex?pretty -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;-reindexed&quot; &#125; &#125;&#x27;done reindex每日索引尽管有上述建议，你仍然可以结合使用_reindex和Painless来reindex每日索引，从而将新模板应用于现有文档。假设有以下文件组成的索引: curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.30/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d{“system.cpu.idle.pct”: 0.908}’ curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.31/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#39;{“system.cpu.idle.pct”: 0.105} metricbeat-*索引的新模板已经加载到Elasticsearch中，但它只适用于新创建的索引。下面的脚本从索引名称中提取日期，并创建一个附加-1的新索引。所有来自metricbeat-2016.05.31的数据将reindex到metricbeat-2016.05.31-1。 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;metricbeat-*&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;metricbeat&quot; &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._index = &#x27;metricbeat-&#x27; + (ctx._index.substring(&#x27;metricbeat-&#x27;.length(),ctx._index.length())) + &#x27;-1&#x27;&quot; &#125;&#125;&#x27; 以前metricbeat索引中的所有文档现在都可以在*-1索引中找到。 curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.30-1/_doc/1?pretty&quot;curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.31-1/_doc/1?pretty&quot; 前一种方法还可以与更改字段名称结合使用，以便仅将现有数据加载到新索引中，并在需要时重命名任何字段。 提取索引中的子集合_reindex可用于提取索引的随机子集进行测试： 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;function_score&quot; : &#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;random_score&quot; : &#123;&#125; &#125; &#125;, &quot;sort&quot;: &quot;_score&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;random_twitter&quot; &#125;&#125;&#x27; _reindex默认按_doc排序，因此random_score不会有任何效果，除非你覆盖sort属性为_score。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"Http状态码及含义","date":"2021-07-20T03:31:58.000Z","path":"wiki/Http状态码及含义/","text":"http状态码由3个十进制数字组成。第一个数字表示状态码的分类，后面的两位表示该分类下不同的状态。分为5个大类。 分类 1** 信息。服务器收到请求，请继续执行请求 2** 成功。请求被成功接收并处理 3** 重定向。需要进一步操作来完成请求 4** 客户端错误。无法完成请求，或请求包含语法错误 5** 服务器错误。服务器在处理请求的过程中发成错误 各个状态说明 100： 继续请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分。 101： 切换协议请求者已要求服务器切换协议，服务器已确认并准备切换。 200： 成功服务器已成功处理了请求。 201： 已创建请求成功并且服务器创建了新的资源。 202： 已接受服务器已接受请求，但尚未处理。 203： 非授权信息服务器已成功处理了请求，但返回的信息可能来自另一来源。 204： 无内容服务器成功处理了请求，但没有返回任何内容。 205： 重置内容服务器成功处理了请求，内容被重置。 206： 部分内容服务器成功处理了部分请求。 300： 多种选择针对请求，服务器可执行多种操作。 301： 永久移动请求的网页已永久移动到新位置，即永久重定向。 302： 临时移动请求的网页暂时跳转到其他页面，即暂时重定向。 303： 查看其他位置如果原来的请求是 POST，重定向目标文档应该通过 GET 提取。 304： 未修改此次请求返回的网页未修改，继续使用上次的资源。 305： 使用代理请求者应该使用代理访问该网页。 307： 临时重定向请求的资源临时从其他位置响应。 400： 错误请求服务器无法解析该请求。 401： 未授权请求没有进行身份验证或验证未通过。 403： 禁止访问服务器拒绝此请求。 404： 未找到服务器找不到请求的网页。 405： 方法禁用服务器禁用了请求中指定的方法。 406： 不接受无法使用请求的内容响应请求的网页。 407： 需要代理授权请求者需要使用代理授权。 408： 请求超时服务器请求超时。 409： 冲突服务器在完成请求时发生冲突。 410： 已删除请求的资源已永久删除。 411： 需要有效长度服务器不接受不含有效内容长度标头字段的请求。 412： 未满足前提条件服务器未满足请求者在请求中设置的其中一个前提条件。 413： 请求实体过大请求实体过大，超出服务器的处理能力。 414： 请求 URI 过长请求网址过长，服务器无法处理。 415： 不支持类型请求的格式不受请求页面的支持。 416： 请求范围不符页面无法提供请求的范围。 417： 未满足期望值服务器未满足期望请求标头字段的要求。 500： 服务器内部错误服务器遇到错误，无法完成请求。 501： 未实现服务器不具备完成请求的功能。 502： 错误网关服务器作为网关或代理，从上游服务器收到无效响应。 503： 服务不可用服务器目前无法使用。 504： 网关超时服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505： HTTP 版本不支持服务器不支持请求中所用的 HTTP 协议版本。","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"操作系统-死锁","date":"2021-07-20T03:29:18.000Z","path":"wiki/操作系统-死锁/","text":"造成死锁的原因当前线程拥有其他线程需要的资源，当前线程等待其他线程释放资源，线程持有资源不可剥夺，线程一直循环等待。 避免死锁的方法1、固定加锁的顺序 2、尽可能缩小锁范围，减少锁粒度 3、使用可释放的定时锁（申请一段时间，超时之后，放弃） 参考资料 死锁是什么？如何避免死锁？ 哲学家就餐问题","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"操作系统进程调度策略","date":"2021-07-20T03:16:04.000Z","path":"wiki/操作系统进程调度策略/","text":"操作系统进程调度策略1. 什么是进程进程是操作系统进行资源分配的基本单位，每个进程都有它自己的内存空间和系统资源。进程实现了多处理机环境下进程调度，分派，切换时，都需要花费较大的时间和空间开销； 为了提升系统的执行效率，减少CPU的空转时间和调度切换的时间，以便于系统的管理，所以有了线程的概念，线程是系统进行资源调度的最小单位。一个进程包含一个或者多个线程，一个进程下的线程共享这个进程的资源。进程与进程之间的资源是不共享的，线程和线程之间的资源也是不共享的。 参考资料 操作系统中的进程调度策略有哪几种","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"进程间通信IPC","date":"2021-07-20T01:59:21.000Z","path":"wiki/进程间通信IPC/","text":"参考资料 进程间通信IPC (InterProcess Communication)","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"elasticsearch统计每年每小时访问量","date":"2021-07-19T14:53:39.000Z","path":"wiki/elasticsearch统计每年每小时访问量/","text":"需求背景，要统计文章在一年的时间内，每个小时的访问情况，按照0点举例子，每个文章，一年内每一天0点的访问次数累加起来； Elasticsearch索引如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;c&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ip&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;p&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ua&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;visitTime&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125; &#125;&#125; Java RestHighLevelClient写法12345678910111213141516171819202122232425262728public void getDateDist() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(&quot;yj_visit_data2&quot;); TermsAggregationBuilder termsAggregation = AggregationBuilders.terms(&quot;article&quot;) .field(&quot;article.keyword&quot;).size(2200) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) .calendarInterval(DateHistogramInterval.HOUR)); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(termsAggregation); sourceBuilder.query(QueryBuilders.rangeQuery(&quot;visitTime&quot;).gt(&quot;1609430400000&quot;).lte(&quot;1625068799000&quot;)); sourceBuilder.timeout(new TimeValue(900000)); SearchRequest request = new SearchRequest(); request.source(sourceBuilder); SearchResponse search = restHighLevelClient.search(request, RequestOptions.DEFAULT); Aggregations aggregations = search.getAggregations(); log.info(&quot;agg -&gt; &#123;&#125;&quot;, aggregations.asList().size()); List&lt;? extends Terms.Bucket&gt; buckets = ((ParsedStringTerms) aggregations.asList().get(0)).getBuckets(); List&lt;ArticleHourData&gt; hourDataList = new ArrayList&lt;&gt;(); for (Terms.Bucket bucket : buckets) &#123; List&lt;? extends Histogram.Bucket&gt; innerBuckets = ((ParsedDateHistogram) bucket.getAggregations().asList().get(0)).getBuckets(); hourDataList.add(calcBucket(innerBuckets, bucket.getKeyAsString())); &#125; log.info(&quot;result ----&gt; &#123;&#125;&quot;, JSONObject.toJSONString(hourDataList)); &#125; 聚合分析123456789101112131415161718192021public ArticleHourData calcBucket(List&lt;? extends Histogram.Bucket&gt; innerBuckets, String article) &#123; log.info(&quot;innerBuckets get(0) ---&gt; &#123;&#125;&quot;, JSON.toJSONString(innerBuckets.get(0))); Map&lt;String, ? extends List&lt;? extends Histogram.Bucket&gt;&gt; hourMap = innerBuckets.stream() .collect(Collectors.groupingBy(bucket -&gt; getHour(bucket.getKeyAsString()))); log.info(&quot;collect ======&gt; &#123;&#125; &quot;, JSONObject.toJSONString(hourMap.keySet())); ArticleHourData hourData = ArticleHourData.builder().article(article).build(); if (hourMap.isEmpty()) &#123; return hourData; &#125; HashMap&lt;String, Long&gt; hashMap = new HashMap&lt;&gt;(); for (String hour : hourMap.keySet()) &#123; List&lt;? extends Histogram.Bucket&gt; list = hourMap.get(hour); if (CollectionUtils.isEmpty(list)) &#123; continue; &#125; hashMap.put(hour, list.stream().mapToLong(Histogram.Bucket::getDocCount).sum()); &#125; hourData.setCountMap(hashMap); return hourData; &#125; 获取时间的小时123456789101112private String getHour(String date) &#123; date = date.replace(&quot;Z&quot;, &quot; UTC&quot;); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSS Z&quot;); Date d = null; try &#123; d = format.parse(date); &#125; catch (ParseException e) &#123; e.printStackTrace(); return null; &#125; return String.valueOf(DateUtil.asLocalDateTime(d).getHour()); &#125; Python写法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from json.decoder import JSONDecoderfrom elasticsearch import Elasticsearchimport logging,jsonfrom datetime import datetimees = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;39.107.117.232&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;elastic&#x27;), timeout = 90000)sqs = &#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;art&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot;, &quot;size&quot;: 5 &#125;, &quot;aggs&quot;: &#123; &quot;art_total&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot; &#125; &#125;, &quot;_time&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;visitTime&quot;, &quot;calendar_interval&quot;: &quot;hour&quot; &#125; &#125; &#125; &#125; &#125;&#125;_search_result = es.search(index=&quot;yj_visit_data2&quot; , body=sqs)_result_json = json.dumps(_search_result,sort_keys=True, indent=4, separators=(&#x27;, &#x27;, &#x27;: &#x27;), ensure_ascii=False)aggregations = _search_result[&#x27;aggregations&#x27;]art = aggregations[&#x27;art&#x27;]buckets = art[&#x27;buckets&#x27;]#print(type(buckets)) ; print(buckets)def getHour(time): return (int)(time[11:13])# 计算每个小时的点击数def countByMonth(dataList , hourTar): _count = 0 for data in dataList: timestamp = data[&#x27;key_as_string&#x27;] hour = getHour(timestamp) if hour == hourTar: _count = (int)(data[&#x27;doc_count&#x27;]) + _count return _count final_list = []# 循环计算每一个文章for outBucket in buckets: simple_result = &#123;&#125; _time = outBucket[&#x27;_time&#x27;] innerBuckets = _time[&#x27;buckets&#x27;] print(&quot;time inner bucker size&quot; , len(innerBuckets)) simple_list = [] for num in range(0,24): simple_list.append(countByMonth(innerBuckets,num)) simple_result[0] = outBucket[&#x27;key&#x27;] simple_result[1] = simple_list final_list.append(simple_result)print(&quot;final result ----&gt; &quot;,final_list)","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch踩坑","date":"2021-07-19T09:46:59.000Z","path":"wiki/elasticsearch踩坑/","text":"search.max_bucketsThis limit can be set by changing the [search.max_buckets] cluster level setting.]]; 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;search.max_buckets&quot;: 100000 &#125;&#125; Required one of fields [field, script][Elasticsearch exception [type=illegal_argument_exception, reason=Required one of fields [field, script], but none were specified. ] 123456789101112@Test public void test1() &#123; NativeSearchQuery nativeSearchQuery = new NativeSearchQuery(QueryBuilders.matchAllQuery(), null); Script script = new Script(&quot;doc[&#x27;article.keyword&#x27;]&quot;); nativeSearchQuery.addAggregation(AggregationBuilders.terms(&quot;art&quot;) .field(&quot;article.keyword&quot;).size(10) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) // 这一行没有写 .calendarInterval(DateHistogramInterval.HOUR))); SearchHits&lt;YjVisitData&gt; search = elasticsearchRestTemplate.search(nativeSearchQuery, YjVisitData.class); System.err.println(&quot;search.getAggregations().asList() &#123;&#125;&quot; + search.getAggregations().asList().size()); &#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"python3基本数据类型","date":"2021-07-18T13:05:52.000Z","path":"wiki/python基本数据类型/","text":"Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。 等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值 标准数据类型Python3 中有六个标准的数据类型： Number（数字） String（字符串） List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 Number（数字）Python3 支持 int、float、bool、complex（复数）。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。 123&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class &#x27;int&#x27;&gt; &lt;class &#x27;float&#x27;&gt; &lt;class &#x27;bool&#x27;&gt; &lt;class &#x27;complex&#x27;&gt; 此外还可以用 isinstance 来判断： 1234&gt;&gt;&gt; a = 111&gt;&gt;&gt; isinstance(a, int)True&gt;&gt;&gt; ⚠️ isinstance 和 type 的区别在于：type()不会认为子类是一种父类类型。isinstance()会认为子类是一种父类类型。 ⚠️ 注意：Python3 中，bool 是 int 的子类，True 和 False 可以和数字相加 True==1，False==0 是会返回 Ture，但可以通过 is 来判断类型。1、Python可以同时为多个变量赋值，如a, b = 1, 2。2、一个变量可以通过赋值指向不同类型的对象。3、数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。4、在混合计算时，Python会把整型转换成为浮点数。 String（字符串）Python中的字符串用单引号 ‘ 或双引号 “ 括起来，同时使用反斜杠 \\ 转义特殊字符。字符串的截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 List（列表）List（列表） 是 Python 中使用最频繁的数据类型。 列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号 [] 之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 Tuple（元组）元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： 1234567891011#!/usr/bin/python3tuple = ( &#x27;abcd&#x27;, 786 , 2.23, &#x27;runoob&#x27;, 70.2 )tinytuple = (123, &#x27;runoob&#x27;)print (tuple) # 输出完整元组print (tuple[0]) # 输出元组的第一个元素print (tuple[1:3]) # 输出从第二个元素开始到第三个元素print (tuple[2:]) # 输出从第三个元素开始的所有元素print (tinytuple * 2) # 输出两次元组print (tuple + tinytuple) # 连接元组 Set（集合）集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。 基本功能是进行成员关系测试和删除重复元素。 可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 创建格式：parame = &#123;value01,value02,...&#125; 或者 set(value) 1234567891011121314151617181920212223242526#!/usr/bin/python3sites = &#123;&#x27;Google&#x27;, &#x27;Taobao&#x27;, &#x27;Runoob&#x27;, &#x27;Facebook&#x27;, &#x27;Zhihu&#x27;, &#x27;Baidu&#x27;&#125;print(sites) # 输出集合，重复的元素被自动去掉# 成员测试if &#x27;Runoob&#x27; in sites : print(&#x27;Runoob 在集合中&#x27;)else : print(&#x27;Runoob 不在集合中&#x27;)# set可以进行集合运算a = set(&#x27;abracadabra&#x27;)b = set(&#x27;alacazam&#x27;)print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素 Dictionary（字典）字典（dictionary）是Python中另一个非常有用的内置数据类型。 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。 键(key)必须使用不可变类型。 在同一个字典中，键(key)必须是唯一的。 #!/usr/bin/python3 dict = &#123;&#125; dict[&#39;one&#39;] = &quot;1 - 菜鸟教程&quot; dict[2] = &quot;2 - 菜鸟工具&quot; tinydict = &#123;&#39;name&#39;: &#39;runoob&#39;,&#39;code&#39;:1, &#39;site&#39;: &#39;www.runoob.com&#39;&#125; print (dict[&#39;one&#39;]) # 输出键为 &#39;one&#39; 的值 print (dict[2]) # 输出键为 2 的值 print (tinydict) # 输出完整的字典 print (tinydict.keys()) # 输出所有键 print (tinydict.values()) # 输出所有值 注意：1、字典是一种映射类型，它的元素是键值对。2、字典的关键字必须为不可变类型，且不能重复。3、创建空字典使用 { }。 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python3基础语法","date":"2021-07-18T12:31:55.000Z","path":"wiki/python3基础语法/","text":"编码默认情况下，Python 3 源码文件以 UTF-8 编码，所有字符串都是 unicode 字符串。 当然你也可以为源码文件指定不同的编码：# -*- coding: cp-1252 -*- 标识符 第一个字符必须是字母表中字母或下划线 _ 。 标识符的其他的部分由字母、数字和下划线组成。 标识符对大小写敏感。在 Python 3 中，可以用中文作为变量名，非 ASCII 标识符也是允许的了。 python保留字import keywordkeyword.kwlist[‘False’, ‘None’, ‘True’, ‘and’, ‘as’, ‘assert’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] 注释Python中单行注释以 # 开头,多行注释可以用多个 # 号，还有 ‘’’ 和 “””： 行与缩进python最具特色的就是使用缩进来表示代码块，不需要使用大括号 {} 。缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数。 多行语句Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠 \\ 来实现多行语句，例如： 123total = item_one + \\ item_two + \\ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠 \\，例如： total = [&#39;item_one&#39;, &#39;item_two&#39;, &#39;item_three&#39;, &#39;item_four&#39;, &#39;item_five&#39;] 数字(Number)类型python中数字有四种类型：整数、布尔型、浮点数和复数。 int (整数) , 如 1, 只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 bool (布尔), 如 True。 float (浮点数), 如 1.23、3E-2 complex (复数), 如 1 + 2j、 1.1 + 2.2j 字符串(String) python中单引号和双引号使用完全相同。 使用三引号(‘’’ 或 “””)可以指定一个多行字符串。 转义符 \\ 反斜杠可以用来转义，使用r可以让反斜杠不发生转义。。 如 r”this is a line with \\n” 则\\n会显示，并不是换行。 按字面意义级联字符串，如”this “ “is “ “string”会被自动转换为this is string。 字符串可以用 + 运算符连接在一起，用 * 运算符重复。 Python 中的字符串有两种索引方式，从左往右以 0 开始，从右往左以 -1 开始。 Python中的字符串不能改变。 Python 没有单独的字符类型，一个字符就是长度为 1 的字符串。 字符串的截取的语法格式如下：变量[头下标:尾下标:步长] 空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。 记住：空行也是程序代码的一部分。 同一行显示多条语句Python 可以在同一行中使用多条语句，语句之间使用分号 ; 分割，以下是一个简单的实例： 12#!/usr/bin/python3import sys; x = &#x27;runoob&#x27;; sys.stdout.write(x + &#x27;\\n&#x27;) 多个语句构成代码组缩进相同的一组语句构成一个代码块，我们称之代码组。 像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。 我们将首行及后面的代码组称为一个子句(clause)。 如下实例： 12345678910111213141516171819202122232425if expression : suiteelif expression : suite else : suite``` ## print 输出print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=&quot;&quot;：## import 与 from...import在 python 用 import 或者 from...import 来导入相应的模块。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数,格式为： from somemodule import somefunction从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import *## 命令行参数很多程序可以执行一些操作来查看一些基本信息，Python可以使用-h参数查看各参数帮助信息： $ python -husage: python [option] … [-c cmd | -m mod | file | -] [arg] …Options and arguments (and corresponding environment variables):-c cmd : program passed in as string (terminates option list)-d : debug output from parser (also PYTHONDEBUG=x)-E : ignore environment variables (such as PYTHONPATH)-h : print this help message and exit [ etc. ] ```我们在使用脚本形式执行 Python 时，可以接收命令行输入的参数;","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python操作elasticsearch","date":"2021-07-18T12:20:25.000Z","path":"wiki/python操作elasticsearch/","text":"下载python对应的elasticsearch依赖包pip3 install elasticsearch==7.10.0 python操作elasticsearch代码1234567from elasticsearch import Elasticsearchprint(&quot;init ...&quot;)es = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;XXXXXX&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;XXXXXX&#x27;))# print(es.get(index=&#x27;yj_ip_pool&#x27;, doc_type=&#x27;_doc&#x27;, id=&#x27;9256058&#x27;))countRes = es.count(index=&#x27;yj_ip_pool&#x27;)print(countRes) 查询效果12345gaolei:awesome-python3-webapp gaolei$ /usr/local/opt/python/bin/python3.7 /Users/gaolei/Documents/DemoProjects/awesome-python3-webapp/www/es_test.pyinit ...&#123;&#x27;count&#x27;: 20095400, &#x27;_shards&#x27;: &#123;&#x27;total&#x27;: 1, &#x27;successful&#x27;: 1, &#x27;skipped&#x27;: 0, &#x27;failed&#x27;: 0&#125;&#125;gaolei:awesome-python3-webapp gaolei$","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"kibana添加用户及控制权限","date":"2021-07-15T15:53:44.000Z","path":"wiki/kibana添加用户及控制权限/","text":"操作步骤： 【修改elasticsearch配置文件】 -&gt; 【重启elasticsearch】 -&gt; 【初始化账号&amp;密码】 -&gt; 【修改kibana配置文件】 -&gt; 【重启kibana】 -&gt; 【初始账号登录kibana】 -&gt; 【创建、配置角色】 -&gt; 【创建新用户】 配置elasticsearch开启自带的xpack的验证功能xpack.security.enabled: true 配置单节点模式discovery.type: single-node 坑1: 集群节点配置报错cluster.initial_master_nodes 坑2: 本次存储节点配置报错maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?node.max_local_storage_nodes: 2 为内置账号设置密码在elasticsearch的bin下执行以下命令：./elasticsearch-setup-passwords interactive 配置kibana123#使用初始用户kibanaelasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;密码&quot; 重启kibana之后使用初始账号 elastic 登录 创建、配置角色 创建新用户 查看新用户访问界面角色里面配置了kibana的访问权限，只开通了discover和dashboard两个入口 索引的话，有好几个索引，但是只配置了一个索引的权限 参考资料 kibana7.2添加登录及权限 Elasticsearch 安装","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch稳定性调优","date":"2021-07-15T07:22:49.000Z","path":"wiki/elasticsearch稳定性调优/","text":"Elasticsearch性能优化总结Elasticsearch调优实践 稳定性调优一 Linux参数调优 修改系统资源限制 👇单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些 echo &quot;* - nofile 655360&quot; &gt;&gt;/etc/security/limits.conf单用户内存地址空间 echo &quot;* - as unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户线程数 echo &quot;* - nproc 2056474&quot; &gt;&gt;/etc/security/limits.conf单用户文件大小 echo &quot;* - fsize unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户锁定内存 echo &quot;* - memlock unlimited&quot; &gt;&gt;/etc/security/limits.conf单进程可以使用的最大map内存区域数量 echo &quot;vm.max_map_count = 655300&quot; &gt;&gt;/etc/sysctl.confTCP全连接队列参数设置， 这样设置的目的是防止节点数较多（比如超过100）的ES集群中，节点异常重启时全连接队列在启动瞬间打满，造成节点hang住，整个集群响应迟滞的情况echo &quot;net.ipv4.tcp_abort_on_overflow = 1&quot; &gt;&gt;/etc/sysctl.conf echo &quot;net.core.somaxconn = 2048&quot; &gt;&gt;/etc/sysctl.conf降低tcp alive time，防止无效链接占用链接数 echo 300 &gt;/proc/sys/net/ipv4/tcp_keepalive_time ES节点配置jvm.options-Xms和-Xmx设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给系统cache使用。 jvm内存建议不要低于2G，否则有可能因为内存不足导致ES无法正常启动或OOMjvm建议不要超过32G，否则jvm会禁用内存对象指针压缩技术，造成内存浪费 elasticsearch.yml设置内存熔断参数，防止写入或查询压力过高导致OOM，具体数值可根据使用场景调整。indices.breaker.total.limit: 30% indices.breaker.request.limit: 6% indices.breaker.fielddata.limit: 3% 调小查询使用的cache，避免cache占用过多的jvm内存，具体数值可根据使用场景调整。indices.queries.cache.count: 500 indices.queries.cache.size: 5% 单机多节点时，主从shard分配以ip为依据，分配到不同的机器上，避免单机挂掉导致数据丢失。cluster.routing.allocation.awareness.attributes: ip node.attr.ip: 1.1.1.1 ES使用方式节点数较多的集群，增加专有master，提升集群稳定性ES集群的元信息管理、index的增删操作、节点的加入剔除等集群管理的任务都是由master节点来负责的，master节点定期将最新的集群状态广播至各个节点。所以，master的稳定性对于集群整体的稳定性是至关重要的。当集群的节点数量较大时（比如超过30个节点），集群的管理工作会变得复杂很多。此时应该创建专有master节点，这些节点只负责集群管理，不存储数据，不承担数据读写压力；其他节点则仅负责数据读写，不负责集群管理的工作。 这样把集群管理和数据的写入/查询分离，互不影响，防止因读写压力过大造成集群整体不稳定。 将专有master节点和数据节点的分离，需要修改ES的配置文件，然后滚动重启各个节点。 专有master节点的配置文件（conf/elasticsearch.yml）增加如下属性：node.master: truenode.data: falsenode.ingest: false数据节点的配置文件增加如下属性（与上面的属性相反）：node.master: falsenode.data: truenode.ingest: true 控制index、shard总数量上面提到，ES的元信息由master节点管理，定期同步给各个节点，也就是每个节点都会存储一份。这个元信息主要存储在clusterstate中，如所有node元信息（indices、节点各种统计参数）、所有index/shard的元信息（mapping, location, size）、元数据ingest等。 ES在创建新分片时，要根据现有的分片分布情况指定分片分配策略，从而使各个节点上的分片数基本一致，此过程中就需要深入遍历clusterstate。当集群中的index/shard过多时，clusterstate结构会变得过于复杂，导致遍历clusterstate效率低下，集群响应迟滞。基础架构部数据库团队曾经在一个20个节点的集群里，创建了4w+个shard，导致新建一个index需要60s+才能完成。 当index/shard数量过多时，可以考虑从以下几方面改进： 降低数据量较小的index的shard数量 把一些有关联的index合并成一个index 数据按某个维度做拆分，写入多个集群 Segment Memory优化前面提到，ES底层采用Lucene做存储，而Lucene的一个index又由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。Lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在Lucene4.0以后采用的数据结构是FST (Finite State Transducer)。Lucene加载segment的时候将其全量装载到内存中，加快查询速度。这部分内存被称为SegmentMemory， 常驻内存，占用heap，无法被GC。 前面提到，为利用JVM的对象指针压缩技术来节约内存，通常建议JVM内存分配不要超过32G。当集群的数据量过大时，SegmentMemory会吃掉大量的堆内存，而JVM内存空间又有限，此时就需要想办法降低SegmentMemory的使用量了，常用方法有下面几个： 定期删除不使用的index 对于不常访问的index，可以通过close接口将其关闭，用到时再打开 通过force_merge接口强制合并segment，降低segment数量 基础架构部数据库团队在此基础上，对FST部分进行了优化，释放高达40%的Segment Memory内存空间。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"启动ELK脚本命令","date":"2021-07-14T15:37:17.000Z","path":"wiki/启动ELK脚本命令/","text":"esuser 授权chown -R esuser /usr/local/elasticsearch/* elastcisearch 启动脚本nohup ./elasticsearch-7.10.0/bin/elasticsearch &gt;&gt; ./elasticsearch-7.10.0/nohup.out 2&gt;&amp;1 &amp; kibana 启动脚本nohup ./bin/kibana &gt;&gt; ./nohup.out 2&gt;&amp;1 &amp; logstash 启动脚本nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf &gt;&gt; /usr/local/logstash/logstash-7.10.0/nohup.out 2&gt;&amp;1 &amp;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch调优实践","date":"2021-07-14T15:12:16.000Z","path":"wiki/elasticsearch调优实践-0/","text":"从性能和稳定性两方面，从linux参数调优、ES节点配置和ES使用方式三个角度入手，介绍ES调优的基本方案。当然，ES的调优绝不能一概而论，需要根据实际业务场景做适当的取舍和调整 Elasticsearch性能优化总结Elasticsearch调优实践 Linux优化关闭交换分区，防止内存置换降低性能。将 /etc/fstab 文件中包含swap的行注释掉sed -i &#39;/swap/s/^/#/&#39; /etc/fstabswapoff -a 磁盘挂载选项noatime：禁止记录访问时间戳，提高文件系统读写性能data=writeback： 不记录data journal，提高文件系统写入性能barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了nobh：关闭buffer_head，防止内核打断大块数据的IO操作mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data 对于SSD磁盘，采用电梯调度算法因为SSD提供了更智能的请求调度算法，不需要内核去做多余的调整 (仅供参考)echo noop &gt; /sys/block/sda/queue/scheduler ES节点配置conf/elasticsearch.yml文件： 适当增大写入buffer和bulk队列长度，提高写入性能和稳定性indices.memory.index_buffer_size: 15%thread_pool.bulk.queue_size: 1024 计算disk使用量时，不考虑正在搬迁的shard在规模比较大的集群中，可以防止新建shard时扫描所有shard的元数据，提升shard分配速度。cluster.routing.allocation.disk.include_relocations: false 三 ES使用方式控制字段的存储选项ES底层使用Lucene存储数据，主要包括行存（StoreFiled）、列存（DocValues）和倒排索引（InvertIndex）三部分。 大多数使用场景中，没有必要同时存储这三个部分，可以通过下面的参数来做适当调整： StoreFiled行存，其中占比最大的是source字段，它控制doc原始数据的存储。在写入数据时，ES把doc原始数据的整个json结构体当做一个string，存储为source字段。查询时，可以通过source字段拿到当初写入时的整个json结构体。 所以，如果没有取出整个原始json结构体的需求，可以通过下面的命令，在mapping中关闭source字段或者只在source中存储部分字段，数据查询时仍可通过ES的docvaluefields获取所有字段的值。注意：关闭source后， update, updatebyquery, reindex等接口将无法正常使用，所以有update等需求的index不能关闭source。 关闭 _source12345678910PUT my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; _source只存储部分字段通过includes指定要存储的字段或者通过excludes滤除不需要的字段 123456789101112131415161718PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;*.count&quot;, &quot;meta.*&quot; ], &quot;excludes&quot;: [ &quot;meta.description&quot;, &quot;meta.other.*&quot; ] &#125; &#125; &#125;&#125; docvalues 控制列存。ES主要使用列存来支持sorting, aggregations和scripts功能，对于没有上述需求的字段，可以通过下面的命令关闭docvalues，降低存储成本。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false &#125; &#125; &#125; &#125;&#125; ndex：控制倒排索引。ES默认对于所有字段都开启了倒排索引，用于查询。对于没有查询需求的字段，可以通过下面的命令关闭倒排索引。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; allES的一个特殊的字段 ES把用户写入json的所有字段值拼接成一个字符串后，做分词，然后保存倒排索引，用于支持整个json的全文检索。这种需求适用的场景较少，可以通过下面的命令将all字段关闭，节约存储成本和cpu开销。（ES 6.0+以上的版本不再支持_all字段，不需要设置）12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; fieldnames该字段用于exists查询，来确认某个doc里面有无一个字段存在。若没有这种需求，可以将其关闭。12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_field_names&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; 开启最佳压缩对于打开了上述_source字段的index，可以通过下面的命令来把lucene适用的压缩算法替换成 DEFLATE，提高数据压缩率。PUT /my_index/_settings&#123; &quot;index.codec&quot;: &quot;best_compression&quot;&#125; bulk批量写入写入数据时尽量使用下面的bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w，具体可根据业务场景选取一个适当的数量。 调整translog同步策略默认情况下，translog的持久化策略是，对于每个写入请求都做一次flush，刷新translog数据到磁盘上。这种频繁的磁盘IO操作是严重影响写入性能的，如果可以接受一定概率的数据丢失（这种硬件故障的概率很小），可以通过下面的命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。 1234567891011PUT my_index&#123;&quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;translog&quot;: &#123; &quot;sync_interval&quot;: &quot;5s&quot;, &quot;durability&quot;: &quot;async&quot; &#125; &#125;&#125;&#125; 调整refresh_interval写入Lucene的数据，并不是实时可搜索的，ES必须通过refresh的过程把内存中的数据转换成Lucene的完整segment后，才可以被搜索。默认情况下，ES每一秒会refresh一次，产生一个新的segment，这样会导致产生的segment较多，从而segment merge较为频繁，系统开销较大。如果对数据的实时可见性要求较低，可以通过下面的命令提高refresh的时间间隔，降低系统开销。 PUT my_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;refresh_interval&quot; : &quot;30s&quot; &#125; &#125;&#125; merge并发控制ES的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。默认值是Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))，当节点配置的cpu核数较高时，merge占用的资源可能会偏高，影响集群的性能，可以通过下面的命令调整某个index的merge过程的并发度： PUT /my_index/_settings&#123; &quot;index.merge.scheduler.max_thread_count&quot;: 2&#125; 写入数据不指定_id，让ES自动产生当用户显示指定id写入数据时，ES会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。这样每次写入时，ES都会耗费一定的资源做查询。如果用户写入数据时不指定doc，ES则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。 所以，在不需要通过id字段去重、update的使用场景中，写入不指定id可以提升写入速率。基础架构部数据库团队的测试结果显示，无id的数据写入性能可能比有_id的高出近一倍，实际损耗和具体测试场景相关。 routing对于数据量较大的index，一般会配置多个shard来分摊压力。这种场景下，一个查询会同时搜索所有的shard，然后再将各个shard的结果合并后，返回给用户。对于高并发的小查询场景，每个分片通常仅抓取极少量数据，此时查询过程中的调度开销远大于实际读取数据的开销，且查询速度取决于最慢的一个分片。开启routing功能后，ES会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。如果查询时指定routing，那么ES只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。 routing的使用方式如下： 12# 写入PUT my_index/my_type/1?routing=user1&#123; &quot;title&quot;: &quot;This is a document&quot;&#125;# 查询GET my_index/_search?routing=user1,user2 &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;document&quot; &#125; &#125;&#125; 为string类型的字段选取合适的存储方式存为text类型的字段（string字段默认类型为text）：做分词后存储倒排索引，支持全文检索，可以通过下面几个参数优化其存储方式： - norms：用于在搜索时计算该doc的_score（代表这条数据与搜索条件的相关度），如果不需要评分，可以将其关闭。 - indexoptions：控制倒排索引中包括哪些信息（docs、freqs、positions、offsets）。对于不太注重score/highlighting的使用场景，可以设为 docs来降低内存/磁盘资源消耗。 - fields: 用于添加子字段。对于有sort和聚合查询需求的场景，可以添加一个keyword子字段以支持这两种功能。 123456789101112131415161718192021222324252627282930313233343536 &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;norms&quot;: false, &quot;index_options&quot;: &quot;docs&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; ``` #### 存为keyword类型的字段不做分词，不支持全文检索。text分词消耗CPU资源，冗余存储keyword子字段占用存储空间。如果没有全文索引需求，只是要通过整个字段做搜索，可以设置该字段的类型为keyword，提升写入速率，降低存储成本。 设置字段类型的方法有两种：一是创建一个具体的index时，指定字段的类型；二是通过创建template，控制某一类index的字段类型。- 通过mapping指定 tags 字段为keyword类型```jsonPUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;tags&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; 通过template，指定my_index*类的index，其所有string字段默认为keyword类型PUT _template/my_template12345678910111213141516171819202122&#123; &quot;order&quot;: 0, &quot;template&quot;: &quot;my_index*&quot;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; ] &#125; &#125;, &quot;aliases&quot;: &#123; &#125; &#125; 查询时，使用query-bool-filter组合取代普通query默认情况下，ES通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。但对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。此时，可以通过query-bool-filter组合来让ES不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。 普通查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;Kimchy&quot; &#125; &#125;&#125; query-bool-filter 加速查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;Kimchy&quot; &#125; &#125; &#125; &#125;&#125; index按日期滚动，便于管理写入ES的数据最好通过某种方式做分割，存入不同的index。常见的做法是将数据按模块/功能分类，写入不同的index，然后按照时间去滚动生成index。这样做的好处是各种数据分开管理不会混淆，也易于提高查询效率。同时index按时间滚动，数据过期时删除整个index，要比一条条删除数据或deletebyquery效率高很多，因为删除整个index是直接删除底层文件，而deletebyquery是查询-标记-删除。 举例说明，假如有[modulea,moduleb]两个模块产生的数据，那么index规划可以是这样的：一类index名称是modulea + {日期}，另一类index名称是module_b+ {日期}。对于名字中的日期，可以在写入数据时自己指定精确的日期，也可以通过ES的ingest pipeline中的index-name-processor实现（会有写入性能损耗）。 按需控制index的分片数和副本数分片（shard）：一个ES的index由多个shard组成，每个shard承载index的一部分数据。 副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。 shard数量（numberofshards）设置过多或过低都会引发一些问题：shard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。 对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理。基础架构部数据库团队的推荐方案是： 对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置35个shard，numberofreplicas设置为1即可（也就是一主一从，共两副本） 。对于数据量较大（100GB以上）的index：一般把单个shard的数据量控制在（20GB50GB）让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"修改mysql表创建时间","date":"2021-07-14T13:42:53.000Z","path":"wiki/修改mysql表创建时间/","text":"修改服务器时间date -s &quot;2021-07-14 21:22:10&quot; 执行DDLalter table mirror_user comment &#39;用户表&#39;; 服务器时间修正ntpdate ntp1.aliyun.com","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"hutool导出excel","date":"2021-07-14T13:01:13.000Z","path":"wiki/hutool导出excel/","text":"如果你仅需一个Java导出excel的工具，👇就可以满足你的临时需求，当然代码下面这么写肯定是不规范的，可以稍后完善！ 添加依赖123456789101112131415161718&lt;!-- https://mvnrepository.com/artifact/cn.hutool/hutool-all --&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; 数据类1234567@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class IPData &#123; private String ip;&#125; Export方法示例12345678910public void export(List&lt;IPData&gt; rows) throws FileNotFoundException &#123; ExcelWriter writer = ExcelUtil.getWriter(true); writer.renameSheet(&quot;所有数据&quot;); //甚至sheet的名称 writer.addHeaderAlias(&quot;ip&quot;, &quot;IP&quot;); writer.write(rows, true); writer.setOnlyAlias(true); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/IP1.xlsx&quot;); writer.flush(fileOutputStream); writer.close(); &#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"git常用命令","date":"2021-07-14T01:55:55.000Z","path":"wiki/git常用命令/","text":"初始化本地仓库git initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin git@github.com:XXXXXgit push -u origin master 批量删除分支远程：git branch -r| grep &#39;ss-1&#39; | sed &#39;s/origin\\///g&#39; | xargs -I &#123;&#125; git push origin :&#123;&#125;本地：git branch -a | grep &#39;feature-re-1&#39; | xargs git branch -D","tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"}]},{"title":"Linux常用命令","date":"2021-07-13T03:44:22.000Z","path":"wiki/Linux常用命令/","text":"用户相关文件相关日志相关 linux 在文档中查找关键字个数grep -o “关键字” 文档名 | wc -l grep -o “关键字” 文档名 | sort | uniq -c 清除history记录vim .bash_history命令模式下（Esc之后输入:） 输入 set nu 每行数据前面显示行号11,20d 回车 11～20行的记录就被删除了然后命令模式下 wq 保存退出就可以了如果在此查看还是有记录，可以退出当前回话之后，再进去查看，就会不再显示删除的记录了","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"}]},{"title":"段合并","date":"2021-07-08T12:57:54.000Z","path":"wiki/段合并/","text":"参考资料 learnku.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"统计去重数据 (近似度量)","date":"2021-07-08T08:09:05.000Z","path":"wiki/统计去重数据/","text":"cardinality用法常用写法如下👇curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;months&quot; : &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125; &#125; &#125;&#125; 精度问题cardinality 度量是一个 近似算法。 它是基于 HyperLogLog++ （HLL）算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。 我们不需要理解技术细节， 但我们最好应该关注一下这个算法的 特性 ： 可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）。 小的数据集精度是非常高的。 我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 要配置精度，我们必须指定 precision_threshold 参数的值。 这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果。参考以下示例： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; ⚠️ ⚠️precision_threshold 接受 0–40000 之间的数字，更大的值还是会被当作 40000 来处理 示例会确保当字段唯一值在 100 以内时会得到非常准确的结果。尽管算法是无法保证这点的，但如果基数在阈值以下，几乎总是 100% 正确的。高于阈值的基数会开始节省内存而牺牲准确度，同时也会对度量结果带入误差。 对于指定的阈值，HLL 的数据结构会大概使用 precision_threshold * 8 字节的内存，所以就必须在牺牲内存和获得额外的准确度间做平衡。 在实际应用中， 100 的阈值可以在唯一值为百万的情况下仍然将误差维持 5% 以内 速度问题如果想要获得唯一值的数目， 通常 需要查询整个数据集合（或几乎所有数据）。 所有基于所有数据的操作都必须迅速，原因是显然的。 HyperLogLog 的速度已经很快了，它只是简单的对数据做哈希以及一些位操作。 但如果速度对我们至关重要，可以做进一步的优化。 因为 HLL 只需要字段内容的哈希值，我们可以在索引时就预先计算好。 就能在查询时跳过哈希计算然后将哈希值从 fielddata 直接加载出来。 预先计算哈希值只对内容很长或者基数很高的字段有用，计算这些字段的哈希值的消耗在查询时是无法忽略的。 尽管数值字段的哈希计算是非常快速的，存储它们的原始值通常需要同样（或更少）的内存空间。这对低基数的字符串字段同样适用，Elasticsearch 的内部优化能够保证每个唯一值只计算一次哈希。 基本上说，预先计算并不能保证所有的字段都更快，它只对那些具有高基数和/或者内容很长的字符串字段有作用。需要记住的是，预计算只是简单的将查询消耗的时间提前转移到索引时，并非没有任何代价，区别在于你可以选择在 什么时候 做这件事，要么在索引时，要么在查询时。 创建索引时添加如下配置： 1234567891011121314151617PUT /cars/&#123; &quot;mappings&quot;: &#123; &quot;transactions&quot;: &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 多值字段的类型是 murmur3 ，这是一个哈希函数。 现在当我们执行聚合时，我们使用 color.hash 字段而不是 color 字段：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 12345678910&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color.hash&quot; &#125; &#125; &#125;&#125; 现在 cardinality 度量会读取 “color.hash“ 里的值（预先计算的哈希值），取代动态计算原始值的哈希。 单个文档节省的时间是非常少的，但是如果你聚合一亿数据，每个字段多花费 10 纳秒的时间，那么在每次查询时都会额外增加 1 秒，如果我们要在非常大量的数据里面使用 cardinality ，我们可以权衡使用预计算的意义，是否需要提前计算 hash，从而在查询时获得更好的性能，做一些性能测试来检验预计算哈希是否适用于你的应用场景。。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"多桶排序","date":"2021-07-08T07:50:24.000Z","path":"wiki/多桶排序/","text":"多值桶（ terms 、 histogram 和 date_histogram ）动态生成很多桶。 Elasticsearch 是如何决定这些桶展示给用户的顺序呢？ 默认的，桶会根据 doc_count 降序排列。这是一个好的默认行为，因为通常我们想要找到文档中与查询条件相关的最大值：售价、人口数量、频率。但有些时候我们希望能修改这个顺序，不同的桶有着不同的处理方式。 内置排序这些排序模式是桶 固有的 能力：它们操作桶生成的数据 ，比如 doc_count 。 它们共享相同的语法，但是根据使用桶的不同会有些细微差别。 让我们做一个 terms 聚合但是按 doc_count 值的升序排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;_count&quot; : &quot;asc&quot; &#125; &#125; &#125; &#125;&#125;&#x27; 用关键字 _count ，我们可以按 doc_count 值的升序排序。 我们为聚合引入了一个 order 对象， 它允许我们可以根据以下几个值中的一个值进行排序： _count按文档数排序。对 terms 、 histogram 、 date_histogram 有效。 _term按词项的字符串值的字母顺序排序。只在 terms 内使用。 _key按每个桶的键值数值排序（理论上与 _term 类似）。 只在 histogram 和 date_histogram 内使用。 按度量排序有时，我们会想基于度量计算的结果值进行排序。 在我们的汽车销售分析仪表盘中，我们可能想按照汽车颜色创建一个销售条状图表，但按照汽车平均售价的升序进行排序。 我们可以增加一个度量，再指定 order 参数引用这个度量即可： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314151617181920&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125;&#x27; 计算每个桶的平均售价。 桶按照计算平均值的升序排序。 我们可以采用这种方式用任何度量排序，只需简单的引用度量的名字。不过有些度量会输出多个值。 extended_stats 度量是一个很好的例子：它输出好几个度量值。 如果我们想使用多值度量进行排序， 我们只需以关心的度量为关键词使用点式路径：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 使用 . 符号，根据感兴趣的度量进行排序。 深度度量排序在前面的示例中，度量是桶的直接子节点。平均售价是根据每个 term 来计算的。 在一定条件下，我们也有可能对 更深 的度量进行排序，比如孙子桶或从孙桶。 我们可以定义更深的路径，将度量用尖括号（ &gt; ）嵌套起来，像这样： my_bucket&gt;another_bucket&gt;metric 。 需要提醒的是嵌套路径上的每个桶都必须是 单值 的。 filter 桶生成 一个单值桶：所有与过滤条件匹配的文档都在桶中。 多值桶（如：terms ）动态生成许多桶，无法通过指定一个确定路径来识别。 目前，只有三个单值桶： filter 、 global 和 reverse_nested 。让我们快速用示例说明，创建一个汽车售价的直方图，但是按照红色和绿色（不包括蓝色）车各自的方差来排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;histogram&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;interval&quot;: 20000, &quot;order&quot;: &#123; &quot;red_green_cars&gt;tats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;red_green_cars&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123;&quot;color&quot;: [&quot;red&quot;, &quot;green&quot;]&#125;&#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123;&quot;extended_stats&quot;: &#123;&quot;field&quot; : &quot;price&quot;&#125;&#125; &#125; &#125; &#125; &#125; &#125;&#125;&#x27; 按照嵌套度量的方差对桶的直方图进行排序。 因为我们使用单值过滤器 filter ，我们可以使用嵌套排序。 按照生成的度量对统计结果进行排序。 本例中，可以看到我们如何访问一个嵌套的度量。 stats 度量是 red_green_cars 聚合的子节点，而 red_green_cars 又是 colors 聚合的子节点。 为了根据这个度量排序，我们定义了路径 red_green_cars&gt;tats.variance 。我们可以这么做，因为 filter 桶是个单值桶。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"过滤和聚合","date":"2021-07-08T07:33:26.000Z","path":"wiki/过滤和聚合/","text":"过滤和聚合聚合范围限定还有一个自然的扩展就是过滤。因为聚合是在查询结果范围内操作的，任何可以适用于查询的过滤器也可以应用在聚合上。 过滤如果我们想找到售价在 $10,000 美元之上的所有汽车同时也为这些车计算平均售价， 可以简单地使用一个 constant_score 查询和 filter 约束： GET /cars/transactions/_search 12345678910111213141516171819&#123; &quot;size&quot; : 0, &quot;query&quot; : &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10000 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;single_avg_price&quot;: &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 从根本上讲，使用 non-scoring 查询和使用 match 查询没有任何区别。查询（包括了一个过滤器）返回一组文档的子集，聚合正是操作这些文档。使用 filtering query 会忽略评分，并有可能会缓存结果数据等等。 过滤桶但是如果我们只想对聚合结果过滤怎么办？ 假设我们正在为汽车经销商创建一个搜索页面， 我们希望显示用户搜索的结果，但是我们同时也想在页面上提供更丰富的信息，包括（与搜索匹配的）上个月度汽车的平均售价。 这里我们无法简单的做范围限定，因为有两个不同的条件。搜索结果必须是 ford ，但是聚合结果必须满足 ford AND sold &gt; now - 1M 。 为了解决这个问题，我们可以用一种特殊的桶，叫做 filter （注：过滤桶） 。 我们可以指定一个过滤桶，当文档满足过滤桶的条件时，我们将其加入到桶内。 查询结果如下：GET /cars/transactions/_search 1234567891011121314151617181920212223242526&#123; &quot;size&quot; : 0, &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;recent_sales&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold&quot;: &#123; &quot;from&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_price&quot;:&#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 使用 过滤 桶在 查询 范围基础上应用过滤器。 avg 度量只会对 ford 和上个月售出的文档计算平均售价。 因为 filter 桶和其他桶的操作方式一样，所以可以随意将其他桶和度量嵌入其中。所有嵌套的组件都会 “继承” 这个过滤，这使我们可以按需针对聚合过滤出选择部分。 后过滤器目前为止，我们可以同时对搜索结果和聚合结果进行过滤（不计算得分的 filter 查询），以及针对聚合结果的一部分进行过滤（ filter 桶）。 我们可能会想，”只过滤搜索结果，不过滤聚合结果呢？” 答案是使用 post_filter 。 它是接收一个过滤器的顶层搜索请求元素。这个过滤器在查询 之后 执行（这正是该过滤器的名字的由来：它在查询之后 post 执行）。正因为它在查询之后执行，它对查询范围没有任何影响，所以对聚合也不会有任何影响。 我们可以利用这个行为对查询条件应用更多的过滤器，而不会影响其他的操作，就如 UI 上的各个分类面。让我们为汽车经销商设计另外一个搜索页面，这个页面允许用户搜索汽车同时可以根据颜色来过滤。颜色的选项是通过聚合获得的： GET /cars/transactions/_search 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;post_filter&quot;: &#123; &quot;term&quot; : &#123; &quot;color&quot; : &quot;green&quot; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;all_colors&quot;: &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; post_filter 元素是 top-level 而且仅对命中结果进行过滤。 查询 部分找到所有的 ford 汽车，然后用 terms 聚合创建一个颜色列表。因为聚合对查询范围进行操作，颜色列表与福特汽车有的颜色相对应。 最后， post_filter 会过滤搜索结果，只展示绿色 ford 汽车。这在查询执行过 后 发生，所以聚合不受影响。 这通常对 UI 的连贯一致性很重要，可以想象用户在界面商选择了一类颜色（比如：绿色），期望的是搜索结果已经被过滤了，而 不是 过滤界面上的选项。如果我们应用 filter 查询，界面会马上变成 只 显示 绿色 作为选项，这不是用户想要的！ ⚠️ ⚠️ ⚠️ 性能考虑（Performance consideration）当你需要对搜索结果和聚合结果做不同的过滤时，你才应该使用 post_filter ， 有时用户会在普通搜索使用 post_filter 。 不要这么做！ post_filter 的特性是在查询 之后 执行，任何过滤对性能带来的好处（比如缓存）都会完全失去。 在我们需要不同过滤时， post_filter 只与聚合一起使用。 总结选择合适类型的过滤（如：搜索命中、聚合或两者兼有）通常和我们期望如何表现用户交互有关。选择合适的过滤器（或组合）取决于我们期望如何将结果呈现给用户。 在 filter 过滤中的 non-scoring 查询，同时影响搜索结果和聚合结果。 filter 桶影响聚合。 post_filter 只影响搜索结果。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 条形图","date":"2021-07-08T07:11:57.000Z","path":"wiki/聚合-条形图/","text":"参考资料","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"嵌套桶","date":"2021-07-08T07:01:16.000Z","path":"wiki/嵌套桶/","text":"两层嵌套在我们使用不同的嵌套方案时，聚合的力量才能真正得以显现。 在前例中，我们已经看到如何将一个度量嵌入桶中，它的功能已经十分强大了。 但真正令人激动的分析来自于将桶嵌套进 另外一个桶 所能得到的结果。 现在，我们想知道每个颜色的汽车制造商的分布： GET /cars/transactions/_search 12345678910111213141516171819202122&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;make&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注意前例中的 avg_price 度量仍然保持原位。 另一个聚合 make 被加入到了 color 颜色桶中。 这个聚合是 terms 桶，它会为每个汽车制造商生成唯一的桶。 这里发生了一些有趣的事。 首先，我们可能会观察到之前例子中的 avg_price 度量完全没有变化，还在原来的位置。 一个聚合的每个 层级 都可以有多个度量或桶， avg_price 度量告诉我们每种颜色汽车的平均价格。它与其他的桶和度量相互独立。 这对我们的应用非常重要，因为这里面有很多相互关联，但又完全不同的度量需要收集。聚合使我们能够用一次数据请求获得所有的这些信息。 另外一件值得注意的重要事情是我们新增的这个 make 聚合，它是一个 terms 桶（嵌套在 colors 、 terms 桶内）。这意味着它会为数据集中的每个唯一组合生成（ color 、 make ）元组。 让我们看看返回的响应（为了简单我们只显示部分结果）： 1234567891011121314151617181920212223242526&#123; &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125; &#125; &#125;&#125; 正如期望的那样，新的聚合嵌入在每个颜色桶中。 现在我们看见按不同制造商分解的每种颜色下车辆信息。 最终，我们看到前例中的 avg_price 度量仍然维持不变。 三层嵌套让我们回到话题的原点，在进入新话题之前，对我们的示例做最后一个修改， 为每个汽车生成商计算最低和最高的价格：GET /cars/transactions/_search 1234567891011121314151617181920212223&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;make&quot; &#125;, &quot;aggs&quot; : &#123; &quot;min_price&quot; : &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125;, &quot;max_price&quot; : &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 我们需要增加另外一个嵌套的 aggs 层级。 然后包括 min 最小度量。 以及 max 最大度量。 得到以下输出（只显示部分结果）： 12345678910111213141516171819202122232425262728293031323334353637&#123;... &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3, &quot;min_price&quot;: &#123; &quot;value&quot;: 10000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 20000 &#125; &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1, &quot;min_price&quot;: &#123; &quot;value&quot;: 80000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 80000 &#125; &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125;,... 有了这两个桶，我们可以对查询的结果进行扩展并得到以下信息： 有四辆红色车。红色车的平均售价是 $32，500 美元。其中三辆红色车是 Honda 本田制造，一辆是 BMW 宝马制造。最便宜的红色本田售价为 $10，000 美元。最贵的红色本田售价为 $20，000 美元。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 高级概念","date":"2021-07-08T06:52:35.000Z","path":"wiki/聚合-高级概念/","text":"桶桶 简单来说就是满足特定条件的文档的集合： 一个雇员属于 男性 桶或者 女性 桶 奥尔巴尼属于 纽约 桶 日期2014-10-28属于 十月 桶当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件。如果匹配到，文档将放入相应的桶并接着进行聚合操作。 桶也可以被嵌套在其他桶里面，提供层次化的或者有条件的划分方案。例如，辛辛那提会被放入俄亥俄州这个桶，而 整个 俄亥俄州桶会被放入美国这个桶。 Elasticsearch 有很多种类型的桶，能让你通过很多种方式来划分文档（时间、最受欢迎的词、年龄区间、地理位置 等等）。其实根本上都是通过同样的原理进行操作：基于条件来划分文档。 指标桶能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些指标的计算。分桶是一种达到目的的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。 大多数 指标 是简单的数学运算（例如最小值、平均值、最大值，还有汇总），这些是通过文档的值来计算。在实践中，指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据。 桶和指标的组合聚合 是由桶和指标组成的。 聚合可能只有一个桶，可能只有一个指标，或者可能两个都有。也有可能有一些桶嵌套在其他桶里面。例如，我们可以通过所属国家来划分文档（桶），然后计算每个国家的平均薪酬（指标）。 由于桶可以被嵌套，我们可以实现非常多并且非常复杂的聚合： 1.通过国家划分文档（桶） 2.然后通过性别划分每个国家（桶） 3.然后通过年龄区间划分每种性别（桶） 4.最后，为每个年龄区间计算平均薪酬（指标） 最后将告诉你每个 &lt;国家, 性别, 年龄&gt; 组合的平均薪酬。所有的这些都在一个请求内完成并且只遍历一次数据！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"elasticsearch-overview","date":"2021-07-08T03:17:58.000Z","path":"wiki/elasticsearch-overview/","text":"学习资料 https://www.codingdict.com/ https://elasticsearch.cn/ https://www.elastic.co/guide/en/ 铭毅天下","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"关于 Elasticsearch 内存占用及分配","date":"2021-07-08T02:39:16.000Z","path":"wiki/关于-Elasticsearch-内存占用及分配/","text":"Elasticsearch 和 Lucene 对内存使用情况： Elasticsearch 限制的内存大小是 JAVA 堆空间的大小，不包括Lucene 缓存倒排索引数据空间。 Lucene 中的 倒排索引 segments 存储在文件中，为提高访问速度，都会把它加载到内存中，从而提高 Lucene 性能。所以建议至少留系统一半内存给Lucene。Node Query Cache (负责缓存f ilter 查询结果)，每个节点有一个，被所有 shard 共享，filter query查询结果要么是 yes 要么是no，不涉及 scores 的计算。集群中每个节点都要配置，默认为：indices.queries.cache.size:10% Indexing Buffer 索引缓冲区，用于存储新索引的文档，当其被填满时，缓冲区中的文档被写入磁盘中的 segments 中。节点上所有 shard 共享。缓冲区默认大小： indices.memory.index_buffer_size: 10%如果缓冲区大小设置了百分百则 indices.memory.min_index_buffer_size 用于这是最小值，默认为 48mb。indices.memory.max_index_buffer_size 用于最大大小，无默认值。 segmentssegments会长期占用内存，其初衷就是利用OS的cache提升性能。只有在Merge之后，才会释放掉标记为Delete的segments，释放部分内存。 Shard Request Cache 用于缓存请求结果，但之缓存request size为0的。比如说 hits.total, aggregations 和 suggestions.默认最大为indices.requests.cache.size:1% Field Data Cache 字段缓存重要用于对字段进行排序、聚合是使用。因为构建字段数据缓存代价昂贵，所以建议有足够的内训来存储。Fielddata 是 「 延迟 」 加载。如果你从来没有聚合一个分析字符串，就不会加载 fielddata 到内存中，也就不会使用大量的内存，所以可以考虑分配较小的heap给Elasticsearch。因为heap越小意味着Elasticsearch的GC会比较快，并且预留给Lucene的内存也会比较大。。如果没有足够的内存保存fielddata时，Elastisearch会不断地从磁盘加载数据到内存，并剔除掉旧的内存数据。剔除操作会造成严重的磁盘I/O，并且引发大量的GC，会严重影响Elastisearch的性能。 默认情况下Fielddata会不断占用内存，直到它触发了fielddata circuit breaker。fielddata circuit breaker会根据查询条件评估这次查询会使用多少内存，从而计算加载这部分内存之后，Field Data Cache所占用的内存是否会超过indices.breaker.fielddata.limit。如果超过这个值，就会触发fielddata circuit breaker，abort这次查询并且抛出异常，防止OOM。 1indices.breaker.fielddata.limit:60% (默认heap的60%) (es7之后改成70%) 如果设置了indices.fielddata.cache.size，当达到size时，cache会剔除旧的fielddata。 indices.breaker.fielddata.limit 必须大于 indices.fielddata.cache.size，否则只会触发fielddata circuit breaker，而不会剔除旧的fielddata。 配置Elasticsearch堆内存Elasticsearch默认安装后设置的内存是 1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10g 注意点给es分配内存时要注意，至少要分配一半儿内存留给 Lucene。分配给 es 的内存最好不要超过 32G ，因为如果堆大小小于 32 GB，JVM 可以利用指针压缩，这可以大大降低内存的使用：每个指针 4 字节而不是 8 字节。如果大于32G 每个指针占用 8字节，并且会占用更多的内存带宽，降低了cpu性能。 还有一点， 要关闭 swap 内存交换空间，禁用swapping。频繁的swapping 对服务器来说是致命的。总结：给es JVM栈的内存最好不要超过32G，留给Lucene的内存越大越好，Lucene把所有的segment都缓存起来，会加快全文检索。 关闭交换区这应该显而易见了，但仍然需要明确的写出来：把内存换成硬盘将毁掉服务器的性能，想象一下：涉及内存的操作是需要快速执行的。如果介质从内存变为了硬盘，一个10微秒的操作变成需要10毫秒。而且这种延迟发生在所有本该只花费10微秒的操作上，就不难理解为什么交换区对于性能来说是噩梦。 最好的选择是禁用掉操作系统的交换区。可以用以下命令： 1sudo swapoff -a 来禁用，你可能还需要编辑 /etc/fstab 文件。细节可以参考你的操作系统文档。 如果实际环境不允许禁用掉 swap，你可以尝试降低 swappiness。此值控制操作系统使用交换区的积极性。这可以防止在正常情况下使用交换区，但仍允许操作系统在紧急情况下将内存里的东西放到交换区。 对于大多数Linux系统来说，这可以用 sysctl 值来配置： 1vm.swappiness = 1 # 将此值配置为1会比0好，在kernal内核的某些版本中，0可能会引起OOM异常。 最后，如果两种方法都不可用，你应该在ElasticSearch的配置中启用 mlockall.file。这允许JVM锁定其使用的内存，而避免被放入操作系统交换区。 在elasticsearch.yml中，做如下设置： 1bootstrap.mlockall: true 查看node节点数据GET /_cat/nodes?v&amp;h=id,ip,port,v,master,name,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count GET /_cat/nodes?v&amp;h=id,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size GET /_cat/nodes?v&amp;h=id,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count 参考文章","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"cross-fields跨字段查询","date":"2021-07-07T06:41:42.000Z","path":"wiki/cross-fields跨字段查询/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » cross-fields 跨字段查询","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"copy_to参数","date":"2021-07-07T06:34:42.000Z","path":"wiki/copy-to参数/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 自定义 _all 字段 Docs » Mapping parameters（映射参数） » Mapping(映射) » copy_to（合并参数）","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"字符串排序与多字段","date":"2021-07-07T03:05:31.000Z","path":"wiki/字符串排序与多字段/","text":"被解析的字符串字段也是多值字段， 但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如 fine old art ， 这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Elasticsearch 在排序过程中没有这样的信息。 你可以使用 min 和 max 排序模式（默认是 min ），但是这会导致排序以 art 或是 old ，任何一个都不是所希望的。 为了以字符串字段进行排序，这个字段应仅包含一项： 整个 not_analyzed 字符串。 但是我们仍需要 analyzed 字段，这样才能以全文进行查询 一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段： analyzed 用于搜索， not_analyzed 用于排序 但是保存相同的字符串两次在 _source 字段是浪费空间的。 我们真正想要做的是传递一个 单字段 但是却用两种方式索引它。所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个 fields 参数 该参数允许你转化一个简单的映射如： 1234&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;&#125; 为一个多字段映射如： 12345678910&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; tweet 主字段与之前的一样: 是一个 analyzed 全文字段。 新的 tweet.raw 子字段是 not_analyzed. 现在，至少只要我们重新索引了我们的数据，使用 tweet 字段用于搜索，tweet.raw 字段用于排序：curl -X GET &quot;localhost:9200/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;, &quot;sort&quot;: &quot;tweet.raw&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"推荐系统-Overview","date":"2021-07-07T02:08:05.000Z","path":"wiki/推荐系统-Overview/","text":"博客资料 深度解析京东个性化推荐系统演进史 用 Mahout 和 Elasticsearch 实现推荐系统 美团推荐算法实践 58同城推荐系统设计与实现 微博推荐系统的架构演进之路 Flink 在小红书推荐系统中的应用 小红书大数据在推荐系统中的应用 快看漫画个性化推荐探索与实践 数据仓库系列篇——唯品会大数据架构 推荐系统基本概念和架构 PAI平台搭建企业级个性化推荐系统 - Aliyun 蘑菇街推荐工程实践 参考资料","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"categories":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"}]},{"title":"flink 提交任务","date":"2021-07-06T15:57:04.000Z","path":"wiki/flink-提交任务/","text":"下面演示如何通过admin页面提交任务 👇 准备task jar1234567891011121314151617181920public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 执行mvn install -DskipTest 可以得到相应的jar admin提交jar 提交完jar包之后，需要设置相关参数，这个根据自己的实际情况来设置，下面是参考样例： Enter Class : com.ibli.flink.StreamWordCount也就是程序入口，我们这是写了一个main方法，如果是程序的话，可以写对应bootstrap的启动类 Program Arguments : –host localhost –port 7777 点击 submit 之后查看提交的任务状态 查看任务 可以看到是有两个任务，并且都是在执行状态；点击一个任务，还可以查看任务详情信息，和一些其他的信息，非常全面； 查看运行时任务列表 查看任务管理列表 点击任务可以跳转到详情页面 👇 下面是执行日志 我们还可以看到任务执行的标准输出结果✅ 任务源数据通过nc 输入数据，由程序读取7777端口输入流并解析数据 123gaolei:geekibli gaolei$ nc -lk 7777hello javahello flink 取消任务如下 再次查看已完成任务列表 如下：","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"apache-flink-overview","date":"2021-07-06T15:39:54.000Z","path":"wiki/apache-flink-overview/","text":"学习初衷推荐系统数据需要实时处理，使用Apache Flink实时计算用户数据，分析用户行为，达到实时业务数据分析和实现业务相关推荐； 学习资料 ashiamd.github.io 尚硅谷2021最新Java版Flink 武老师清华硕士，原IBM-CDL负责人 Apache Flink® — Stateful Computations over Data Streams Apache Flink® - 数据流上的有状态计算 https://space.bilibili.com/33807709?spm_id_from=333.788.b_765f7570696e666f.2 https://github.com/flink-china/flink-training-course https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/ [官方文档 推荐 ‼️]","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"most_fields类型","date":"2021-07-06T12:54:23.000Z","path":"wiki/most-fields类型/","text":"多字段映射首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍： DELETE /my_index 1234567891011121314151617181920PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; title 字段使用 english 英语分析器来提取词干。 title.std 字段使用 standard 标准分析器，所以没有词干提取。 接着索引一些文档： 12345PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; 这里用一个简单 match 查询 title 标题字段是否包含 jumping rabbits （跳跃的兔子）： 12345678GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;jumping rabbits&quot; &#125; &#125;&#125; 因为有了 english 分析器，这个查询是在查找以 jump 和 rabbit 这两个被提取词的文档。两个文档的 title 字段都同时包括这两个词，所以两个文档得到的评分也相同： 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125; ]&#125; 如果只是查询 title.std 字段，那么只有文档 2 是匹配的。尽管如此，如果同时查询两个字段，然后使用 bool 查询将评分结果 合并 ，那么两个文档都是匹配的（ title 字段的作用），而且文档 2 的相关度评分更高（ title.std 字段的作用）： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; 我们希望将所有匹配字段的评分合并起来，所以使用 most_fields 类型。这让 multi_match 查询用 bool 查询将两个字段语句包在里面，而不是使用 dis_max (最佳字段) 查询。 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8226396, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.10741998, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125; ]&#125; 文档 2 现在的评分要比文档 1 高。 用广度匹配字段 title 包括尽可能多的文档——以提升召回率——同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。 每个字段对于最终评分的贡献可以通过自定义值 boost 来控制。比如，使 title 字段更为重要，这样同时也降低了其他信号字段的作用： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title^10&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; title 字段的 boost 的值为 10 使它比 title.std 更重要。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 多数字段","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"multi_match 查询","date":"2021-07-06T12:37:26.000Z","path":"wiki/multi-match-查询/","text":"multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。 📒 📒 📒 multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即：best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。 默认情况下，查询的类型是 best_fields ，这表示它会为每个字段生成一个 match 查询，然后将它们组合到 dis_max 查询的内部，如下： 1234567891011121314151617181920212223&#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, ], &quot;tie_breaker&quot;: 0.3 &#125;&#125; 上面这个查询用 multi_match 重写成更简洁的形式： 123456789&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;body&quot; ], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125;&#125; ⚠️ ⚠️ ⚠️ best_fields 类型是默认值，可以不指定。 如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。 查询字段名称的模糊匹配字段名称可以用 模糊匹配 的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中，例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: &quot;*_title&quot; &#125;&#125; 提升单个字段的权重可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: [ &quot;*_title&quot;, &quot;chapter_title^2&quot; ] &#125;&#125; chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » multi_match 查询 Elasticsearch Guide [7.x] » Query DSL » Full text queries » Multi-match query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"dis_max查询","date":"2021-07-06T12:09:23.000Z","path":"wiki/dis-max查询/","text":"假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例： 1234567891011PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;&#125; 用户输入词组 Brown fox 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词： 现在运行以下 bool 查询： 12345678910&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 但是我们发现查询的结果是文档 1 的评分更高： 为了理解导致这样的原因，需要回想一下 bool 是如何计算评分的： 它会执行 should 语句中的两个查询。加和两个查询的评分。乘以匹配语句的总数。除以所有语句总数（这里为：2）。 文档 1 的两个字段都包含 brown 这个词，所以两个 match 语句都能成功匹配并且有一个评分。文档 2 的 body 字段同时包含 brown 和 fox 这两个词，但 title 字段没有包含任何词。这样， body 查询结果中的高分，加上 title 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。 在本例中， title 和 body 字段是相互竞争的关系，所以就需要找到单个 最佳匹配 的字段。 如果不是简单将每个字段的评分结果加在一起，而是将 最佳匹配 字段的评分作为查询的整体评分，结果会怎样？这样返回的结果可能是： 同时 包含 brown 和 fox 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。 dis_max 查询不使用 bool 查询，可以使用 dis_max 即分离 最大化查询 （Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ： 12345678910&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 得到我们想要的结果为： Top-level parameters for dis_maxedit queries(Required, array of query objects) Contains one or more query clauses. Returned documents must match one or more of these queries. If a document matches multiple queries, Elasticsearch uses the highest relevance score. tie_breaker(Optional, float) Floating point number between 0 and 1.0 used to increase the relevance scores of documents matching multiple query clauses. Defaults to 0.0. You can use the tie_breaker value to assign higher relevance scores to documents that contain the same term in multiple fields than documents that contain this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields. If a document matches multiple clauses, the dis_max query calculates the relevance score for the document as follows: Take the relevance score from a matching clause with the highest score.Multiply the score from any other matching clauses by the tie_breaker value.Add the highest score to the multiplied scores.If the tie_breaker value is greater than 0.0, all matching clauses count, but the clause with the highest score counts most. dis_max，只是取分数最高的那个query的分数而已，完全不考虑其他query的分数，这种一刀切的做法，可能导致在有其他query的影响下，score不准确的情况，这时为了使用结果更准确，最好还是要考虑到其他query的影响;使用 tie_breaker 将其他query的分数也考虑进去, tie_breaker 参数的意义，将其他query的分数乘以tie_breaker，然后综合考虑后与最高分数的那个query的分数综合在一起进行计算，这样做除了取最高分以外，还会考虑其他的query的分数。tie_breaker的值，设置在在0~1之间，是个小数就行，没有固定的值 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 最佳字段 Elasticsearch中文文档 Elasticsearch Guide [7.x] » Query DSL » Compound queries » Disjunction max query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch被破坏的相似度","date":"2021-07-06T11:50:11.000Z","path":"wiki/elasticsearch被破坏的相似度/","text":"在讨论更复杂的 多字段搜索 之前，让我们先快速解释一下为什么只在主分片上 创建测试索引 。 用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤：用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。 为了理解为什么会这样，可以设想，我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 foo 。可能是分片 1 有其中 3 个 foo 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。 在 什么是相关度？中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 词频/逆向文档频率 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 逆向文档频率 将 某个词在索引内所有文档出现的百分数 考虑在内，出现的频率越高，它的权重就越低。 但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。 因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 foo 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， foo 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。 在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。 为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 match 查询 里介绍的那样，如果只有一个分片，那么本地的 IDF 就是 全局的 IDF。 第二个方式就是在搜索请求后添加 ?search_type=dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。 不要在生产环境上使用 dfs_query_then_fetch 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。 参考资料 Elasticsearch: 权威指南 » 基础入门 » 排序与相关性 » 什么是相关性? Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 被破坏的相关度！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"canal同步es后部分字段为null","date":"2021-07-06T08:11:15.000Z","path":"wiki/canal同步es后部分字段为null/","text":"现象 配置文件如下： 123456789101112131415dataSourceKey: defaultDS # 源数据源的key, 对应上面配置的srcDataSources中的值destination: example # cannal的instance或者MQ的topicgroupId: g1 # 对应MQ模式下的groupId, 只会同步对应groupId的数据esMapping: _index: rd_member_fans_info # es 的索引名称 _type: _doc # es 的doc名称 _id: _id # es 的_id, 如果不配置该项必须配置下面的pk项_id则会由es自动分配# pk: id # 如果不需要_id, 则需要指定一个属性为主键属性# # sql映射 sql: &#x27;SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.create_time,t.update_time,t.`status` FROM rd_member_fans_info t&#x27;# objFields:# _labels: array:; # 数组或者对象属性, array:; 代表以;字段里面是以;分隔的# _obj: object # json对象 etlCondition: &quot;where t.update_time&gt;=&#123;&#125;&quot; # etl 的条件参数 commitBatch: 3000 # 提交批大小 ⚠️ ⚠️sql执行是没有问题的！ canal-adapter 获取binlog数据也没有问题，显示日志如下： 12021-07-06 15:39:24.588 [pool-1-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:3,&quot;redtom_id&quot;:1,&quot;fans_redtom_id&quot;:1,&quot;fans_username&quot;:&quot;1&quot;,&quot;fans_introduce&quot;:&quot;1&quot;,&quot;fans_avatar&quot;:&quot;1&quot;,&quot;is_each_following&quot;:1,&quot;follow_channel&quot;:1,&quot;create_time&quot;:1625556851000,&quot;update_time&quot;:1625556851000,&quot;status&quot;:2&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625557164000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;rd_member_fans_info&quot;,&quot;ts&quot;:1625557164587,&quot;type&quot;:&quot;INSERT&quot;&#125; 然后看一下我创建索引的mapping 解决方法调整sql如下： SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.status as is_deleted , t.create_time,t.update_time FROM rd_member_fans_info t 调整了那些东西呢？ status 的顺序提前而已！ 测试执行一下命令：curl http://127.0.0.1:8081/etl/es7/rd_member_fans_info.yml -X POST canal-adapter 日志如下： 122021-07-06 16:21:33.519 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member_fans_info2021-07-06 16:21:33.527 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - 数据全量导入完成, 一共导入 3 条数据, 耗时: 7 查看es数据：","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch-analyzer","date":"2021-07-06T07:02:01.000Z","path":"wiki/elasticsearch-analyzer/","text":"测试常见分析器GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 12345&#123; &quot;analyzer&quot;: &quot;english&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;stop&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; 默认分析器虽然我们可以在字段层级指定分析器，但是如果该层级没有指定任何的分析器，那么我们如何能确定这个字段使用的是哪个分析器呢？ 分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下： 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 在搜索时，顺序有些许不同： 查询自己定义的 analyzer ，否则 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 有时，在索引时和搜索时使用不同的分析器是合理的。我们可能要想为同义词建索引（例如，所有 quick 出现的地方，同时也为 fast 、 rapid 和 speedy 创建索引）。但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 quick 、 fast 、 rapid 或 speedy 。 为了区分，Elasticsearch 也支持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。 如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样： 查询自己定义的 analyzer ，否则字段映射里定义的 search_analyzer ，否则字段映射里定义的 analyzer ，否则索引设置中名为 default_search 的分析器，默认为索引设置中名为 default 的分析器，默认为standard 标准分析器 保持简单多数情况下，会提前知道文档会包括哪些字段。最简单的途径就是在创建索引或者增加类型映射时，为每个全文字段设置分析器。这种方式尽管有点麻烦，但是它让我们可以清楚的看到每个字段每个分析器是如何设置的。 通常，多数字符串字段都是 not_analyzed 精确值字段，比如标签（tag）或枚举（enum），而且更多的全文字段会使用默认的 standard 分析器或 english 或其他某种语言的分析器。这样只需要为少数一两个字段指定自定义分析：或许标题 title 字段需要以支持 输入即查找（find-as-you-type） 的方式进行索引。 可以在索引级别设置中，为绝大部分的字段设置你想指定的 default 默认分析器。然后在字段级别设置中，对某一两个字段配置需要指定的分析器。 📒 📒 📒对于和时间相关的日志数据，通常的做法是每天自行创建索引，由于这种方式不是从头创建的索引，仍然可以用 索引模板（Index Template） 为新建的索引指定配置和映射。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 控制分析","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"Parameter index out of range (1 > number of parameters, which is 0).","date":"2021-07-06T05:03:16.000Z","path":"wiki/Parameter-index-out-of-range-1-number-of-parameters-which-is-0/","text":"问题记录123456789101112132021-07-06 12:39:31.179 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:39:31.186 [http-nio-8081-exec-2] ERROR com.alibaba.otter.canal.client.adapter.support.Util - sqlRs has error, sql: SELECT COUNT(1) FROM ( select t.redtom_id as id, t.username, t.nickname, t.avatar, t.status, t.mobile, t.mobile_region_no, t.email, t.gender, t.password,t.salt,t.birthday,t.introduce,t.country,t.region,t.level,t.is_vip,t.follows ,t.fans,t.likes_num, t.collects_num, t.instagram_account, t.youtube_account, t.facebook_account, t.twitter_account,t.create_ip, t.create_time,t.update_time from rd_member r where t.create_time&gt;=&#x27;&#123;0&#125;&#x27;) _CNT2021-07-06 12:39:31.188 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0).java.lang.RuntimeException: java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0). at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:65) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:62) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] 如何解决我执行的操作如下：👇curl http://127.0.0.1:8081/etl/es7/customer.yml -X POST -d &quot;params=2019-08-31 00:00:00&quot; 但是我的 es7/rd_member.yml的配置文件如下： etlCondition:&quot;where a.c_time&gt;=&#39;&#123;0&#125;&#39;&quot; # etl 的条件参数 应该改成：etlCondition:&quot;where a.c_time&gt;=&#123;&#125;&quot; # etl 的条件参数","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"field name is null or empty","date":"2021-07-06T04:53:44.000Z","path":"wiki/field-name-is-null-or-empty/","text":"canal adapter 报错信息123456789101112131415161718192021222021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO o.a.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring FrameworkServlet &#x27;dispatcherServlet&#x27;2021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization started2021-07-06 12:46:31.968 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization completed in 9 ms2021-07-06 12:46:31.995 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:46:32.027 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - field name is null or emptyjava.lang.IllegalArgumentException: field name is null or empty at org.elasticsearch.index.query.BaseTermQueryBuilder.&lt;init&gt;(BaseTermQueryBuilder.java:113) ~[na:na] at org.elasticsearch.index.query.TermQueryBuilder.&lt;init&gt;(TermQueryBuilder.java:75) ~[na:na] at org.elasticsearch.index.query.QueryBuilders.termQuery(QueryBuilders.java:202) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.lambda$executeSqlImport$1(ESEtlService.java:141) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:60) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.executeSqlImport(ESEtlService.java:64) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:105) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) ~[na:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke 问题排查操作是向数据库中插入一条数据，通过canal-adapter同步到elasticsearch中，接口发生以上错误！现象是canal-adapter检测到和mysql的数据变化，但是同步到es的时候发生了错误；猜想大概是某个为空导致存到es的时候发生异常； 然后查看es7下的mapping配置： 发现我的sql查id的时候写错了，别名应该写成_id,对应elasticsearch的_id","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"elasticsearch映射","date":"2021-07-05T14:54:05.000Z","path":"wiki/elasticsearch映射/","text":"Elasticsearch 支持如下简单域类型： 字符串: string （es7之后编程text） 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 查看索引的mappingGET /gb/_mapping/tweet 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 自定义域映射尽管在很多情况下基本域数据类型已经够用，但你经常需要为单独域自定义映射，特别是字符串域。自定义映射允许你执行下面的操作： 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： 12345&#123; &quot;number_of_clicks&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;&#125; 默认， string (text) 类型域会被认为包含全文。就是说，它们的值在索引前，会通过一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。 string 域映射的两个最重要属性是 index 和 analyzer 。 indexindex 属性控制怎样索引字符串。它可以是下面三个值： analyzed首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。 no不索引这个域。这个域不会被搜索到。 (比如一些隐私信息) string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ： ⚠️ ⚠️其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。 analyzer对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english; 123456&#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 更新映射当你首次创建一个索引的时候，可以指定类型的映射。你也可以使用 /_mapping 为新类型（或者为存在的类型更新映射）增加映射。⚠️ ⚠️尽管你可以 增加 一个存在的映射，你不能 修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。 我们可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed 。 为了描述指定映射的两种方式，我们先删除 gd 索引：DELETE /gb然后创建一个新索引，指定 tweet 域使用 english 分析器： 12345678910111213141516171819202122PUT /gb &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 稍后，我们决定在 tweet 映射增加一个新的名为 tag 的 not_analyzed 的文本域，使用 _mapping ： 123456789PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中 测试映射你可以使用 analyze API 测试字符串域的映射。比较下面两个请求的输出： 1234567891011GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125; tweet 域产生两个词条 black 和 cat ， tag 域产生单独的词条 Black-cats 。换句话说，我们的映射正常工作。 参考资料 Elasticsearch权威指南","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"分析与分析器","date":"2021-07-05T14:43:44.000Z","path":"wiki/分析与分析器/","text":"分析包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 ，之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。 内置分析器但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：&quot;Set the shape to semi-transparent by calling set_trans(5)&quot; 标准分析器标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。 什么时候使用分析器当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。 全文查询，理解每个域是如何定义的，因此它们可以做正确的事： 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。 测试分析器有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本： 12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条： 12345678910111213141516171819202122232425&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。 每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于​keep_types token 过滤器​。 analyze API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。 你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域—​不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。 要做到这一点，我们必须手动指定这些域的映射。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"倒排索引","date":"2021-07-05T14:17:00.000Z","path":"wiki/倒排索引/","text":"Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容： The quick brown fox jumped over the lazy dogQuick brown foxes leap over lazy dogs in summer为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。 如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如： Quick 可以小写化为 quick 。foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的， dogs 可以为提取为 dog 。jumped 和 leap 是同义词，可以索引为相同的单词 jump 。现在索引看上去像这样：这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！ 这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch分页查询","date":"2021-07-05T14:08:23.000Z","path":"wiki/elasticsearch分页查询/","text":"和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size显示应该返回的结果数量，默认是 10from显示应该跳过的初始结果数量，默认是 0如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果： GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 ⚠️ ⚠️ ⚠️考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。 在分布式系统中深度分页 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页—​结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因 参考资料 elasticsearch权威指南 干货 | 全方位深度解读 Elasticsearch 分页查询 Paginate search results","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"多索引多类型搜索","date":"2021-07-05T14:02:34.000Z","path":"wiki/多索引多类型搜索/","text":"如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。 然而，经常的情况下，你想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示： /_search在所有的索引中搜索所有的类型/gb/_search在 gb 索引中搜索所有的类型/gb,us/_search在 gb 和 us 索引中搜索所有的文档/g*,u*/_search在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search在所有的索引中搜索 user 和 tweet 类型当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的—​只是会涉及到更多的分片。 注意 ⚠️搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch重要配置","date":"2021-07-05T13:22:32.000Z","path":"wiki/elasticsearch重要配置/","text":"虽然Elasticsearch仅需要很少的配置，但有许多设置需要手动配置，并且在进入生产之前绝对必须进行配置。 path.data 和 path.logscluster.namenode.namebootstrap.memory_locknetwork.hostdiscovery.zen.ping.unicast.hostsdiscovery.zen.minimum_master_nodespath.data 和 path.logs如果使用.zip或.tar.gz归档，则数据和日志目录是$ES_HOME的子文件夹。 如果这些重要的文件夹保留在其默认位置，则存在将Elasticsearch升级到新版本时被删除的高风险。 在生产使用中，肯定得更改数据和日志文件夹的位置： 123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch RPM和Debian发行版已经使用数据和日志的自定义路径。 path.data 设置可以设置为多个路径，在这种情况下，所有路径将用于存储数据（属于单个分片的文件将全部存储在同一数据路径上）： 12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 cluster.name节点只能在群集与群集中的所有其他节点共享其cluster.name时才能加入群集。 默认名称为elasticsearch，但您应将其更改为描述集群用途的适当名称。cluster.name: logging-prod确保不要在不同的环境中重复使用相同的集群名称，否则可能会导致加入错误集群的节点。 node.name默认情况下，Elasticsearch将使用随机生成的uuid的第一个字符作为节点id。 请注意，节点ID是持久化的，并且在节点重新启动时不会更改，因此默认节点名称也不会更改。配置一个更有意义的名称是值得的，这是重启节点后也能一直保持的优势：node.name: prod-data-2node.name也可以设置为服务器的HOSTNAME，如下所示： 12node.name: $&#123;HOSTNAME&#125;bootstrap.memory_lock 没有JVM被交换到磁盘上这事对于节点的健康来说是至关重要的。一种实现方法是将bootstrap.memory_lock设置为true。要使此设置生效，需要首先配置其他系统设置。 有关如何正确设置内存锁定的更多详细信息，请参阅启用bootstrap.memory_lock。 network.host默认情况下，Elasticsearch仅仅绑定在本地回路地址——如：127.0.0.1与[::1]。这在一台服务器上跑一个开发节点是足够的。提示 事实上，多个节点可以在单个节点上相同的$ES_HOME位置一同运行。这可以用于测试Elasticsearch形成集群的能力,但这种配置方式不推荐用于生产环境。 为了将其它服务器上的节点形成一个可以相互通讯的集群，你的节点将不能绑定在一个回路地址上。 这里有更多的网路配置，通常你只需要配置network.host：network.host: 192.168.1.10network.host也可以配置成一些能识别的特殊的值，譬如：_local_、_site、_global_，它们可以结合指定:ip4与ip6来使用。更多相信信息请参见：网路配置 重要 👇 一旦你自定义了network.host的配置，Elasticsearch将假设你已经从开发模式转到了生产模式，并将升级系统检测的警告信息为异常信息。更多信息请参见：开发模式vs生产模式 discovery.zen.ping.unicast.hosts（单播发现）开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的回路地址，并扫描9300年到9305的端口去连接同一机器上的其他节点,试图连接到相同的服务器上运行的其他节点。它提供了不需要任何配置就能自动组建集群的体验。当与其它机器上的节点要形成一个集群时，你需要提供一个在线且可访问的节点列表。像如下来配置： 1234discovery.zen.ping.unicast.hosts: - 192.168.1.10:9300 - 192.168.1.11 #① - seeds.mydomain.com #② ① 未指定端口时，将使用默认的transport.profiles.default.port值，如果此值也为设置则使用transport.tcp.port ② 主机名将被尝试解析成能解析的多个IP discovery.zen.minimum_master_nodes为防止数据丢失，配置discovery-zen-minimum_master_nodes将非常重要，他规定了必须至少要有多少个master节点才能形成一个集群。没有此设置时，一个集群在发生网络问题是可能会分裂成多个集群——脑裂——这将导致数据丢失。更多详细信息请参见：通过minimum_master_nodes避免脑裂为避免脑裂，你需要根据master节点数来设置法定人数：(master_eligible_nodes / 2) + 1换句话说，如果你有三个master节点，最小的主节点数因被设置为(3/2)+1或者是2discovery.zen.minimum_master_nodes: 2 参考资料 elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch操作索引","date":"2021-07-05T13:11:01.000Z","path":"wiki/elasticsearch操作索引/","text":"创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 查看索引的mappingGET yj_visit_data/_mapping 1234567891011121314151617181920212223242526&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查询所有GET yj_visit_data/_search 12345&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 删除所有POST yj_visit_data/_delete_by_query 123456&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125; 通过文章删除POST yj_visit_data/_delete_by_query 1234567&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 根据文章查询GET yj_visit_data/_search 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 修改索引1234POST customer/_doc/1&#123; &quot;name&quot;:&quot;2&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch基础api","date":"2021-07-05T12:53:09.000Z","path":"wiki/elasticsearch基础cat_api/","text":"cat API集群健康状态GET _cat/health?v&amp;pretty 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1625489855 12:57:35 my-application yellow 1 1 35 35 0 0 23 0 - 60.3% 或者直接在服务器上调用rest接口：curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’ 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1475247709 17:01:49 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 我们可以看到我们名为 my-application 的集群与 yellow 的 status。 无论何时我们请求集群健康，我们可以获得 green，yellow，或者 red 的 status。Green 表示一切正常（集群功能齐全）， yellow 表示所有数据可用，但是有些副本尚未分配（集群功能齐全），red 意味着由于某些原因有些数据不可用。注意，集群是 red，它仍然具有部分功能（例如，它将继续从可用的分片中服务搜索请求），但是您可能需要尽快去修复它，因为您已经丢失数据了。 另外，从上面的响应中，我们可以看到共计 1 个 node（节点）和 0 个 shard（分片），因为我们还没有放入数据的。注意，因为我们使用的是默认的集群名称（elasticsearch），并且 Elasticsearch 默认情况下使用 unicast network（单播网络）来发现同一机器上的其它节点。有可能您不小心在您的电脑上启动了多个节点，然后它们全部加入到了单个集群。在这种情况下，你会在上面的响应中看到不止 1 个 node（节点）。 查看集群分布GET _cat/nodes?v&amp;pretty 12ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name172.19.0.1 20 61 15 0.02 0.04 0.29 cdhilmrstw * redtom-es-1 查看所有索引GET _cat/indices?v&amp;pretty 1234health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open rd-logstash-2021.06.19 p5iej71MQVW12s2awNv8nw 1 1 61236 0 16.3mb 16.3mbyellow open demo_index k6VTs7tdS0ysot-rPwxG9A 1 1 1 0 5.5kb 5.5kbgreen open kibana_sample_data_flights A7c5DViGSISii8FA0dNlGw 1 0 13059 0 5.6mb 5.6mb 查看所有索引的数量GET _cat/count?v&amp;pretty 12epoch timestamp count1625490245 13:04:05 838913 磁盘分配情况GET _cat/allocation?v&amp;pretty 123shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 35 308.7mb 20.1gb 215.9gb 236.1gb 8 172.19.0.1 172.19.0.1 redtom-es-1 23 UNASSIGNED 查看shard情况GET _cat/shards?v&amp;pretty 12345678index shard prirep state docs store ip nodeyj_visit_data 0 p STARTED 0 208b 172.19.0.1 redtom-es-1yj_visit_data 0 r UNASSIGNED demo_index 0 p STARTED 1 5.5kb 172.19.0.1 redtom-es-1demo_index 0 r UNASSIGNED rbtags 0 p STARTED 0 208b 172.19.0.1 redtom-es-1.kibana_1 0 p STARTED 280 11.5mb 172.19.0.1 redtom-es-1.kibana_task_manager_1 0 p STARTED 5 5.8mb 172.19.0.1 redtom-es-1 yj_visit_data 设置了一个副本分区，但是没有副节点，所以节点状态显示未分配； 参考资料 Elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"canal同步mysql数据到elasticsearch","date":"2021-07-05T03:26:50.000Z","path":"wiki/canal同步mysql数据到elasticsearch/","text":"首先安装elk推荐大家到elasic中文社区去下载 👉 【传送】⚠️ elastcisearch | logstash | kibana 的版本最好保持一直，否则会出现很多坑的，切记！ 安装ELK的步骤这里就不做介绍了，可以查看 👉 【TODO】 下载安装canal-adaptercanal github传送门 👉 【Alibaba Canal】 canal-client 模式可以参照canal给出的example项目和官方文档给出的例子来测试 依赖配置12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 创建maven项目保证canal-server 已经正确启动 👈 然后启动下面服务，操作数据库即可看到控制台的日志输出； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.redtom.canal.deploy;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import java.net.InetSocketAddress;import java.util.List;/** * @Author gaolei * @Date 2021/6/30 2:57 下午 * @Version 1.0 */@Slf4j@Componentclass CanalClient implements InitializingBean &#123; private final static int BATCH_SIZE = 1000; @Override public void afterPropertiesSet() throws Exception &#123; // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(&quot;***.***.***.***&quot;, 11111), &quot;example&quot;, &quot;canal&quot;, &quot;canal&quot;); try &#123; //打开连接 connector.connect(); //订阅数据库表,全部表 connector.subscribe(&quot;.*\\\\..*&quot;); //回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有ack的地方开始拿 connector.rollback(); while (true) &#123; // 获取指定数量的数据 Message message = connector.getWithoutAck(BATCH_SIZE); //获取批量ID long batchId = message.getId(); //获取批量的数量 int size = message.getEntries().size(); //如果没有数据 if (batchId == -1 || size == 0) &#123; try &#123; //线程休眠2秒 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; //如果有数据,处理数据 printEntry(message.getEntries()); &#125; //进行 batch id 的确认。确认之后，小于等于此 batchId 的 Message 都会被确认。 connector.ack(batchId); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; connector.disconnect(); &#125; &#125; /** * 打印canal server解析binlog获得的实体类信息 */ private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) &#123; //开启/关闭事务的实体类型，跳过 continue; &#125; //RowChange对象，包含了一行数据变化的所有特征 //比如isDdl 是否是ddl变更操作 sql 具体的ddl sql beforeColumns afterColumns 变更前后的数据字段等等 CanalEntry.RowChange rowChage; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;ERROR ## parser of eromanga-event has an error , data:&quot; + entry.toString(), e); &#125; //获取操作类型：insert/update/delete类型 CanalEntry.EventType eventType = rowChage.getEventType(); //打印Header信息 log.info(&quot;headers:&#123;&#125; &quot;, String.format(&quot;================》; binlog[%s:%s] , name[%s,%s] , eventType : %s&quot;, entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); //判断是否是DDL语句 if (rowChage.getIsDdl()) &#123; log.info(&quot;================》;isDdl: true,sql: &#123;&#125;&quot;, rowChage.getSql()); &#125; //获取RowChange对象里的每一行数据，打印出来 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; //如果是删除语句 if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); //如果是新增语句 &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); //如果是更新的语句 &#125; else &#123; //变更前的数据 log.info(&quot;-------&gt;; before&quot;); printColumn(rowData.getBeforeColumnsList()); //变更后的数据 log.info(&quot;-------&gt;; after&quot;); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; for (CanalEntry.Column column : columns) &#123; log.info(&quot; &#123;&#125; : &#123;&#125; update= &#123;&#125;&quot;, column.getName(), column.getValue(), column.getUpdated()); &#125; &#125;&#125; canal-adapter 模式adapter 配置文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546server: port: 8081spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1 batchSize: 1 retries: 0 timeout: accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 172.25.101.75:11111 canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 canal.tcp.username: canal.tcp.password: srcDataSources: defaultDS: url: jdbc:mysql://xxxx:pppp/database?useUnicode=true username: root password: pwd canalAdapters: - instance: example # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: es7 hosts: 172.25.101.75:9300 # 127.0.0.1:9200 for rest mode properties: mode: transport # or rest# # security.auth: test:123456 # only used for rest mode cluster.name: my-application# - name: kudu# key: kudu# properties:# kudu.master.address: 127.0.0.1 # &#x27;,&#x27; split multi address 我的elasticsearch是7.10.0版本的application.yml bootstrap.yml es6 es7 hbase kudu logback.xml META-INF rdb所以：👇 123cd es7biz_order.yml customer.yml mytest_user.ymlvim customer.yml customer.yml 配置文件如下： 123456789101112dataSourceKey: defaultDSdestination: examplegroupId: g1esMapping: _index: customer _id: id relations: customer_order: name: customer sql: &quot;select t.id, t.name, t.email from customer t&quot; etlCondition: &quot;where t.c_time&gt;=&#123;&#125;&quot; commitBatch: 3000 创建表结构12345678910CREATE TABLE `customer` ( `id` bigint(20) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, `email` varchar(255) DEFAULT NULL, `order_id` int(11) DEFAULT NULL, `order_serial` varchar(255) DEFAULT NULL, `order_time` datetime DEFAULT NULL, `customer_order` varchar(255) DEFAULT NULL, `c_time` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 测试canal-adapter同步效果创建一条记录122021-07-05 11:50:53.725 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;1&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457053000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457053724,&quot;type&quot;:&quot;INSERT&quot;&#125;Affected indexes: customer Elastcisearch 效果 1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;1&quot;, &quot;email&quot; : &quot;1&quot;, &quot;customer_order&quot; : &#123; &quot;name&quot; : &quot;customer&quot; &#125; &#125; &#125; ] &#125;&#125; 修改数据122021-07-05 11:54:36.402 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457275000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:[&#123;&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;customer_order&quot;:&quot;1&quot;&#125;],&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457276401,&quot;type&quot;:&quot;UPDATE&quot;&#125;Affected indexes: customer Elastcisearch 效果 删除一条数据122021-07-05 11:56:51.524 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457411000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457411523,&quot;type&quot;:&quot;DELETE&quot;&#125;Affected indexes: customer Elastcisearch 效果 参考资料 使用canal client-adapter完成mysql到es数据同步教程(包括全量和增量) es 同步问题 #1514 Github issue canal v1.1.4 文档手册 Sync es","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"mysql配置binlog","date":"2021-07-05T03:06:36.000Z","path":"wiki/binlog配置/","text":"开启binlog[mysqld]log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 查看binlog状态mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"mysql常用命令","date":"2021-07-05T03:06:36.000Z","path":"wiki/mysql常用命令/","text":"binlog相关命令mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+ 用户&amp;权限创建用户并授权root用户登录： mysql -u root -p 然后输入密码创建用户： create user &#39;yjuser&#39;@&#39;%&#39; identified by &#39;u-bx.com&#39;;授权用户只读权限： grant SELECT on mirror.* to &#39;yjuser&#39;@&#39;%&#39; IDENTIFIED by &#39;u-bx.com&#39;;刷新权限：flush privileges; 查看当前用户select User();","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"flink简单上手","date":"2021-07-04T14:23:43.000Z","path":"wiki/flink简单上手/","text":"mac 安装 flink1、执行 brew install apache-flink 命令 123456789gaolei:/ gaolei$ brew install apache-flinkUpdating Homebrew...==&gt; Auto-updated Homebrew! Updated 1 tap (homebrew/services).No changes to formulae.==&gt; Downloading https://archive.apache.org/dist/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgz######################################################################## 100.0%🍺 /usr/local/Cellar/apache-flink/1.9.1: 166 files, 277MB, built in 15 minutes 29 seconds 2、执行flink启动脚本 12/usr/local/Cellar/apache-flink/1.9.1/libexec/bin./start-cluster.sh WordCount批处理Demo创建maven项目，导入依赖 注意自己的flink版本 👇👇 12345678910111213141516171819202122&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写批处理程序123456789101112131415161718192021222324public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125; &#125; 准备数据源文件123456hello sparkhello worldhello javahello flinkhow are youwhat is your name 执行结果123456789101112(is,1)(what,1)(you,1)(flink,1)(name,1)(world,1)(hello,4)(your,1)(are,1)(java,1)(how,1)(spark,1) flink 处理流式数据1、通过 nc -lk &lt;port&gt; 打开一个socket服务，监听7777端口 用于模拟实时的流数据 2、java代码处理流式数据 123456789101112131415161718192021222324252627public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数（设置成1即单线程处理） // env.setMaxParallelism(32); // 从文件中读取数据// String inputPath = &quot;/tmp/Flink_Tutorial/src/main/resources/hello.txt&quot;;// DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 4、在首次启动的时候遇到一个错误 ❌Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/streaming/api/datastream/DataStream处理方法可参照 参考资料 👇 参考资料 Exception in thread “main” java.lang.NoClassDefFoundError 解决方案 https://flink.apache.org/zh/downloads.html https://www.cnblogs.com/zlshtml/p/13796793.html","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Apache Flink","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink简介/","text":"官方地址请戳👉 【传送】 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink 是一个框架和分布式处理器引擎，用于对无界和有界数据进行状态计算； Why Flink 流数据更真实地反应了我们的生活方式 传统的数据架构是基于有限数据集的 我们的目标1、低延迟 毫秒级响应2、高吞吐 能够处理海量数据 分布式3、结果的准确性和良好的容错性 Where need Flink 电商和市场营销数据报表、广告投放、业务流程需要 物联网（IOT）传感器实时数据采集和显示，实时报警，交通运输业 电信业基站流量调配 银行和金融业实时结算和通知推送、实时检测异常行为 传统数据处理架构 传统的数据处理架构如上👆CRM(用户关系系统)， Order System(订单系统), Web App (用户点击时间)，当用户出发行为之后需要系统作出响应，首先由上层的计算层处理计算逻辑，计算层的逻辑计算依赖下面的存储层，计算层计算完成之后，将响应返回给客户端。这种基于传统数据库方式无法满足高并发场景，数据库的并发量都是很低的。 分析处理流程 分析处理流程架构如上👆，数据先有传统的关系数据库，经过提取，清洗过滤等，将数据存放到数据仓库，然后通过一些sql处理，生成数据报表和一些其他的查询。 问题也很明显，实时性太差了，处理流程太长，无法满足毫秒级需求 数据来源不唯一，能满足海量数据和高并发的需求，但是无法满足实时的需求 有状态的流式处理 把当前做流式计算所需要的数据不存放在数据库中，而是简单粗暴的直接放到本地内存中； 内存不稳定？周期性的检查点，数据存盘和故障检测； lambda架构用两台系统同时保障低延迟和结果准确； 这套架构分成两个流程，上面为批处理流程，数据收集到一定程序，交给批处理器处理，最终产生一个批处理结果 下面的流程为流式处理流程，保证能快速得到结果 最终有我们在应用层根据实际问题选择具体的处理结果交给应用程序这种架构有什么缺陷？可能得到的结果是不准确的，我们可以先快速的得到一个实时计算的结果，隔一段时间之后在来看批处理产生的结果。实现两台系统和维护两套系统，成本很大； 第三代流式处理架构Apache Flink 可以完美解决上面的问题👆Strom无法满足海量数据； Sparking Stream 无法满足低延迟； 基于事件驱动 （Event-driven） 处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。 无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流&lt;/&gt; 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 其他特点 支持事件时间（event-time）和处理时间（processing-time）语义 精确一次的状态一致性保证 低延迟 每秒处理数百万个事件，毫秒级延迟 与众多常用的存储系统链接 高可用，动态扩展，支持7*24全天运行 参考资料 1、尚硅谷 2021 Flink Java版2、Apache Flink Documentation","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Flink 运行时架构","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink运行时架构/","text":"Flink运行时组件 JobManager (作业管理器) JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成： ResourceManagerResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。 DispatcherDispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。 JobMasterJobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。 TaskManager （任务管理器） Flink中的工作进程，通常在flink中会有多个TaskManager运行，每一个TaskMaganer都包含一定数量的插槽（slots）. 插槽的数量限制了TaskManager能够执行的任务数量； 启动之后，TaskManager会向资源管理器注册他的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用，JobManager就可以向插槽分配任务（tasks）来执行了 在执行的过程中，一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据。 任务提交流程 任务调度原理Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。 Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run …中运行。 可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。 思考问题🤔 怎样实现并行计算？ 多线程 并行的任务，需要占用多少solt？ 一个流处理程序，到底包含多少个任务？ Tasks 和算子链并行度（Parallelism） 一个特定算子的子任务（subtask）的个数被称之为并行度； 一般情况下，一个Stream的并行度就是其所有算子中最大的并行度。整个流也有一个并行度，就是所有算子所有任务的并行度之和；对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。链行为是可以配置的；请参考链文档以获取详细信息。 TaskManager 和 Slots","tags":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"拥塞避免","date":"2021-07-04T08:46:13.000Z","path":"wiki/拥塞避免/","text":"拥塞避免拥塞控制的慢启动是以指数方式快速的通过试探来扩大拥塞窗口的，但是一旦发生网络丢包，则肯定是很多报文段都会都是，因为窗口时称被增长的；为了解决这种问题，需要引入– 拥塞避免 什么是拥塞避免拥塞避免为了解决慢启动下，当拥塞窗口超出网络带宽时发生的大量丢包问题，它提出一个「慢启动阈值」的概念，当拥塞窗口到达这个阈值之后，不在以指数方式增长，而选择涨幅比较缓慢的「线性增长」，计算方式： cwnd += SMSS*SMSS/cwnd 当拥塞窗口在线性增长时发生丢包，将慢启动阈值设置为当前窗口的一半，慢启动窗口恢复初始窗口（init wnd）； 拥塞避免和慢启动是结合使用的，当发生网络丢包是，拥塞控制采用快速重传和快速启动来解决丢包问题！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-四次挥手/断开连接","date":"2021-07-04T08:34:11.000Z","path":"wiki/TCP-四次挥手-断开连接/","text":"TCP断开连接四次挥手 开始客户端和服务端都是处理【established】状态 客户端发送「FIN」报文之后，进入FIN-WAIT-1状态 服务端收到客户端的FIN之后，恢复一个ACK，同时进入CLOSE_WAIT状态 客户端接收到ACK之后，进入到FIN-WAIT-2状态 服务端接着发送FIN报文，同时进入LAST-ACK状态 客户端接收到服务端的FIN报文之后，发送ACK报文，并进入TIME_WAIT状态 客户端在经历2个MSL时间之后，进入CLOSE状态 服务端接收到客户端的ACK之后，进入CLOSE状态 并不是所有的四次挥手都是上述流程，当客户端和服务端同时发送关闭连接的请求如下👇： 可以看到双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN_WAIT_1 都进入到 CLOSING 这个过度状态然后再到 TIME_WAIT。 挥手一定需要四次吗？ 假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？ 为什么要有 TIME_WAIT? 断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。 那么为什么要等 2MSL 呢？ 就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。 假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。 等待 2MSL 会产生什么问题？ 如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"快速重传/快速恢复","date":"2021-07-04T08:33:35.000Z","path":"wiki/快速重传-快速恢复/","text":"快速重传和快速恢复快速重传 d 为何会接收到以个失序数据段？ 若报文丢失，将会产生连续的失序ACK段 若网络路径与设备导致数据段失序，将会产生少量的失序ACK段 若报文重复，将会产生少量的失序ACK段 当发送端发送pkt0是正常的，由于滑动窗口为满，发送方可以继续发送pkt1，pkt2； 加入pkt1发生了丢包，虽然pkt2接收端接收成功了，但是没有pkt1的数据段，接收端还是发送ACK1的确认报文； 在没有「快速重传」的情况下，发送端需要等到RTO之后，才可以重新发送pkt1 重传成功之后，接收端其实收到了pkt2之前的所有数据段，所以发送ACK3的确认报文 这种需要等待RTO才可以重传的方式效率是比较低的，因此需要快速重传来进行优化； 快速重传和累积确认 当发送方连续发送pkt3，pkt4，pkt5，pkt6四个数据端，但是pkt5在网络中丢包了，那后面发送的pkt6，pkt7，pkt8的确认报文都返回ACK5，希望发送方吃昂传pkt5的数据段；这个时候，发送方收到连续3个相同的确认报文，便立即重新发送pkt5的数据段； 接收方: 当接收到一个失序数据段时，立刻发送它所期待的缺口 ACK 序列号 当接收到填充失序缺口的数据段时，立刻发 送它所期待的下一个 ACK 序列号 发送方 当接收到3个重复的失序 ACK 段(4个相同的失序ACK段)时，不再等待重传定时器的触发，立刻基于快速重传机制重发报文段 当pkt5重新发送并被接收端接收之后，接收端发送ACK9的确认报文，而不是再分别发送ACK6，ACK7，ACK8，这个称谓「 累计确认 」。 快速恢复 快速重传下一定要进入慢启动吗? 接受端收到重复ACK，意味着网络仍在流动，而如果要重新进入慢启动，会导致网络突然减少数据流，拥塞窗口恢复初始窗口，所以，「在快速恢复下发生丢包的场景下」，应该使用快速恢复，简单的讲，就是将慢启动阈值设置成当前拥塞窗口的一半，而拥塞窗口也适当放低，而不是一下字恢复到初始窗口大小； 快速恢复的流程如上图👆所示！ 快速恢复的具体操作： 将 ssthresh 设置为当前拥塞窗口 cwnd 的一半，设当前 cwnd 为 ssthresh 加上 3*MSS 每收到一个重复 ACK，cwnd 增加 1 个 MSS 当新数据 ACK 到达后，设置 cwnd 为 ssthresh","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-拥塞控制之慢启动","date":"2021-07-04T08:33:19.000Z","path":"wiki/TCP-拥塞控制之慢启动/","text":"由于TCP是面向字节流的传输协议，可以发送不定长的字节流数据，TCP连接发送数据时会“先天性”尝试占用整个带宽，而当所有的TCP连接都尝试占用网络带宽时，就会造成网络的堵塞，而TCP慢启动算法则是为了解决这一场景； 全局思考 拥塞控制要面向整体思考，如上👆网络拓扑图，当左边的网络节点通过路由交换设备向右边的设备传输报文的时候，中间的某一链路的带宽肯定是一定的，这里假设1000M带宽，当左边R1以700Mb/s的速度向链路中发送数据，同时R2以600Mb/s的速率发送报文，那势必会有300Mb的数据报丢失；「路由交换设备基于存储转发来实现报文的发送」大量报文都是时，路由设备的缓冲队列肯定是慢的，这也会造成某些数据报在网络链路中停留时间过长，从而导致TCP通讯变慢，甚至网络瘫痪； 理想的情况下，当链路带宽占满以后，链路以最大带宽传输数据，当然显示中是不可能的，当发生轻度拥塞时，链路的吞吐量就开始下降了，发展到严重阻塞时，链路的吞吐量会严重地下降，甚至瘫痪； 那么，慢启动是如何发挥作用的呢？ 拥塞窗口 拥塞窗口cwnd(congestion window) 通告窗口rwnd(receiver‘s advertised window) 其实就是RCV.WND，标志在TCP首部的Window字段！ 发送窗口swnd = min(cwnd，rwnd) 前面学习滑动窗口的时候提到发送窗口大致等于接受窗口，当引入拥塞窗口时，发送窗口就是拥塞窗口和对方接受窗口的最小值 每收到一个ACK，cwnd扩充一倍 慢启动的窗口大小如何设置呢？如上所示，起初拥塞窗口设置成1个报文段大小，当发送端发送一个报文段并且没有发生丢包时，调整拥塞窗口为2个报文段大小，如果还没有发生丢包，一次类推，知道发生丢包停止；发送窗口以「指数」的方式扩大；慢启动是无法确知网络拥塞程度的情况下，以试探性地方式快速扩大拥塞窗口； 慢启动初始窗口慢启动的拥塞窗口真的就如上面所说的以一个报文段大小作为初始值吗？ 慢启动初始窗口 IW(Initial Window)的变迁 1 SMSS:RFC2001(1997) 2 - 4 SMSS:RFC2414(1998) IW = min (4SMSS, max (2SMSS, 4380 bytes)) 10 SMSS:RFC6928(2013) IW = min (10MSS, max (2MSS, 14600)) 其实在实际情况下，互联网中的网页都在10个mss左右，如果还是从1个mss开始，则会浪费3个RTT的时间；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-如何减少小报文提升网络效率","date":"2021-07-04T08:32:55.000Z","path":"wiki/TCP-如何减少小报文提升网络效率/","text":"如何减少小报文提升网络效率每一个TCP报文段都包含20字节的IP头部和20字节的TCP首部，如果报文段的数据部分很少的话，网络效率会很差； SWS(Silly Window syndrome) 糊涂窗口综合症 如上图👆所示场景，在之前的滑动窗口已经了解过，随着服务端处理连接数据能力越来越低，服务端的可用窗口不断压缩，最终导致窗口关闭； SWS 避免算法SWS 避免算法对发送方和接收方都做客 接收方 David D Clark 算法:窗口边界移动值小于 min(MSS, 缓存/2)时，通知窗口为 0 发送方 Nagle 算法:1、TCP_NODELAY 用于关闭 Nagle 算法2、没有已发送未确认报文段时，立刻发送数据3、存在未确认报文段时，直到:1-没有已发送未确认报文段，或者 2-数据长度达到MSS时再发送 TCP delayed acknowledgment 延迟确认实际情况下，没有携带任何数据的ACK报文也会造成网络效率低下的，因为确认报文也包含40字节的头部信息，但仅仅是为了传输ACK=1这样的信息，为了解决这种情况，TCP有一种机制，叫做延迟确认，如下👇： 当有响应数据要发送时,ack会随着响应数据立即发送给对方. 如果没有响应数据,ack的发送将会有一个延迟,以等待看是否有响应数据可以一起发送 如果在等待发送ack期间,对方的第二个数据段又到达了,这时要立即发送ack 那个延迟的时间如何设置呢？ 上面👆是Linux操作系统对于TCP延时的定义。 HZ是什呢？其实那是和操作系统的时钟相关的，具体的操作系统间各有差别；如何查看Linux操作系统下的HZ如何设置呢？ 1cat /boot/config- `-uname -r` | grep &#x27;^GONFIG_HZ=&#x27; TCP_CORK sendfile 零拷贝技术","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-滑动窗口","date":"2021-07-04T08:32:40.000Z","path":"wiki/TCP-滑动窗口/","text":"滑动窗口 之前学习了PAR方式的TCP超时和重传，其实在考虑发送方发送数据报的同时，也应该考虑接收方对于数据的处理能力，由此引出本次学习的主题 – 滑动窗口 发送端窗口滑动窗口按照传输数据方向分为两种，发送端窗口和接收端窗口；下面先看一下发送端窗口👇： 上图分为四个部分： 已发送并收到 Ack 确认的数据:1-31 字节 已发送未收到 Ack 确认的数据:32-45 字节 未发送但总大小在接收方处理范围内:46-51 字节 未发送但总大小超出接收方处理范围:52-字节 可用窗口和发送窗口 如上图这里可以引出两个概念：「可用窗口」和「发送窗口」 【 可用窗口 】： 就是上图中的第三部分，属于还未发送，但是在接收端可以处理范围内的部分；【 发送窗口 】： 就是发送端可以发送的最大报文大小，如上图中的第二部分+第三部分合成发送窗口； 可用窗口耗尽 可用窗口会在一个短暂的停留，当处于未发送并且接受端可以接受范围内的数据传输完成之后，可用窗口耗尽；当然上面仅仅说的一瞬时的状态，这个状态下，已经发送的报文段还没有确认，并且发送窗口大小没有发生变化，此时发送窗口达到最大状态； 窗口移动 如果在发送窗口中已经发送的报文段已经得到接受端确认之后，那部分数据就会被移除发送窗口，在发送窗口大小不发生变化的情况下，发送窗口向右➡️移动5个字节，因为左边已经发送的5个字节得到确认之后，被移除发送窗口； 可用窗口如何计算 再次引出三个概念： SND.WND SND 指的是发送端，WND指的是window，也就是发送端窗口的意思 SND.UNA UNA 就是un ACK的意思，指的是已经发送但是没有没有确认 它指向窗口的第一个字节处 SND.NXT NXT 是next的位置，是发送方接下来要发送的位置，它指向可用窗口的第一个字节处 那就很容易得出可用窗口的大小了，计算公式如下： Usable Window Size = SND.UNA + SND.WND - SND.NXT 接收端窗口上面介绍了发送端窗口的一些概念，下面👇是接收端窗口的学习： 已经接收并且已经确认 :28-31 字节 还未接收并且接收端可以接受:32-51 字节 还未接收并且超出接收处理能力:51-57 字节 这里引出两个概念： RCV.WND RCV是接收端的意思，WND是接受端窗口的大小 RCV.NXT NXT表示的是接受端接收窗口的开始位置，也就是接收方接下来处理的第一个字节； RCV.WND的大小接受端的内存以及缓冲区大小有关，在某种意义上说，接受端的窗口大小和发送端大小大致相同；接受端可接收的数据能力可以通过TCP首部的Window字段设置，但是接受端的处理能力是可能随时变化的，所以接受端和服务端的窗口大小大致是一样的； 流量控制下面👇根据一个例子来阐述流量控制，模拟一个GET请求，客户端向服务端请求一个260字节的文件，大致流程如下，比较繁琐： 这里假设MSS和窗口的大小不发生变化，同时客户端和发送端状态如下：【 客户端 】： 发送窗口默认360字节 接收窗口设定200字节【 服务端 】： 发送窗口设定200字节 接收窗口设定360字节 Step1： 客户端发送140字节的数据到服务端 【客户端】发送140字节，【SND.NXT】从1-&gt;141 【服务端】状态不变，等待接收客户端传输的140字节 Step2: 服务端接收140字节，发送80字节响应以及ACK 【 客户端 】发送140字节之后等待【 服务端 】的ACK 【 服务端 】可用窗口右移，【RCV.NXT】从1-&gt;141【 服务端 】发送80字节数据，【SND.NXT】从241-&gt;321 Step3: 客户端接收响应ACK，并且发送ACK 【 客户端 】发出的140字节得到确认，【SND.UNA】右移140字节【 客户端 】接收80字节数据，【RCV.NXT】右移80字节，从241-&gt;321 Step4: 服务端发送一个280字节的文件，但是280字节超出了客户端的接收窗口，所以客户端分成两部分传输，先传输120字节； 【 服务端 】发送120字节，【SND.NXT】向右移动120字节，从321-&gt;441 Step5: 客户端接收文件第一部分，并发送ACK 【 客户端 】接收120字节，【RCV.NXT】从321-&gt;441 Step6：服务端接收到第二步80字节的ACK [ 服务器 ] 80字节得到ACK 【SND.UNA】从241-&gt;321 Step7: 服务端接收到第4步的确认 【 服务端 】之前发送文件第一部分的120字节得到确认，【SND.UNA】右移动120，从321-&gt;441 Step8: 服务端发送文件第二部分的160字节 【 服务端 】： 发送160字节，【SND.NXT】向右移动160字节，从441-&gt;601 Step9: 客户端接收到文件第二部分160字节，同时发送ACK 【 客户端 】接收160字节，【RCV.NXT】向右移动160字节，从441-&gt;601 Step10: 服务端收到文件第二部分的ACK 【 服务端 】发送的160字节得到确认，【SND.UNA】向右一定160字节，从441-&gt;601；至此客户端收到服务端发送的完整的文件； 上面通过表格列举服务端和客户端每个状态在每个步骤的状态，如果不是很好理解，可以看如下示意图辅助理解： 客户端交互流程 服务端交互流程 上面👆是模拟一个GET请求，服务端发送一个280字节的文件给到客户端，客户端的接收窗口是200字节场景加，客户端和服务端的数据传输与交互流程，通过这个流程来学习滑动窗口的移动状态和流量控制的大致流程； 滑动窗口与操作系统缓冲区上面👆讲述的时候，都是假设窗口大小是不变的，而实际上，发送端和接受端的滑动窗口的字节数都吃存储在操作系统缓冲区的，操作系统的缓冲区受操作系统控制，当应用进程增加是，每个进程分配的内存减少，缓冲区减少，分配给每个连接的窗口就会压缩。**而且滑动窗口的大小也受应用进程读取缓冲区数据速度有关**； 应用进程读取缓冲区数据不及时造成窗口收缩step1: 客户端发送140字节 客户端发送到140字节之后，可用窗口收缩到220字节，发送窗口不变 Step2: 服务端接收140字节 但是应用进程仅仅读取40字节 服务端应用进程仅仅读取40字节，仍有100字节占用缓冲区大小，导致接受窗口收缩，服务端发送ACK报文时，在首部Window带上接收窗口的大小260 Step3: 客户端收到确认报文之后，发送窗口收缩到260 Step4: 客户端继续发送180字节数据 客户端发送180字节之后，可用窗口变成80字节 Step5: 服务端接收到180字节 假设应用程序仍然不读取这180字节，最终也导致服务端接收窗口再次收缩180字节，仅剩下80字节，在发送确认报文时，设置首部window=80 Step6: 客户端收到80字节的窗口时，调整发送窗口大小为80字节，可用窗口也是80字节 Step7: 客户端仍然发送80字节到服务端，此时可用窗口为空 Step8: 服务端应用进程继续不读区这80字节的缓冲区数据，最终导致服务端接收窗口大小为0，不能再接收任何数据，同时发送ACK报文； Step9：客户端收到确认报文之后，调整发送窗口大小为0，这个状态叫做「 窗口关闭 」 窗口收缩导致的丢包 Step1：客户端服务端开始的窗口大小都是360字节，客户端发送140字节数据 客户端发送140字节之后，可用窗口变成220字节 Step2：服务端应用进程骤增，进程缓存区平均分配，造成服务端接收窗口减少，从360变成240字节； 假设接收了140字节之后，应用进程没有读取，那个可用窗口进一步压缩，变成100字节； Step3：假设同一个连接在没有收到服务端确认之后，又发送了180个字节的数据（Retramission） 先发送了140字节，后发送了180字节，都没有得到确认，客户端可用窗口大小变成40字节 Step4：服务端收到上面👆第三步发送的180字节的数据，但是接受窗口的大小只有100字节，所以不能接收 服务端拒绝接收180字节 Step5：此时客户端才收到之前140字节的确认报文，才知道接收窗口发生了变化 客户端由于没有收到180字节的确认，加入客户端正在准备发送180字节数据，得到接受端的窗口大小是100字节之后，须强制将右侧窗口向左收缩80字节； 窗口关闭这个例子和上面的例子都发生了「 窗口关闭 」 窗口关闭： 发送端的发送窗口变成0的状态； 上面讲的两种情况一般不会发生的，因为操作系统不会既收缩窗口，同时减少连接缓存；而是一般先使用窗口收缩策略，之后在压缩缓冲区的方式来规避以上问题；发生窗口关闭之后，发送端不会被动的等待服务端的通知，而是会采用定时嗅探的方式去查看服务端接收窗口是否开放； Linux中对TCP缓冲区的调整方式 net.ipv4.tcp_rmem = 4096 87380 6291456 读缓存最小值、默认值、最大值，单位字节，覆盖 net.core.rmem_max net.ipv4.tcp_wmem = 4096 16384 4194304 写缓存最小值、默认值、最大值，单位字节，覆盖net.core.wmem_max net.ipv4.tcp_mem = 1541646 2055528 3083292 系统无内存压力、启动压力模式阀值、最大值，单位为页的数量 net.ipv4.tcp_moderate_rcvbuf = 1 开启自动调整缓存模式","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-RTO重传计数器的计算","date":"2021-07-04T08:32:27.000Z","path":"wiki/TCP-RTO重传计数器的计算/","text":"之前的文章已经介绍了TCP超时重传的过程中使用了定时器的策略，当定时器规定时间内未收到确认报文之后，就会触发报文的重传，同时定时器复位；那么定时器超时时间（RTO Retramission Timeout）是如何计算的呢？ 什么是RTT？了解RTO如何计算之前，首先明确一个概念「 RTT 」； 如上图所示，从client发送第一个「SYN」报文，到Server接受到报文，并且返回「SYN ACK」报文之后，client接受到Server的「ACK」报文之后，client所经历的时间，叫做1个RTT时间； 如何在重传下有效测量RTT？ 如上图两种情况：第一种，左侧a图所示，当一端发送的数据报丢失后要进行重传，到重传之后接收到确认报文之后，这种场景下该如何计算RTT呢？开始时间是按照第一次发送数据报时间呢还是按照重传数据报的时间呢？ 按照常理来说，如右侧b图所示，RTT时间应该以RTT2为准； 第二种，左侧b图所示，第一次发送数据报文时，由于网络时延导致RTO时间内没有收到接收段的确认报文，发送端进行重发，但是在刚刚重发之后就收到了第一次报文的确认报文，那这种情况RTT该如何计算呢？ 如右侧a图所示，RTT时间应该以RTT1为准； 就像上面提及的两种情况，一会以第一个RTT1为准，一会以RTT2为准，那么TCP协议如何正确的计算出RTT呢？ 使用Timestamp方式计算RTT之前的文章中在介绍TCP超时与重传的笔记中有介绍通过使用Timtstamp的方式来区分相同Seq序列号的不同报文，其实在TCP报文首部存储Timestamp的时候，会存储报文的发送时间和确认时间，如下所示： 如何计算RTO？上面👆说到了RTT如何计算，那个RTO和RTT有什么关系呢？ RTO的取值将会影响到TCP的传输效率以及网络的吞吐量； 通常来说RTO应该略大于RTT，如果RTO小于RTT，则会造成发送端频繁重发，可能会造成网络阻塞；如果RTO设置的过大，则接受端已经收到确认报文之后的一段时间内仍然不能发送其他报文，会造成两端性能的浪费和网络吞吐量的下降； 平滑RTO网络的RTT是不断的变化的，所以计算RTO的时候，应当考虑RTO的平滑性，尽量避免RTT波动带来的干扰，以抵挡瞬时变化； 平滑RTO在文档RFC793定义，给出如下计算方式： SRTT (smoothed round-trip time) = ( α * SRTT ) + ((1 - α) * RTT) α 从 0到 1(RFC 推荐 0.9)，越大越平滑 RTO = min[ UBOUND, max[ LBOUND, (β * SRTT) ] ] 如 UBOUND为1分钟，LBOUND为 1 秒钟， β从 1.3 到 2 之间 这种计算方式不适用于 RTT 波动大(方差大)的场景,如果网络的RTT波动很大，会造成RTO调整不及时； 追踪RTT方差计算RTO RFC6298(RFC2988)，其中α = 1/8， β = 1/4，K = 4，G 为最小时间颗粒: 首次计算 RTO，R为第 1 次测量出的 RTT123SRTT(smoothed round-trip time) = RRTTVAR(round-trip time variation) = R/2RTO = SRTT + max (G, K*RTTVAR) 后续计算 RTO，R’为最新测量出的 RTT123SRTT= (1-α)*SRTT+α*R’RTTVAR=(1-β)*RTTVAR+β*|SRTT-R’|RTO = SRTT + max (G, K*RTTVAR)","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP超时与重传","date":"2021-07-04T08:32:08.000Z","path":"wiki/TCP超时与重传/","text":"背景 如上图👆所示，设备A向设备B发送消息，消息在网络中会由于各种各样的问题导致丢失，那么该如何解决上述问题呢？ 采用定时器重传 PAR：Positive Acknowledgment with Retransmission 最简单的思路是在发送方设置「 定时器 」： 当设备A发送第一条消息之后，在定时器规定的时间内，如果收到设备B的确认报文，则设备A继续发送下一个报文，同时定时器复位； 如果第一条消息发送时间超出了定时器规定的时间，则设备A将重新发送第一条消息，同时重新设置定时器； 这种方式是串型发送的，只有第一个消息发送成功之后，才可以发送下一条消息，「 效率极差 」； 并发能力PAR 基于上述PAR效率低下的方式进行改造，在发送端采用并发+定时器的方式进行数据发送； 首先设备A可以同时发送多个消息或者报文段，每个报文段具有一个标志字段【#XX】去标志唯一，每个报文段连接具有自己的定时器； 设备B规定时间内收到设备A发送的数据之后并且设备A得到设备B的确认之后，设备A将定时器清除 同PAR一样，设备B没有在规定的时间内发送确认报文，设备A将这个报文所对应的定时器复位，重新发送这个报文 并发发送带来的问题采用并发的方式发送消息或者报文段固然提升了发送端的性能，但是发送端发送的消息可能接受端不能完全处理，这是双方报文处理速度或者效率不一致的问题； 所以对于接收端设备B，应该明确自己可能接受的数据量，并且在确认报文中同步到发送端设备A，设备A根据设备B的处理能力来调整发送数据的大小；也就是上图中的「 limit」； 继续延伸Sequment序列号和Ack序列号的设计理念或者设计初衷是「 解决应用层字节流的可靠发送 」 跟踪「应用层」的发送端数据是否送达 确定「接收端有序的」接收到「字节流」 序列号的值针对的是字节而不是报文 ⚠️⚠️⚠️ TCP的定位就是面向字节流的！ TCP序列号如何设计的 通过TCP报文头我们可以知道，Sequment序列号包括32位长度；也就是说一个Sequment可以发送2的32次方个字节，大约4G的数量，Sequment就无法表示了，当传输的数据超过“4G”之后，如果这个连接依然要使用的话，Sequment会重新复用；Sequment复用会产生一个问题，也就是序列号回绕；👇 序列号回绕 序列号回绕 (Protect Against Wrapped Sequence numbers) 当一个连接要发送6G的数据是，A、B、C、D分别发送1G的数据，如果继续使用此连接，E下一次发送数据1G，Seq序列号复用，E报文段的序列号和A报文段的序列号表示相同 按照上面的逻辑继续发送数据，F报文段的Seq标志和B报文段的是一样的； 加入B报文段在发送过程中丢失了，直到接受端接收了F报文段的同时B报文段到达接受端，接受端该如何区分相同Seq序列号不同数据的报文段呢？ 其实TCP解决这个问题很简单，就是在每个报文段上添加Tcp Timestamp时间戳，类似于版本号的理念； 接收端收到相同Seq序列号的报文段是可以根据时间戳来进行区分；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP最大报文段（MSS）","date":"2021-07-04T08:31:56.000Z","path":"wiki/TCP最大报文段（MSS）/","text":"MSS产生的背景我们都知道TCP协议是运输在传输层的协议，它是面向【字节流】的传输协议；它的上层，应用层传输的数据是无限制的，但是它的下层也就是网络层和链路层由于路由等转发设备有内存等限制是不可能无限制传输任何大小的报文的，它们一定会限制报文的长度，因此 TCP协议要完成的工作是将从应用层接受到的任意长度数据，切割成多个报文段，MSS就是如何切割报文段的依据。 什么是MSSMSS（Max Segment Size）：仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879 MSS 选择目的 尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率 防止 Segment 被某个设备的 IP 层基于 MTU 拆分 IP层基于MTU的数据拆分是效率极差的，一个报文段丢失，所有的报文段都要重传 MSS默认大小 默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部) MSS在什么时候使用 握手阶段协商 MSS 这个在TCP三次握手的文章中已经提及过了！ MSS 分类 发送方最大报文段: SMSS:SENDER MAXIMUM SEGMENT SIZE 接收方最大报文段: RMSS:RECEIVER MAXIMUM SEGMENT SIZE 在TCP常用选项中可以看到【MSS】的选项 TCP流与报文段在数据传输中的状态 从上图可以看到，左边客户端在发送字节流数据给到右边客户端，客户端发送一个连续的字节流，会在TCP层按照MSS大小规定进行拆分成多个小的报文段，分别传送到另一个客户端或者其他的接收端；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP三次握手","date":"2021-07-04T08:31:42.000Z","path":"wiki/TCP三次握手/","text":"握手🤝的目的 同步Sequence序列号 初始化序列号ISN （Inital Sequence Number） 交换TCP通讯的参数 比如最大报文段参数（MSS）、窗口比例因子（Window）、选择性确认（SACK）、制定校验和算法； 三次握手握手过程 TCP三次握手的大致流程图如上👆 使用tcpdump抓包分析三次🤝握手报文中Seq和Ack的变化 1tcpdump port 80 -c 3 -S 第一次握手🤝1IP upay.60734 &gt; 100.100.15.23.http: Flags [S], seq 3800409106, win 29200, options [mss 1460,sackOK,TS val 839851765 ecr 0,nop,wscale 7], length 0 客户端upay访问服务端80端口，发送一个「 seq=3800409106 」 ，同时标志位SYN=1，声明此次握手是要建立连接； 第二次握手🤝1IP 100.100.15.23.http &gt; upay.60734: Flags [S.], seq 1981710286, ack 3800409107, win 14600, options [mss 1440,nop,nop,sackOK,nop,wscale 7], length 0 第二次握手，服务端收到客户端的申请连接强求（SYN=1）之后，在服务端自己准备好的情况下，给客户端发送 「 ACK=1 SYN=1 」的确认报文，SYN=1同样也是声明此次报文是建立连接的报文请求； ack= 3800409107 也就是第一个客户端发给服务端的seq+1（ack是接收方下次期望接口报文的开始位置） 第三次握手握手1IP upay.60734 &gt; 100.100.15.23.http: Flags [.], ack 1981710287, win 229, length 0 客户端收到服务器返回的确认报文，确认可以进行连接，发送「 ack = 1981710287 」的确认报文，之后就完成了三次握手，TCP的连接就创建成功了，接下来双方就可以发送数据报了； TCP连接创建构成中状态的变更 首先客户端和服务端都是【CLOSED】状态，客户端发起连接请求之后，进入【SYN-SENT】状态，这个状态维持的时间很短，我们使用netstat去查看tcp连接状态的时候，基本上都不会看到这个状态，而服务端是在【LISTEN】状态，等待客户端的请求； 服务端收到客户端请求之后，发送「SYN ACK」确认报文，同时服务端进入【SYN-RECEIVED】状态，等待客户端的确认报文； 客户端收到服务端的同步确认请求之后，发送「ACK」确认报文，同时进入【ESTABLISHED】状态，准备后续的数据传输； 服务端收到三次握手最后的确认报文之后，进入【ESTABLISHED】状态，至此，一个TCP连接算是建立完成了，后面就是双方的通信了； TCB（Transmission Control Block） 保存连接使用的源端口、目的端口、目的 ip、序号、 应答序号、对方窗口大小、己方窗口大小、tcp 状态、tcp 输入/输出队列、应用层输出队 列、tcp 的重传有关变量等 TCP性能优化和安全问题 正如我们了解的TCP三次握手🤝的流程，当有大量SYN请求到达服务端时，会进入到【SYN队列】，服务端收到第二次确认报文之后，会进入【ESTABLISHED】状态，服务端操作系统内核会将连接放入到【ACCEPT】队列中，当Nginx或者Tomcat这些应用程序在调用accept（访问内核）的时候，就是在【ACCEPT】队列中取出连接进行处理； 由此可见，【SYN】队列和【ACCEPT】是会影响服务器连接性能的重要因素，所以对于高并发的场景下，这两个队列一定是要设置的比较大的； 如何设置SYN队列大小服务器端 SYN_RCV 状态 net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数 客户端 SYN_SENT 状态（服务端作为客户端，比如Ngnix转发等） net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数 net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围 Fast Open机制 TCP如何对连接的次数以及连接时间进行优化的呢？这里提到Fast Open机制；比如我们有一个Http Get请求，正常的三次握手🤝到收到服务端数据需要2个RTT的时间；FastOpen做出如下优化： 第一次创建连接的时候，也是要经历2个RTT时间，但是在服务端发送确认报文的时候，在报文中添加一个cookie； 等到下次客户端再需要创建请求的时候，直接将【SYN】和cookie一并带上，可以一次就创建连接，经过一个RTT客户端就可以收到服务端的数据； 如何Linux上打开TCP Fast Open net.ipv4.tcp_fastopen:系统开启 TFO 功能 0:关闭 1:作为客户端时可以使用 TFO 2:作为服务器时可以使用 TFO 3:无论作为客户端还是服务器，都可以使用 TFO SYN攻击什么是SYN攻击？ 正常的服务通讯都是由操作系统内核实现的请求报文来创建连接的，但是，可以人为伪造大量不同IP地址的SYN报文，也就是上面👆状态变更图中的SYN请求，但是收到服务端的ACK报文之后，却不发送对于服务端的ACK请求，也就是没有第三次挥手，这样会造成大量处于【SYN-RECEIVED】状态的TCP连接占用大量服务端资源，导致正常的连接无法创建，从而导致系统崩坏； SYN攻击如何查看1netstat -nap | grep SYN_RECV 如果存在大量【SYN-RECEIVED】的连接，就是发生SYN攻击了； 如何规避SYN攻击？ net.core.netdev_max_backlog 接收自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn_backlog SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接 设置SYN Timeout 由于SYN Flood攻击的效果取决于服务器上保持的SYN半连接数，这个值=SYN攻击的频度 x SYN Timeout，所以通过缩短从接收到SYN报文到确定这个报文无效并丢弃改连接的时间，例如设置为20秒以下，可以成倍的降低服务器的负荷。但过低的SYN Timeout设置可能会影响客户的正常访问。 设置SYN Cookie (net.ipv4.tcp_syncookies = 1) 就是给每一个请求连接的IP地址分配一个Cookie，如果短时间内连续受到某个IP的重复SYN报文，就认定是受到了攻击，并记录地址信息，以后从这个IP地址来的包会被一概丢弃。这样做的结果也可能会影响到正常用户的访问。 当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接 由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等 TCP_DEFER_ACCEPT这个是做什么呢？ 正如上面👆操作系统内核展示图所示，内核中维护两个队列【SYN】队列和【ACCEPT】队列，只有当收到客户端的ACK报文之后，连接会进入到【ACCEPT】，同时服务器的状态是【ESTABLISHED】状态，此时操作系统并不会去激活应用进程，而是会等待，知道收到真正的data分组之后，才会激活应用进程，这是为了提高应用进程的执行效率，避免应用进程的等待； TCP三次握手为什么不能是两次或者四次 参见文章：敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP头部","date":"2021-07-04T08:31:22.000Z","path":"wiki/TCP头部/","text":"带着问题学习 如何校验报文段是否损坏？ 如何CRC校验 seq和ack是如何计算的？ tcp校验位都有那些？ 6个 分别是什么含义？ tcp如何计算首部长度？ 偏移量 TCP Retransmission 重传？ tcp spurious retransmission 又是什么呢？ tcp dup ack 是什么？ ack与ACK有什么区别？ 分别有什么作用？ TCP头部结构 学习TCP协议首先要看一下它的报文段是如何组成的；TCP报文段组成由两部分，第一部分是报文头部，第二部分是数据部分； 先看一下报文头，也就是TCP首部的组成； 16位端口16位端口号：告知主机该报文段是来自哪里（源端口Source Port）以及传给哪个上层协议或应用程序（目的端口Destination Port）的。进行TCP通信时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号（比如DNS协议对应端口53，HTTP协议对应80，这些端口号可在/etc/services文件中找到）。 序列号（Seq）占32位，也就是4字节长度，序号范围自然也是是0~2^32-1。TCP是面向字节流的，TCP连接中传送的字节流中的每个字节都按顺序编号。整个要传送的字节流的起始序号必须要在连接建立时设置。首部中的序号字段值指的是本报文段所发送的数据的第一个字节的序号。 TCP用序列号对数据包进行标记，以便在到达目的地后重新重装，假设当前的序列号为 s，发送数据长度为 l，则下次发送数据时的序列号为 s + l。在建立连接时通常由计算机生成一个随机数作为序列号的初始值。 **这里存在一个疑问，第一次建立TCP连接的时候，网上一些博客上说seq是client随机生成的，也有的博客说是seq=1； 这里经过我抓包后，看到第一次创建TCP连接的时候，确实是1; ** 确认应答号（Ack）Ack占32位，4个字节长度，表示期望收到对方下一个报文段的序号值。 用作对另一方发送来的TCP报文段的响应。其值是收到的TCP报文段的序号值加1。假设主机A和主机B进行TCP通信，那么A发送出的TCP报文段不仅携带自己的序号，而且包含对B发送来的TCP报文段的确认号。反之，B发送出的TCP报文段也同时携带自己的序号和对A发送来的报文段的确认号。TCP的可靠性，是建立在「每一个数据报文都需要确认收到」的基础之上的。 就是说，通讯的任何一方在收到对方的一个报文之后，都要发送一个相对应的「确认报文」，来表达确认收到。 那么，确认报文，就会包含确认号。 若确认号=N，则表明：到序号N-1为止的所有数据都已正确收到。 数据偏移 Offset占 0.5 个字节 (4 位)。 这个字段实际上是指出了TCP报文段的首部长度 ，它指出了TCP报文段的数据起始处距离TCP报文的起始处有多远。 注意数据起始处和报文起始处的意思，上面👆已经写到，TCP报文段的组成有两部分，TCP报文首部和数据部分，偏移量记录的是报文段开始和数据开始的长度，也就是报文首部的长度； 一个数据偏移量 = 4 byte，由于4位二进制数能表示的最大十进制数字是 15，因此数据偏移的最大值是 60 byte，这也侧面限制了TCP首部的最大长度。 保留Reserved占 0.75 个字节 (6 位)。 保留为今后使用，但目前应置为 0。 标志位 TCP Flags标志位，一共有6个，分别占1位，共6位。 每一位的值只有 0 和 1，分别表达不同意思。 如上图是使用wireshard抓包展示截图； ACK(Acknowlegemt) ：确认序号有效 当 ACK = 1 的时候，确认号（Acknowledgemt Number）有效。 一般称携带 ACK 标志的 TCP 报文段为「确认报文段」。为0表示数据段不包含确认信息，确认号被忽略。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 设置为 1。 RST(Reset)：重置连接 当 RST = 1 的时候，表示 TCP 连接中出现严重错误，需要释放并重新建立连接。 一般称携带 RST 标志的 TCP 报文段为「复位报文段」。 SYN(SYNchronization)：发起了一个新连接 当 SYN = 1 的时候，表明这是一个请求连接报文段。 一般称携带 SYN 标志的 TCP 报文段为「同步报文段」。 在 TCP 三次握手中的第一个报文就是同步报文段，在连接建立时用来同步序号。对方若同意建立连接，则应在响应的报文段中使 SYN = 1 和 ACK = 1。 PSH (Push): 推送 当 PSH = 1 的时候，表示该报文段高优先级，接收方 TCP 应该尽快推送给接收应用程序，而不用等到整个 TCP 缓存都填满了后再交付。 FIN：释放一个连接 当 FIN = 1 时，表示此报文段的发送方的数据已经发送完毕，并要求释放 TCP 连接。一般称携带 FIN 的报文段为「结束报文段」。在 TCP 四次挥手释放连接的时候，就会用到该标志。 窗口大小 Window Size占16位。该字段明确指出了现在允许对方发送的数据量，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 窗口大小的值是指，从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量。 例如，假如确认号是701，窗口字段是 1000。这就表明，从 701 号算起，发送此报文段的一方还有接收 1000 （字节序号是 701 ~ 1700） 个字节的数据的接收缓存空间。 校验和 TCP Checksum占16位。 由发送端填充，接收端对TCP报文段执行【CRC算法】，以检验TCP报文段在传输过程中是否损坏，如果损坏这丢弃。 检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。 紧急指针 Urgent Pointer占 2 个字节。 仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。 当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 因此，紧急指针指出了紧急数据的末尾在报文段中的位置。 选项 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 下面是常用选项： MTU（最大传输单元）MTU（最大传输单元）是【链路层】中的网络对数据帧的一个限制，以以太网为例，MTU 为 1500 个字节。一个IP 数据报在以太网中传输，如果它的长度大于该 MTU 值，就要进行分片传输，使得每片数据报的长度小于MTU。分片传输的 IP 数据报不一定按序到达，但 IP 首部中的信息能让这些数据报片按序组装。IP 数据报的分片与重组是在网络层进完成的。 MSS （最大分段大小）MSS 是 TCP 里的一个概念（首部的选项字段中）。MSS 是 TCP 数据包每次能够传输的最大数据分段，TCP 报文段的长度大于 MSS 时，要进行分段传输。TCP 协议在建立连接的时候通常要协商双方的 MSS 值，每一方都有用于通告它期望接收的 MSS 选项（MSS 选项只出现在 SYN 报文段中，即 TCP 三次握手的前两次）。MSS 的值一般为 MTU 值减去两个首部大小（需要减去 IP 数据包包头的大小 20Bytes 和 TCP 数据段的包头 20Bytes）所以如果用链路层以太网，MSS 的值往往为 1460。而 Internet 上标准的 MTU 为 576，那么如果不设置，则MSS的默认值就为 536 个字节。TCP报文段的分段与重组是在运输层完成的。 seq和ack的计算逻辑 CRC校验参考资料TCP协议中的seq/ack序号是如何变化的？TCP协议详解TCP协议详解（一）：TCP头部结构TCP和UDP报文头格式TCP协议详解吃透TCP协议 传送门 👇 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP协议","date":"2021-07-04T08:30:55.000Z","path":"wiki/TCP协议/","text":"TCP协议学习笔记📒 下面是本人在学习TCP协议的过程中，记录的笔记，按照学习的过程从前到后整理在这里！可能会有很多的知识没有罗列，只是记录的大概框架，如果有问题或错误，欢迎指正！ 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手 学习资料 敖丙Github整理的笔记 有大概10篇左右的文章，都是高质量的，原地址请点击着👉 【Github】 极客时间《Web协议详解与抓包实战》– 陶辉老师 这门课程专门讲解网络协议的，包括Http/Https,TLS协议，TCP协议，IP协议等； 《计算机网络 自顶向下方法》第7版 很多名校计算机网络课程在使用的教材，非常权威！ 面试题https://www.nowcoder.com/index","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"Welcome to GeekIBLi","date":"2021-07-04T07:44:33.000Z","path":"wiki/index/","text":"道阻 且长不错的学习网站推荐掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io字节跳动后端面试题集知乎 Java快速进阶通道To Be Top Javaer - Java工程师成神之路Div-wangJava 全栈知识体系https://github.com/crossoverJie/JCSprouthttps://github.com/ZhongFuCheng3y/3y ‼️算法图文分析 工具网站ProcessOn示说 「 提供了很多优质的PPT 还有很多大厂的沙龙视屏以及材料」 技术团队推荐小米信息部技术团队有赞技术团队美团技术团队 面试题合集2020年大厂Java面试前复习的正确姿势（800+面试题附答案解析）Java集合面试题（总结最全面的面试题） 两年学说话 一生学闭嘴","tags":[],"categories":[{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}]},{"title":"2021年后端大厂-Spring","date":"2021-01-09T06:24:05.000Z","path":"wiki/2021年后端大厂-Spring/","text":"spring支持几种bean scope？Spring bean 支持 5 种 scope： Singleton - 每个 Spring IoC 容器仅有一个单实例。 Prototype - 每次请求都会产生一个新的实例。 Request - 每一次 HTTP 请求都会产生一个新的实例，并且该 bean 仅在当前 HTTP 请求内有效。 Session - 每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效。 Global-session - 类似于标准的 HTTP Session 作用域，不过它仅仅在基于portlet 的 web 应用中才有意义。 Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portlet 所共享。在 globalsession 作用域中定义的 bean 被限定于全局 portlet Session 的生命周期范围内。如果你在 web 中使用 global session 作用域来标识 bean，那么 web会自动当成 session 类型来使用。 仅当用户使用支持 Web 的 ApplicationContext 时，最后三个才可用。 Spring bean的生命周期是怎样的spring bean 容器的生命周期流程如下： （1）Spring 容器根据配置中的 bean 定义中实例化 bean。（2）Spring 使用依赖注入填充所有属性，如 bean 中所定义的配置。（3）如果 bean 实现BeanNameAware 接口，则工厂通过传递 bean 的 ID 来调用setBeanName()。（4）如果 bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 setBeanFactory()。（5）如果存在与 bean 关联的任何BeanPostProcessors，则调用preProcessBeforeInitialization() 方法。（6）如果为 bean 指定了 init 方法（ 的 init-method 属性），那么将调 用它。（7）最后，如果存在与 bean 关联的任何 BeanPostProcessors，则将调用 postProcessAfterInitialization() 方法。（8）如果 bean 实现DisposableBean 接口，当 spring 容器关闭时，会调用 destory()。（9）如果为bean 指定了 destroy 方法（ 的 destroy-method 属性），那么将 调用它。 什么是Spring的装配当 bean 在 Spring 容器中组合在一起时，它被称为装配或 bean 装配。Spring容器需要知道需要什么 bean 以及容器应该如何使用依赖注入来将 bean 绑定在一起，同时装配 bean。 自动装配有哪些方式在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。 在Spring框架xml配置中共有5种自动装配： （1）no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。 （2）byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。 （3）byType：通过参数的数据类型进行自动装配。 （4）constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。 （5）autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。 描述一下 DispatcherServlet 的工作流程（1）向服务器发送 HTTP 请求，请求被前端控制器 DispatcherServlet 捕获。 （2） DispatcherServlet 根据 -servlet.xml 中的配置对请求的 URL 进行解析，得到请求资源标识符（URI）。然后根据该 URI，调用 HandlerMapping获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以HandlerExecutionChain 对象的形式返回。 （3） DispatcherServlet 根据获得的 Handler，选择一个合适的HandlerAdapter。（附注：如果成功获得 HandlerAdapter 后，此时将开始执行拦截器的 preHandler(…)方法）。 （4）提取 Request 中的模型数据，填充 Handler 入参，开始执行 Handler（ Controller)。在填充 Handler 的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作： · HttpMessageConveter：将请求消息（如 Json、xml 等数据）转换成一个对象，将对象转换为指定的响应信息。 · 数据转换：对请求消息进行数据转换。如 String 转换成 Integer、Double 等。 · 数据根式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等。 · 数据验证：验证数据的有效性（长度、格式等），验证结果存储到BindingResult 或 Error 中。 （5）Handler(Controller)执行完成后，向 DispatcherServlet 返回一个ModelAndView 对象； （6）根据返回的 ModelAndView，选择一个适合的 ViewResolver（必须是已经注册到 Spring 容器中的 ViewResolver)返回给 DispatcherServlet。 （7） ViewResolver 结合 Model 和 View，来渲染视图。 （8）视图负责将渲染结果返回给客户端。 面试请不要再问我Spring Cloud底层原理https://juejin.cn/post/6844903705553174541 静态代理/动态代理狂神说 https://mp.weixin.qq.com/s/McxiyucxAQYPSOaJSUCCRQ 什么是Spring IOCSpring入门这一篇就够了 https://www.tianmaying.com/tutorial/spring-ioc Spring依赖注入Spring【依赖注入】就是这么简单 3y java后端开发三年！你还不了解Spring 依赖注入，凭什么给你涨薪 对象创建循环依赖问题如何解决循环依赖问题 一级缓存：Map&lt;String, Object&gt; singletonObjects第一级缓存的作用？ 用于存储单例模式下创建的Bean实例（已经创建完毕）。 该缓存是对外使用的，指的就是使用Spring框架的程序员。 存储什么数据？ K：bean的名称 V：bean的实例对象（有代理对象则指的是代理对象，已经创建完毕） 第二级缓存：Map&lt;String, Object&gt; earlySingletonObjects第二级缓存的作用？ 用于存储单例模式下创建的Bean实例（该Bean被提前暴露的引用,该Bean还在创建中）。 该缓存是对内使用的，指的就是Spring框架内部逻辑使用该缓存。 为了解决第一个classA引用最终如何替换为代理对象的问题（如果有代理对象）请爬楼参考演示案例 存储什么数据？ K：bean的名称 V：bean的实例对象（有代理对象则指的是代理对象，该Bean还在创建中） 第三级缓存：Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories第三级缓存的作用？ 通过ObjectFactory对象来存储单例模式下提前暴露的Bean实例的引用（正在创建中）。 该缓存是对内使用的，指的就是Spring框架内部逻辑使用该缓存。 此缓存是解决循环依赖最大的功臣 存储什么数据？ K：bean的名称 V：ObjectFactory，该对象持有提前暴露的bean的引用","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"categories":[]},{"title":"2021年后端大厂面试题合集","date":"2021-01-07T04:40:25.000Z","path":"wiki/2021年后端大厂面试题合集/","text":"记录2021年各互联网大厂面试问题以及答案链接，达到快速复习效果 面试题合集《我们一起进大厂》https://aobing.blog.csdn.net/category_9424379.html 1、Spring面试题1、Spring启动流程 https://aobing.blog.csdn.net/article/details/110383213 https://geekibli.github.io/wiki/Spring梳理启动脉络/ https://blog.csdn.net/hjukyjhg56/article/details/108529552 （面试总结版） 2、Spring的ioc和aop说一说 3、如果让你实现一个ioc，你要怎么做？ https://aobing.blog.csdn.net/article/details/110383213 https://geekibli.github.io/wiki/Spring梳理启动脉络/ 4、aop实现原理，以及jdk动态代理会遇到的问题 ，那cglib就没有什么问题了吗？ 2、Java虚拟机操作堆和栈谁比较快？为什么？ 3、网络面试题什么是加盐 Java基础为什么重写equals要重写Hashcode 10、逻辑推理字节最爱问的智力题，你会几道？（二）","tags":[],"categories":[]},{"title":"2021年后端大厂-计算机网络","date":"2021-01-01T06:17:33.000Z","path":"wiki/2021年后端大厂-计算机网络/","text":"计算机网络 面试合集链接优秀简历模板及计算机网络八股文 面试题目为什么需要TCP协议？TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 IP 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。 因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接收的网络包是无损坏、无间隔、非冗余和按序的。 URI和URL的区别URI(Uniform Resource Identifier)：中文全称为统一资源标志符，主要作用是唯一标识一个资源。 URL(Uniform Resource Location)：中文全称为统一资源定位符，主要作用是提供资源的路径。 DNS的工作流程 domain name system 集群式运行（高可用） 主机向本地域名服务器的查询一般是采用递归查询，而本地域名服务器向根域名的查询一般是采用迭代查询。 详情参见https://www.nowcoder.com/discuss/682094?source_id=profile_create_nctrack&amp;channel=-1 键入网址后，期间发生了什么？三歪问我：键入网址后，期间发生了什么？ 了解ARP协议吗?ARP协议属于网络层的协议，主要作用是实现从IP地址转换为MAC地址。在每个主机或者路由器中都建有一个ARP缓存表，表中有IP地址及IP地址对应的MAC地址。 有了IP地址，为什么还要用MAC地址？简单来说，标识网络中的一台计算机，比较常用的就是IP地址和MAC地址，但计算机的IP地址可由用户自行更改，管理起来相对困难，而MAC地址不可更改，所以一般会把IP地址和MAC地址组合起来使用。 说一下ping的过程ping是ICMP(网际控制报文协议)中的一个重要应用，ICMP是网络层的协议。ping的作用是测试两个主机的连通性。 ping的工作过程： 向目的主机发送多个ICMP回送请求报文 根据目的主机返回的回送报文的时间和成功响应的次数估算出数据包往返时间及丢包率。 路由器和交换机的区别？ 交换机 数据链库层 识别MAC地址并根据MAC地址转发数据帧 路由器 网络层 识别IP地址并根据IP地址转发数据包，维护数据表并基于数据表进行最佳路径选择 TCP和UDP协议的区别TCP建立连接，可靠传输，一对一，流量控制 UDP无需创建连接，尽量交付传输，支持一对多，没有流量控制，首部开销小 TCP协议如何保证可靠传输 主要有校验和、序列号、超时重传、流量控制及拥塞避免等几种方法。 三次握手和四次挥手敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题 为什么不是握手不是两次 三次握手才可以阻止历史重复连接的初始化（主要原因） 三次握手才可以同步双方的初始序列号 三次握手才可以避免资源浪费（避免建立多余的连接） 为什么是四次握手 1、被断开连接一方的ACK（响应主动断开连接的FIN）报文和自己的FIN报文要分开发送，因为中间报处理可能为处理完的数据，所以不能两个报文同时发送或者不能合并成一个报文发送 为什么需要TIME_WAIT 防止具有相同「四元组」的「旧」数据包被收到； 保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭； 这个比较好理解，就是为了确保被断开连接的一方能收到断开连接相应自己FIN报文的ACK报文，上图👆中的最后一个ACK报文。 为什么要等待两个MSL只有主动断开连接的一方在连接断开后要进行TIME_WAIT ， 那TIME_WAIT的时间为什么是2个MSL（Maximum Segment Lifetime 报文最大生存时间）。 TIME_WAIT 过多有什么危害？ 第一是内存资源占用； 第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口； 如何计算TCP数据长度IP报长度-IP首部长度-TCP首部长度 Socket编程https://mp.weixin.qq.com/s/rX3A_FA19n4pI9HicIEsXg 建立一个 socket 连接要经过哪些步骤 服务端和客户端初始化 socket，得到文件描述符； 服务端调用 bind，将绑定在 IP 地址和端口; 服务端调用 listen，进行监听； 服务端调用 accept，等待客户端连接； 客户端调用 connect，向服务器端的地址和端口发起连接请求； 服务端 accept 返回用于传输的 socket 的文件描述符； 客户端调用 write 写入数据；服务端调用 read 读取数据； 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。 这里需要注意的是，服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。 所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作监听 socket，一个叫作已完成连接 socket。 成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。 301 和 302区别301适合永久重定向 301比较常用的场景是使用域名跳转。 比如，我们访问 http://www.baidu.com 会跳转到 https://www.baidu.com，发送请求之后，就会返回301状态码，然后返回一个location，提示新的地址，浏览器就会拿着这个新的地址去访问。 注意： 301请求是可以缓存的， 即通过看status code，可以发现后面写着from cache。 或者你把你的网页的名称从php修改为了html，这个过程中，也会发生永久重定向。 302用来做临时跳转 比如未登陆的用户访问用户中心重定向到登录页面。访问404页面会重新定向到首页。 服务器500，501，502，503，504，505","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"categories":[]},{"title":"2021年后端大厂-MySQL","date":"2021-01-01T03:55:30.000Z","path":"wiki/2021年后端大厂-MySQL/","text":"没有别的目的，就是为了快速复习 Mysql面试合集什么是回表https://blog.csdn.net/xuyw10000/article/details/95971218 https://juejin.cn/post/6844904062329028621 覆盖索引的一个优化办法是将要查询的列添加到索引项中去","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[]},{"title":"2021年后端大厂-算法面试合集","date":"2021-01-01T01:43:34.000Z","path":"wiki/2021年后端大厂-算法面试合集/","text":"算法类利用堆求 TopK 问题及其应用案例解决思路https://juejin.cn/post/6963096695333158926 https://juejin.cn/post/6844903831487152135","tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"categories":[]}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"},{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"},{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"},{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"},{"name":"mybatis","slug":"Spring-Family/mybatis","permalink":"http://example.com/categories/Spring-Family/mybatis/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"},{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"},{"name":"MongoDB","slug":"DataBase/MongoDB","permalink":"http://example.com/categories/DataBase/MongoDB/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"},{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"},{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"},{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"},{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"简单","slug":"简单","permalink":"http://example.com/tags/%E7%AE%80%E5%8D%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"},{"name":"位运算","slug":"位运算","permalink":"http://example.com/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"哈希表","slug":"哈希表","permalink":"http://example.com/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"排序","slug":"排序","permalink":"http://example.com/tags/%E6%8E%92%E5%BA%8F/"},{"name":"贪心","slug":"贪心","permalink":"http://example.com/tags/%E8%B4%AA%E5%BF%83/"},{"name":"动态规划","slug":"动态规划","permalink":"http://example.com/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"双指针","slug":"双指针","permalink":"http://example.com/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/tags/Apache-Flink/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"},{"name":"mybatis","slug":"mybatis","permalink":"http://example.com/tags/mybatis/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"mongodb","slug":"mongodb","permalink":"http://example.com/tags/mongodb/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"},{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"},{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}]}