{"pages":[{"title":"Tags","date":"2021-07-04T07:22:56.335Z","path":"tags/index.html","text":""},{"title":"About","date":"2021-07-04T09:22:18.003Z","path":"about/index.html","text":""},{"title":"Categories","date":"2021-07-04T07:22:56.334Z","path":"categories/index.html","text":""}],"posts":[{"title":"JVM-Xms,Xmx和Xss","date":"2021-07-28T13:04:14.000Z","path":"wiki/JVM-Xms-Xmx和Xss/","text":"性能调优参数Xms，Xmx，Xss的含义 s -Xss 规定了每个线程虚拟机栈及堆栈的大小，一般情况下，256k是足够的，此配置将会影响此进程中并发线程数的大小。 s -Xms 表示初始化JAVA堆的大小及该进程刚创建出来的时候，他的专属JAVA堆的大小，一旦对象容量超过了JAVA堆的初始容量，JAVA堆将会自动扩容到-Xmx大小。 s -Xmx 表示java堆可以扩展到的最大值，在很多情况下，通常将-Xms和-Xmx设置成一样的，因为当堆不够用而发生扩容时，会发生内存抖动影响程序运行时的稳定性。 w 堆内存分配： JVM初始分配的内存由-Xms指定，默认是物理内存的1/64JVM最大分配的内存由-Xmx指定，默认是物理内存的1/4默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。对象的堆内存由称为垃圾回收器的自动内存管理系统回收。非堆内存分配：JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。-Xmn2G：设置年轻代大小为2G。-XX:SurvivorRatio，设置年轻代中Eden区与Survivor区的比值。 参考资料1、类似-Xms、-Xmn这些参数的含义：2、JVM三大性能调优参数Xms，Xmx，Xss的含义，你又知道多少呢","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-自定义类加载器","date":"2021-07-28T13:02:32.000Z","path":"wiki/JVM-自定义类加载器/","text":"如何自定义类加载器为什么要自定义加载器 原因：1、存放在自定义路径上的类，需要通过自定义类加载器去加载。【注意：AppClassLoader加载classpath下的类】2、类不一定从文件中加载，也可能从网络中的流中加载，这就需要自定义加载器去实现加密解密。3、可以定义类的实现机制，实现类的热部署,如OSGi中的bundle模块就是通过实现自己的ClassLoader实现的，如tomcat实现的自定义类加载模型。 如何实现自定义加载器 i 实现自定义类加载有以下两步：1、继承ClassLoader2、重写findClass，在findClass里获取类的字节码，并调用ClassLoader中的defineClass方法来加载类，获取class对象。注意：如果要打破双亲委派机制，需要重写loadClass方法。如下：是一个自定义 的类加载器 1234567891011121314151617181920public static class MyClassLoader extends ClassLoader&#123; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] data=null; try &#123; data= loadByte(name); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return this.defineClass(data,0,data.length); &#125; private byte[] loadByte(String name) throws IOException &#123; File file = new File(&quot;/Users/admin/test/&quot;+name); FileInputStream fi = new FileInputStream(file); int len = fi.available(); byte[] b = new byte[len]; fi.read(b); return b; &#125; &#125; 下面是要加载的类： 12345public class Demo&#123;public void say()&#123;System.out.println(&quot;hello&quot;);&#125;&#125; 该类编译后的class 文件放置在/Users/admin/test/下,然后执行如下代码去加载： 1234567MyClassLoader classLoader = new MyClassLoader(); Class clazz = classLoader.loadClass(&quot;Demo.class&quot;); Object o=clazz.newInstance(); Method method = clazz.getMethod(&quot;say&quot;); method.invoke(o);输出:hello 能不能自己写一个java.lang.String 1、代码书写后可以编译不会报错2、在另一个类中加载java.lang.String，通过反射调用自己写的String类里的方法，得到结果NoSuchMethod，说明加载的还是原来的String，因为通过双亲委派机制，会把java.lang.String一直提交给启动类加载器去加载，通过他加载，加载到的永远是/lib下面的java.lang.String3、在这个自己写的类中写上main方法public static void main(String[] args)执行main方法报错，因为这个String并不是系统的java.lang.String，所以JVM找不到main方法的签名 参考资料JVM:如何实现一个自定义类加载器？原文链接：https://blog.csdn.net/qq_28605513/article/details/85014451","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"分布式-一致性哈希算法","date":"2021-07-28T13:01:05.000Z","path":"wiki/分布式-一致性哈希算法/","text":"一致性哈希算法一致性哈希算法(Consistent Hashing Algorithm)是一种分布式算法，常用于负载均衡。Memcached client也选择这种算法，解决将key-value均匀分配到众多Memcached server上的问题。它可以取代传统的取模操作，解决了取模操作无法应对增删Memcached Server的问题(增删server会导致同一个key,在get操作时分配不到数据真正存储的server，命中率会急剧下降)。 哈希指标 i 评估一个哈希算法的优劣，有如下指标，而一致性哈希全部满足： 均衡性(Balance)：将关键字的哈希地址均匀地分布在地址空间中，使地址空间得到充分利用，这是设计哈希的一个基本特性。 单调性(Monotonicity): 单调性是指当地址空间增大时，通过哈希函数所得到的关键字的哈希地址也能映射的新的地址空间，而不是仅限于原先的地址空间。或等地址空间减少时，也是只能映射到有效的地址空间中。简单的哈希函数往往不能满足此性质。 分散性(Spread): 哈希经常用在分布式环境中，终端用户通过哈希函数将自己的内容存到不同的缓冲区。此时，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load): 负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷 一致性哈希 i 将节点通过hash映射到hash环上，理想的情况是多个节点直接分布均匀 当我们的对象通过hash算法分配在hash环上的时候，它是固定分配到一个节点的空间上的，当我们在BC之间插入一个节点时，仅仅会影响到BC这一段空间上的数据，而不是整个环上的数据都要跟着变化； i 现实情况下，节点之间可能分配不均匀 这和传统的hash取模一样，同样会数据倾斜的问题！ i 虚拟节点 这个时候虚拟节点就此诞生，下面让我们来看一下虚拟节点在一致性Hash中的作用。当我们在Hash环上新增若干个点，那么每个点之间的距离就会接近相等。按照这个思路我们可以新增若干个片/表，但是成本有限，我们通过复制多个A、B、C的副本({A1-An},{B1-Bn},{C1-Cn})一起参与计算，按照顺时针的方向进行数据分布，按照下图示意: 此时A=[A,C1)&amp;[A1,C2)&amp;[A2,B4)&amp;[A3,A4)&amp;[A4,B1)；B=[B,A1)&amp;[B2,C)&amp;[B3,C3)&amp;[B4,C4)&amp;[B1,A)；C=[C1,B)&amp;[C2,B2)&amp;[C,B3)&amp;[B3,C3)&amp;[C4,A3)；由图可以看出分布点越密集，平衡性约好。 算法实现一致性哈希算法有多种具体的实现，包括 Chord 算法，KAD 算法等，都比较复杂。 参考资料1 、一致性哈希算法的原理与实现2、浅谈一致性Hash原理及应用","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"}]},{"title":"分布式-CAP理论","date":"2021-07-28T10:15:10.000Z","path":"wiki/分布式-CAP理论/","text":"CAP 原则CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP原则的精髓就是要么AP，要么CP，要么AC，但是不存在CAP。 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值，即写操作之后的读操作，必须返回该值。（分为弱一致性、强一致性和最终一致性） 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 取舍策略 i CA without P 如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库RDBMS：Oracle、MySQL就是CA。 i CP without A 如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。 i AP wihtout C 要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。 解决方案——BASEBASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果。 核心思想：即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 i 基本可用Basically Available 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 i 软状态Soft state 软状态也称弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 i 最终一致性Eventually consistent 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 参考资料原文链接：https://blog.csdn.net/lixinkuan328/article/details/95535691","tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"}]},{"title":"Mysql-事务特性与实现原理","date":"2021-07-28T10:13:07.000Z","path":"wiki/Mysql-事务特性与实现原理/","text":"事务特性与实现原理事务特性 i 原子性(Atomicity) 事务中的所有操作作为一个整体像原子一样不可分割，要么全部成功,要么全部失败。 i 一致性(Consistency) 事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态。一致性状态是指:1.系统的状态满足数据的完整性约束(主码,参照完整性,check约束等) 2.系统的状态反应数据库本应描述的现实世界的真实状态,比如转账前后两个账户的金额总和应该保持不变。 i 隔离性(Isolation) 并发执行的事务不会相互影响,其对数据库的影响和它们串行执行时一样。比如多个用户同时往一个账户转账,最后账户的结果应该和他们按先后次序转账的结果一样。 i 持久性(Durability) 事务一旦提交,其对数据库的更新就是持久的。任何事务或系统故障都不会导致数据丢失。 在事务的ACID特性中,C即一致性是事务的根本追求,而对数据一致性的破坏主要来自两个方面1.事务的并发执行2.事务故障或系统故障 事务实现原理 并发控制技术保证了事务的隔离性,使数据库的一致性状态不会因为并发执行的操作被破坏。 日志恢复技术保证了事务的原子性,使一致性状态不会因事务或系统故障被破坏。同时使已提交的对数据库的修改不会因系统崩溃而丢失,保证了事务的持久性。 回滚日志（undo）undo log属于 「 逻辑日志 」，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。 undo log用于存放数据被修改前的值，如果修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。另外InnoDB MVCC事务特性也是基于undo日志实现的。 因此，undo log有两个作用：提供回滚和多个行版本控制(MVCC)。 重做日志（redo）redo log重做日志记录的是新数据的备份，属于物理日志。在事务提交前，只要将redo log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是redo log已经持久化。系统可以根据redo log的内容，将所有数据恢复到最新的状态。 redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。 MySQL中redo log刷新规则采用一种称为Checkpoint的机制（利用LSN实现），为了确保安全性，又引入double write机制。 事务基本操作开启事务：start transaction回滚事务：rollback提交事务：commit 参考资料1、详细分析MySQL事务日志(redo log和undo log)2、数据库事务的概念及其实现原理3、数据库事务实现原理","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Mysql-事务隔离级别","date":"2021-07-28T10:12:49.000Z","path":"wiki/Mysql-事务隔离级别/","text":"数据库隔离级别SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。另外，这篇分布式事务不理解？一次给你讲清楚！推荐大家阅读。 i Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 i Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。 i Repeatable Read（可重读） 这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 i Serializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 事务隔离级别产生的问题 w 脏读(Drity Read) 某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 12345事务A第一次读取到price=100同时事务B更新update price=120，但是此时的事务B还未commit事务A读取的price=120事务B-&gt;rollback操作事务A读取到的是脏数据 w不可重复读(Non-repeatable read) 在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 1234事务A第一次读取到price=100同时事务B更新update price=120，并commit事务A读取的price=120事务A多次读取的结果不一致 w 幻读(Phantom Read) 幻读和不可重复读的区别在于，幻读主要表现在数据的删除和插入，而不可重复读表现在数据的更新。 1234事务A第一次读取到price=100同时事务B更新delete price=100 这条记录，并commit事务A读取的price=100price这条记录已经不存在，但是事务A还是可以读取到 1、在可重复读隔离级别下，普通查询是快照读，是不会看到别的事务插入的数据的，幻读只在当前读下才会出现。 2、幻读专指新插入的行，读到原本存在行的更新结果不算。因为当前读的作用就是能读到所有已经提交记录的最新值。 参考资料1、mysql数据库的隔离级别2、MYSQL数据库的四种隔离级别3、MySQL幻读","tags":[],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Mysql-MVCC多版本并发控制","date":"2021-07-28T10:12:28.000Z","path":"wiki/Mysql-MVCC多版本并发控制/","text":"MVCC多版本并发控制MVCC，全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。 MVCC在MySQL InnoDB中的实现主要是为了 「 提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读 」 当前读和快照读 i 当前读 像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 i 快照读 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本 说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现 MVCC模型在MySQL中的具体实现则是由 3个隐式字段，undo日志 ，Read View 等去完成的，具体可以看下面的MVCC实现原理 MVCC有什么好处，解决了什么问题多版本并发控制（MVCC）是一种用来「 解决读-写冲突的无锁并发控制 」，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题 MVCC的实现原理MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。所以我们先来看看这个三个point的概念 隐式字段每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段 w DB_TRX_ID 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID w DB_ROLL_PTR 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） w DB_ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了 undo日志undo log主要分为两种： s insert undo log 代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 s update undo log 事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 Read View(读视图)什么是Read View，说白了Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大) 所以我们知道 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 Read View遵循一个可见性算法，主要是将要被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本; 参考资料1、MVCC多版本并发控制2、MVCC浅析3、乐观锁、悲观锁和MVCC，今天让你一次搞懂4、面试官：谈谈你对Mysql的MVCC的理解？5、Mysql中MVCC的使用及原理详解","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"Java对象头","date":"2021-07-28T09:59:59.000Z","path":"wiki/Java对象头/","text":"Java对象头JOL查看对象头信息在项目中引入以下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 写一个main方法，创建一个Object，然后打印对象信息： 1234 public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable());&#125; 打印结果如下： 12345678java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 由此可知，new Object()在内存中占16个字节，组成部分8字节的markword+4字节的class point+4字节的对齐； Java对象在内存中的布局 markword i 存储sync锁标志，分代年龄等一些关键信息 8字节 class pointer i 指向当前对象所属类类型 4字节 查看java命令默认带的参数命令： java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=134217728-XX:MaxHeapSize=2147483648-XX:+PrintCommandLineFlags-XX:+UseCompressedClassPointers 压缩类指针 4字节-XX:+UseCompressedOops 普通对象指针压缩 4字节-XX:+UseParallelGC instance data i 寸尺当前对象的实例数据 padding i 对齐填充，当对象所占字节数不能被8整除之后，进行填充对齐。 目前的操作系统基本上都是64位的； 顺丰面试题，new Object()在内存中占多少个字节1、如果创建的是空对象，没有实例数据 默认开启了class pointer指针压缩 w 8字节markword + 4字节class pointer + 4字节 padding 如果关闭了类指针压缩 w 8字节markword + 8字节class pointer 2、如果创建的对象有实力数据，如下对象： 1Person（int age , String name） 默认开启了class pointer指针压缩 w 8字节markword + 4字节class pointer + 4字节int + 4字节String + 4字节padding对齐","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-CAS原理和底层实现","date":"2021-07-28T09:59:17.000Z","path":"wiki/Java-CAS原理和底层实现/","text":"CAS原理和底层实现 什么是CASCAS是（compare and swap） 的缩写，它能在不加锁的情况下，在多线程的环境下，保证多线程一致性的改动某一值； ABA问题ABA问题是一个线程在CAS比较值和原来是否相等的过程中，别的线程修改过这个值，但是又改回去了，倒置当前线程比较的时候，发现是相等的，但是，中间是被修改过的； 添加版本号，比较值的时候同时比较版本号 CAS底层原理AtomicInteger:123456789101112public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125; &#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; Unsafe:1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 运用： 12345678910111213141516171819202122232425262728package com.mashibing.jol;import sun.misc.Unsafe;import java.lang.reflect.Field;public class T02_TestUnsafe &#123; int i = 0; private static T02_TestUnsafe t = new T02_TestUnsafe(); public static void main(String[] args) throws Exception &#123; //Unsafe unsafe = Unsafe.getUnsafe(); Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); Field f = T02_TestUnsafe.class.getDeclaredField(&quot;i&quot;); long offset = unsafe.objectFieldOffset(f); System.out.println(offset); boolean success = unsafe.compareAndSwapInt(t, offset, 0, 1); System.out.println(success); System.out.println(t.i); //unsafe.compareAndSwapInt() &#125;&#125; jdk8u: unsafe.cpp: cmpxchg = compare and exchange 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(&quot;Unsafe_CompareAndSwapInt&quot;); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END jdk8u: atomic_linux_x86.inline.hpp 93行is_MP = Multi Processor 12345678inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) &quot;cmpxchgl %1,(%3)&quot; : &quot;=a&quot; (exchange_value) : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest), &quot;r&quot; (mp) : &quot;cc&quot;, &quot;memory&quot;); return exchange_value;&#125; jdk8u: os.hpp is_MP() 12345678910static inline bool is_MP() &#123; // During bootstrap if _processor_count is not yet initialized // we claim to be MP as that is safest. If any platform has a // stub generator that might be triggered in this phase and for // which being declared MP when in fact not, is a problem - then // the bootstrap routine for the stub generator needs to check // the processor count directly and leave the bootstrap routine // in place until called after initialization has ocurred. return (_processor_count != 1) || AssumeMP;&#125; jdk8u: atomic_linux_x86.inline.hpp 1#define LOCK_IF_MP(mp) &quot;cmp $0, &quot; #mp &quot;; je 1f; lock; 1: &quot; 最终实现：底层对应一个汇编指令「lock comxchg」，但是comxchg这条指令不是原子性的，他不能保证在比较的时候，别的线程会不会改变值；而保证线程安全的则是lock这条指令，lock这条指令在执行后面执行的时候锁定一个「北桥信号」，而不是采用纵线锁的方式； CAS在JDK中的实现1、AtomitInteger2、ConcurrentHashMap","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-FutureTask原理","date":"2021-07-28T09:58:47.000Z","path":"wiki/Java-FutureTask原理/","text":"FutureTaskFuture方法介绍123456789101112131415161718public interface Future&lt;V&gt; &#123; // 取消任务 可中断的方式取消 boolean cancel(boolean mayInterruptIfRunning); // 判断任务是否处于取消状态 boolean isCancelled(); // 判断异步任务是否执行完成 ==这里使用轮训的方式监听== boolean isDone(); // 获取异步线程的执行结果，如果没有执行完成，则一直阻塞到有结果返回； V get() throws InterruptedException, ExecutionException; // 获取异步线程的执行结果，如果没有执行完成，则一直阻塞到设置的时间，有结果返回，没有结果则抛出异常； V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 简单示范Callable&amp;Future（1）向线程池中提交任务的submit方法不是阻塞方法，而Future.get方法是一个阻塞方法（2）submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果，所以一般需要使用future.isDone先判断任务是否全部执行完成，完成后再使用future.get得到结果。（也可以用get (long timeout, TimeUnit unit)方法可以设置超时时间，防止无限时间的等待） 1234567891011121314151617181920212223242526272829303132333435public class FutureTest implements Callable&lt;Integer&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ @Override public Integer call() throws Exception &#123; System.err.println(&quot;start call method...&quot;); Thread.sleep(3000); return 1111; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; System.err.println(&quot;main method start....&quot;); FutureTest futureTest = new FutureTest(); Future1Test future1Test = new Future1Test(); long time = System.currentTimeMillis(); ExecutorService executorService = Executors.newFixedThreadPool(2); Future&lt;String&gt; future = executorService.submit(future1Test); if (!future.isDone()) &#123; System.err.println(&quot;future not done !&quot;); &#125; Future&lt;Integer&gt; future1 = executorService.submit(futureTest); // submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果 // 这里先提交了future1Test，休眠了4s, futureTest休眠了3s，但是等我们get到结果的时候，是消耗的4s时间的； System.err.println(&quot;cost time: &quot; + (System.currentTimeMillis() - time)); System.err.println(&quot;future: &quot; + future.get()); System.err.println(&quot;future1: &quot; + future1.get()); System.err.println(&quot;main method end....&quot;); executorService.shutdown(); &#125;&#125; 执行结果12345678main method start....future not done !// 说明了第一 get()方法是阻塞，第二线程池任务都执行完成之后，按提交任务顺序get结果返回值cost time: 4start call method...future: future 2 testfuture1: 1111main method end.... 注意点 线程池执行任务有两种方式execute和submit，execute是不带返回值的，submit是有返回值的; main方法中可以不使用线程池，可以直接创建线程，调用start方法就可以，切记只有在演示代码的时候后。手动直接创建线程的方式还是不要用，因为一旦请求变多，则会创建无数的线程，线程数大于CPU核数，进而导致CPU频繁切换上下分进行调度，性能严重下降。 而且线程的数据是存放在内存中的，会占用大量的内存，增加垃圾回收的压力。严重的会发生OOM; 异常main方法中我们使用的是Future future接收异步任务执行的放回结果，但实际上Future其实是一个interface，并不能接收返回结果的，那实际我们调用future.get()是，是实例了一个FutureTask对象来接受的； FutureTask讲解下面主要针对Future的实现类FutureTask的几个重要方法展开 FutureTask继承关系1234567891011public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123;...&#125;// 下面是RunnableFuture接口的继承关系public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; /** * Sets this Future to the result of its computation * unless it has been cancelled. */ void run();&#125; FutureTask 重要的成员变量 12345678910111213141516171819202122/** The underlying callable; nulled out after running */private Callable&lt;V&gt; callable;/** The result to return or exception to throw from get() *///任务执行结果或者任务异常private Object outcome; // non-volatile, protected by state reads/writes/** The thread running the callable; CASed during run() *///执行任务的线程private volatile Thread runner;/** Treiber stack of waiting threads *///等待节点，关联等待线程private volatile WaitNode waiters;private static final sun.misc.Unsafe UNSAFE;//state字段的内存偏移量 这个在线程池执行任务的时候进行状态判断的时候会用到private static final long stateOffset;//runner字段的内存偏移量private static final long runnerOffset;//waiters字段的内存偏移量private static final long waitersOffset; 定义任务的生命周期 12345678private volatile int state;private static final int NEW = 0;private static final int COMPLETING = 1;private static final int NORMAL = 2;private static final int EXCEPTIONAL = 3;private static final int CANCELLED = 4;private static final int INTERRUPTING = 5;private static final int INTERRUPTED = 6; NORMAL:指的是任务能够正常执行状态 EXCEPTIONAL：表示任务执行异常 CANCELLED：取消状态，之后的状态都表示任务取消或终端 下面看一下FutureTask中几个重要的方法 执行结果 | report方法 Returns result or throws exception for completed task.主要是上报异步任务执行的结果或返回任务执行发生的异常 1234567891011121314/** * Returns result or throws exception for completed task. * * @param s completed state value */ @SuppressWarnings(&quot;unchecked&quot;) private V report(int s) throws ExecutionException &#123; Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); &#125; 判断逻辑就是根据参数，也是是任务状态，根据不同的状态处理相应的逻辑。比如NORNAL状态，表示任务正常执行，直接返回结果就可以。如果状态大于CANCELLED，说明任务被取消或终端，会抛出CancellationException()；如果不是异常状态，则抛出ExecutionException； 任务执行 | run() 执行异步任务 1234567891011121314151617181920212223242526272829303132333435363738394041public void run() &#123; // 如果状态 state 不是 NEW，或者设置 runner 值失败 // 表示有别的线程在此之前调用 run 方法，并成功设置了 runner 值 // 保证了只有一个线程可以运行 try 代码块中的代码。 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; //以上state值变更的由CAS操作保证原子性 try &#123; Callable&lt;V&gt; c = callable; //只有c不为null且状态state为NEW的情况 if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //调用callable的call方法，并获得返回结果 result = c.call(); //运行成功 ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; setException(ex); &#125; if (ran) //设置结果 set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125; 核心逻辑就是调用Callable的call方法，==result=c.call();== 并且对任务执行的结果或异常信息进行处理； 获取结果 | get() throws InterruptedException, ExecutionException 获取异步任务执行的结果或异常信息 123456public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125; get方法执行两个操作： 判断任务的状态,如果没有执行完成，调用awaitDone方法 任务完成，调用我们上面说的report方法，返回任务执行结果 任务阻塞 | awaitDone(boolean timed, long nanos) 等到任务执行完成 也是get方法阻塞特性的关键所在 123456789101112131415161718192021222324252627282930313233343536373839404142434445private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; // CPU轮转 for (;;) &#123; // 如果线程中断了，将线程移除等待队列，抛出中断异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; // 如果任务状态大于完成，则直接返回； if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; // 如果任务完成，但是返回值outcome还没有设置，可以先让出线程执行权，让其他线程执行 else if (s == COMPLETING) // cannot time out yet Thread.yield(); // 下面是任务还没有执行完成的状态，将线程添加到等待队列 else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // 判断get方法是否设置了超时时间 else if (timed) &#123; nanos = deadline - System.nanoTime(); // 如果超出设置的时间，线程移除等到队列 if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; // 没有设置超时时间，线程直接阻塞，直到任务完成 else LockSupport.park(this); &#125; &#125; 主要执行步骤： 判断线程是否被中断，如果被中断了，就从等待的线程栈中移除该等待节点，然后抛出中断异常 读取state,判断任务是否已经完成，如果已经完成或者任务已经取消，此时调用get方法的线程不会阻塞，会直接获取到结果或者拿到异常信息； 如果s == COMPLETING，说明任务已经结束，但是结果还没有保存到outcome中，==此时线程让出执行权，给其他线程先执行；== 如果任务没有执行完成，则需要创建等待节点，等待插入到阻塞队列 判断queued，这里是将c中创建节点q加入队列头。使用Unsafe的CAS方法，对waiters进行赋值，waiters也是一个WaitNode节点，相当于队列头，或者理解为队列的头指针。通过WaitNode可以遍历整个阻塞队列 然后判断超时时间，时间是在调用get方法的时候传输进来的，如果有超时时间，则设置超时时间，如果超出时间，则将线程移除等待队列；如果没有设置时间，则直接阻塞线程； 取消任务 | cancel(boolean mayInterruptIfRunning)123456789101112131415161718192021222324252627@Param mayInterruptIfRunning 是否中断public boolean cancel(boolean mayInterruptIfRunning) &#123; /* * 在状态还为NEW的时候，根据参数中的是否允许传递， * 将状态流转到INTERRUPTING或者CANCELLED。 */ if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // in case call to interrupt throws exception if (mayInterruptIfRunning) &#123; try &#123; Thread t = runner; if (t != null) t.interrupt(); &#125; finally &#123; // final state UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true; &#125; 123456789101112131415161718192021222324252627282930313233343536private void finishCompletion() &#123; for (WaitNode q; (q = waiters) != null;) &#123; // 必须将栈顶CAS为null，否则重读栈顶并重试。 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; // 遍历并唤醒栈中节点对应的线程。 for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; WaitNode next = q.next; if (next == null) break; // 将next域置为null，这样对GC友好。 q.next = null; q = next; &#125; break; &#125; &#125; /* * done方法是暴露给子类的一个钩子方法。 * * 这个方法在ExecutorCompletionService.QueueingFuture中的override实现是把结果加到阻塞队列里。 * CompletionService谁用谁知道，奥秘全在这。 */ done(); /* * callable置为null主要为了减少内存开销, * 更多可以了解JVM memory footprint相关资料。 */ callable = null;&#125; Callable&amp;Future使用场景 异步任务需要拿到返回值 多线程并发调用，顺序组装返回值，一些并发框架中会看到相应体现 还有一些分布式任务调度的场景，远程调用需要回填执行结果 还有很多通信框架中都有体现 参考资料 (1) future.get方法阻塞问题的解决，实现按照任务完成的先后顺序获取任务的结果(2) Java多线程引发的性能问题以及调优策略(3) 可取消的异步任务——FutureTask用法及解析(4) FutureTask源码解读","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-join方法原理解析","date":"2021-07-28T09:58:22.000Z","path":"wiki/Java-join方法原理解析/","text":"join方法12345join重载方法1 join()2 join(long millis) //参数为毫秒3 join(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 功能演示123456789101112131415public class JoinDemo implements Runnable&#123; public void run() &#123; System.err.println(&quot;join thread demo &quot;); &#125; public static void main(String[] args) throws Exception &#123; System.err.println(&quot;main thread start... &quot;); Runnable r = new JoinDemo(); Thread t = new Thread(r); t.setName(&quot;ibli joinTest ...&quot;); t.start();// t.join(); System.err.println(&quot;main thread end... &quot;); &#125;&#125; 以上将t.join();注释掉，执行的一种可能结果如下： 12345678main thread start... main thread end... join thread demo还有可能是这种结果：main thread start... join thread demomain thread end... 但是把注释去掉，结果如下： 123main thread start... join thread demo main thread end... 这是一个非常简单的demo,效果是显而易见的。当main线程去调用t.join()是，会将自己当前线程阻塞，等到t线程执行完成到达完结状态，main线程才可以继续执行。 我们看一下join()设置超时时间的方法： 123456789101112131415161718192021222324public class JoinDemo implements Runnable&#123; public void run() &#123; System.err.println(&quot;join thread demo &quot;); try &#123; // 线程睡眠4s Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; List&lt;String&gt; strings = null; System.err.println(strings.get(0)); &#125; public static void main(String[] args) throws Exception &#123; System.err.println(&quot;main thread start... &quot;); Runnable r = new JoinDemo(); Thread t = new Thread(r); t.setName(&quot;ibli joinTest ...&quot;); t.start(); // 但是主线程join的超时时间是1s t.join(1000); System.err.println(&quot;main thread end... &quot;); &#125;&#125; 执行效果： 123456main thread start... join thread demo main thread end... Exception in thread &quot;ibli joinTest ...&quot; java.lang.NullPointerException at com.ibli.threadTest.api.JoinDemo.run(JoinDemo.java:14) at java.lang.Thread.run(Thread.java:748) 上面的执行结果可以看到，子线程设置了4s的超时时间，但是主线程在1秒超时后，并没有等待子线程执行完毕，就被唤醒执行后续操作了；这样的预期是否符合你的预期呢？下面我们按照join的源码去分析吧！ join方法原理下面是join的原理图 join()源码 首先会调用join(0)方法，其实是join的重载方法； 123public final void join() throws InterruptedException &#123; join(0);&#125; 下面是join的核心实现： 1234567891011121314151617181920212223242526public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; // 首先校验参数是否合法 if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; // 如果join方法没有参数，则相当于直接调用wait方法 if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 下面是isAlive方法的源码 1public final native boolean isAlive(); 这是一个本地方法，作用是判断当前的线程是否处于活动状态。什么是活动状态呢？活动状态就是线程已经启动且尚未终止。线程处于正在运行或准备开始运行的状态，就认为线程是“存活”的。 这里有一个点要注意，join为什么阻塞的是主线程，而不是子线程呢？ 不理解的原因是阻塞主线程的方法是放在previousThread这个实例作用，让大家误以为应该阻塞previousThread线程。实际上主线程会持有previousThread这个对象的锁，然后调用wait方法去阻塞，而这个方法的调用者是在主线程中的。所以造成主线程阻塞。 其实join()方法的核心在于wait(),在主线程中调用t.join()相当于在main方法中添加 new JoinDemo().wait();是一样的效果；在这里只不过是wait方法写在了子线程的方法中。 再次重申一遍，join方法的作用是在主线程阻塞，等在子线程执行完之后，由子线程唤醒主线程，再继续执行主线程调用t.join()方法之后的逻辑。 那么主线程是在什么情况下知道要继续执行呢？就是上面说的，主线程其实是由join的子线程在执行完成之后调用的notifyAll()方法，来唤醒等待的线程。怎么证明呢？ 其实大家可以去翻看JVM的源码实现，Thread.cpp文件中，有一段代码： 123456void JavaThread::exit(bool destroy_vm, ExitType exit_type) &#123; // Notify waiters on thread object. This has to be done after exit() is called // on the thread (if the thread is the last thread in a daemon ThreadGroup the // group should have the destroyed bit set before waiters are notified). ensure_join(this);&#125; 其中调用ensure_join方法 123456789101112131415161718static void ensure_join(JavaThread* thread) &#123; // We do not need to grap the Threads_lock, since we are operating on ourself. Handle threadObj(thread, thread-&gt;threadObj()); assert(threadObj.not_null(), &quot;java thread object must exist&quot;); ObjectLocker lock(threadObj, thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception(); // Thread is exiting. So set thread_status field in java.lang.Thread class to TERMINATED. java_lang_Thread::set_thread_status(threadObj(), java_lang_Thread::TERMINATED); // Clear the native thread instance - this makes isAlive return false and allows the join() // to complete once we&#x27;ve done the notify_all below //这里是清除native线程，这个操作会导致isAlive()方法返回false java_lang_Thread::set_thread(threadObj(), NULL); // 在这里唤醒等待的线程 lock.notify_all(thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception();&#125; 在JVM的代码中，线程执行结束的最终调用了lock.notify_all(thread)方法来唤醒所有处于等到的线程 使用场景 比如我们使用Callable执行异步任务，需要在主线程处理任务的返回值时，可以调用join方法； 还有一些场景希望线程之间顺序执行的； join()方法与sleep()的比较我们先说一下sleep方法： 让当前线程休眠指定时间。 休眠时间的准确性依赖于系统时钟和CPU调度机制。 不释放已获取的锁资源，如果sleep方法在同步上下文中调用，那么其他线程是无法进- 入到当前同步块或者同步方法中的。 可通过调用interrupt()方法来唤醒休眠线程。 sleep是静态方法，可以在任何地方调用 相比与sleep方法sleep是静态方法，而且sleep的线程不是放锁资源，而join方法是对象方法，并且在等待的过程中会释放掉对象锁； 关于join方法会释放对象锁，那到底是释放的那个对象的锁呢，可以参照 关于join() 是否会释放锁的一些思考 参考资料 1、Java多线程中join方法的理解2、Thread.join的作用和原理3、Thread.join的作用和原理 d 山脚太拥挤 我们更高处见。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java内存模型","date":"2021-07-28T09:57:51.000Z","path":"wiki/Java内存模型/","text":"Java内存模型首先需要思考什么是JMM，以及为什么会有JMM。 Java Memory Model简称JMM, 是一系列的Java虚拟机平台对开发者提供的多线程环境下的内存可见性、是否可以重排序等问题的无关具体平台的统一的保证。(可能在术语上与Java运行时内存分布有歧义，后者指堆、方法区、线程栈等内存区域)。 JMM规范的内容 1.所有变量存储在主内存 2.主内存是虚拟机内存的一部分 3.每条线程有自己的工作内存 4.线程的工作内存保存变量的主内存副本 5.线程对变量的操作必须在工作内存中进行 6.不同线程之间无法直接访问对方工作内存中的变量 7.线程间变量值的传递均需要通过主内存来完成 JMM并不是一个客观存在的东西，它实际是为了规范Java虚拟机制定到一套标准。那为什么需要这套标准呢？ 其实我们都知道JVM是运行在操作系统之上的。而目前的操作系统都是基于冯诺伊曼设置的计算机系统体系来的。CPU是计算机中用来执行控制和计算的核心组建，所有的计算任务全部在CPU中完成，但是我们的所有变量的数据全部存储在主内存中。CPU在执行计算时，需要去主内存加载数据，CPU执行运算的速度极快，这就设计一个CPU执行速度和数据加载速度不一致的问题。 在操作系统级别解决这个问题的办法是引入了CPU缓存。每个CPU都有自己私有的L1缓存和L2缓存，当执行计算时，会优先去CPU自己的缓存中寻找数据，没有的话才会重新加载内存数据。这种方式一定程度上解决了CPU计算和数据加载不一致的问题。 但是也会引入一个新的问题，就是数据一致性问题。 缓存一致性与MESI协议首先看一下什么是MESI协议 缓存一致性协议给缓存行（通常为64字节）定义了个状态：独占（exclusive）、共享（share）、修改（modified）、失效（invalid），用来描述该缓存行是否被多处理器共享、是否修改。所以缓存一致性协议也称MESI协议。 独占（exclusive）：仅当前处理器拥有该缓存行，并且没有修改过，是最新的值。 共享（share）：有多个处理器拥有该缓存行，每个处理器都没有修改过缓存，是最新的值。 修改（modified）：仅当前处理器拥有该缓存行，并且缓存行被修改过了，一定时间内会写回主存，会写成功状态会变为S。 失效（invalid）：缓存行被其他处理器修改过，该值不是最新的值，需要读取主存上最新的值。 如何解决缓存一致性问题呢？ 如上图所示，共享变量是存储在主内存Memory中，在CPU计算时，每一个CPU都有改变量的独立拷贝，每个CPU可以去读取甚至修改共享变量的值，但是为了保证数据的一致性，一个CPU modify了变量的值，需要通知其他的CPU这个变量的最新值是什么。那么可以怎么做呢。 在初始状态，每个CPU还没有加载共享变量，所有每一个CPU的缓存行的状态都是invalid； 当CPU0去使用这个共享变量的时候，首先去自己的缓存中查找，肯定是缓存不命中的，也就是cache miss,这个时候去主内存Memory中去加载，当共享变量的值加载到CPU0的缓存后，CPU缓存行状态变成shared,也就是共享状态； 如果这个时候有其他的CPU也读取了共享变量的值，它们的cache line 的状态同样也是shared共享状态；此时一个CPU如果修改共享变量的值，而没有通知其他的CPU,就会造成缓存一致性问题； 当CPU0尝试去修改共享变量的值时，它会发出一个read invalidate命令，同时CPU0的缓存行状态设置为exclusive(独占),同时将其他加载了这个共享变量的cacheline的状态设置为invalid。通俗一点就是CPU0独占的这个变量的缓存行，其他的CPU缓存的共享变量都失效了； CPU0接下来修改共享变量的值，它会将cacheline的状态修改为modified,其实也是独占共享变量的cacheline，只不过是此时缓存行的数据和主内存Memory的数据不一致的，而exclusive虽然也是独占状态，但是共享变量的值是一样的，modified的值需要write back到Memory中去的，而exclusive是不需要的； 在cacheline没有替换出CPU0的cache之前，当有其他CPU来读取共享变量，此时肯定是cache miss ,因为CPU0的modify操作已经将它的缓存失效了。如果CPU0的状态是modified状态，它必须响应其他CPU的读操作，会告知其他CPU主内存的数据是dirty data。所以其他的CPU的状态可能会变成shared。如果CPU0还没有write back操作，其他的CPU状态还是invalid状态。 Store Buffer正如上面所描述的，在CPU0进行共享变量的修改，会同步修改其他CPU的cacheline状态为invalid，这个操作是和共享变量的写操作同步进行的，因此共享变量的写操作的性能是非常差的。在修改其他的CPU cacheline状态时，CPU0其实是处于阻塞状态的。所以为了优化这个问题，提出了Store Buffer的解决方案。 这样的话，写操作不必等到cacheline被加载，而是直接写到store buffer中，然后去执行后续的操作。由于是store buffer相当于是异步处理，在这里可能会出现因为并发执行导致的执行执行交叉问题，具体解决方法是依赖于内存屏障。具体可以参考这篇文章：Linux内核同步机制之（三）：memory barrier Invalidate Queue处理失效的缓存也不是简单的，需要读取主存。并且存储缓存也不是无限大的，那么当存储缓存满的时候，处理器还是要等待失效响应的。为了解决上面两个问题，引进了失效队列（invalidate queue）。处理失效的工作如下： 收到失效消息时，放到失效队列中去。 为了不让处理器久等失效响应，收到失效消息需要马上回复失效响应。 为了不频繁阻塞处理器，不会马上读主存以及设置缓存为invlid，合适的时候再一块处理失效队列。 happens- before原则 虽然指令重排提高了并发的性能，但是Java虚拟机会对指令重排做出一些规则限制，并不能让所有的指令都随意的改变执行位置，主要有以下几点： 1、单线程每个操作，happen-before于该线程中任意后续操作；2、volatile写happen-before与后续对这个变量的读；3、synchronized解锁happen-before后续对这个锁的加锁；4、final变量的写happen-before于final域对象的读，happen-before后续对final变量的读；5、传递性规则，A先于B，B先于C，那么A一定先于C发生；","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-synchronzied底层原理","date":"2021-07-28T09:57:19.000Z","path":"wiki/Java-synchronzied底层原理/","text":"synchronzied底层原理synchronzied四个层级实现 Java代码 通过添加synchronzied给对象或者方法或者代码块 字节码层级通过一组 MONITORENTER/MONITOREXIT指令 JVM层级：锁升级过程 汇编执行通过 lock comxchg指令保证原子操作 JDK早期，synchronized 叫做重量级锁， 因为申请锁资源必须通过kernel, 系统调用 1234567891011121314151617181920;hello.asm;write(int fd, const void *buffer, size_t nbytes)section data msg db &quot;Hello&quot;, 0xA len equ $ - msgsection .textglobal _start_start: mov edx, len mov ecx, msg mov ebx, 1 ;文件描述符1 std_out mov eax, 4 ;write函数系统调用号 4 int 0x80 mov ebx, 0 mov eax, 1 ;exit函数系统调用号 int 0x80 优化后的synchronized如下👇： Java层级12345678public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable()); synchronized (object)&#123; System.out.println(ClassLayout.parseInstance(object).toPrintable()); &#125; &#125; 字节码层级12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// access flags 0x9 public static main([Ljava/lang/String;)V // parameter args TRYCATCHBLOCK L0 L1 L2 null TRYCATCHBLOCK L2 L3 L2 null L4 LINENUMBER 13 L4 NEW java/lang/Object DUP INVOKESPECIAL java/lang/Object.&lt;init&gt; ()V ASTORE 1 L5 LINENUMBER 14 L5 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 1 INVOKESTATIC org/openjdk/jol/info/ClassLayout.parseInstance (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout; INVOKEVIRTUAL org/openjdk/jol/info/ClassLayout.toPrintable ()Ljava/lang/String; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L6 LINENUMBER 16 L6 ALOAD 1 DUP ASTORE 2 MONITORENTER L0 LINENUMBER 17 L0 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 1 INVOKESTATIC org/openjdk/jol/info/ClassLayout.parseInstance (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout; INVOKEVIRTUAL org/openjdk/jol/info/ClassLayout.toPrintable ()Ljava/lang/String; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L7 LINENUMBER 18 L7 ALOAD 2 MONITOREXIT L1 GOTO L8 L2 FRAME FULL [[Ljava/lang/String; java/lang/Object java/lang/Object] [java/lang/Throwable] ASTORE 3 ALOAD 2 MONITOREXIT L3 ALOAD 3 ATHROW L8 LINENUMBER 19 L8 FRAME CHOP 1 RETURN L9 LOCALVARIABLE args [Ljava/lang/String; L4 L9 0 LOCALVARIABLE object Ljava/lang/Object; L5 L9 1 MAXSTACK = 2 MAXLOCALS = 4&#125; i 主要通过MONITORENTER 和 MONITOREXIT 两个字节码指令控制加锁过程 JVM层级通过锁升级过程实现加锁；无锁 -&gt; 偏向锁 -&gt; 自旋锁（轻量级锁 自适应锁）-&gt; 重量级锁锁升级过程可以查看 锁升级过程 复制理解 汇编指令级别linux操作系统安装hsdis插件，查看java代码的汇编指令： 1234567891011121314public class T &#123; static volatile int i = 0; public static void n() &#123; i++; &#125; public static synchronized void m() &#123;&#125; publics static void main(String[] args) &#123; for(int j=0; j&lt;1000_000; j++) &#123; m(); n(); &#125; &#125;&#125; 执行以下命令： 1java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly T C1 Compile Level 1 (一级优化) C2 Compile Level 2 (二级优化) 找到m() n()方法的汇编码，会看到 lock comxchg …..指令","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-锁消除和锁膨胀","date":"2021-07-28T09:57:00.000Z","path":"wiki/Java-锁消除和锁膨胀/","text":"锁消除和锁粗化锁消除 （lock eliminate）1234public void add(String str1,String str2)&#123; StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2);&#125; 我们都知道 StringBuffer 是线程安全的，因为它的关键方法都是被 synchronized 修饰过的，但我们看上面这段代码，我们会发现，sb 这个引用只会在 add 方法中使用，不可能被其它线程引用（因为是局部变量，栈私有），因此 sb 是不可能共享的资源，JVM 会自动消除 StringBuffer 对象内部的锁。 锁粗化 （lock coarsening）123456789public String test(String str)&#123; int i = 0; StringBuffer sb = new StringBuffer(): while(i &lt; 100)&#123; sb.append(str); i++; &#125; return sb.toString():&#125; JVM 会检测到这样一连串的操作都对同一个对象加锁（while 循环内 100 次执行 append，没有锁粗化的就要进行 100 次加锁/解锁），此时 JVM 就会将加锁的范围粗化到这一连串的操作的外部（比如 while 虚幻体外），使得这一连串操作只需要加一次锁即可。 锁降级https://www.zhihu.com/question/63859501 其实，只被VMThread访问，降级也就没啥意义了。所以可以简单认为锁降级不存在！","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-锁升级过程","date":"2021-07-28T09:56:39.000Z","path":"wiki/Java-锁升级过程/","text":"锁升级使用JOL查看加锁之后的对象信息12345678public static void main(String[] args) &#123; Object object = new Object(); System.out.println(ClassLayout.parseInstance(object).toPrintable()); synchronized (object)&#123; System.out.println(ClassLayout.parseInstance(object).toPrintable()); &#125; &#125; 查看打印结果： 1234567891011121314151617java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totaljava.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) f0 b8 d0 0f (11110000 10111000 11010000 00001111) (265337072) 4 4 (object header) 00 70 00 00 (00000000 01110000 00000000 00000000) (28672) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 锁升级过程 w 锁升级过程： new - 偏向锁 - 轻量级锁 （无锁, 自旋锁，自适应自旋）- 重量级锁 自旋锁什么时候升级为重量级锁？ 为什么有自旋锁还需要重量级锁？ i 自旋是消耗CPU资源的，如果锁的时间长，或者自旋线程多，CPU会被大量消耗重量级锁有等待队列，所有拿不到锁的进入等待队列，不需要消耗CPU资源 偏向锁是否一定比自旋锁效率高？ i 不一定，在明确知道会有多线程竞争的情况下，偏向锁肯定会涉及锁撤销，这时候直接使用自旋锁JVM启动过程，会有很多线程竞争（明确），所以默认情况启动时不打开偏向锁，过一段儿时间再打开 synchronized优化的过程和markword息息相关用markword中最低的三位代表锁状态 其中1位是偏向锁位 两位是普通锁位 Object o = new Object()锁 = 0 01 无锁态注意：如果偏向锁打开，默认是匿名偏向状态 o.hashCode()001 + hashcode 1200000001 10101101 00110100 0011011001011001 00000000 00000000 00000000 little endian big endian 00000000 00000000 00000000 01011001 00110110 00110100 10101101 00000000 默认synchronized(o)00 -&gt; 轻量级锁默认情况 偏向锁有个时延，默认是4秒why? 因为JVM虚拟机自己有一些默认启动的线程，里面有好多sync代码，这些sync代码启动时就知道肯定会有竞争，如果使用偏向锁，就会造成偏向锁不断的进行锁撤销和锁升级的操作，效率较低。 1-XX:BiasedLockingStartupDelay=0 如果设定上述参数new Object () - &gt; 101 偏向锁 -&gt;线程ID为0 -&gt; Anonymous BiasedLock打开偏向锁，new出来的对象，默认就是一个可偏向匿名对象101 如果有线程上锁上偏向锁，指的就是，把markword的线程ID改为自己线程ID的过程偏向锁不可重偏向 批量偏向 批量撤销 如果有线程竞争撤销偏向锁，升级轻量级锁线程在自己的线程栈生成LockRecord ，用CAS操作将markword设置为指向自己这个线程的LR的指针，设置成功者得到锁 如果竞争加剧竞争加剧：有线程超过10次自旋， -XX:PreBlockSpin， 或者自旋线程数超过CPU核数的一半， 1.6之后，加入自适应自旋 Adapative Self Spinning ， JVM自己控制升级重量级锁：-&gt; 向操作系统申请资源，linux mutex , CPU从3级-0级系统调用，线程挂起，进入等待队列，等待操作系统的调度，然后再映射回用户空间 (以上实验环境是JDK11，打开就是偏向锁，而JDK8默认对象头是无锁)偏向锁默认是打开的，但是有一个时延，如果要观察到偏向锁，应该设定参数 如果计算过对象的hashCode，则对象无法进入偏向状态！ i 轻量级锁重量级锁的hashCode存在与什么地方？答案：线程栈中，轻量级锁的LR中，或是代表重量级锁的ObjectMonitor的成员中 关于epoch: (不重要) 批量重偏向与批量撤销渊源：从偏向锁的加锁解锁过程中可看出，当只有一个线程反复进入同步块时，偏向锁带来的性能开销基本可以忽略，但是当有其他线程尝试获得锁时，就需要等到safe point时，再将偏向锁撤销为无锁状态或升级为轻量级，会消耗一定的性能，所以在多线程竞争频繁的情况下，偏向锁不仅不能提高性能，还会导致性能下降。于是，就有了批量重偏向与批量撤销的机制。 原理以class为单位，为每个class维护解决场景批量重偏向（bulk rebias）机制是为了解决：一个线程创建了大量对象并执行了初始的同步操作，后来另一个线程也来将这些对象作为锁对象进行操作，这样会导致大量的偏向锁撤销操作。批量撤销（bulk revoke）机制是为了解决：在明显多线程竞争剧烈的场景下使用偏向锁是不合适的。 一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向。每个class对象会有一个对应的epoch字段，每个处于偏向锁状态对象的Mark Word中也有该字段，其初始值为创建该对象时class中的epoch的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其epoch字段改为新值。下次获得锁时，发现当前对象的epoch值和class的epoch不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其Mark Word的Thread Id 改成当前线程Id。当达到重偏向阈值后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑。 没错，我就是厕所所长 加锁，指的是锁定对象 锁升级的过程 JDK较早的版本 OS的资源 互斥量 用户态 -&gt; 内核态的转换 重量级 效率比较低 现代版本进行了优化 无锁 - 偏向锁 -轻量级锁（自旋锁）-重量级锁 偏向锁 - markword 上记录当前线程指针，下次同一个线程加锁的时候，不需要争用，只需要判断线程指针是否同一个，所以，偏向锁，偏向加锁的第一个线程 。hashCode备份在线程栈上 线程销毁，锁降级为无锁 有争用 - 锁升级为轻量级锁 - 每个线程有自己的LockRecord在自己的线程栈上，用CAS去争用markword的LR的指针，指针指向哪个线程的LR，哪个线程就拥有锁 自旋超过10次，升级为重量级锁 - 如果太多线程自旋 CPU消耗过大，不如升级为重量级锁，进入等待队列（不消耗CPU）-XX:PreBlockSpin 自旋锁在 JDK1.4.2 中引入，使用 -XX:+UseSpinning 来开启。JDK 6 中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。 自适应自旋锁意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 偏向锁由于有锁撤销的过程revoke，会消耗系统资源，所以，在锁争用特别激烈的时候，用偏向锁未必效率高。还不如直接使用轻量级锁。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"JVM-strace","date":"2021-07-28T09:52:23.000Z","path":"wiki/JVM-strace/","text":"strace 命令查看操作系统日志strace -ff -o out java ***.class -ff : 跟踪进程下所有线程用到的系统命令-o : 将跟踪的操作系统日志输出 i 下面查看JDK1.8下，BIO模式都有哪些系统命令的执行 123456789101112131415161718192021222324252627282930313233package com.ibli.javaBase.io.bio;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.net.ServerSocket;import java.net.Socket;/** * @Author gaolei * @Date 2021/4/3 2:55 下午 * @Version 1.0 */public class SockerIo &#123; public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(9090); // 阻塞 Socket client = serverSocket.accept(); InputStream inputStream = client.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 读阻塞 System.err.println(bufferedReader.readLine()); while (true)&#123; &#125; &#125;&#125; i 服务端 1、javac SockerIo.java 得到SockerIo.class然后，使用strace启动java程序👇：2、strace -ff -0 out java SockerIo得到如下日志： i 客户端使用nc连接9090端口，然后请求数据 nc 127.0.0.1 9090 发送如下数据 strace查看日志 查看主线程日志：如上图，👆文件最大的是主线程日志： 根据上面👆strace命令跟踪的日志可以看到，JDK1.8下的BIO的多路复用器是使用的「poll」","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"Java-NIO核心组件--channel","date":"2021-07-28T09:51:42.000Z","path":"wiki/Java-NIO核心组件-channel/","text":"NIO核心组件 - ChannelSocketChannel 和 ServerSocketChannel学习此部分可以对比Socket和ServerSocket 服务端代码 123456789101112131415161718192021222324252627282930313233343536373839public class NioSocketServer01 &#123; public static void main(String[] args) &#123; try &#123; // ServerSocketChannel 支持阻塞/非阻塞 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 设置成非阻塞。默认阻塞true serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(8080)); // 循环监听客户端连接 while (true) &#123; // 如果有客户端连接，则返回一个socketChannel实例，否则socketChannel=null SocketChannel socketChannel = serverSocketChannel.accept(); // 代码执行到此处，说明有客户端链接 if (socketChannel != null) &#123; // 读取客户端发送的数据，并输出 ByteBuffer buffer = ByteBuffer.allocate(1024); socketChannel.read(buffer); System.err.println(new String(buffer.array())); // 将数据在写会客户端 buffer.flip(); socketChannel.write(buffer); //验证客户端 socketChannel设置成false时，从服务端read数据的操作变成非阻塞的 //ByteBuffer buffer = ByteBuffer.allocate(1024); //buffer.put(&quot;this is server!&quot;); //buffer.flip(); //socketChannel.write(buffer); &#125; else &#123; Thread.sleep(1000L); System.err.println(&quot;no client&quot;); &#125; &#125; &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 客户端代码 12345678910111213141516171819202122232425262728293031public class NioSocketClient1 &#123; public static void main(String[] args) &#123; try &#123; SocketChannel socketChannel = SocketChannel.open(); // 默认阻塞IO true socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(&quot;localhost&quot;, 8080)); // finishConnect的主要作用就是确认通道连接已建立，方便后续IO操作（读写）不会因连接没建立而导致NotYetConnectedException异常。 if (socketChannel.isConnectionPending()) &#123; // finishConnect一直阻塞到connect建立完成 socketChannel.finishConnect(); &#125; ByteBuffer byteBuffer = ByteBuffer.allocate(1024); byteBuffer.put(&quot;hello world&quot;.getBytes()); byteBuffer.flip(); socketChannel.write(byteBuffer); byteBuffer.clear(); int r = socketChannel.read(byteBuffer); // 非阻塞方法 byteBuffer的数据还是上面put的 if (r &gt; 0) &#123; System.out.println(&quot;get msg:&#123;&#125;&quot; + new String(byteBuffer.array())); &#125; else &#123; System.out.println(&quot;server no back&quot;); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO核心组件--selector","date":"2021-07-28T09:51:30.000Z","path":"wiki/Java-NIO核心组件-selector/","text":"多路复用器select1、select选择器会告诉客户端哪些连接有数据要读取，但是读取的操作还是用户自己触发的，这种叫做「同步」 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.ibli.javaBase.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.*;import java.util.Iterator;import java.util.Set;/** * @Author gaolei * @Date 2021/4/3 4:09 下午 * @Version 1.0 */public class SelectMultiple &#123; private ServerSocketChannel server = null; private Selector selector = null; int port = 9090; public void initServer() throws IOException &#123; server = ServerSocketChannel.open(); server.configureBlocking(false); server.bind(new InetSocketAddress(port)); server.register(selector, SelectionKey.OP_ACCEPT); &#125; public void start() throws IOException &#123; initServer(); System.err.println(&quot;server started ....&quot;); while (true) &#123; // selector.select() 调用系统内核的select while (selector.select() &gt; 0) &#123; // 从多路复用器中选择有效的key Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectionKeys.iterator(); while (iter.hasNext()) &#123; SelectionKey key = iter.next(); if (key.isAcceptable()) &#123; acceptHandle(key); &#125; else if (key.isReadable()) &#123; readHandle(key); &#125; &#125; &#125; &#125; &#125; public void acceptHandle(SelectionKey key) throws IOException &#123; ServerSocketChannel ssc = (ServerSocketChannel) key.channel(); SocketChannel client = ssc.accept(); client.configureBlocking(false); ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1024); client.register(selector, SelectionKey.OP_READ, byteBuffer); System.err.println(&quot;client arrived &quot; + client.getRemoteAddress()); &#125; public void readHandle(SelectionKey key) throws IOException &#123; SocketChannel client = (SocketChannel) key.channel(); ByteBuffer buffer = (ByteBuffer) key.attachment(); buffer.clear(); int read = 0; while (true) &#123; read = client.read(buffer); if (read &gt; 0) &#123; // 服务端读到的数据，再写一遍给到客户端 buffer.flip(); while (buffer.hasRemaining()) &#123; client.write(buffer); &#125; buffer.clear(); &#125; else if (read == 0) &#123; break; &#125; else &#123; // client 发生错误 或者断开 read == -1 // 导致空转 最终CPU达到100% client.close(); break; &#125; &#125; &#125;&#125; i 上面的写法是一个selector既担任boss又担任worker","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO核心组件--buffer","date":"2021-07-28T09:51:10.000Z","path":"wiki/Java-NIO核心组件-buffer/","text":"Buffer 读写NIO之BufferBuffer作为NIO三大核心组件之一，本质上是一块可以写入数据，以及从中读取数据的内存，实际上也是一个byte[]数据,只是在NIO中被封装成了NIO Buffer对象并提供了一组方法来访问这个内存块。 下面是一个简单的Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// 读取一个text.txt文件，生成一个新的text1.txt文件public class FirstNioDemo &#123; public static void main(String[] args) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(&quot;/Users/gaolei/Desktop/text.txt&quot;); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/text1.txt&quot;); FileChannel inChannel = fileInputStream.getChannel(); FileChannel outChannel = fileOutputStream.getChannel(); // 声明缓冲区大小为1024字节 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // 从通道中读取数据 inChannel.read(byteBuffer); // 读模式切换为写模式 byteBuffer.flip(); //把缓冲区的数据写到通道 outChannel.write(byteBuffer); // 数据写完之后清空全部缓冲区 byteBuffer.clear(); //关闭文件流 fileInputStream.close(); fileOutputStream.close(); &#125;&#125;``` &gt; 执行结果：生成/Users/gaolei/Desktop/text1.txt文件 **Buffer进行数据读写操作的一般步骤** 1、写入数据到Buffer 2、调用flip()方法 3、从Buffer中读取数据 4、调用clear()方法或者compact()方法 &gt; clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 ### Buffer三个核心的属性 - capacity 容量 与buffer处在什么模式无关- position 游标位置 指向下一个存放/读取数据的位置 范围（0 ～ capacity–1）- limit ### 读写操作中Buffer三大属性的变化初始状态 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71f90dfd671f80eb9f6142f135b7c2dfc92.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 第一次读取数据 position处于起始位置，limit和capacity都处于结尾 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-41b47d9e54d58c7b39caf9e514fc9b5261f.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 第二次读取数据 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-07f3d1aa1f886b592b386cd4d846810911d.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; 当写数据的时候，需要调用flip方法： 当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position） &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b9323701bbb34a6c12f61d5ac2652ab7eeb.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; Clear方法 &lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-71f90dfd671f80eb9f6142f135b7c2dfc92.png&quot; height=&quot;230&quot; width=&quot;395&quot;&gt; ### JAVA NIO下的Buffer分类- ByteBuffer- MappedByteBuffer- CharBuffer- DoubleBuffer- FloatBuffer- IntBuffer- LongBuffer- ShortBuffer&gt; Java基本类型除了布尔类型，都有其对应的Buffer ### ByteBuffer使用&gt; 下面以ByteBuffer为例子看一下Buffer如何使用```java// 创建一个byteBuffer，设置容量为1024字节ByteBuffer byteBuffer = ByteBuffer.allocate(1024); 1、如下代码，其实调用了new HeapByteBuffer(capacity, capacity)来创建一个buffer 12345public static ByteBuffer allocate(int capacity) &#123; if (capacity &lt; 0) throw new IllegalArgumentException(); return new HeapByteBuffer(capacity, capacity); &#125; 2、创建了buffer之后要往里面写数据，除了上面从channel中读取数据之外，还可以调用put方法,如下 12ByteBuffer byteBuffer = ByteBuffer.allocate(1024);byteBuffer.put(&quot;hello world&quot;.getBytes()); 3、如果写将buffer中的数据写出去，必须先调用flap方法 flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。 4、将数据写到通道中 inChannel.write(buf); 5、数据写出到通道之后，要将缓存清空，一般调用clear方法clear方法 12345678public final Buffer clear() &#123; //position将被设回0 position = 0; //limit被设置成 capacity的值 limit = capacity; mark = -1; return this; &#125; Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。compact方法如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。 1234567891011public ByteBuffer compact() &#123; //compact()方法将所有未读的数据拷贝到Buffer起始处。 System.arraycopy(hb, ix(position()), hb, ix(0), remaining()); //position设到最后一个未读元素正后面 position(remaining()); //limit属性设置成capacity limit(capacity()); discardMark(); return this; &#125;现在Buffer准备好写数据了，但是不会覆盖未读的数据 零拷贝原理– 零拷贝，第一次接触零拷贝是在kafka的数据存储部分–IO流程：内存映射缓冲区比普通IO操作文件快很多，甚至比channel还要快很多。因为避免了很多系统调用（System.read System.write）。减少了内核缓冲区的数据拷贝到用户缓冲区。 举个栗子： 123456789101112public static void main(String[] args) throws IOException &#123; FileChannel in = FileChannel.open(Paths.get(&quot;/Users/gaolei/Desktop/text.txt&quot;), StandardOpenOption.READ); FileChannel out = FileChannel.open(Paths.get(&quot;/Users/gaolei/Desktop/text1.txt&quot;), StandardOpenOption.READ, StandardOpenOption.CREATE, StandardOpenOption.WRITE); MappedByteBuffer inBuffer = in.map(FileChannel.MapMode.READ_ONLY, 0, in.size()); MappedByteBuffer outBuffer = out.map(FileChannel.MapMode.READ_WRITE, 0, in.size()); byte[] bytes = new byte[inBuffer.limit()]; inBuffer.get(bytes); outBuffer.put(bytes); in.close(); out.close(); &#125; 普通的网络IO拷贝流程1、首先系统从磁盘上拷贝文件到内核空间缓冲区2、然后在内核空间拷贝数据到用户空间3、第三次，用户缓冲区再将数据拷贝到内核部分的socket缓冲4、内核在将存储在socket缓冲区的数据拷贝并发送到网卡缓冲区以上一个常规的网络IO经历了4次数据拷贝； 设置缓冲区的意义在于提升性能，当用户空间仅仅需要一小部分数据的时候，操作系统会在磁盘上读取一块数据方法内核缓冲区，这个叫做局部性原理。 零拷贝减去了内核空间数据到用户空间数据的拷贝，从而提升IO性能。假设读取的文件很大，操作系统需要读取磁盘大量数据到内核空间，这时候内核缓冲区的作用是很难体现的。因为如果用户空间需要少量数据的时候是可以直接在内核空间获取的（局部性原理）。正式因为有了零拷贝，操作系统在磁盘读取数据之后，可以直接发送到网卡缓冲区，从而大大提升IO性能。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-NIO","date":"2021-07-28T09:50:38.000Z","path":"wiki/Java-NIO/","text":"Java NIO w Java NIO 对于Java BIO的优化 Java 非阻塞IO 及时不使用线程池，也可以处理多个客户端请求 12345678910111213141516171819202122232425262728293031323334353637383940public static void main(String[] args) throws IOException, InterruptedException &#123; LinkedList&lt;SocketChannel&gt; clients = new LinkedList&lt;&gt;(); ServerSocketChannel ss = ServerSocketChannel.open(); ss.bind(new InetSocketAddress(9090)); ss.configureBlocking(false); while (true) &#123; Thread.sleep(1000L); // 非阻塞 SocketChannel client = ss.accept(); if (client == null) &#123; System.err.println(&quot;client is null&quot;); &#125; else &#123; client.configureBlocking(false); int port = client.socket().getPort(); System.err.println(&quot;client port &quot; + port); clients.add(client); &#125; ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4096); // 串型话 // 真实场景下 每一个client一个独自的buffer for (SocketChannel c : clients) &#123; // -1 出现空轮训 int num = c.read(byteBuffer); if (num &gt; 0) &#123; byteBuffer.flip(); byte[] aaa = new byte[byteBuffer.limit()]; byteBuffer.get(aaa); String b = new String(aaa); System.err.println(c.socket().getPort() + &quot; : &quot; + b); // 清空 循环下一次client在使用 byteBuffer.clear(); &#125; &#125; &#125;&#125; 以上可以实现，一个线程可以处理多个客户端链接，服务端非阻塞接收，接收之后，读取数据也是非阻塞的； i NIO的非阻塞是操作系统内部实现的，底层调用了linux内核的accept函数 d Java的NIO有什么弊端 服务端还是会进行空转 不管有没有客户端连接建立，服务端都要不断执行accept方法 不管客户端连接有没有传输数据，都会执行一遍read操作 资源浪费问题 还是会存在C10k的问题","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java-传统的BIO","date":"2021-07-28T09:50:21.000Z","path":"wiki/Java-传统的BIO/","text":"传统的BIOSocket 和 ServerSocket1234567891011121314151617public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(9090); // 阻塞 Socket client = serverSocket.accept(); InputStream inputStream = client.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 读阻塞 System.err.println(bufferedReader.readLine()); while (true)&#123; &#125; &#125; i new ServerSocket(9090); 这个java程序创建new ServerSocket(9090);会调用操作系统内核，也就是系统调用，比如linux操作系统，应用进程也就是我们的java进程，会调用linux的内核方法，创建一个socket，在linux系统中就是一个文件描述符fd，最终对得到： 123socket() = XXfdbind(XXfd,9090)listen(XXfd) socket 的read方法 ，读取客户端发送的数据，如果没有，则一直阻塞 serverSocket的accept方法，等待客户端的链接，如果没有链接，则一直阻塞等待 serverSocket 一次只能处理一个客户端请求 BIO程序有哪些弊端？ 服务端一次处理一个请求，并发非常低 没有客户端请求，服务端一直阻塞，占用资源 如果在bio的基础上，利用多线程处理客户端请求？ d C10K问题 来一个链接，服务端创建一个线程 ，去处理请求，服务端继续监听客户端，是不是可以增加并发？有什么问题？ 线程消耗内存资源 如果一下子过来10万个请求呢？服务器要创建10万个线程，内存就崩了。 如果搞一个线程池呢？ 并发度最大为最大线程数？ 并发度已经定死了？","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"JVM-jstat","date":"2021-07-28T09:46:25.000Z","path":"wiki/JVM-jstat/","text":"jstatjstat是一个简单的实用工具，在JDK中存在，用于提供与JVM性能相关的统计信息，例如垃圾收集，编译活动。 jstat的主要优势在于，它可以在运行JVM且无需任何先决条件的情况下动态捕获这些指标。 这是什么意思？ 例如，如果要捕获与垃圾回收相关的统计信息，则需要在启动JVM之前传递以下参数： i -Xlog:gc*:file={file-path} 此参数将启用GC日志并将其打印在指定的文件路径中。 假设您尚未传递此参数，那么将不会生成与GC相关的统计信息。 这是jstat可以派上用场的地方。 您可以动态地连接到JVM并捕获GC，编译相关的统计信息如下所示。 jstat操作执行命令： 1jstat -gc -t 11656 10000 30 -gc ：将显示与垃圾收集相关的统计信息 自JVM启动以来的-t时间戳将被打印 11656：目标JVM进程ID 10000：每10,000毫秒（即10秒）将打印一次统计信息。 30 ：将打印30次迭代的统计信息。 因此，以上选项将导致JVM打印指标300秒（即10秒x 30次迭代）。 （请注意，除了-gc之外，您还可以传递其他各种选项来生成不同的数据集。有关不同选项的更多详细信息，请参见此处 。）打印结果： 1234Timestamp S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 34486.1 1536.0 1536.0 0.0 878.8 226816.0 132809.2 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 34496.3 1536.0 1536.0 0.0 878.8 226816.0 138030.9 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 34506.3 1536.0 1536.0 0.0 878.8 226816.0 195648.1 218112.0 113086.4 120664.0 111284.9 14464.0 12928.3 355 3.523 6 1.126 4.649 字段解读S0C –幸存者0区域的容量，以KB为单位 S1C –幸存者1区域的容量，以KB为单位 S0U –幸存者0区域使用的空间以KB为单位 S1U –幸存者1区域以KB为单位使用空间 EC –伊甸园地区容量（KB） 欧盟–伊甸园地区的已利用空间（以KB为单位） OC –旧区域容量（KB） OU –旧区域的已利用空间，以KB为单位 MC –元空间区域容量，以KB为单位 MU –元空间区域使用的空间以KB为单位 CCSC –压缩类空间区域的容量，以KB为单位 CCSU –压缩类空间区域以KB为单位使用空间 YGC –迄今为止发生的年轻GC事件的数量 YGCT –到目前为止，年轻GC花费的时间 FGC –迄今为止已发生的完全GC事件的数量 FGCT –到目前为止已花费的完整GC时间 GCT –到目前为止所花费的GC时间总量（基本上是YGCT + FGCT） 参考资料jstat分析_jstat –分析","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-jstack","date":"2021-07-28T09:46:12.000Z","path":"wiki/JVM-jstack/","text":"jstackjstack 功能主要分为两个功能： a． 针对活着的进程做本地的或远程的线程dump； b． 针对core文件做线程dump。 jstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack 操作方式 i jps -l | grep keyword -&gt; pidjstack pid jstack结果如下； 123456789101112131415161718&quot;lettuce-nioEventLoop-4-1&quot; #639 daemon prio=5 os_prio=0 tid=0x00007ff27025d800 nid=0x258f runnable [0x00007ff262af7000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked &lt;0x000000008988d6a8&gt; (a io.netty.channel.nio.SelectedSelectionKeySet) - locked &lt;0x000000008988d770&gt; (a java.util.Collections$UnmodifiableSet) - locked &lt;0x000000008988d600&gt; (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101) at io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68) at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:803) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:748) 参考资料原文链接：https://blog.csdn.net/weixin_30013175/article/details/113901522","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-类加载机制","date":"2021-07-28T09:45:58.000Z","path":"wiki/JVM-类加载机制/","text":"类加载机制类加载机制具体流程Java 的类加载过程可以分为 5 个阶段：载入、验证、准备、解析和初始化。这 5 个阶段一般是顺序发生的，但在动态绑定的情况下，解析阶段发生在初始化阶段之后。 1）Loading（载入） JVM 在该阶段的主要目的是将字节码从不同的数据源（可能是 class 文件、也可能是 jar 包，甚至网络）转化为二进制字节流加载到内存中，并生成一个代表该类的 java.lang.Class 对象。 2）Verification（验证） JVM 会在该阶段对二进制字节流进行校验，只有符合 JVM 字节码规范的才能被 JVM 正确执行。该阶段是保证 JVM 安全的重要屏障，下面是一些主要的检查。 确保二进制字节流格式符合预期（比如说是否以 cafe bene 开头）。是否所有方法都遵守访问控制关键字的限定。方法调用的参数个数和类型是否正确。确保变量在使用之前被正确初始化了。检查变量是否被赋予恰当类型的值。3）Preparation（准备） JVM 会在该阶段对类变量（也称为静态变量，static 关键字修饰的）分配内存并初始化（对应数据类型的默认初始值，如 0、0L、null、false 等）。 也就是说，假如有这样一段代码： public String chenmo = “沉默”;public static String wanger = “王二”;public static final String cmower = “沉默王二”;chenmo 不会被分配内存，而 wanger 会；但 wanger 的初始值不是“王二”而是 null。 需要注意的是，static final 修饰的变量被称作为常量，和类变量不同。常量一旦赋值就不会改变了，所以 cmower 在准备阶段的值为“沉默王二”而不是 null。 4）Resolution（解析） 该阶段将常量池中的符号引用转化为直接引用。 what？符号引用，直接引用？ 符号引用以一组符号（任何形式的字面量，只要在使用时能够无歧义的定位到目标即可）来描述所引用的目标。 在编译时，Java 类并不知道所引用的类的实际地址，因此只能使用符号引用来代替。比如 com.Wanger 类引用了 com.Chenmo 类，编译时 Wanger 类并不知道 Chenmo 类的实际内存地址，因此只能使用符号 com.Chenmo。 直接引用通过对符号引用进行解析，找到引用的实际内存地址。 5）Initialization（初始化） 该阶段是类加载过程的最后一步。在准备阶段，类变量已经被赋过默认初始值，而在初始化阶段，类变量将被赋值为代码期望赋的值。换句话说，初始化阶段是执行类构造器方法的过程。 oh，no，上面这段话说得很抽象，不好理解，对不对，我来举个例子。 String cmower = new String(“沉默王二”);上面这段代码使用了 new 关键字来实例化一个字符串对象，那么这时候，就会调用 String 类的构造方法对 cmower 进行实例化。 什么是双亲委派聊完类加载过程，就不得不聊聊类加载器。 一般来说，Java 程序员并不需要直接同类加载器进行交互。JVM 默认的行为就已经足够满足大多数情况的需求了。不过，如果遇到了需要和类加载器进行交互的情况，而对类加载器的机制又不是很了解的话，就不得不花大量的时间去调试ClassNotFoundException 和 NoClassDefFoundError 等异常。 对于任意一个类，都需要由它的类加载器和这个类本身一同确定其在 JVM 中的唯一性。也就是说，如果两个类的加载器不同，即使两个类来源于同一个字节码文件，那这两个类就必定不相等（比如两个类的 Class 对象不 equals）。 站在程序员的角度来看，Java 类加载器可以分为三种。 1）启动类加载器（Bootstrap Class-Loader），加载 jre/lib 包下面的 jar 文件，比如说常见的 rt.jar。 2）扩展类加载器（Extension or Ext Class-Loader），加载 jre/lib/ext 包下面的 jar 文件。 3）应用类加载器（Application or App Clas-Loader），根据程序的类路径（classpath）来加载 Java 类。 来来来，通过一段简单的代码了解下。 public class Test &#123; public static void main(String[] args) &#123; ClassLoader loader = Test.class.getClassLoader(); while (loader != null) &#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; &#125; &#125; 每个 Java 类都维护着一个指向定义它的类加载器的引用，通过 类名.class.getClassLoader() 可以获取到此引用；然后通过 loader.getParent() 可以获取类加载器的上层类加载器。 这段代码的输出结果如下： sun.misc.Launcher$AppClassLoader@73d16e93sun.misc.Launcher$ExtClassLoader@15db9742第一行输出为 Test 的类加载器，即应用类加载器，它是 sun.misc.Launcher$AppClassLoader 类的实例；第二行输出为扩展类加载器，是 sun.misc.Launcher$ExtClassLoader 类的实例。那启动类加载器呢？ 按理说，扩展类加载器的上层类加载器是启动类加载器，但在我这个版本的 JDK 中， 扩展类加载器的 getParent() 返回 null。所以没有输出。 双亲委派机制： 双亲委派如何破坏线程上下文加载器 参考资料1、Java类加载机制2、通俗易懂的双亲委派机制","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"JVM-jmap","date":"2021-07-28T09:45:41.000Z","path":"wiki/JVM-jmap/","text":"jmap命令jmap是一个多功能的命令。它可以生成 java 程序的 dump 文件， 也可以查看堆内对象示例的统计信息、查看 ClassLoader 的信息以及 finalizer 队列。 jmap -heap pid1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Attaching to process ID 7183, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.242-b08using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2051014656 (1956.0MB) NewSize = 42991616 (41.0MB) MaxNewSize = 683671552 (652.0MB) OldSize = 87031808 (83.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 233308160 (222.5MB) used = 161611280 (154.12452697753906MB) free = 71696880 (68.37547302246094MB) 69.26945032698384% usedFrom Space: capacity = 1572864 (1.5MB) used = 899896 (0.8582077026367188MB) free = 672968 (0.6417922973632812MB) 57.213846842447914% usedTo Space: capacity = 1572864 (1.5MB) used = 0 (0.0MB) free = 1572864 (1.5MB) 0.0% usedPS Old Generation capacity = 223346688 (213.0MB) used = 115841432 (110.4749984741211MB) free = 107505256 (102.5250015258789MB) 51.866196466723515% used41772 interned Strings occupying 4324472 bytes. 参考资料jvm 性能调优工具之 jmapJVM调试工具-jmap通过jstack与jmap分析一次线上故障","tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"}]},{"title":"SpringBean循环依赖","date":"2021-07-28T09:34:20.000Z","path":"wiki/SpringBean循环依赖/","text":"Spring Bean 循环依赖为什么会存在循环依赖 如上图👆所示，A对象的一个属性是B,B对象的一个属性是A,而Spring中的bean默认情况下都是单例的\b，所以这两个Bean就产生了循环依赖的问题！ i 那么循环依赖的问题出现在什么情况呢 想一下属性赋值的方式有几种呢？ 构造器赋值 这种形式循环依赖问题无法解决 GET/SET方法 调用SET方法进行赋值的时候，可以通过三级缓存的策略来解决循环依赖的问题 所以，三级缓存的策略是针对于使用SET方法对属性赋值的场景下的！ 循环依赖如何解决 在实例化的过程中，将处于半成品的对象全部放到缓存中，方便后续来进行调用；只要有了当前对象的引用地址，那么后续来进行赋值即可； d 能不能将创建好的对象也放到缓存中呢？ 不能，如果放在一起将无法区分对象是成品对象还是半成品对象了所以再次引出多级缓存的概念，可以创建两个缓存对象，一个用来存放已经实例化的半成品对象，另一个存放完成实例化并且完成初始化的成品对象，这个应该比较好理解吧！ i 思考一下以上的设计有没有问题呢？ 为什么需要三级缓存？Spring在解决对象Bean循环依赖的问题的解决方案是使用了「三级缓存」；为什么需要三级缓存，也就是三个Map对象； i org.springframework.beans.factory.support.DefaultSingletonBeanRegistry 123456// 一级缓存private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap(256);// 二级缓存private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap(16);// 三级缓存private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap(16); w 三级缓存中分别保存的是什么内容 一级缓存： 成品对象 二级缓存： 半成品对象 三级缓存； lambda表达式 w 如果只有二级缓存可不可行 在Spring源码中，只有addSingleton方法和doCreateBean方法中向三级缓存中添加东西的； org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#addSingletonFactory 123456789protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized(this.singletonObjects) &#123; this.singletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125; org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#getSingleton(java.lang.String, boolean) 1234567891011121314151617181920@Nullable protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; this.isSingletonCurrentlyInCreation(beanName)) &#123; synchronized(this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, &quot;Bean name must not be null&quot;); synchronized(this.singletonObjects) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; if (this.singletonsCurrentlyInDestruction) &#123; throw new BeanCreationNotAllowedException(beanName, &quot;Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)&quot;); &#125; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(&quot;Creating shared instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; this.beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = this.suppressedExceptions == null; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = new LinkedHashSet(); &#125; try &#123; singletonObject = singletonFactory.getObject(); newSingleton = true; &#125; catch (IllegalStateException var16) &#123; singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; throw var16; &#125; &#125; catch (BeanCreationException var17) &#123; BeanCreationException ex = var17; if (recordSuppressedExceptions) &#123; Iterator var8 = this.suppressedExceptions.iterator(); while(var8.hasNext()) &#123; Exception suppressedException = (Exception)var8.next(); ex.addRelatedCause(suppressedException); &#125; &#125; throw ex; &#125; finally &#123; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = null; &#125; this.afterSingletonCreation(beanName); &#125; if (newSingleton) &#123; this.addSingleton(beanName, singletonObject); &#125; &#125; return singletonObject; &#125; &#125;","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"Spring加载配置文件原理","date":"2021-07-28T09:33:48.000Z","path":"wiki/Spring加载配置文件原理/","text":"Spring如何加载配置文件到应用程序加载Xml文件配置，获取对象 i xml文件 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;user&quot; class=&quot;com.ibli.javaBase.reflection.User&quot;&gt; &lt;property name=&quot;age&quot; value=&quot;12&quot;/&gt; &lt;property name=&quot;name&quot; value=&quot;gaolei&quot;/&gt; &lt;property name=&quot;sex&quot; value=&quot;male&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; i 测试类 1234567public class IocDemo &#123; public static void main(String[] args) &#123; ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;spring-ioc.xml&quot;); User user = (User) ac.getBean(&quot;user&quot;); System.out.println(user); &#125;&#125; Spring 加载Xml文件流程 w 首先猜想一下宏观的流程 我们可以大体猜想流程是什么样的，如下👇 接下来debug源码看一下具体流程： i ClassPathXmlApplicationContext调用refresh方法 12345678public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); this.setConfigLocations(configLocations); if (refresh) &#123; // Spring 启动入口 this.refresh(); &#125; &#125; Spring 启动入口 this.refresh(); 👆 i 调用AbstractRefreshableApplicationContext下的refreshBeanFactory org.springframework.context.support.AbstractRefreshableApplicationContext#refreshBeanFactory 1234567891011121314151617181920protected final void refreshBeanFactory() throws BeansException &#123; if (this.hasBeanFactory()) &#123; this.destroyBeans(); this.closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = this.createBeanFactory(); beanFactory.setSerializationId(this.getId()); this.customizeBeanFactory(beanFactory); // 从这里进入下一步 👇 this.loadBeanDefinitions(beanFactory); synchronized(this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException var5) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + this.getDisplayName(), var5); &#125; &#125; 关键方法是this.loadBeanDefinitions(beanFactory); i 找到XmlBeanDefinitionReader 这是读取配置的关键所在 关键对象 XmlBeanDefinitionReader 这个在 「梳理Spring启动脉络」中提到了，Spring提供的抽象接口！ 12345678910protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化beanDefinitionReader对象 this.initBeanDefinitionReader(beanDefinitionReader); // 加载配置文件 获得BeanDefinitions this.loadBeanDefinitions(beanDefinitionReader); &#125; i 继续调用 loadBeanDefinitions 这个有很多重载方法，一直点下去就行！ 12345678910111213protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = this.getConfigResources(); if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = this.getConfigLocations(); //spring-ioc.xml if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125; &#125; configLocations 就是我们Xml配置文件的路径 i 接下来一直调用loadBeanDefinitions方法 直到这一步 👇 org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.support.EncodedResource) 1234567891011121314151617181920212223242526272829try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); Throwable var4 = null; try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; var6 = this.doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; catch (Throwable var24) &#123; var4 = var24; throw var24; &#125; finally &#123; if (inputStream != null) &#123; if (var4 != null) &#123; try &#123; inputStream.close(); &#125; catch (Throwable var23) &#123; var4.addSuppressed(var23); &#125; &#125; else &#123; inputStream.close(); &#125; &#125; &#125; &#125; 这里看到 InputStream 很明显，这里是通过IO流读取制定位置的文件的 ! i 获取到文件输入流之后，将输入流转换成Document文件去解析 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException 12// 转换成Document的关键方法Document doc = this.doLoadDocument(inputSource, resource); i 调用doRegisterBeanDefinitions方法 org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#doRegisterBeanDefinitions调用parseBeanDefinitions方法去解析数据 i 调用DefaultBeanDefinitionDocumentReader的parseBeanDefinitions方法 来解析Element org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#parseBeanDefinitions 1234567891011121314151617181920protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for(int i = 0; i &lt; nl.getLength(); ++i) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element)node; if (delegate.isDefaultNamespace(ele)) &#123; this.parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125; &#125; i 调用parseDefaultElement方法 org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader#parseDefaultElement 123456789101112private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, &quot;import&quot;)) &#123; this.importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, &quot;alias&quot;)) &#123; this.processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, &quot;bean&quot;)) &#123; this.processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, &quot;beans&quot;)) &#123; this.doRegisterBeanDefinitions(ele); &#125; &#125; 这里看到if (delegate.nodeNameEquals(ele, &quot;bean&quot;)) 会不会很兴奋呢，接下来就是解析的方法了👇 i 跳转到 processBeanDefinition(ele, delegate); 12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 是的 就是这个方法了 👉 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, this.getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException var5) &#123; this.getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, var5); &#125; this.getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; i parseBeanDefinitionElement 将元素数据解析到beanDefinition org.springframework.beans.factory.xml.BeanDefinitionParserDelegate#parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Nullable public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(&quot;id&quot;); String nameAttr = ele.getAttribute(&quot;name&quot;); List&lt;String&gt; aliases = new ArrayList(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, &quot;,; &quot;); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(id) &amp;&amp; !aliases.isEmpty()) &#123; beanName = (String)aliases.remove(0); if (this.logger.isTraceEnabled()) &#123; this.logger.trace(&quot;No XML &#x27;id&#x27; specified - using &#x27;&quot; + beanName + &quot;&#x27; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; this.checkNameUniqueness(beanName, aliases, ele); &#125; // 将element数据最终转换成一个beanDefinition对象 是不是很惊奇 哈哈哈 👉 AbstractBeanDefinition beanDefinition = this.parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName(beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (this.logger.isTraceEnabled()) &#123; this.logger.trace(&quot;Neither XML &#x27;id&#x27; nor &#x27;name&#x27; specified - using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception var9) &#123; this.error(var9.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; else &#123; return null; &#125; &#125;","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"Spring梳理启动脉络","date":"2021-07-28T09:33:20.000Z","path":"wiki/Spring梳理启动脉络/","text":"Spring是如何启动的Spring最大的核心就是Bean容器；容器： 从对象创建，使用和销毁全部由容器帮我们控制，用户仅仅使用就可以。 两大核心 IOC 控制反转 AOP 面向切面编程 思考：我们是如何使用Spring的呢？ i 加入从配置文件中加载bean 我们猜想一下大致流程是怎样的 123&lt;bean id=getPerson class=com.ibli.Person&gt;&lt;property name=id value=1&gt;&lt;property name=age value=20&gt; 配置文件如上👆，这里是伪代码！ 先猜想大致流程： 通过上面猜想创建的对象流程，创建出对象，对象已经好了，就是使用了，那么如何使用呢？ 一般情况下我们会可以这样使用，写一下伪代码吧👇： 12创建一个ApplicationContext对象Object obj = applicationContext.getBean(&quot;bean name); 思考，创建的对象如何存储？ 或者容器到底是什么呢？ 应该可以猜到是Map结构，具体是什么Map,先不管； i 1、首先容器是创建好的，容器创建好之后，才可以加载配置文件 也就是我们猜想的Map i 2、加载配置文件 配置文件可能会有多种方式，比如XML格式，property格式，yaml格式，注解格式，这个格式各不相同，又是如何加载的呢？Spring提供了一个接口，BeanDefinitionReader,它有一个抽象实现类AbstractBeanDefinitionReader，不同配置文件的Reader来继承这个抽象类，实现它们自己的逻辑； 123public class PropertiesBeanDefinitionReader extends AbstractBeanDefinitionReaderpublic class GroovyBeanDefinitionReader extends AbstractBeanDefinitionReader implements GroovyObjectpublic class XmlBeanDefinitionReader extends AbstractBeanDefinitionReader i 3、读取的配置文件会转换成Spring定义的格式，也就是BeanDefinition； BeanDefinition定义了类的所有相关的数据； 此时得到的BeanDefinition的属性值只是「符号类型」,并不是真正的属性值； 我们可能会见过这中加载数据源的方式👇 12345&lt;bean id=dataSource class=com.alibab.durid.pool.DruidDataSource&gt;&lt;property name=url value=$&#123;jdbc.url&#125;&gt;&lt;property name=username value=$&#123;jdbc.username&#125;&gt;&lt;property name=password value=$&#123;jdbc.password&#125;&gt;&lt;/bean&gt; 数据源的具体配置是放在配置文件中的，当通过XmlBeanDefinitionReader读取并解析到的BeanDefinition，仅仅是将Xml中的文件数据存放到BeanDefinition中，属性的值是${jdbc.url}而不是真正的我们数据源的地址； i 4、得到所有的BeanDefinition之后，通过BeanFactoryPostProcessor来处理上一步骤中，属性value不是真实数据的问题 比如PlaceHolderConfigurerSupport(占位符处理) 经过工厂后置处理器处理之后，BeanDefinition的属性值就是真实需要的数据了； i 5、BeanDefinition数据准备完成之后，由BeanFactory来完成Bean的创建 实例化 对象中分配堆内存等操作 反射调用无参构造函数 创建对象 但是属性是空的 初始化 i 6、初始化之前需要准备的工作 1、准备BeanPostProcessors2、观察者模式，准备监听器 事件 广播器 i 7、初始化环节有很多步骤 对象的填充 其实就是调用get/set方法对属性赋值 调用aware方法 如果我们的对象中的属性是BeanFactory 我们不用自己去完成setBeanFactory方法，只需要当前类实现BeanFactoryAware方法即可 123public interface BeanFactoryAware extends Aware &#123; void setBeanFactory(BeanFactory var1) throws BeansException;&#125; 处理before操作 调用init方法 执行after方法 before和after此处是调用的BeanPostProcessor的方法 1234// 前置方法postProcessBeforeInitialization// 后置方法postProcessAfterInitialization i 8、执行到此，完成对象的创建，得到一个可以使用的对象","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"}]},{"title":"","date":"2021-07-28T09:32:41.602Z","path":"wiki/Spring-Overview/","text":"title: Spring-Overviewtoc: truedate: 2021-07-28 17:32:41tags: Springcategories: [Spring Family , Spring Framework] SpringThe Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. A key element of Spring is infrastructural support at the application level: Spring focuses on the “plumbing” of enterprise applications so that teams can focus on application-level business logic, without unnecessary ties to specific deployment environments. 学习方法 先梳理脉络，先宏观，再细节 Spring源码注释很重要 见名知意 学习spring的命名规范 猜测和验证 坚持看 不要三分钟热度 学习资料 w Spring框架官方网站 【官网】可以下载Spirng源码在本地查看更舒服！ 源码请戳 👉👉 【源码】 👈👈 w Spring5最新完整教程IDEA版通俗易懂 视频教程 狂神说 （这个大佬在B站很火的）原链接请点击👉 【传送】 w 24集彻底搞懂aop ioc mvc底层原理 视频目录比较好 👉 【2020年史上最新Spring源码合集，24集彻底搞懂aop ioc mvc底层原理。】 w 手撕SpringIOC源码 马士兵教育 【400分钟学完Spring源码设计及原理，手撕SpringIOC源码，从我做起】 w 图灵学院公开课 课程目录还不错 5个小时 21年录制比较新 【2021年新版Java-Spring底层原理，阿里P8大佬全套讲解】 w Mybatis + Spring 源码解读 VIP公开课【终于有字节跳动技术大牛把【mybatis底层原理：spring整理mybatis】讲明白了】","tags":[],"categories":[]},{"title":"mysql乐观锁实现分布式锁","date":"2021-07-28T09:21:46.000Z","path":"wiki/mysql乐观锁实现分布式锁/","text":"基于数据表乐观锁实现分布式锁整体的实际思路要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。当我们要锁住某个方法或资源的时候，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。 基于数据表实现分布式锁的几个要点1、这把锁依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。2、这把锁没有失效时间，一旦解决操作失败，就会导致记录一直在数据库中，其他线程无法在获得锁。3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁的操作。4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据库表中数据已经存在了。 当然，我们也可以有其它方式解决上面的问题： 1、数据库是单点？那就搞两个数据库，数据库之前双向同步，一旦挂掉快速切换到备库上。2、没有失效时间？可以做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。3、非阻塞？可以写一个while循环，直到insert成功再返回成功。4、非重入？可以在数据库表中加一个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库中可以查到的话，就直接把锁分配给它即可。 乐观锁&amp;悲观锁乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。 乐观锁的实现实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。 1、使用版本号实现乐观锁使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。 12345671.查询出商品信息select (status,status,version) from t_goods where id=#&#123;id&#125;2.根据商品信息生成订单3.修改商品status为2update t_goodsset status=2,version=version+1where id=#&#123;id&#125; and version=#&#123;version&#125;; 需要注意的是，乐观锁机制往往基于系统中数据存储逻辑，因此也具备一定的局限性。由于乐观锁机制是在我们的系统中实现的，对于来自外部系统的用户数据更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。在系统设计阶段，我们应该充分考虑到这些情况，并进行相应的调整（如将乐观锁策略在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。 这一点其实在微服务架构中只要做好数据隔离就可以避免，比如user这张数据表，按照边界划分应该属于用户中心服务的，其他服务比如仓储，物流等需要用户的信息，应该有用户中心暴露出接口，而不是仓储去数据库查询user这张表的数据，甚至update user的数据。 2、使用时间戳一般都是使用update_time字段，并且这个字段肯定是跟随数据库时间配置的，即 update on current_timestamp ； 乐观锁的优点与不足乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。能够提升数据库的吞吐量；但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。 参考资料基于数据库的分布式锁实现分布式锁方式（一、基于数据库的分布式锁）分布式锁看这篇就够了乐观锁与悲观锁深入学习","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"","date":"2021-07-28T09:05:46.241Z","path":"wiki/mybatis配置文件解析/","text":"title: mybatis配置文件解析toc: truedate: 2021-07-28 17:05:46tags: mybatiscategories: [Spring Family] Mybatis配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- 引入数据库属性文件 --&gt; &lt;properties resource=&quot;database.properties&quot;&gt; &lt;!-- &lt;property name=&quot;username&quot; value=&quot;sa&quot;&gt;&lt;/property&gt; --&gt; &lt;/properties&gt; &lt;!-- mybatis配置文件 --&gt; &lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;false&quot;/&gt; &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt; &lt;/settings&gt; &lt;!-- 别名的配置 --&gt; &lt;typeAliases&gt; &lt;!-- &lt;typeAlias type=&quot;com.xit.pojo.User&quot; alias=&quot;user&quot;/&gt; --&gt; &lt;package name=&quot;com.xit.pojo&quot;/&gt; &lt;/typeAliases&gt; &lt;!-- 配置运行环境 --&gt; &lt;environments default=&quot;default&quot;&gt; &lt;environment id=&quot;default&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;!-- 由JDBC管理事务 --&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!-- 配置数据源：连接池 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 引入Mapper映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;com/xit/pojo/UserMapper.xml&quot;/&gt; &lt;!-- URL方式 --&gt; &lt;!-- &lt;mapper url=&quot;file:///C:/eclipse-workspace/mybatis-01/src/com/xit/pojo/UserMapper.xml&quot;/&gt; --&gt; &lt;/mappers&gt;&lt;/configuration&gt; Mybatis有几部分全局配置properties=&gt;settings=&gt;typeAliases=&gt;typeHandlers=&gt;objectFactory=&gt;plugins=&gt;environment=&gt;databaseIdProvider=&gt;mappers Mybatis 加载Mapper文件有几种方式？以上是Mybatis官方文档介绍的样例👆，原文链接请点击这 有4种方式；按照优先级从高到底依次是： package resource url class 下面是Mybatis加载mybatis-config.xml文件配置的源码，从代码中也可以看到加载的4中方式和优先级！👇org.apache.ibatis.builder.xml.XMLConfigBuilder#typeHandlerElement 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private void mapperElement(XNode parent) throws Exception &#123; if (parent != null) &#123; Iterator var2 = parent.getChildren().iterator(); while(true) &#123; while(var2.hasNext()) &#123; XNode child = (XNode)var2.next(); String resource; if (&quot;package&quot;.equals(child.getName())) &#123; resource = child.getStringAttribute(&quot;name&quot;); this.configuration.addMappers(resource); &#125; else &#123; resource = child.getStringAttribute(&quot;resource&quot;); String url = child.getStringAttribute(&quot;url&quot;); String mapperClass = child.getStringAttribute(&quot;class&quot;); XMLMapperBuilder mapperParser; InputStream inputStream; if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(resource); inputStream = Resources.getResourceAsStream(resource); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, resource, this.configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(url); inputStream = Resources.getUrlAsStream(url); mapperParser = new XMLMapperBuilder(inputStream, this.configuration, url, this.configuration.getSqlFragments()); mapperParser.parse(); &#125; else &#123; if (resource != null || url != null || mapperClass == null) &#123; throw new BuilderException(&quot;A mapper element may only specify a url, resource or class, but not more than one.&quot;); &#125; Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass); this.configuration.addMapper(mapperInterface); &#125; &#125; &#125; return; &#125; &#125; &#125;``` ## Mybatis有几种执行器mybatis有3中执行器； ```textpackage org.apache.ibatis.session;public enum ExecutorType &#123; SIMPLE, // 默认 REUSE, BATCH; private ExecutorType() &#123; &#125;&#125;","tags":[],"categories":[]},{"title":"mybatis-工作原理","date":"2021-07-28T08:52:38.000Z","path":"wiki/mybatis-工作原理/","text":"Mybatis工作原理Mybatis整体框架 工作原理解析 1）读取MyBatis配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。 读取配置文件将mybatis-config.xml转换为org.apache.ibatis.session.Configuration类，这里mybatis包含9个全局配置； 2）加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。 扫描Mapping目录下的***Mapper.xml文件； 3）构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。 1234567@Bean(name = &quot;sqlSessionFactory&quot;)public SqlSessionFactory sqlSessionFactory(HikariDataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:com/****/mapping/**/*.xml&quot;)); return bean.getObject();&#125; 生成工厂实例： 123456789101112131415161718192021222324252627public class SqlSessionFactoryBean implements FactoryBean&lt;SqlSessionFactory&gt;, InitializingBean, ApplicationListener&lt;ApplicationEvent&gt; &#123; private static final Log LOGGER = LogFactory.getLog(SqlSessionFactoryBean.class); private Resource configLocation; private Configuration configuration; private Resource[] mapperLocations; private DataSource dataSource; private TransactionFactory transactionFactory; private Properties configurationProperties; private SqlSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); private SqlSessionFactory sqlSessionFactory; private String environment = SqlSessionFactoryBean.class.getSimpleName(); private boolean failFast; private Interceptor[] plugins; private TypeHandler&lt;?&gt;[] typeHandlers; private String typeHandlersPackage; private Class&lt;?&gt;[] typeAliases; private String typeAliasesPackage; private Class&lt;?&gt; typeAliasesSuperType; private DatabaseIdProvider databaseIdProvider; private Class&lt;? extends VFS&gt; vfs; private Cache cache; private ObjectFactory objectFactory; private ObjectWrapperFactory objectWrapperFactory; public SqlSessionFactoryBean() &#123; &#125;&#125; 4）创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。 SqlSession对象完成和数据库的交互： 5）Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。 3种执行期类型（Simple Pre Batch） Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。 6）MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。 借助MappedStatement中的结果映射关系，将返回结果转化成HashMap、JavaBean等存储结构并返回。 7）输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。8）输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。 参考资料【1】MyBatis的工作原理 C语言网","tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://example.com/tags/mybatis/"}],"categories":[{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"mybatis","slug":"Spring-Family/mybatis","permalink":"http://example.com/categories/Spring-Family/mybatis/"}]},{"title":"Redis-缓存穿透、击穿和雪崩","date":"2021-07-28T08:51:22.000Z","path":"wiki/Redis-缓存穿透、击穿和雪崩/","text":"缓存击穿，穿透和雪崩背景首先说一下为什么会写这片文章，因为这个对我来说是印象非常深刻的，那是还在实习的时候，当时接了一个任务（其实就是练手的），大致需求是写一个白名单，然后有一个功能对白名单开放。因为是新功能，需要在部分地区试点，如果没有问题才会放开到全国城市运行。就是这么一个小的功能，让当时的我，呵呵。我记得那是第一次使用Redis,根本不知道什么是缓存雪崩啊，缓存击穿啊，穿透呀这些，还有更可怕的缓存一致性问题（TODO 下期再说）。 当时团队十几位大佬review我的代码，哼。这就是为什么印象会深刻一些吧，关键那是我第一个review代码，还是第一次使用redis，整个review就像是十几个大佬在面试我。真是怀疑人生了。。我是废物，别笑。 12345678910// 大致伪代码public boolean isPermit(String cityCode)&#123; // 先查询缓存中是否存在 String tmpCity = getCache(cityCode); if(StringUtil.isNotBlank(tmpCity))&#123; reture Boolean.TRUE; &#125; // 缓存中没有在去查数据库 reture judgeForDb(cityCode);&#125; 我上面的例子其实就是典型的缓存穿透的问题。因为仅仅是开放了十几个城市来试点功能，所以大部分的查询都是缓存不命中的。 下面引出今天我们的三个关键词： 缓存穿透 要查询的数据，缓存中基本上没有，所以大概率情况下缓存是不命中的，而是去数据库中去查询数据，导致缓存相当于是一个摆设。如果Key不是热点访问还可以，如果是热点Key，而且并发量也会很大的情况下，绝大多数的请求都会打到数据库上，很容易造成数据库宕机。 解决方案1、布隆过滤器进行校验，bloom filter典型应用场景（用户名是否存在，黑名单机制，单词错误检测）2、缓存空值方法，这个网上也是说的比较多的，这种方式也是可以的 在此我说一下我的解决方案：1、校验一个城市是否是城市白名单的城市时，直接查询缓存，存储的数据结构是使用的hash；2、如果没有查到这个城市，则认为这个城市不在白名单中，因为我是先查的hash，然后在查询hash中的具体城市。如果hash的值都没有查到，那说明缓存失效了；3、针对缓存失效的情况，可以再查DB来更新缓存，这个时候有人要较真了，会不会出线缓存击穿的情况呀，这个看具体场景，因为我的业务是不需要的，所以这里就直接查库更新缓存了，如果需要的话，那可以上缓存击穿的解决方案，或者看看上面两种解决方案有没有合适的，哈哈哈；4、因为城市是在后台进行配置的，所以我是在增删改的操作时，保证了缓存数据的一致性的前提下，才选择相信缓存的。这是我当时的解决方案。 缓存击穿 某个热点Key在一段时间内失效了，此时有大量请求瞬时抵达，会严重增加数据库的压力。因为我们的缓存数据一般都是要设置过期时间的，当缓存失效时，会去查询数据库同时更新缓存数据。 解决方案1、可以加锁，来保证只有第一个请求进来时达到数据库上，然后更新缓存，第二个请求进来是就会命中缓存，当然如果是分布式服务，那就需要使用分布式锁了。2、合理设置缓存时间，可以将热点Key时间设置长一些，或者根据业务将失效时间设置在业务量比较小的波段，都是可以的。有的解决方案会不设置Key的过期时间，这个，看情况吧，不建议这样。 如果大量的key不设置过期时间，则长期占用内存也是不好的。 缓存雪崩 缓存雪崩顾名思义，就是大量Key在一段时间或者瞬时失效，或者Redis服务重启（所有Key失效，因为是存在内存中），从而导致大量请求打到数据库上，增加数据库压力 解决方案1、将热点key设置不同波段的过期时间，把过期时间散列开。2、也可以使用分布式锁来限制高并发的请求，和缓存击穿的解决方案同理。3、对于Redis重启或宕机的问题，可以考虑集群部署，并保证数据的同步和一致性； i 生活远不止眼前的苟且","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-字符串底层原理","date":"2021-07-28T08:50:26.000Z","path":"wiki/Redis-字符串底层原理/","text":"Redis底层实现及原理 关键词 SDS embstr 二进制安全 空间预分配 String类型不同的编码方式 使用整数存储： 只对长度小于或等于 21 字节，并且可以被解释为整数的字符串进行编码 使用EMBSTR 编码： 尝试将 RAW 编码的字符串编码为 EMBSTR 编码， 使用SDS编码: 这个对象没办法进行编码，尝试从 SDS 中移除所有空余空间 下面举个例子看一下👇 embstr与动态字符串 embstr的创建只需分配一次内存，而raw为两次（一次为sds分配对象，另一次为redisObject分配对象，embstr省去了第一次）。 相对地，释放内存的次数也由两次变为一次。 embstr的redisObject和sds放在一起，更好地利用缓存带来的优势 但是redis并未提供任何修改embstr的方式，即embstr是只读的形式。对embstr的修改实际上是先转换为raw再进行修改。 SDS(simple dynamic string)SDS定义123456789struct sdshdr&#123; //记录buf数组中已使用字节的数量 //等于 SDS 保存字符串的长度 int len; //记录 buf 数组中未使用字节的数量 int free; //字节数组，用于保存字符串 char buf[];&#125; SDS有什么优点1、常数复杂度获取字符串长度sdshdr 中由于 len 属性的存在，获取 SDS 字符串的长度只需要读取 len 属性，时间复杂度为 O(1)，而对于 C 语言来说， 获取字符串的长度通常是遍历字符串计数来实现的，时间复杂度为 O(n)。 2、杜绝缓冲区溢出我们知道在 C 语言中使用 strcat 函数来进行两个字符串的拼接，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。而对于 SDS 数据类型，在进行字符修改的时候， 会首先根据记录的 len属性检查内存空间是否满足需求，如果不满足，会进行相应的空间扩展，然后在进行修改操作，所以不会出现缓冲区溢出。 3、减少修改字符串时带来的内存重分配次数C语言由于不记录字符串的长度，所以如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。而对于SDS，由于len属性和free属性的存在，对于修改字符串SDS实现了空间预分配和惰性空间释放两种策略： 3.1 字符串长度增加操作时，进行空间预分配 对字符串进行空间扩展的时候，扩展的内存比实际需要的多，这样可以减少连续执行字符串增长操作所需的内存重分配次数。 3.2 字符串长度减少操作时，惰性空间释放 对字符串进行缩短操作时，程序不立即使用内存重新分配来回收缩短后多余的字节，而是使用 free 属性将这些字节的数量记录下来，等待后续使用。（当然SDS也提供了相应的API，当我们有需要时，也可以手动释放这些未使用的空间。 4、二进制安全因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取； 而所有 SDS 的API 都是以处理二进制的方式来处理 buf 里面的元素，并且 SDS不是以空字符串来判断是否结束，而是以 len 属性表示的长度来判断字符串是否结束。 5、兼容部分C字符串函数虽然 SDS 是二进制安全的，但是一样遵从每个字符串都是以空字符串结尾的惯例，这样可以重用 C 语言库&lt;string.h&gt; 中的一部分函数。 为什么字符串长度大于44就是用raw方式编码这个是因为C语言函数库分配内存的长度只能是2/4/8/16/32/64；最大分配64位的长度；但是redisObj的长度加上字符串对象头的长度，占用20位，所以字符串长度最多是44位，超过这个长度，就是用raw方式进行编码； – 《Redis深度历险-String数据结构》 参考资料1、《闲扯Redis二》String数据类型之底层解析2、每个程序员都应该知道的Redis知识 - String底层原理3、Redis详解（四）—— redis的底层数据结构4、redis string底层数据结构","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-overview","date":"2021-07-28T08:49:42.000Z","path":"wiki/Redis-overview/","text":"Redis （Remote Dictionary Server）Redis简介Redis 本质上是一个 Key-Value 类型的内存数据库，很像 memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据 flush 到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value DB。Redis 的出色之处不仅仅是性能，Redis 最大的魅力是支持保存多种数据结构，此外单个 value 的最大限制是 1GB，不像 memcached 只能保存 1MB 的数据，因此 Redis 可以用来实现很多有用的功能。比方说用他的 List 来做 FIFO 双向链表，实现一个轻量级的高性 能消息队列服务，用他的 Set 可以做高性能的 tag 系统等等。另外 Redis 也可以对存入的 Key-Value 设置 expire 时间，因此也可以被当作一 个功能加强版的 memcached 来用。Redis 的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此 Redis 适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 如何设置密码及验证密码？设置密码：config set requirepass 123456授权密码：auth 123456 Redis 有哪几种数据淘汰策略？noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但 DEL 和几个例外）allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。allkeys-random: 回收随机的键使得新添加的数据有空间存放。volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 Redis 有哪些适合的场景？（1）会话缓存（Session Cache）最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会话比其他存储（如 Memcached）的优势在于：Redis 提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 （2）全页缓存（FPC）除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似 PHP 本地 FPC。再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）队列Redis 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本地程序语言（如 Python）对 list 的 push/pop操作。如果你快速的在 Google 中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用 Redis 作为 broker，你可以从这里去查看。 （4）排行榜/计数器Redis 在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis 只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可：当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行：ZRANGE user_scores 0 10 WITHSCORESAgora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数据的，你可以在这里看到。###（5）发布/订阅最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！ Redis 常见的性能问题和解决方案1、master 最好不要做持久化工作，如 RDB 内存快照和 AOF 日志文件2、如果数据比较重要，某个 slave 开启 AOF 备份，策略设置成每秒同步一次3、为了主从复制的速度和连接的稳定性，master 和 Slave 最好在一个局域网内4、尽量避免在压力大得主库上增加从库5、主从复制不要采用网状结构，尽量是线性结构，Master&lt;–Slave1&lt;—-Slave2 ….","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-哈希表实现","date":"2021-07-28T08:46:45.000Z","path":"wiki/Redis-哈希表实现/","text":"Redis之Hash表底层实现 关键词 字段dict | 渐进式哈希 | ziplist | 哈希表 字典底层结构 dict 字典结构体12345678typedf struct dict&#123; dictType *type;//类型特定函数，包括一些自定义函数，这些函数使得key和 //value能够存储 void *private;//私有数据 dictht ht[2];//两张hash表 int rehashidx;//rehash索引，字典没有进行rehash时，此值为-1 unsigned long iterators; //正在迭代的迭代器数量&#125;dict; type和private这两个属性是为了实现字典多态而设置的，当字典中存放着不同类型的值，对应的一些复制，比较函数也不一样，这两个属性配合起来可以实现多态的方法调用； ht[2]，两个hash表 rehashidx，这是一个辅助变量，用于记录rehash过程的进度，以及是否正在进行rehash等信息，当此值为-1时，表示该dict此时没有rehash过程 iterators，记录此时dict有几个迭代器正在进行遍历过程 dictht 哈希表结构体1234567typedf struct dictht&#123; dictEntry **table;//存储数据的数组 二维 unsigned long size;//数组的大小 unsigned long sizemask;//哈希表的大小的掩码，用于计算索引值，总是等于 //size-1 unsigned long used;//// 哈希表中中元素个数&#125;dictht; dictEntry 哈希数组结构12345678910typedf struct dictEntry&#123; void *key;//键 union&#123; void val; unit64_t u64; int64_t s64; double d; &#125;v;//值 struct dictEntry *next；//指向下一个节点的指针&#125;dictEntry; 注意这里还有一个指向下一个哈希表节点的指针，我们知道哈希表最大的问题是存在哈希冲突，如何解决哈希冲突，有开放地址法和链地址法。这里采用的便是链地址法，通过next这个指针可以将多个哈希值相同的键值对连接在一起，用来解决哈希冲突。 扩容与缩容当哈希表保存的键值对太多或者太少时，就要通过 rerehash(重新散列）来对哈希表进行相应的扩展或者收缩。具体步骤： 1、如果执行扩展操作，会基于原哈希表创建一个大小等于 ht[0].used*2n 的哈希表（也就是每次扩展都是根据原哈希表已使用的空间扩大一倍创建另一个哈希表）。相反如果执行的是收缩操作，每次收缩是根据已使用空间缩小一倍创建一个新的哈希表。2、重新利用上面的哈希算法，计算索引值，然后将键值对放到新的哈希表位置上。3、所有键值对都迁徙完毕后，释放原哈希表的内存空间。 触发扩容的条件： 1、服务器目前没有执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。 2、服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。ps：负载因子 = 哈希表已保存节点数量 / 哈希表大小。 为什么扩容的时候要考虑BIGSAVE的影响，而缩容时不需要？ BGSAVE时，dict要是进行扩容，则此时就需要为dictht[1]分配内存，若是dictht[0]的数据量很大时，就会占用更多系统内存，造成内存页过多分离，所以为了避免系统耗费更多的开销去回收内存，此时最好不要进行扩容； 缩容时，结合缩容的条件，此时负载因子&lt;0.1，说明此时dict中数据很少，就算为dictht[1]分配内存，也消耗不了多少资源； 渐进式哈希什么叫渐进式 rehash？也就是说扩容和收缩操作不是一次性、集中式完成的，而是分多次、渐进式完成的。如果保存在Redis中的键值对只有几个几十个，那么 rehash 操作可以瞬间完成，但是如果键值对有几百万，几千万甚至几亿，那么要一次性的进行 rehash，势必会造成Redis一段时间内不能进行别的操作。所以Redis采用渐进式 rehash,这样在进行渐进式rehash期间，字典的删除查找更新等操作可能会在两个哈希表上进行，第一个哈希表没有找到，就会去第二个哈希表上进行查找。但是进行增加操作，一定是在新的哈希表上进行的。 渐进式哈希其实就是慢慢的，一步一步的将hash表的数据迁移到另一个hash表中 redis会有一个定时任务去检测是否需要进行rehash rehash的过程中会在字典dict中维护一个rehashidx的标志 在rehash的过程中，两个hash表中都会有数据，此时如果有数据新增，将会存在ht[1]也就是第二个哈希表上； 在rehash的过程中，如果有删改查，则优先选择第一张表，如果第一张表没有查到数据，则查找第二章哈希表； 参考资料1、Redis详解（四）—— redis的底层数据结构2、Redis底层数据结构之hash3、Redis Hash数据结构的底层实现4、图解redis五种数据结构底层实现(动图哦)5、redis hash底层数据结构6、Redis底层数据结构之hash","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"Redis-list底层实现","date":"2021-07-28T08:45:33.000Z","path":"wiki/Redis-list底层实现/","text":"Redis List 底层实现 关键字 连锁更新问题 | quicklist | ziplist | linkedlist List底层数据结构在 3.0 版本的 Redis 中，List 类型有两种实现方式：数据结构底层采用压缩列表ziplist或linkedlist两种数据结构进行存储，首先以ziplist进行存储，在不满足ziplist的存储要求后转换为linkedlist列表。当列表对象同时满足以下两个条件时，列表对象使用ziplist进行存储，否则用linkedlist存储。 ziplist转换成linkedlist的条件1、触发一下任意一条即进行转换： 列表对象保存的所有字符串元素的长度小于64字节 列表对象保存的元素数量小于512个。 2、redis.conf配置文件 12list-max-ziplist-value 64 list-max-ziplist-entries 512 3、ziplist和linkedlist底层实现1、使用压缩列表（ziplist）实现的列表对象。2、使用双端链表（linkedlist）实现的列表对象。 在 3.2 版本后新增了 quicklist 数据结构实现了 list，现在就来分析下 quicklist 的结构。 quicklistziplist会引入频繁的内存申请和释放，而linkedlist由于指针也会造成内存的浪费，而且每个节点是单独存在的，会造成很多内存碎片，所以结合两个结构的特点，设计了quickList。quickList 是一个 ziplist 组成的双向链表。每个节点使用 ziplist 来保存数据。本质上来说， quicklist 里面保存着一个一个小的 ziplist。 quicklist表头结构12345678910111213141516171819typedef struct quicklist &#123; //指向头部(最左边)quicklist节点的指针 quicklistNode *head; //指向尾部(最右边)quicklist节点的指针 quicklistNode *tail; //ziplist中的entry节点计数器 unsigned long count; /* total count of all entries in all ziplists */ //quicklist的quicklistNode节点计数器 unsigned int len; /* number of quicklistNodes */ //保存ziplist的大小，配置文件设定，占16bits int fill : 16; /* fill factor for individual nodes */ //保存压缩程度值，配置文件设定，占16bits，0表示不压缩 unsigned int compress : 16; /* depth of end nodes not to compress;0=off */&#125; quicklist; head 和 tail 分别指向这个双端链表的表头和表尾, quicklist 存储的节点是一个叫做 quicklistNode 的结构, 如果这个 quicklist 是空的,那么 head 和 tail 会同时成为空指针, 如果这个双端链表的大小为 1, 那么 head 和 tail 会同时指向一个相同的节点 count 是一个计数器, 表示当前这个 list 结构一共存储了多少个元素, 它的类型是 unsigned long, 所以一个 list 能存储的最多的元素在 字长为 64 bit 的机器上是 (1 &lt;&lt; 64) - 1, 字长为 32 bit 的机器上是 (1 &lt;&lt; 32) - 1 len 表示了这个双端链表的长度(quicklistNodes 的数量) fill 表示了单个节点(quicklistNode)的负载比例(fill factor), 这是什么意思呢 Lists 结构使用了一种特殊的编码方式来节省空间, Lists 中每一个节点所能存储的东西可以通过最大长度或者一个最大存储的空间大小来限制,对于想限制每个节点最大存储空间的用户, 用 -5 到 -1 来表示这个限制值 5: 最大存储空间: 64 Kb &lt;– 通常情况下不要设置这个值 4: 最大存储空间: 32 Kb &lt;– 非常不推荐 3: 最大存储空间: 16 Kb &lt;– 不推荐 2: 最大存储空间: 8 Kb &lt;– 推荐 1: 最大存储空间: 4 Kb &lt;– 推荐 对于正整数则表示最多能存储到你设置的那个值, 当前的节点就装满了通常在 -2 (8 Kb size) 或 -1 (4 Kb size) 时, 性能表现最好但是如果你的使用场景非常独特的话, 调整到适合你的场景的值！！！！ redis.conf, 其中有一个可配置的参数叫做 list-max-ziplist-size, 默认值为 -2, 它控制了 quicklist 中的 fill 字段的值, 负数限制 quicklistNode 中的 ziplist 的字节长度, 正数限制 quicklistNode 中的 ziplist 的最大长度 compress 则表示 quicklist 中的节点 quicklistNode, 除开最两端的 compress 个节点之后, 中间的节点都会被压缩 Lists 在某些情况下是会被压缩的, 压缩深度是表示除开 list 两侧的这么多个节点不会被压缩, 剩下的节点都会被尝试进行压缩, 头尾两个节点一定不会被进行压缩,因为要保证 push/pop 操作的性能, 有以下的值可以设置:0: 关闭压缩功能 1: 深度 1 表示至少在 1 个节点以后才会开始尝试压缩, 方向为从头到尾或者从尾到头 12[head]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[tail][head], [tail] 永远都是不会被压缩的状态; 中间的节点则会被压缩 2 不会尝试压缩 head 或者 head-&gt;next 或者 tail-&gt;prev 或者 tail 但是会压缩这中间的所有节点 1[head]-&gt;[next]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[prev]-&gt;[tail] 3: 以此类推，最大为2的16次方。 quicklistNode 节点123456789101112131415161718192021222324252627282930typedef struct quicklistNode &#123; struct quicklistNode *prev; //前驱节点指针 struct quicklistNode *next; //后继节点指针 //不设置压缩数据参数recompress时指向一个ziplist结构 //设置压缩数据参数recompress指向quicklistLZF结构 unsigned char *zl; //压缩列表ziplist的总长度 unsigned int sz; /* ziplist size in bytes */ //ziplist中包的节点数，占16 bits长度 unsigned int count : 16; /* count of items in ziplist */ //表示是否采用了LZF压缩算法压缩quicklist节点，1表示压缩过，2表示没压缩，占2 bits长度 unsigned int encoding : 2; /* RAW==1 or LZF==2 */ //表示一个quicklistNode节点是否采用ziplist结构保存数据，2表示压缩了，1表示没压缩，默认是2，占2bits长度 unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ //标记quicklist节点的ziplist之前是否被解压缩过，占1bit长度 //如果recompress为1，则等待被再次压缩 unsigned int recompress : 1; /* was this node previous compressed? */ //测试时使用 unsigned int attempted_compress : 1; /* node can&#x27;t compress; too small */ //额外扩展位，占10bits长度 unsigned int extra : 10; /* more bits to steal for future usage */&#125; quicklistNode; prev 和 next 分别指向当前 quicklistNode 的前一个和后一个节点 zl 指向实际的 ziplist sz 存储了当前这个 ziplist 的占用空间的大小, 单位是字节 count 表示当前有多少个元素存储在这个节点的 ziplist 中, 它是一个 16 bit 大小的字段, 所以一个 quicklistNode 最多也只能存储 65536 个元素 encoding 表示当前节点中的 ziplist 的编码方式, 1(RAW) 表示默认的方式存储, 2(LZF) 表示用 LZF 算法压缩后进行的存储 container 表示 quicklistNode 当前使用哪种数据结构进行存储的, 目前支持的也是默认的值为 2(ZIPLIST), 未来也许会引入更多其他的结构 recompress 是一个 1 bit 大小的布尔值, 它表示当前的 quicklistNode 是不是已经被解压出来作临时使用 attempted_compress 只在测试的时候使用 extra 是剩下多出来的 bit, 可以留作未来使用 quicklistLZF 结构定义1234Copytypedef struct quicklistLZF &#123; unsigned int sz; //压缩后的ziplist大小 char compressed[];//柔性数组，存放压缩后的ziplist字节数组&#125; quicklistLZF; 当指定使用lzf压缩算法压缩ziplist的entry节点时，quicklistNode结构的zl成员指向quicklistLZF结构; 参考资料1、Redis列表list 底层原理2、Redis中string、list的底层数据结构原理3、《闲扯Redis五》List数据类型底层之quicklist4、Redis源码剖析和注释（七）— 快速列表(quicklist)5、redis 列表结构 底层实现(quicklist)","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"}]},{"title":"kafka安装与初体验","date":"2021-07-28T08:43:24.000Z","path":"wiki/kafka安装与初体验/","text":"Kafka的安装安装zookeeper1brew install zookeeper 默认端口：2181默认安装位置：/usr/local/Cellar/zookeeper配置文件位置：/usr/local/etc/zookeeper日志文件位置：/usr/local/var/log/zookeeper/zookeeper.log 启动zookeeper1nohup zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; 安装Kafka1brew install kafka 默认端口：9092默认安装位置：/usr/local/Cellar/kafka配置文件位置：/usr/local/etc/kafka日志文件位置：/usr/local/var/lib/kafka-logs 启动kafkanohup kafka-server-start /usr/local/etc/kafka/server.properties &amp; 订阅发布Demo创建一个Topic1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看创建的Topic1kafka-topics --list --zookeeper localhost:2181 生产者生产消息1kafka-console-producer --broker-list localhost:9092 --topic test 消费者消费消息1kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic test1 --from-beginning –from-beginning: 将从第一个消息开始接收 SpringBoot集成Kafka源码地址：https://gitee.com/IBLiplus/kafka-demo.git项目启动前按照上述安装启动步骤，在本地启动kafka.创建Maven项目，引入一下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.5.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 添加如下配置，端口号可以自己定配置文件： 123456789101112131415161718192021222324252627282930313233server.port=9010spring.kafka.bootstrap-servers= 127.0.0.1:9092# 发生错误后，消息重发的次数。spring.kafka.producer.retries= 0#当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。spring.kafka.producer.batch-size= 16384# 设置生产者内存缓冲区的大小。spring.kafka.producer.buffer-memory= 33554432# 键的序列化方式spring.kafka.producer.key-serializer= org.apache.kafka.common.serialization.StringSerializer# 值的序列化方式spring.kafka.producer.value-serializer = org.apache.kafka.common.serialization.StringSerializer# acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。# acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。# acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。spring.kafka.producer.acks= 1# 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5Dspring.kafka.consumer.auto-commit-interval= 1S# 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：# latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）# earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录spring.kafka.consumer.auto-offset-reset= earliest# 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量spring.kafka.consumer.enable-auto-commit= false# 键的反序列化方式spring.kafka.consumer.key-deserializer= org.apache.kafka.common.serialization.StringDeserializer# 值的反序列化方式spring.kafka.consumer.value-deserializer= org.apache.kafka.common.serialization.StringDeserializer# 在侦听器容器中运行的线程数。spring.kafka.listener.concurrency= 5#listner负责ack，每调用一次，就立即commitspring.kafka.listener.ack-mode= manual_immediatespring.kafka.listener.missing-topics-fatal= false 生产者生产消息： 123456789101112131415161718192021222324252627282930@Componentpublic class ProductDemo &#123; Logger log = LoggerFactory.getLogger(ProductDemo.class); @Resource private KafkaTemplate&lt;String, Object&gt; kafkaTemplate; //自定义topic public static final String TOPIC_TEST = &quot;topic.test&quot;; public static final String TOPIC_GROUP1 = &quot;topic.group1&quot;; public static final String TOPIC_GROUP2 = &quot;topic.group2&quot;; public void send(String obj) &#123; log.info(&quot;准备发送消息为：&#123;&#125;&quot;, obj); //发送消息 ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(TOPIC_TEST, obj); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() &#123; @Override public void onFailure(Throwable throwable) &#123; //发送失败的处理 log.info(TOPIC_TEST + &quot; - 生产者 发送消息失败：&quot; + throwable.getMessage()); &#125; @Override public void onSuccess(SendResult&lt;String, Object&gt; stringObjectSendResult) &#123; //成功的处理 log.info(TOPIC_TEST + &quot; - 生产者 发送消息成功：&quot; + stringObjectSendResult.toString()); &#125; &#125;); &#125;&#125; 消费消息 1234567891011121314151617181920212223242526@Componentpublic class ConsumerDemo &#123; Logger log = LoggerFactory.getLogger(ConsumerDemo.class); @KafkaListener(topics = ProductDemo.TOPIC_TEST, groupId = ProductDemo.TOPIC_GROUP1) public void topic_test(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) &#123; Optional message = Optional.ofNullable(record.value()); if (message.isPresent()) &#123; Object msg = message.get(); log.info(&quot;topic_test 消费了： Topic:&quot; + topic + &quot;,Message:&quot; + msg); ack.acknowledge(); &#125; &#125; @KafkaListener(topics = ProductDemo.TOPIC_TEST, groupId = ProductDemo.TOPIC_GROUP2) public void topic_test1(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) &#123; Optional message = Optional.ofNullable(record.value()); if (message.isPresent()) &#123; Object msg = message.get(); log.info(&quot;topic_test1 消费了： Topic:&quot; + topic + &quot;,Message:&quot; + msg); ack.acknowledge(); &#125; &#125;&#125; 测试接口： 123456789@Resourceprivate ProductDemo productDemo;@GetMapping(&quot;/kafka/test&quot;)public void testKafka()&#123; logger.info(&quot;start test&quot;); productDemo.send(&quot;hello kafka&quot;); logger.info(&quot;end test&quot;);&#125; i 山脚太拥挤 我们更高处见。","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"}]},{"title":"docker整体架构图","date":"2021-07-28T08:20:03.000Z","path":"wiki/docker整体架构图/","text":"Docker的总架构图 docker是一个C/S模式的架构，后端是一个松耦合架构，模块各司其职。 1、用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。2、Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；3、Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。4、Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；5、当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；6、当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。7、libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"}]},{"title":"docker常用手册","date":"2021-07-28T07:54:20.000Z","path":"wiki/docker常用手册/","text":"docker中文文档 http://www.dockerinfo.net/documentdocker doc https://docs.docker.com/engine/reference/commandline/docker/docker 中文社区 https://www.docker.org.cn/ 搜索可用镜像docker search tutorial 检查运行的镜像docker inspect efe 发布镜像docker push 下载镜像docker pull 在容器中安装新的程序apt-get updateapt-get install vim 保存对容器的修改docker commit pid","tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"}]},{"title":"docker-compost安装mongodb","date":"2021-07-28T07:24:01.000Z","path":"wiki/docker-compost安装mongodb/","text":"mongo 配置文件 -&gt; https://www.cnblogs.com/xibuhaohao/p/12580331.html docker-compose 配置文件123456789mongo: image: mongo:4.4.7 #根据需要选择自己的镜像 ports: - 27017:27017 #对外暴露停供服务的端口，正式生产的时候理论不用暴露。 volumes: - ./mongodb/data/db:/data/db # 挂载数据目录 - ./mongodb/data/log:/var/log/mongodb # 挂载日志目录 - ./mongodb/data/config:/etc/mongo # 挂载配置目录 # command: --config /docker/mongodb/mongod.conf # 配置文件 按照上面👆配置文件设置目录/data/db/mongodb/datals -lconfig db log mongo 配置文件1234567891011121314151617181920212223242526272829303132333435363738# Where and how to store data.storage: dbPath: /data/db/mongodb/data/db journal: enabled: true# engine:# mmapv1:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /data/db/mongodb/data/log# network interfacesnet: port: 27017 bindIp: 0.0.0.0# how the process runsprocessManagement: timeZoneInfo: /usr/share/zoneinfo#security:#operationProfiling:#replication:#sharding:## Enterprise-Only Options:#auditLog:#snmp: bindIp: 0.0.0.0 允许远程访问 docker-compose启动mongodocker-compose up -ddocker ps 进入dockerdocker psdocker exec -it xxxxxxxxxx bash mongo创建数据库登录mongo 查看数据库show dbs 创建数据库use wechat_spider 然后 db 查看 创建用户1234567db.createUser( &#123; user:&quot;wechat&quot;, pwd:&quot;123456&quot;, roles:[&#123;role:&quot;readWrite&quot;,db:&quot;wechat_spider&quot;&#125;] &#125; ) Java客户端链接配置mvn12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt;&lt;/dependency&gt; 配置文件12345678910111213spring: data: mongodb: username: &#x27;wechat&#x27; password: &#x27;123456&#x27;# port: 3333 port: 27017 database: wechat_spider# host: 123.56.77.177 host: 39.107.117.232 repositories: type: auto domain1234567891011121314151617181920212223242526272829import org.springframework.data.mongodb.core.mapping.Document;/** * @Author gaolei * @Date 2021/7/28 上午10:02 * @Version 1.0 */@Document(collection = &quot;passenger&quot;)public class Passenger &#123; private String name; private String password; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; controller12345678910111213141516171819@RestControllerpublic class TestContoller &#123; @Autowired private MongoTemplate mongoTemplate; @RequestMapping(&quot;/insert&quot;) public String insert() &#123; Passenger passenger = new Passenger(); passenger.setName(&quot;hello&quot;); passenger.setPassword(&quot;world1&quot;); passenger = mongoTemplate.insert(passenger); if (passenger != null) &#123; return &quot;success&quot;; &#125; else &#123; return &quot;false&quot;; &#125; &#125;&#125;","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://example.com/tags/mongodb/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MongoDB","slug":"DataBase/MongoDB","permalink":"http://example.com/categories/DataBase/MongoDB/"}]},{"title":"基于BIO实现简易tomcat","date":"2021-07-28T07:11:55.000Z","path":"wiki/基于BIO实现简易tomcat/","text":"基于传统的BIO手写一个简易Tomcat本文主要基于传统的BIO来实现一个简单的Http请求处理过程；1、Servlet请求无非就是doGet/doPost，所以我们定义抽象Servlet记忆GET/POST方法；2、基于Socket和ServerSocket实现CS通信；3、模拟Spring加载配置文件，注册请求以及控制器； GlRequest 封装一个请求 当然是一个很简单的请求，这里只处理请求的URL和请求方法；获取请求，也就是输入流，解析数据Url和Method，并做相应的处理； 12345678910111213141516171819202122232425262728293031public class GlRequest &#123; private String url; private String method; public GlRequest(InputStream is) &#123; try &#123; // 解析http请求的具体内容； String content = &quot;&quot;; byte[] buff = new byte[1024]; int len = 0; if ((len = is.read(buff)) &gt; 0) &#123; content = new String(buff, 0, len); &#125; String line = content.split(&quot;\\\\n&quot;)[0]; String [] arr = line.split(&quot;\\\\s&quot;); this.method = arr[0]; this.url = arr[1].split(&quot;\\\\?&quot;)[0]; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public String getUrl() &#123; return this.url; &#125; public String getMethod() &#123; return this.method; &#125;&#125; GlResponse 定义返回值response 处理请求返回值，将业务处理的结果通过输出流输出；输出大致分为两部分，第一是返回的数据，第二是返回数据的Header; 1234567891011121314151617public class GlResponse &#123; private OutputStream outputStream; public GlResponse(OutputStream os) &#123; this.outputStream = os; &#125; public void write(String string) throws Exception &#123; StringBuffer sb = new StringBuffer(); sb.append(&quot;HTTP/1.1 200 OK\\n&quot;) .append(&quot;Content-Type: text/html;\\n&quot;) .append(&quot;\\r\\n&quot;) .append(string); outputStream.write(sb.toString().getBytes()); &#125;&#125; GlServlet 定义抽象servlet，定义GET方法和POST方法 定义抽象的Servlet和doGet方法和doPost方法，具体的业务去实现自己的方法和逻辑； 12345678910111213141516public abstract class GlServlet &#123; private final static String GET = &quot;GET&quot;; public void service(GlRequest request, GlResponse response) throws Exception &#123; if (GET.equals(request.getMethod())) &#123; doGet(request, response); &#125; else &#123; doPost(request, response); &#125; &#125; public abstract void doGet(GlRequest request, GlResponse response) throws Exception; public abstract void doPost(GlRequest request, GlResponse response) throws Exception; &#125; FirstServlet 具体的业务Servlet实现抽象Servlet的方法12345678910111213public class FirstServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; // 具体的逻辑 this.doPost(request, response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This is first servlet from BIO&quot;); &#125;&#125; SecondServlet 具体的业务Servlet实现抽象Servlet方法123456789101112public class SecondServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; doPost(request,response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This second request form BIO&quot;); &#125;&#125; web-bio.properties 配置文件 配置请求和处理器，Spring中是通过Controller下的@XXXMapping注解去扫描并加载到工厂的； 12345servlet.one.className=com.ibli.netty.tomcat.bio.servlet.FirstServletservlet.one.url=/firstServlet.doservlet.two.className=com.ibli.netty.tomcat.bio.servlet.SecondServletservlet.two.url=/secondServlet.do GlTomcat测试类 启动服务端，在网页中访问本地8080端口，输入配置文件中定义的url进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class GlTomcat &#123; private ServerSocket server; private final Integer PORT = 8080; private Properties webXml = new Properties(); private Map&lt;String, GlServlet&gt; servletMapping = new HashMap&lt;String, GlServlet&gt;(); /** * 模拟项目main方法，启动加载配置 * * @param args 启动参数 */ public static void main(String[] args) &#123; new GlTomcat().start(); &#125; /** * Tomcat的启动入口 */ private void start() &#123; //1、加载web配置文件，解析配置 init(); //2、启动服务器socket，等待用户请求 try &#123; server = new ServerSocket(this.PORT); System.err.println(&quot;Gl tomcat started in port &quot; + this.PORT); while (true) &#123; Socket client = server.accept(); // 3、获得请求信息，解析HTTP协议的内容 process(client); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 加载配置文件 */ private void init() &#123; try &#123; String WEB_INF = this.getClass().getResource(&quot;/&quot;).getPath(); FileInputStream fis = new FileInputStream(WEB_INF + &quot;web-bio.properties&quot;); webXml.load(fis); for (Object k : webXml.keySet()) &#123; String key = k.toString(); if (key.endsWith(&quot;.url&quot;)) &#123; //servlet.two.url String servletName = key.replaceAll(&quot;\\\\.url&quot;, &quot;&quot;); String url = webXml.getProperty(key); //servlet.two.className String className = webXml.getProperty(servletName + &quot;.className&quot;); //反射创建servlet实例 // load-on-startup &gt;=1 :web启动的时候初始化 0：用户请求的时候才启动 GlServlet obj = (GlServlet) Class.forName(className).newInstance(); // 将url和servlet建立映射关系 servletMapping.put(url, obj); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 解析客户端请求 * * @param client 客户端 */ private void process(Socket client) throws Exception &#123; InputStream is = null; OutputStream os = null; try &#123; //请求 is = client.getInputStream(); //封装返回值 os = client.getOutputStream(); GlRequest request = new GlRequest(is); GlResponse response = new GlResponse(os); String url = request.getUrl(); if (servletMapping.containsKey(url)) &#123; servletMapping.get(url).service(request, response); &#125; else &#123; response.write(&quot;404 Not found!&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (os != null) &#123; os.flush(); os.close(); &#125; if (is != null) &#123; is.close(); client.close(); &#125; &#125; &#125;&#125; 打印请求信息123456789101112Request content : GET /fitstServlet.do HTTP/1.1Host: localhost:8080Connection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 OPR/74.0.3911.160Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9Sec-Fetch-Site: noneSec-Fetch-Mode: navigateSec-Fetch-User: ?1Sec-Fetch-Dest: documentAccept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9 客户端发送请求及结果展示 请求： http://localhost:8080/firstServlet.do","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"netty实现简易RPC调用","date":"2021-07-27T12:01:08.000Z","path":"wiki/netty实现简易RPC调用/","text":"基于Netty手写一个RPC简易远程调用 抽象协议12345678910111213141516171819202122@Datapublic class InvokerProtocol implements Serializable &#123; // 基于二进制流调用协议 /** * 类名 */ private String className; /** * 方法名 */ private String methodName; /** * 形参 */ private Class&lt;?&gt;[] params; /** * 实参 */ private Object[] values;&#125; 注册中心RpcRegistry 基于Netty实现的RPC注册中心 1、 ServerBootstrap 启动8080端口，等待客户端链接；2、 RegisterHandler用来处理RPC接口的发现和注册； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class RpcRegistry &#123; private Integer post; public RpcRegistry(Integer post) &#123; this.post = post; &#125; private void start() &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap(); server.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; // 接受客户端请求的处理 ChannelPipeline pipeline = ch.pipeline(); //配置通用解码器 int fieldLength = 4; pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, fieldLength, 0, fieldLength)); pipeline.addLast(new LengthFieldPrepender(fieldLength)); //对象编码器 pipeline.addLast(&quot;encoder&quot;, new ObjectEncoder()); pipeline.addLast(&quot;decoder&quot;, new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(new RegisterHandler()); &#125; &#125;) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); try &#123; ChannelFuture future = server.bind(this.post).sync(); System.out.println(&quot;Rpc registry started in port &quot; + this.post); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; new RpcRegistry(8080).start(); &#125; &#125; RegisterHandler 执行RPC的发现和注册 1、扫描固定包下或者路径下的类;2、接口为key，具体实例作为value； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class RegisterHandler extends ChannelInboundHandlerAdapter &#123; /** * 注册中心容器 */ private static final ConcurrentHashMap&lt;String, Object&gt; REGISTRY_MAP = new ConcurrentHashMap&lt;String, Object&gt;(); private List&lt;String&gt; classNameList = new ArrayList&lt;String&gt;(); public RegisterHandler() &#123; // 1、扫描所有需要注册的类 scannerClass(&quot;com.ibli.netty.rpc.provider&quot;); // 执行注册 doRegistry(); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Object result; InvokerProtocol request = (InvokerProtocol) msg; if (REGISTRY_MAP.containsKey(request.getClassName())) &#123; Object provider = REGISTRY_MAP.get(request.getClassName()); Method method = provider.getClass().getMethod(request.getMethodName(), request.getParams()); result = method.invoke(provider, request.getValues()); ctx.write(result); ctx.flush(); ctx.close(); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; private void doRegistry() &#123; if (classNameList.isEmpty()) &#123; return; &#125; for (String className : classNameList) &#123; try &#123; Class&lt;?&gt; clazz = Class.forName(className); Class&lt;?&gt; i = clazz.getInterfaces()[0]; REGISTRY_MAP.put(i.getName(), clazz.newInstance()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void scannerClass(String packageName) &#123; URL url = this.getClass().getClassLoader().getResource(packageName.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File dir = new File(url.getFile()); for (File file : dir.listFiles()) &#123; if (file.isDirectory()) &#123; scannerClass(packageName + &quot;.&quot; + file.getName()); &#125; else &#123; classNameList.add(packageName + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;).trim()); &#125; &#125; &#125;&#125; API以及实现RPC接口 定义一个简单的服务接口 作为一个微服务对外暴露的API; 123456789public interface IRpcService &#123; int add(int a, int b); int mul(int a, int b); int sub(int a, int b); int div(int a, int b);&#125; RPC接口实现 provider实现具体的接口，提供具体的服务； 1234567891011121314151617public class RpcServiceImpl implements IRpcService &#123; public int add(int a, int b) &#123; return a + b; &#125; public int mul(int a, int b) &#123; return a * b; &#125; public int sub(int a, int b) &#123; return a - b; &#125; public int div(int a, int b) &#123; return a / b; &#125;&#125; RPC调用方调用RPC12345678public class RpcConsumer &#123; public static void main(String[] args) &#123; IRpcService rpc = RpcProxy.create(IRpcService.class); System.err.println(rpc.add(1,3)); System.err.println(rpc.mul(3,3)); System.err.println(rpc.sub(14,3)); &#125;&#125; RpcProxy 动态代理对象请求RPC 通过Netty Bootstrap访问8080端口； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class RpcProxy &#123; public static &lt;T&gt; T create(Class&lt;?&gt; clazz) &#123; MethodProxy proxy = new MethodProxy(clazz); Class&lt;?&gt;[] interfaces = clazz.isInterface() ? new Class[]&#123;clazz&#125; : clazz.getInterfaces(); T result = (T) Proxy.newProxyInstance(clazz.getClassLoader(), interfaces, proxy); return result; &#125; public static class MethodProxy implements InvocationHandler &#123; private Class&lt;?&gt; clazz; public MethodProxy(Class&lt;?&gt; clazz) &#123; this.clazz = clazz; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); &#125; else &#123; return rpcInvoke(proxy, method, args); &#125; &#125; private Object rpcInvoke(Object proxy, Method method, Object[] args) &#123; //封装请求的内容 InvokerProtocol msg = new InvokerProtocol(); msg.setClassName(this.clazz.getName()); msg.setMethodName(method.getName()); msg.setParams(method.getParameterTypes()); msg.setValues(args); final RpcProxyHandler consumerHandler = new RpcProxyHandler(); EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap client = new Bootstrap(); client.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; //接收课客户端请求的处理流程 ChannelPipeline pipeline = ch.pipeline(); int fieldLength = 4; //通用解码器设置 pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, fieldLength, 0, fieldLength)); //通用编码器 pipeline.addLast(new LengthFieldPrepender(fieldLength)); //对象编码器 pipeline.addLast(&quot;encoder&quot;, new ObjectEncoder()); //对象解码器 pipeline.addLast(&quot;decoder&quot;, new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(&quot;handler&quot;, consumerHandler); &#125; &#125;) .option(ChannelOption.TCP_NODELAY, true); ChannelFuture future = client.connect(&quot;localhost&quot;, 8080).sync(); future.channel().writeAndFlush(msg).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; group.shutdownGracefully(); &#125; return consumerHandler.getResponse(); &#125; &#125;&#125; RPC调用方接受并处理调用结果123456789101112131415161718public class RpcProxyHandler extends ChannelInboundHandlerAdapter &#123; private Object response; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; this.response = msg; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); &#125; public Object getResponse() &#123; return this.response; &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"netty实现简易tomcat","date":"2021-07-27T11:54:15.000Z","path":"wiki/netty实现简易tomcat/","text":"基于Netty手写一个简易的Tomcat容器本文主要基于传统的BIO来实现一个简单的Http请求处理过程；1、Servlet请求无非就是doGet/doPost，所以我们定义抽象Servlet记忆GET/POST方法；2、基于Netty API实现CS通信；3、模拟Spring加载配置文件，注册请求以及控制器； Netty版本12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.6.Final&lt;/version&gt;&lt;/dependency&gt; GlRequest 基于Netty&amp;HttpRequest的API操作，非常简单1234567891011121314151617181920212223242526272829303132public class GlRequest &#123; private ChannelHandlerContext ctx; private HttpRequest req; public GlRequest(ChannelHandlerContext ctx, HttpRequest req) &#123; this.ctx = ctx; this.req = req; &#125; public String getUrl() &#123; return this.req.uri(); &#125; public String getMethod() &#123; return this.req.method().name(); &#125; public Map&lt;String, List&lt;String&gt;&gt; getParams() &#123; QueryStringDecoder decoder = new QueryStringDecoder(req.uri()); return decoder.parameters(); &#125; public String getParam(String name) &#123; Map&lt;String, List&lt;String&gt;&gt; params = getParams(); List&lt;String&gt; strings = params.get(name); if (strings == null) &#123; return null; &#125; return strings.get(0); &#125;&#125; GlResponse 基于Netty&amp;FullHttpResponse的API操作 FullHttpResponse作为返回请求的主体； 123456789101112131415161718192021222324252627282930313233public class GlResponse &#123; private ChannelHandlerContext ctx; private HttpRequest req; public GlResponse(ChannelHandlerContext ctx, HttpRequest req) &#123; this.req = req; this.ctx = ctx; &#125; public void write(String string) throws Exception &#123; if (string == null || string.length() == 0) &#123; return; &#125; try &#123; FullHttpResponse response = new DefaultFullHttpResponse( HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(string.getBytes(&quot;UTF-8&quot;)) ); response.headers().set(&quot;Content-Type&quot;, &quot;text/html&quot;); ctx.write(response); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; ctx.flush(); ctx.close(); &#125; &#125;&#125; GlServlet 定义抽象servlet，定义GET方法和POST方法 定义抽象的Servlet和doGet方法和doPost方法，具体的业务去实现自己的方法和逻辑； 12345678910111213141516public abstract class GlServlet &#123; private final static String GET = &quot;GET&quot;; public void service(GlRequest request, GlResponse response) throws Exception &#123; if (GET.equals(request.getMethod())) &#123; doGet(request, response); &#125; else &#123; doPost(request, response); &#125; &#125; public abstract void doGet(GlRequest request, GlResponse response) throws Exception; public abstract void doPost(GlRequest request, GlResponse response) throws Exception; &#125; FirstServlet 具体的业务Servlet实现抽象Servlet的方法12345678910111213public class FirstServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; // 具体的逻辑 this.doPost(request, response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This is first servlet from NIO&quot;); &#125;&#125; SecondServlet 具体的业务Servlet实现抽象Servlet方法123456789101112public class SecondServlet extends GlServlet &#123; @Override public void doGet(GlRequest request, GlResponse response) throws Exception &#123; doPost(request,response); &#125; @Override public void doPost(GlRequest request, GlResponse response) throws Exception &#123; response.write(&quot;This second request form NIO&quot;); &#125;&#125; web-nio.properties 配置文件 配置请求和处理器，Spring中是通过Controller下的@XXXMapping注解去扫描并加载到工厂的； 12345servlet.one.className=com.ibli.netty.tomcat.nio.servlet.FirstServletservlet.one.url=/firstServlet.doservlet.two.className=com.ibli.netty.tomcat.nio.servlet.SecondServletservlet.two.url=/secondServlet.do GlTomcat 启动服务端，在网页中访问本地8080端口，输入配置文件中定义的url进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class GlTomcat &#123; private final Integer PORT = 8080; private Properties webXml = new Properties(); private Map&lt;String, GlServlet&gt; servletMapping = new HashMap&lt;String, GlServlet&gt;(); public static void main(String[] args) &#123; new GlTomcat().start(); &#125; /** * Tomcat的启动入口 */ private void start() &#123; //1、加载web配置文件，解析配置 init(); // Boss线程 EventLoopGroup bossGroup = new NioEventLoopGroup(); // Worker线程 EventLoopGroup workGroup = new NioEventLoopGroup(); //2、创建Netty服务端对象 ServerBootstrap server = new ServerBootstrap(); //3、 配置服务端参数 server.group(bossGroup, workGroup) // 配置主线程的处理逻辑 .channel(NioServerSocketChannel.class) // 子线程的回调逻辑 .childHandler(new ChannelInitializer() &#123; @Override protected void initChannel(Channel client) &#123; // 处理具体的回调逻辑 // 责任链模式 //返回-编码 client.pipeline().addLast(new HttpResponseEncoder()); //请求-解码 client.pipeline().addLast(new HttpRequestDecoder()); //用户自己的逻辑处理 client.pipeline().addLast(new GlTomcatHandler()); &#125; &#125;) // 配置主线程可分配的最大线程数 .option(ChannelOption.SO_BACKLOG, 128) //保持长链接 .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = null; try &#123; future = server.bind(this.PORT).sync(); System.err.println(&quot;Gl tomcat started in pory &quot; + this.PORT); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; /** * 加载配置文件 * 这其实使用了策略模式 */ private void init() &#123; try &#123; String WEB_INF = this.getClass().getResource(&quot;/&quot;).getPath(); FileInputStream fis = new FileInputStream(WEB_INF + &quot;web-nio.properties&quot;); webXml.load(fis); for (Object k : webXml.keySet()) &#123; String key = k.toString(); if (key.endsWith(&quot;.url&quot;)) &#123; //servlet.two.url String servletName = key.replaceAll(&quot;\\\\.url&quot;, &quot;&quot;); String url = webXml.getProperty(key); //servlet.two.className String className = webXml.getProperty(servletName + &quot;.className&quot;); //反射创建servlet实例 // load-on-startup &gt;=1 :web启动的时候初始化 0：用户请求的时候才启动 GlServlet obj = (GlServlet) Class.forName(className).newInstance(); // 将url和servlet建立映射关系 servletMapping.put(url, obj); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 处理用户请求 */ public class GlTomcatHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof HttpRequest) &#123; HttpRequest req = (HttpRequest) msg; GlRequest request = new GlRequest(ctx,req); GlResponse response = new GlResponse(ctx,req); String url = request.getUrl(); if (servletMapping.containsKey(url))&#123; servletMapping.get(url).service(request,response); &#125; else &#123; response.write(&quot;404 Not Fount&quot;); &#125; &#125; &#125; &#125;&#125; 测试结果 请求 : http://localhost:8080/secoundServlet.do 这的地址写错误 ⚠️ 请求 : http://localhost:8080/secondServlet.do","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"}]},{"title":"Java位运算","date":"2021-07-27T11:47:25.000Z","path":"wiki/Java位运算/","text":"在计算机中所有数据都是以二进制的形式储存的。位运算其实就是直接对在内存中的二进制数据进行操作，因此处理数据的速度非常快。在实际编程中，如果能巧妙运用位操作，完全可以达到四两拨千斤的效果，正因为位操作的这些优点，所以位操作在各大IT公司的笔试面试中一直是个热点问题。 位操作基础基本的位操作符有与、或、异或、取反、左移、右移这6种，它们的运算规则如下所示： 在这6种操作符，只有~取反是单目操作符，其它5种都是双目操作符。 位操作只能用于整形数据，对float和double类型进行位操作会被编译器报错。 位操作符的运算优先级比较低，因为尽量使用括号来确保运算顺序，否则很可能会得到莫明其妙的结果。比如要得到像1，3， 5，9这些2^i+1的数字。写成int a = 1 « i + 1;是不对的，程序会先执行i + 1，再执行左移操作。应该写成int a = (1 « i) + 1; 另外位操作还有一些复合操作符，如&amp;=、|=、 ^=、«=、»=。 12345678package com.king.bit;public class BitMain &#123; public static void main(String [] args) &#123; int a = -15, b = 15; System.out.println(a &gt;&gt; 2); // -4：-15 = 1111 0001(二进制)，右移二位，最高位由符号位填充将得到1111 1100即-4 System.out.println(b &gt;&gt; 2); // 3：15=0000 1111(二进制)，右移二位，最高位由符号位填充将得到0000 0011即3 &#125;&#125; 常用位操作小技巧下面对位操作的一些常见应用作个总结，有判断奇偶、交换两数、变换符号及求绝对值。这些小技巧应用易记，应当熟练掌握。 判断奇偶只要根据最未位是0还是1来决定，为0就是偶数，为1就是奇数。因此可以用if ((a &amp; 1) == 0)代替if (a % 2 == 0)来判断a是不是偶数。下面程序将输出0到100之间的所有偶数： 12345for (int i = 0; i &lt; 100; i ++) &#123; if ((i &amp; 1) == 0) &#123; // 偶数 System.out.println(i); &#125;&#125; 交换两数123456int c = 1, d = 2;c ^= d;d ^= c;c ^= d;System.out.println(&quot;c=&quot; + c);System.out.println(&quot;d=&quot; + d); 可以这样理解： 第一步 a=b 即a=(ab)；第二步 b=a 即b=b(ab)，由于运算满足交换律，b(ab)=bba。由于一个数和自己异或的结果为0并且任何数与0异或都会不变的，所以此时b被赋上了a的值；第三步 a=b 就是a=ab，由于前面二步可知a=(ab)，b=a，所以a=ab即a=(ab)a。故a会被赋上b的值； 变换符号变换符号就是正数变成负数，负数变成正数。如对于-11和11，可以通过下面的变换方法将-11变成11 11111 0101(二进制) –取反-&gt; 0000 1010(二进制) –加1-&gt; 0000 1011(二进制) 同样可以这样的将11变成-11 10000 1011(二进制) –取反-&gt; 0000 0100(二进制) –加1-&gt; 1111 0101(二进制) 因此变换符号只需要取反后加1即可。完整代码如下： 123int a = -15, b = 15;System.out.println(~a + 1);System.out.println(~b + 1); 求绝对值位操作也可以用来求绝对值，对于负数可以通过对其取反后加1来得到正数。对-6可以这样： 11111 1010(二进制) –取反-&gt;0000 0101(二进制) -加1-&gt; 0000 0110(二进制) 来得到6。 因此先移位来取符号位，int i = a » 31;要注意如果a为正数，i等于0，为负数，i等于-1。然后对i进行判断——如果i等于0，直接返回。否之，返回~a+1。完整代码如下： 12int i = a &gt;&gt; 31;System.out.println(i == 0 ? a : (~a + 1)); 现在再分析下。对于任何数，与0异或都会保持不变，与-1即0xFFFFFFFF异或就相当于取反。因此，a与i异或后再减i（因为i为0或-1，所以减i即是要么加0要么加1）也可以得到绝对值。所以可以对上面代码优化下： 12int j = a &gt;&gt; 31;System.out.println((a ^ j) - j); 注意这种方法没用任何判断表达式，而且有些笔面试题就要求这样做，因此建议读者记住该方法（_讲解过后应该是比较好记了）。 位操作与空间压缩筛素数法在这里不就详细介绍了，本文着重对筛素数法所使用的素数表进行优化来减小其空间占用。要压缩素数表的空间占用，可以使用位操作。下面是用筛素数法计算100以内的素数示例代码（注2）： 1234567891011121314151617// 打印100以内素数：// （1）对每个素数，它的倍数必定不是素数；// （2）有很多重复访问如flag[10]会在访问flag[2]和flag[5]时各访问一次；int max = 100;boolean[] flags = new boolean[max];int [] primes = new int[max / 3 + 1];int pi = 0;for (int m = 2; m &lt; max ; m ++) &#123; if (!flags[m]) &#123; primes[pi++] = m; for(int n = m; n &lt; max; n += m) &#123; flags[n] = true; &#125; &#125;&#125;System.out.println(Arrays.toString(primes)); 运行结果如下： 1[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0] 在上面程序是用bool数组来作标记的，bool型数据占1个字节（8位），因此用位操作来压缩下空间占用将会使空间的占用减少八分之七。 下面考虑下如何在数组中对指定位置置1，先考虑如何对一个整数在指定位置上置1。对于一个整数可以通过将1向左移位后与其相或来达到在指定位上置1的效果，代码如下所示： 1234// 在一个数指定位上置1int e = 0;e |= 1 &lt;&lt; 10;System.out.println(e); 同样，可以1向左移位后与原数相与来判断指定位上是0还是1（也可以将原数右移若干位再与1相与）。 12345//判断指定位上是0还是1if ((e &amp; (1 &lt;&lt; 10)) != 0) System.out.println(&quot;指定位上为1&quot;);else System.out.println(&quot;指定位上为0&quot;); 扩展到数组上，我们可以采用这种方法，因为数组在内存上也是连续分配的一段空间，完全可以“认为”是一个很长的整数。先写一份测试代码，看看如何在数组中使用位操作： 1234567891011int[] bits = new int[40];for (int m = 0; m &lt; 40; m += 3) &#123; bits[m / 32] |= (1 &lt;&lt; (m % 32));&#125;// 输出整个bitsfor (int m = 0; m &lt; 40; m++) &#123; if (((bits[m / 32] &gt;&gt; (m % 32)) &amp; 1) != 0) System.out.print(&#x27;1&#x27;); else System.out.print(&#x27;0&#x27;);&#125; 运行结果如下： 11001001001001001001001001001001001001001 可以看出该数组每3个就置成了1，证明我们上面对数组进行位操作的方法是正确的。因此可以将上面筛素数方法改成使用位操作压缩后的筛素数方法： 12345678910111213int[] flags2 = new int[max / 32 + 1];pi = 0;for (int m = 2; m &lt; max ; m ++) &#123; if ((((flags2[m / 32] &gt;&gt; (m % 32)) &amp; 1) == 0)) &#123; primes[pi++] = m; for(int n = m; n &lt; max; n += m) &#123; flags2[n / 32] |= (1 &lt;&lt; (n % 32)); &#125; &#125;&#125; System.out.println();System.out.println(Arrays.toString(primes)); 运行结果如下： 1[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0] 位操作工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.king.bit; /** * Java 位运算的常用方法封装 */public class BitUtils &#123; /** * 获取运算数指定位置的值 * 例如： 0000 1011 获取其第 0 位的值为 1, 第 2 位 的值为 0 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @return 指定位置的值(0 or 1) */ public static byte getBitValue(byte source, int pos) &#123; return (byte) ((source &gt;&gt; pos) &amp; 1); &#125; /** * 将运算数指定位置的值置为指定值 * 例: 0000 1011 需要更新为 0000 1111, 即第 2 位的值需要置为 1 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @param value * 只能取值为 0, 或 1, 所有大于0的值作为1处理, 所有小于0的值作为0处理 * * @return 运算后的结果数 */ public static byte setBitValue(byte source, int pos, byte value) &#123; byte mask = (byte) (1 &lt;&lt; pos); if (value &gt; 0) &#123; source |= mask; &#125; else &#123; source &amp;= (~mask); &#125; return source; &#125; /** * 将运算数指定位置取反值 * 例： 0000 1011 指定第 3 位取反, 结果为 0000 0011; 指定第2位取反, 结果为 0000 1111 * * @param source * * @param pos * 指定位置 (0&lt;=pos&lt;=7) * * @return 运算后的结果数 */ public static byte reverseBitValue(byte source, int pos) &#123; byte mask = (byte) (1 &lt;&lt; pos); return (byte) (source ^ mask); &#125; /** * 检查运算数的指定位置是否为1 * * @param source * 需要运算的数 * @param pos * 指定位置 (0&lt;=pos&lt;=7) * @return true 表示指定位置值为1, false 表示指定位置值为 0 */ public static boolean checkBitValue(byte source, int pos) &#123; source = (byte) (source &gt;&gt;&gt; pos); return (source &amp; 1) == 1; &#125; /** * 入口函数做测试 * * @param args */ public static void main(String[] args) &#123; // 取十进制 11 (二级制 0000 1011) 为例子 byte source = 11; // 取第2位值并输出, 结果应为 0000 1011 for (byte i = 7; i &gt;= 0; i--) &#123; System.out.printf(&quot;%d &quot;, getBitValue(source, i)); &#125; // 将第6位置为1并输出 , 结果为 75 (0100 1011) System.out.println(&quot;\\n&quot; + setBitValue(source, 6, (byte) 1)); // 将第6位取反并输出, 结果应为75(0100 1011) System.out.println(reverseBitValue(source, 6)); // 检查第6位是否为1，结果应为false System.out.println(checkBitValue(source, 6)); // 输出为1的位, 结果应为 0 1 3 for (byte i = 0; i &lt; 8; i++) &#123; if (checkBitValue(source, i)) &#123; System.out.printf(&quot;%d &quot;, i); &#125; &#125; &#125;&#125; BitSet类BitSet类：大小可动态改变, 取值为true或false的位集合。用于表示一组布尔标志。 此类实现了一个按需增长的位向量。位 set 的每个组件都有一个 boolean 值。用非负的整数将 BitSet 的位编入索引。可以对每个编入索引的位进行测试、设置或者清除。通过逻辑与、逻辑或和逻辑异或操作，可以使用一个 BitSet 修改另一个 BitSet 的内容。默认情况下，set 中所有位的初始值都是 false。 每个位 set 都有一个当前大小，也就是该位 set 当前所用空间的位数。注意，这个大小与位 set 的实现有关，所以它可能随实现的不同而更改。位 set 的长度与位 set 的逻辑长度有关，并且是与实现无关而定义的。 除非另行说明，否则将 null 参数传递给 BitSet 中的任何方法都将导致 NullPointerException。 在没有外部同步的情况下，多个线程操作一个 BitSet 是不安全的。 构造函数: BitSet() or BitSet(int nbits)，默认初始大小为64。 123456789101112131415161718192021222324252627282930public void set(int pos): 位置pos的字位设置为true。public void set(int bitIndex, boolean value): 将指定索引处的位设置为指定的值。public void clear(int pos): 位置pos的字位设置为false。public void clear(): 将此 BitSet 中的所有位设置为 false。public int cardinality(): 返回此 BitSet 中设置为 true 的位数。public boolean get(int pos): 返回位置是pos的字位值。public void and(BitSet other): other同该字位集进行与操作，结果作为该字位集的新值。public void or(BitSet other): other同该字位集进行或操作，结果作为该字位集的新值。public void xor(BitSet other): other同该字位集进行异或操作，结果作为该字位集的新值。public void andNot(BitSet set): 清除此 BitSet 中所有的位,set – 用来屏蔽此 BitSet 的 BitSetpublic int size(): 返回此 BitSet 表示位值时实际使用空间的位数。public int length(): 返回此 BitSet 的“逻辑大小”：BitSet 中最高设置位的索引加 1。public int hashCode(): 返回该集合Hash 码， 这个码同集合中的字位值有关。public boolean equals(Object other): 如果other中的字位同集合中的字位相同，返回true。public Object clone(): 克隆此 BitSet，生成一个与之相等的新 BitSet。public String toString(): 返回此位 set 的字符串表示形式。 例1：标明一个字符串中用了哪些字符 1234567891011121314151617181920212223242526package com.king.bit;import java.util.BitSet;public class WhichChars &#123; private BitSet used = new BitSet(); public WhichChars(String str) &#123; for (int i = 0; i &lt; str.length(); i++) used.set(str.charAt(i)); // set bit for char &#125; public String toString() &#123; String desc = &quot;[&quot;; int size = used.size(); for (int i = 0; i &lt; size; i++) &#123; if (used.get(i)) desc += (char) i; &#125; return desc + &quot;]&quot;; &#125; public static void main(String args[]) &#123; WhichChars w = new WhichChars(&quot;How do you do&quot;); System.out.println(w); &#125;&#125; 例2： 1234567891011121314151617181920package com.king.bit;import java.util.BitSet;public class MainTestThree &#123; /** * @param args */ public static void main(String[] args) &#123; BitSet bm = new BitSet(); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(0); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(1); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); System.out.println(bm.get(65)); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); bm.set(65); System.out.println(bm.isEmpty() + &quot;--&quot; + bm.size()); &#125;&#125; 例3： 12345678910111213141516171819202122package com.king.bit;import java.util.BitSet;public class MainTestFour &#123; /** * @param args */ public static void main(String[] args) &#123; BitSet bm1 = new BitSet(7); System.out.println(bm1.isEmpty() + &quot;--&quot; + bm1.size()); BitSet bm2 = new BitSet(63); System.out.println(bm2.isEmpty() + &quot;--&quot; + bm2.size()); BitSet bm3 = new BitSet(65); System.out.println(bm3.isEmpty() + &quot;--&quot; + bm3.size()); BitSet bm4 = new BitSet(111); System.out.println(bm4.isEmpty() + &quot;--&quot; + bm4.size()); &#125; &#125; 位操作技巧123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 1. 获得int型最大值System.out.println((1 &lt;&lt; 31) - 1);// 2147483647， 由于优先级关系，括号不可省略System.out.println(~(1 &lt;&lt; 31));// 2147483647 // 2. 获得int型最小值System.out.println(1 &lt;&lt; 31);System.out.println(1 &lt;&lt; -1); // 3. 获得long类型的最大值System.out.println(((long)1 &lt;&lt; 127) - 1); // 4. 乘以2运算System.out.println(10&lt;&lt;1); // 5. 除以2运算(负奇数的运算不可用)System.out.println(10&gt;&gt;1); // 6. 乘以2的m次方System.out.println(10&lt;&lt;2); // 7. 除以2的m次方System.out.println(16&gt;&gt;2); // 8. 判断一个数的奇偶性System.out.println((10 &amp; 1) == 1);System.out.println((9 &amp; 1) == 1); // 9. 不用临时变量交换两个数（面试常考）a ^= b;b ^= a;a ^= b; // 10. 取绝对值（某些机器上，效率比n&gt;0 ? n:-n 高）int n = -1;System.out.println((n ^ (n &gt;&gt; 31)) - (n &gt;&gt; 31));/* n&gt;&gt;31 取得n的符号，若n为正数，n&gt;&gt;31等于0，若n为负数，n&gt;&gt;31等于-1若n为正数 n^0-0数不变，若n为负数n^-1 需要计算n和-1的补码，异或后再取补码，结果n变号并且绝对值减1，再减去-1就是绝对值 */ // 11. 取两个数的最大值（某些机器上，效率比a&gt;b ? a:b高）System.out.println(b&amp;((a-b)&gt;&gt;31) | a&amp;(~(a-b)&gt;&gt;31)); // 12. 取两个数的最小值（某些机器上，效率比a&gt;b ? b:a高）System.out.println(a&amp;((a-b)&gt;&gt;31) | b&amp;(~(a-b)&gt;&gt;31)); // 13. 判断符号是否相同(true 表示 x和y有相同的符号， false表示x，y有相反的符号。)System.out.println((a ^ b) &gt; 0); // 14. 计算2的n次方 n &gt; 0System.out.println(2&lt;&lt;(n-1)); // 15. 判断一个数n是不是2的幂System.out.println((n &amp; (n - 1)) == 0);/*如果是2的幂，n一定是100... n-1就是1111....所以做与运算结果为0*/ // 16. 求两个整数的平均值System.out.println((a+b) &gt;&gt; 1); // 17. 从低位到高位,取n的第m位int m = 2;System.out.println((n &gt;&gt; (m-1)) &amp; 1); // 18. 从低位到高位.将n的第m位置为1System.out.println(n | (1&lt;&lt;(m-1)));/*将1左移m-1位找到第m位，得到000...1...000n在和这个数做或运算*/ // 19. 从低位到高位,将n的第m位置为0System.out.println(n &amp; ~(0&lt;&lt;(m-1)));/* 将1左移m-1位找到第m位，取反后变成111...0...1111n再和这个数做与运算*/","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java注解","date":"2021-07-27T11:45:12.000Z","path":"wiki/Java注解/","text":"Java基础之注解机制详解 注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它是框架学习和设计者必须掌握的基础。 注解基础注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它主要的作用有以下四方面：— 生成文档，通过代码里标识的元数据生成javadoc文档。— 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。— 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理， 例如使用反射注入实例。这么来说是比较抽象的，我们具体看下注解的常见分类： Java自带的标准注解， 包括@Override、@Deprecated和@SuppressWarnings，分别用于标明重写某个方法、标明某个类或方法过时、标明要忽略的警告，用这些注解标明后编译器就会进行检查。 元注解，元注解是用于定义注解的注解，包括@Retention、@Target、@Inherited、@Documented，@Retention用于标明注解被保留的阶段，@Target用于标明注解使用的范围，@Inherited用于标明注解可继承，@Documented用于标明是否生成javadoc文档。 自定义注解，可以根据自己的需求定义注解，并可用元注解对自定义注解进行注解。接下来我们通过这个分类角度来理解注解。 Java内置注解我们从最为常见的Java内置的注解开始说起，先看下下面的代码： 123456789101112131415161718192021222324252627282930313233class A&#123; public void test() &#123; &#125;&#125;class B extends A&#123; /** * 重载父类的test方法 */ @Override public void test() &#123; &#125; /** * 被弃用的方法 */ @Deprecated public void oldMethod() &#123; &#125; /** * 忽略告警 * * @return */ @SuppressWarnings(&quot;rawtypes&quot;) public List processList() &#123; List list = new ArrayList(); return list; &#125;&#125; Java 1.5开始自带的标准注解，包括@Override、@Deprecated和@SuppressWarnings： @Override：表示当前的方法定义将覆盖父类中的方法 @Deprecated：表示代码被弃用，如果使用了被@Deprecated注解的代码则编译器将发出警告 @SuppressWarnings：表示关闭编译器警告信息 我们再具体看下这几个内置注解，同时通过这几个内置注解中的元注解的定义来引出元注解。 内置注解 - @Override我们先来看一下这个注解类型的定义： 1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override &#123;&#125; 从它的定义我们可以看到，这个注解可以被用来修饰方法，并且它只在编译时有效，在编译后的class文件中便不再存在。这个注解的作用我们大家都不陌生，那就是告诉编译器被修饰的方法是重写的父类的中的相同签名的方法，编译器会对此做出检查，若发现父类中不存在这个方法或是存在的方法签名不同，则会报错。 内置注解 - @Deprecated这个注解的定义如下： 12345 @Documented@Retention(RetentionPolicy.RUNTIME)@Target(value=&#123;CONSTRUCTOR, FIELD, LOCAL_VARIABLE, METHOD, PACKAGE, PARAMETER, TYPE&#125;)public @interface Deprecated &#123;&#125; 从它的定义我们可以知道，它会被文档化，能够保留到运行时，能够修饰构造方法、属性、局部变量、方法、包、参数、类型。这个注解的作用是告诉编译器被修饰的程序元素已被“废弃”，不再建议用户使用。 内置注解 - @SuppressWarnings这个注解我们也比较常用到，先来看下它的定义： 12345@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.SOURCE)public @interface SuppressWarnings &#123;String[] value();&#125; 它能够修饰的程序元素包括类型、属性、方法、参数、构造器、局部变量，只能存活在源码时，取值为String[]。它的作用是告诉编译器忽略指定的警告信息，它可以取的值如下所示： // TODO 参考资料https://www.pdai.tech/md/java/basic/java-basic-x-annotation.html","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java泛型","date":"2021-07-27T11:39:32.000Z","path":"wiki/Java泛型/","text":"Java泛型1、泛型定义 使用泛型机制编写的程序代码要比那些杂乱地使用Object变量，然后在进行强制类型转换的代码具有更好的安全性和可读性。 –《Java核心技术》 泛型是在编译时期作用的； 泛型变量使用大写形式，在Java库中，一般使用变量E表示集合的元素类型，K和V表示表的关键字与值的类型。 2、通配符2.1 无边界通配符无边界通配符又成为非限定通配符 1234567891011121314public static void main(String[] args) &#123; List&lt;String&gt; list1 = new ArrayList&lt;&gt;(); list1.add(&quot;1&quot;); list1.add(&quot;2&quot;); list1.add(&quot;3&quot;); list1.add(&quot;4&quot;); loop(list1); &#125; public static void loop(List&lt;?&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; 2.2 上边界通配符上边界通配符和下边界通配符都属于限定通配符 12345678910111213141516public static void main(String[] args) &#123; //List中的类型必须是Number的子类，不然会报编译错误 List&lt;Integer&gt; list1 = new ArrayList&lt;&gt;(); list1.add(1); list1.add(2); list1.add(3); list1.add(4); loop(list1); &#125; // 传进来的list的类型必须是Number或Number的子类才可以 public static void loop(List&lt;? extends Number&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; ? extends Number如果限定的类型有多个，之间使用 &amp; 进行分割 2.3 下边界通配符1234567891011121314151617181920public static void main(String[] args) &#123; //List的泛型是Number 添加的元素只要是Number下的类型就可以 List&lt;Number&gt; list1 = new ArrayList&lt;&gt;(); list1.add(1); list1.add(2L); list1.add(new BigDecimal(22)); list1.add(4); loop(list1); &#125; /** * 通用类型必须是Number到Object之间的类型 * * @param list */ public static void loop(List&lt;? super Number&gt; list) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125; 3、泛型的使用 泛型必须先声明，再使用，不然会有编译错误；泛型的声明是用过一对&lt;&gt;来完成，约定使用一个大写的字母来表示;通配符不能用作返回值; 123456public &lt;T&gt; T testA(T t, Test1&lt;T&gt; test1) &#123; System.out.println(&quot;这是传入的T:&quot; + t); t = test1.t; System.out.println(&quot;这是赋值后的T:&quot; + t); return t;&#125; 要从泛型类取数据时，用extends； 要往泛型类写数据时，用super； 既要取又要写，就不用通配符（即extends与super都不用）。 3.1 泛型类12345public class Demo&lt;K, V&gt; &#123; public &lt;K&gt; K test(V v) &#123; return null; &#125;&#125; 3.2 泛型方法1234567891011121314151617181920212223242526272829303132333435public class DemoTest4&lt;K, V&gt; &#123; /** * &lt;T&gt; 代表泛型的声明 * * @param t 本方法声明的泛型类型 * @param &lt;T&gt; 本方法声明的泛型类型 * @return */ public &lt;T&gt; T test(T t) &#123; return null; &#125; /** * 普通的泛型方法 * * @param k 类中定义的泛型类型 * @param &lt;X&gt; 本方法中声明的泛型类型 * @return */ public &lt;X&gt; X aa(K k) &#123; return (X) null; &#125; /** * 静态方法中是无法使用类中声明的泛型类型的 * 可以使用在本方法中声明的泛型类型 * * @return */ public static &lt;X&gt; X bb() &#123; return null; &#125;&#125; 3.3 泛型接口首先看一下不使用泛型接口的Demo 12345678910111213141516171819202122先定义接口，声明两个方法public interface IGeneric &#123; Integer aa(Integer a); Integer bb(Integer b);&#125;//然后创建一个类来实现方法：public class IntegerDemo implements IGeneric&#123; @Override public Integer aa(Integer a) &#123; return null; &#125; @Override public Integer bb(Integer b) &#123; return null; &#125;&#125; 上面是没有使用泛型的接口设计，但是aa方法的操作类型相当于在接口中写死了，如果此时我们需要一个String类型的aa方法，那是不是还要在声明一个String类型的接口，然后再去实现呢，这样是不是显得代码很臃肿，代码重复；所以我们可以看一下使用泛型之后是怎么样的。 1234567891011121314151617181920212223242526272829303132定义泛型接口public interface IGenericInte&lt;T&gt; &#123; T aa(T a); T bb(T b);&#125;下面是根据不同类型的实现类泛型传如Integer类型public class IGenericInteger implements IGenericInte&lt;Integer&gt; &#123; @Override public Integer aa(Integer a) &#123; return null; &#125; @Override public Integer bb(Integer b) &#123; return null; &#125;&#125;泛型传入String类型public class IGenericString implements IGenericInte&lt;String&gt; &#123; @Override public String aa(String a) &#123; return null; &#125; @Override public String bb(String b) &#123; return null; &#125;&#125; 4、泛型擦除在虚拟机上没有泛型类型对象，所有的对象都属于普通类。Java在处理泛型类型的时候，会处理成一个相应的原始类型。 擦除类型变量，并替换为限定类型，如果没有限定类型，默认使用Object替代。如果有限定类型，并且是多个，会使用第一个限定的类型来替换。 1234public interface IGenericInte&lt;T&gt; &#123; T aa(T a); T bb(T b);&#125; 像上面这个T是一个无限定的变量，泛型擦除之后会直接使用Object替换。当然调用泛型方法时，如果擦除返回类型，编译器插入强制类型转换 12Pair&lt;Employee&gt; buddies = ....Employee buddy = buddies.getFirst(); 擦除getFirst的返回类型后将返回Object类型。编译器自动插入Employee的强制类型转换，也就是说，编译器调用方法是其实是执行了一下两个虚拟机指令： 对原始方法Pair.getFirst()方法的调用 将返回的Object类型强制转换为Employee类型 1public static &lt;T extends Comparable&gt; T foo(T [] args) 在擦除类型之后变成： 1public static Comparable T foo(Comparable [] args) 参数类型T已经被擦除，只留下限定类型Comparable; 总之有关Java泛型转换的事实： 虚拟机没有泛型，只有普通的类和方法 所有的类型参数都用它们的限定类型替换 ==桥方法被合成来保证多态== 为了保持类型安全型，必要时插入强制类型转换 第一条应该很好理解，这也是为什么会有泛型擦除这个概念，是因为JVM不能操作泛型；第二条就是解释泛型如何进行类型的擦除；第三条是泛型方法可能与多态的理念矛盾，所以使用桥方法来过渡或兼容；第四条上面也有提到，会出现强制类型转换的情况； 5、泛型的约束与局限性当然泛型的设计在java中并没有那么完美，它确实可以解决代码结构重用等问题，但是也是有一些局限性，下面是我根据《Java核心技术》进行的总结： 5.1 不能使用基础数据类型实例化类型参数原因是类型擦除之后，如果使用Object原始类型，Object是无法存储基本数据类型的值。所以只能通过其包装类型声明； 5.2 运行时查询类型只适用与原始类型1234567public class DemoTest5&lt;T&gt; &#123; public static void main(String[] args) &#123; DemoTest5&lt;String&gt; demoTest5 = new DemoTest5&lt;&gt;(); DemoTest5&lt;Integer&gt; demoTest4 = new DemoTest5&lt;&gt;(); System.err.println(demoTest4.getClass().equals(demoTest5.getClass())); &#125;&#125; demoTest4.getClass().equals(demoTest5.getClass())其实比较的是DemoTest5这个类类型，我们输出一下demoTest4.getClass()的结果看一下： 1class com.ibli.javaBase.generics.DemoTest5 所以这里有一道非常经典的面试题，如何判断一个泛型他的具体类型是什么，这里我们可以使用反射去拿到泛型的具体类型； 5.3 不能创造参数化类型的数组对于参数化类型的数组，在类型擦除之后，会变成Object[]类型，如果此时试图存储一个String类型的元素，就会抛出一个Array-StoreException异常；主要目的还是处于到数组安全的保护，可以参考几篇文章: 1、如果Java不支持参数化类型数组，那么Arrays.asList()如何处理它们？2、java不能创建参数化类型的泛型数组3、java.lang.ArrayStoreException 5.4 Varargs警告向参数个数可变的方法传递一个泛型类型的实例的场景，编译器会发出警告！抑制这种警告的方式有两种： 在调用方法上增加注解@SuppressWarnings(“unchecked”) 还可以使用@SafeVarargs注解直接标注方法 参考 java不能创建参数化类型的泛型数组 5.5 不能实例化类型变量不能使用new T(..) 或则new T[…]和T.class这样的表达式的类型变量；因为类型擦除后，T变成Object，显然我们在这里并不是想要创建一个Object实例。解决办法是在调用者提供一个构造器表达式，下面是用Supplier函数实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Pair&lt;T&gt; &#123; private T first; private T second; public T getFirst() &#123; return first; &#125; public void setFirst(T first) &#123; this.first = first; &#125; public T getSecond() &#123; return second; &#125; public void setSecond(T second) &#123; this.second = second; &#125; public Pair(T first, T second) &#123; this.first = first; this.second = second; &#125; public static &lt;T&gt; Pair&lt;T&gt; build(Supplier&lt;T&gt; constr) &#123; return new Pair&lt;&gt;(constr.get(), constr.get()); &#125; /** * Cannot infer type arguments for Pair2&lt;&gt; * 当函数头返回值为Pair时,无法推断,改为Pair2后可以推断. * @param c1 * @return */ public static &lt;T&gt; Pair&lt;T&gt; build(Class&lt;T&gt; c1)&#123; try &#123; return new Pair&lt;&gt;(c1.newInstance(),c1.newInstance()); &#125; catch (InstantiationException | IllegalAccessException e) &#123; return null; &#125; &#125;&#125; Supplier是一个函数接口，返回一个无参数并且返回类型为T的函数： 12345678910@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; /** * Gets a result. * * @return a result */ T get();&#125; 12345678910111213141516171819202122232425262728293031323334public class TestMakePair &#123; public static void main(String[] args) &#123; /** * 1.接受Supplier&lt;T&gt;--它是一个函数式接口。表示无参数且返回类型为T的函数。 * 因为不能实例化类型变量，如： * public Pair() &#123;first = new T();second = new T();&#125; * 所以最好的方式是让调用者提供一个构造器表达式.形式如下: * @param constr * @return */ Pair&lt;String&gt; pair = Pair.build(String::new); System.out.println(pair.getFirst().length()); /** * public void buildT()&#123; 2.传统的方式是通过Class.newInstance方法来构造泛型对象. 但由于细节过于复杂,T.class是不合法的.它会被擦除为Object.class.如下: Illegal class literal for the type parameter T T.class.newInstance(); &#125; * 3. * T.class是不合法的,但若API涉及如下 * reason:因为String.class是Class&lt;String&gt;的一个实例. */ Pair&lt;String&gt; pair1 = Pair.build(String.class); System.out.println(pair1.getFirst().length()); &#125;&#125;执行结果：00 5.6 不能构造泛型数组就像不能实例化一个泛型实例一样，也不能实例化数组。数组本身也有类型，用来监控存储在JVM中的数组，这个类型会被擦除，例如： 1234public static &lt;T extends Comparable&gt; T[] foo(T[] a)&#123; T[] mm = new T[2]; ...&#125; 类型擦除，会让这个方法永远构造Comparabel[2]数组； 5.7 泛型类的静态上下文中类型变量无效这个应该是比较好理解的，上文也提到过了，泛型类型是作用在泛型类上的，一些静态的方法或这静态的属性不能够使用泛型类的变量类型，编译器会直接报错； 5.8 不能抛出或者捕获泛型类的实例Java既不能抛出也不能捕获泛型类对象，实际上，甚至泛型类扩展Throwable都是不合法的。 12345678public static &lt;T extends Throwable&gt; void doWork(Class&lt;T&gt; t)&#123; try&#123; ... &#125;catch (T ex)&#123; 此处无法捕获 catch必须捕获具体的异常 .... &#125;&#125; 在异常规范中使用类型变量是允许的，如下： 123456789public static &lt;T extends Throwable&gt; void doWork(Class&lt;T&gt; t) throws T &#123; try&#123; ... &#125;catch (Throwable ex)&#123; t.initCause(ex); throw t; &#125;&#125; 5.9 可以消除对受查异常的检查Java异常处理要求必须为所有的受查异常提供一个处理器，但是使用泛型，可以规避这一点； 1234@SuppressWarnings(&quot;unchecked&quot;)public static &lt;T extends Throwable&gt; void throwAs(Throwable e) throws T&#123; throw (T)e;&#125; 调用上面的方法，编译器会认为t是一个非受查异常; 5.10 注意擦除后的冲突比如一个泛型类的equals方法，擦除之后，和Object的equals冲突；解决办法是重新命名引发错误的方法； 6、泛型的继承关系如果Manage extends Employee,那么Pair&lt; Manage &gt;是Pair&lt; Employee &gt;的子类吗？ 不是的！但是泛型类可以扩展或实现其他的泛型类，很典型的一个例子ArrayList: 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;&#125; ArrayList[E]继承了AbstractList[E]; 对于Java泛型的一些思考 编译器如何推断出具体的类型？ 参考资料：深入理解 Java 泛型 ------------------- 他日若遂凌云志 敢笑黄巢不丈夫 -------------------","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java反射","date":"2021-07-27T11:39:08.000Z","path":"wiki/Java反射/","text":"Java反射 反向探知，在程序运行是动态的获取类的相关属性这种动态获取类的内容以及动态调用对象的方法和获取属性的机制，叫做java反射机制； 反射的优缺点 优点增加了程序的灵活性，避免的固有逻辑写死到程序中代码简介，提高程序的复用性 缺点相比于直接调用，反射有比较大的性能消耗内部暴露和安全隐患 （因为反射可以操作private成员变量和调用private成员方法） 反射的基本操作获取类对象的4种方式123456789101112// 调用forName方法得到一个对象，这也是最容易想到的方式Class&lt;?&gt; object = Class.forName(&quot;com.ibli.javaBase.reflection.User&quot;);// 通过实例对象调用getClass方法Teacher teacher = new Teacher();Class&lt;?&gt; objectT = teacher.getClass();// 通过类加载器的方式Class&lt;?&gt; loader = ClassLoader.getSystemClassLoader().loadClass(&quot;com.ibli.javaBase.reflection.User&quot;);//通过一个类.classClass&lt;?&gt; tt = Teacher.class; 基本信息操作 类修饰符 PUBLIC PRIVATE PROTECTED STATIC FINAL SYNCHRONIZED VOLATILE TRANSIENT NATIVE INTERFACE ABSTRACT modifiers 1 2 4 8 16 32 64 128 256 512 1024 12345678910111213141516// 类的修饰符 具体的值可以参考JDK API文档中的定义 返回值是int类型 public：1System.err.println(tt.getModifiers());// 包名System.err.println(tt.getPackage());// 类的名称System.err.println(tt.getName());// 父类System.err.println(tt.getSuperclass());// 类加载器System.err.println(tt.getClassLoader());// 简称System.err.println(tt.getSimpleName());// 类实现的所有的接口System.err.println(tt.getInterfaces().length);// 所有的注解类型System.err.println(tt.getAnnotations().length); 执行结果： 123456781package com.ibli.javaBase.reflectioncom.ibli.javaBase.reflection.Teacherclass java.lang.Objectsun.misc.Launcher$AppClassLoader@18b4aac2Teacher00 查看类的变量12345678910111213141516// User extend Person(aa,bb)Class&lt;User&gt; obj = User.class;User user = obj.newInstance();// 能够拿到类的所有的变量Field[] fields = obj.getDeclaredFields();for (Field field : fields)&#123; System.out.println(field.getModifiers() + &quot; &quot; + field.getName());&#125;System.out.println(&quot; &quot;);// 只能够拿到类的public的变量Field[] fields1 = obj.getFields();for (Field field : fields1)&#123; System.out.println(field.getModifiers() + &quot; &quot; + field.getName());&#125;System.out.println(&quot; &quot;); 执行结果： 123456782 age2 name1 sex10 height 1 sex1 aa1 bb 结论： getDeclaredFields（1）getDeclaredFields能够获取本类的所有成员变量，无论是public还是private;（2）但是不能获取父类的任何属性；（3）可以获取static类型的属性； getFields（1）只能够获取本类的public属性；（2）能够获取父类的public属性；（3）可以获取static类型的属性； 修改属性1234567891011// 设置Person中的变量aaField aaField = obj.getField(&quot;aa&quot;);aaField.setInt(user,111);System.err.println(user.getAa());// 设置User私有成员变量Field ageField = obj.getDeclaredField(&quot;age&quot;);// 设置访问权限ageField.setAccessible(true);ageField.set(user,333);System.err.println(user.getAge()); 执行结果： 12111333 查看方法1234567891011121314151617Class&lt;User&gt; obj = User.class;User user = obj.newInstance();// 可以获取父类的方法Method[] methods = obj.getMethods();for (Method method : methods) &#123; System.out.println(method.getModifiers() + &quot; &quot; + method.getName());&#125;System.err.println(&quot; ----- &quot;);// 获取本类中的所有方法Method[] methods1 = obj.getDeclaredMethods();for (Method method : methods1) &#123; System.out.println(method.getModifiers() + &quot; &quot; + method.getName());&#125;System.err.println(&quot; 。。。。。。 &quot;);// 执行结果就不展示了 结论： getDeclaredMethods（1）可以获取本类中的所有方法；（2）可以获取本类的静态方法 getMethods（1）可以获取本类中的所有==公有==方法；（2）可以获取父类中的所有==公有==方法；（3）可以获取本类和父类的公有静态方法； 调用方法123456789// 访问私有方法Method sleep = obj.getDeclaredMethod(&quot;sleep&quot;);sleep.setAccessible(true);sleep.invoke(user);// 如果是静态方法，invoke第一个参数传null即可Method say = obj.getDeclaredMethod(&quot;say&quot;,String.class);say.setAccessible(true);say.invoke(null,&quot;hello java&quot;); 执行结果： 12Im sleeping!say hello java 构造器的使用123456789101112Class&lt;User&gt; obj = User.class;// 查询共有的构造器Constructor&lt;?&gt;[] constructors = obj.getConstructors();for (Constructor&lt;?&gt; constructor : constructors)&#123; System.out.println(constructor.getModifiers() + &quot; &quot; + constructor.getName());&#125;// 可以获取私有的构造器Constructor&lt;?&gt;[] constructors1 = obj.getDeclaredConstructors();for (Constructor&lt;?&gt; constructor : constructors1)&#123; System.err.println(constructor.getModifiers() + &quot; &quot; + constructor.getName());&#125; 执行结果： 1234561 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User2 com.ibli.javaBase.reflection.User1 com.ibli.javaBase.reflection.User 结论： getConstructors（1）获得本类所有的公有构造器 getDeclaredConstructors（1）获得本类所有的构造器（public&amp;private） 实例化对象1234567// 使用newInstance创建对象 调用无参构造器User user = obj.newInstance();// 获取构造器来实例化对象Constructor&lt;User&gt; constructor = obj.getDeclaredConstructor(Integer.class, String.class);constructor.setAccessible(true);User temp = constructor.newInstance(22, &quot;java&quot;);System.err.println(temp.getAge() + &quot; &quot; + temp.getName()); 执行结果： 22 java 反射性能为什么差 可以从两方面考虑，第一个是反射生成Class对象时性能差，第二是通过反射调用对象方式是的性能差； （1） 调用forName 本地方法（2）每次newInstance 都会进行一次安全检查（3）在默认情况下，方法的反射调用为委派实现，委派给本地实现来进行方法调用。在调用超过 15次之后，委派实现便会将委派对象切换至动态实现。这个动态实现的字节码是自动生成的，它将直接使用 invoke 指令来调用目标方法。 方法的反射调用会带来不少性能开销，原因主要有三个： 变长参数方法导致的Object数组 基本类型的自动装箱、拆箱 (参考资料2) 还有最重要的方法内联。 参考资料(1)反射为什么慢(2)关于装箱拆箱为什么会影响效率(3)jvm之方法内联优化 反射使用的场景 JDBC封装 Spring IOC jdbcTemplate Mybatis使用大量反射 使用反射注意点 在获取Field,method,construtor的时候，应尽量避免是用getDelcaredXXX(),应该传进参数获取指定的字段，方法和构造器； 使用缓存机制缓存反射操作相关元数据的原因是因为反射操作相关元数据的实时获取是比较耗时的 --------------------- 前途浩浩荡荡 万事尽可期待。----------------------- 反射在IOC中的应用","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java并发编程之同步锁","date":"2021-07-26T14:27:05.000Z","path":"wiki/Java并发编程之同步锁/","text":"","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java多线程之ThreadLocal","date":"2021-07-26T08:28:01.000Z","path":"wiki/Java多线程之ThreadLocal/","text":"ThreadLocalMap结构 ThreadLocal set流程 参考资料 Java面试必问：ThreadLocal终极篇","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"基础面试题目","date":"2021-07-26T02:05:07.000Z","path":"wiki/基础面试题目/","text":"1.String不可变String 对象的不可变性 了解了 String 对象的实现后，你有没有发现在实现代码中 String 类被 final 关键字修饰了，而且变量 char 数组也被 final 修饰了。 我们知道类被 final 修饰代表该类不可继承，而 char[] 被 final+private 修饰，代表了 String 对象不可被更改。Java 实现的这个特性叫作 String 对象的不可变性，即 String 对象一旦创建成功，就不能再对它进行改变。 Java 这样做的好处在哪里呢？ 第一，保证 String 对象的安全性。假设 String 对象是可变的，那么 String 对象将可能被恶意修改。 第二，保证 hash 属性值不会频繁变更，确保了唯一性，使得类似 HashMap 容器才能实现相应的 key-value 缓存功能。 第三，可以实现字符串常量池。在 Java 中，通常有两种创建字符串对象的方式，一种是通过字符串常量的方式创建，如 String str=“abc”；另一种是字符串变量通过 new 形式的创建，如 String str = new String(“abc”)。 当代码中使用第一种方式创建字符串对象时，JVM 首先会检查该对象是否在字符串常量池中，如果在，就返回该对象引用，否则新的字符串将在常量池中被创建。这种方式可以减少同一个值的字符串对象的重复创建，节约内存。 2.String 和 StringBuilder、StringBuffer 的区别？https://www.cnblogs.com/weibanggang/p/9455926.html 3.描述一下 JVM 加载 class 文件的原理机制？https://www.cnblogs.com/williamjie/p/11167920.html 4.char 型变量中能不能存贮一个中文汉字，为什么？正确答案： char型变量是用来存储Unicode编码的字符的，unicode编码字符集中包含了汉字， 所以，char型变量中当然可以存储汉字啦。不过，如果某个特殊的汉字没有被包含在unicode编码字符集中， 那么，这个char型变量中就不能存储这个特殊汉字。 补充说明：unicode编码占用两个字节，所以，char类型的变量也是占用两个字节 5.抽象类（abstract class）和接口（interface）有什么异同？https://blog.csdn.net/aptentity/article/details/68942916 6.静态嵌套类(Static Nested Class)和内部类（Inner Class）的不同？https://blog.csdn.net/machinecat0898/article/details/80071242 7.抽象的（abstract）方法是否可同时是静态的（static）,（native）， synchronized 修饰？ 答：都不能。抽象方法需要子类重写，而静态的方法是无法被重写的，因此二者是矛盾的。本地方法是由本地代码（如C代码）实现的方法，而抽象方法是没有实现的，也是矛盾的。synchronized和方法的实现细节有关，抽象方法不涉及实现细节，因此也是相互矛盾的。 8.如何实现对象克隆https://www.cnblogs.com/fnlingnzb-learner/p/10649509.html 9.内部类可以引用它的包含类（外部类）的成员吗？有没有什么限制https://www.cnblogs.com/aademeng/articles/11084885.htmlhttps://www.cnblogs.com/dolphin0520/p/3811445.html 静态内部类：它是用static修饰的，在访问限制上它只能访问外部类中的static所修饰的成员变量或者是方法成员内部类：成员内部类是最普通的内部类，它可以无条件访问外部类的所有成员属性和成员方法（包括private成员和静态成员）。【注意】当成员内部类拥有和外部类同名的成员变量或者方法时，会发生隐藏现象，即默认情况下访问的是成员内部类的成员。如果要访问外部类的同名成员，需要以下面的形式进行访问：局部内部类：局部内部类是定义在外围类的方法中的，在访问的时候它可以直接访问外围类的所有成员！但是不能随便访问局部变量，除非这个局部变量被final修饰。匿名内部类： 10.Java 中的 final 关键字有哪些用法？https://www.cnblogs.com/dotgua/p/6357951.html多线程下的final语义 👇https://www.codercc.com/backend/basic/juc/concurrent-keywords/final.html#_1-final%E7%9A%84%E7%AE%80%E4%BB%8B 修饰变量基本类型的变量，值是不可以变化的引用类型的变量，引用是不可以变化的，但是可以修改引用的值方法参数： 保证这个变量在这个方法中的值不会发生变化 修饰方法它表示该方法不能被覆盖。这种使用方式主要是从设计的角度考虑，即明确告诉其他可能会继承该类的程序员，不希望他们去覆盖这个方法。这种方式我们很容易理解，然而，关于private和final关键字还有一点联系，这就是类中所有的private方法都隐式地指定为是final的，由于无法在类外使用private方法，所以也就无法覆盖它 修饰类用final修饰的类是无法被继承的 11.Thread 类的 sleep()方法和对象的 wait()方法都可以让线程暂停执行，它们有什么区别?sleep()方法是Thread类 sleep是Thread的静态native方法,可随时调用,会使当前线程休眠,并释放CPU资源,但不会释放对象锁; wait()方法是Object类 wait()方法是Object的native方法,只能在同步方法或同步代码块中使用,调用会进入休眠状态,并释放CPU资源与对象锁,需要我们调用notify/notifyAll方法唤醒指定或全部的休眠线程,再次竞争CPU资源. 注意:sleep(long millis)存在睡眠时间,不算特点因为wait()方法存在重载wait(long timeout),即设置了等待超时时间它们两个都需要再次抢夺CPU资源 12.线程的 sleep()方法和 yield()方法有什么区别？sleep()方法在给其他线程运行机会时不考虑线程的优先级。因此会给低优先级的线程运行的机会，而yield()方法只会给相同优先级或更高优先级的线程运行的机会。线程执行sleep()方法后会转入阻塞状态，所以执行sleep()方法的线程在指定的时间内肯定不会被执行，而yield()方法只是使当前线程重新回到就绪状态，所以执行yield()方法的线程有可能在进入到就绪状态后又立马被执行。 13.线程的基本状态以及状态之间的关系 https://blog.csdn.net/zhangdongnihao/article/details/104029972 https://juejin.cn/post/6885159254764814349 14.访问修饰符 public,private,protected,以及不写（默认）时的区别？ 15.请说出与线程同步以及线程调度相关的方法。（1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；（2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常；（3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关；（4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； 补充：Java 5 通过 Lock 接口提供了显式的锁机制（explicit lock），增强了灵活性以及对线程的协调。Lock 接口中定义了加锁（lock()）和解锁（unlock()）的方法，同时还提供了 newCondition()方法来产生用于线程之间通信的 Condition 对象；此外，Java 5 还提供了信号量机制（semaphore），信号量可以用来限制对某个共享资源进行访问的线程的数量。在对资源进行访问之前，线程必须得到信号量的许可（调用 Semaphore 对象的 acquire()方法）；在完成对资源的访问后，线程必须向信号量归还许可（调用 Semaphore 对象的 release()方法）。 16.synchronized 关键字的用法？12345678910111213141516171819202122public class SyncDemo &#123; final Object lock = new Object(); public synchronized void m1()&#123;&#125; public void m2()&#123; synchronized (SyncDemo.class)&#123; // TODO &#125; synchronized (lock)&#123; &#125; synchronized (this)&#123; &#125; &#125; public synchronized static void m3()&#123; &#125;&#125; 17.Java 中如何实现序列化，有什么意义？序列化就是一种用来处理对象流的机制，所谓对象流也就是将对象的内容进行流化。可以对流化后的对象进行读写操作，也可将流化后的对象传输于网络之间。序列化是为了解决对象流读写操作时可能引发的问题（如果不进行序列化可能会存在数据乱序的问题）。要实现序列化，需要让一个类实现 Serializable 接口，该接口是一个标识性接口，标注该类对象是可被序列化的，然后使用一个输出流来构造一个对象输出流并通过 writeObject(Object)方法就可以将实现对象写出（即保存其状态）；如果需要反序列化则可以用一个输入流建立对象输入流，然后通过 readObject 方法从流中读取对象。序列化除了能够实现对象的持久化之外，还能够用于对象的深度克隆 18.阐述 JDBC 操作数据库的步骤下面的代码以连接本机的 Oracle 数据库为例，演示 JDBC 操作数据库的步骤。（1） 加载驱动。Class.forName(&quot;oracle.jdbc.driver.OracleDriver&quot;);（2） 创建连接。Connection con = DriverManager.getConnection(&quot;jdbc:oracle:thin:@localhost:1521:orcl&quot;,&quot;scott&quot;, &quot;tiger&quot;);（3） 创建语句。 123PreparedStatement ps = con.prepareStatement(&quot;select * from emp where sal between ? and ?&quot;);ps.setint(1, 1000);ps.setint(2, 3000); （4）执行语句。ResultSet rs = ps.executeQuery();（5）处理结果。 1234while(rs.next()) &#123; System.out.println(rs.getint(&quot;empno&quot;) + &quot; - &quot; + rs.getString(&quot;ename&quot;));&#125; （6） 关闭资源。 12345678910finally &#123; if(con != null) &#123; try &#123; con.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 提示：关闭外部资源的顺序应该和打开的顺序相反，也就是说先关闭 ResultSet、再关闭 Statement、在关闭 Connection。上面的代码只关闭了 Connection（连接），虽然通常情况下在关闭连接时，连接上创建的语句和打开的游标也会关闭，但不能保证总是如此，因此应该按照刚才说的顺序分别关闭。此外，第一步加载驱动在 JDBC 4.0 中是可以省略的（自动从类路径中加载驱动），但是我们建议保留。 19.Statement 和 PreparedStatement 有什么区别？哪个性能更好？与 Statement 相比，①PreparedStatement 接口代表预编译的语句，它主要的优势在于可以减少 SQL 的编译错误并增加 SQL 的安全性（减少 SQL 注射攻击的可能性）；②PreparedStatement 中的 SQL 语句是可以带参数的，避免了用字符串连接拼接 SQL 语句的麻烦和不安全；③当批量处理 SQL 或频繁执行相同的查询时，PreparedStatement 有明显的性能上的优势，由于数据库可以将编译优化后的SQL语句缓存起来，下次执行相同结构的语句时就会很快（不用再次编译和生成执行计划）。 补充：为了提供对存储过程的调用，JDBC API 中还提供了 CallableStatement 接口。存储过程（Stored Procedure）是数据库中一组为了完成特定功能的 SQL 语句的集合，经编译后存储在数据库中，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。虽然调用存储过程会在网络开销、安全性、性能上获得很多好处，但是存在如果底层数据库发生迁移时就会有很多麻烦，因为每种数据库的存储过程在书写上存在不少的差别。 20.在进行数据库编程时，连接池有什么作用？由于创建连接和释放连接都有很大的开销（尤其是数据库服务器不在本地时，每次建立连接都需要进行 TCP 的三次握手，释放连接需要进行 TCP 四次握手，造成的开销是不可忽视的），为了提升系统访问数据库的性能，可以事先创建若干连接置于连接池中，需要时直接从连接池获取，使用结束时归还连接池而不必关闭连接，从而避免频繁创建和释放连接所造成的开销，这是典型的用空间换取时间的策略（浪费了空间存储连接，但节省了创建和释放连接的时间）。池化技术在Java 开发中是很常见的，在使用线程时创建线程池的道理与此相同。基于 Java 的开源数据库连接池主要有：C3P0、Proxool、DBCP、BoneCP、Druid 等。 补充：在计算机系统中时间和空间是不可调和的矛盾，理解这一点对设计满足性能要求的算法是至关重要的。大型网站性能优化的一个关键就是使用缓存，而缓存跟上面讲的连接池道理非常类似，也是使用空间换时间的策略。可以将热点数据置于缓存中，当用户查询这些数据时可以直接从缓存中得到，这无论如何也快过去数据库中查询。当然，缓存的置换策略等也会对系统性能产生重要影响，对于这个问题的讨论已经超出了这里要阐述的范围。 21.什么是 DAO 模式?DAO（Data Access Object）顾名思义是一个为数据库或其他持久化机制提供了抽象接口的对象，在不暴露底层持久化方案实现细节的前提下提供了各种数据访问操作。在实际的开发中，应该将所有对数据源的访问操作进行抽象化后封装在一个公共API中。用程序设计语言来说，就是建立一个接口，接口中定义了此应用程序中将会用到的所有事务方法。在这个应用程序中，当需要和数据源进行交互的时候则使用这个接口，并且编写一个单独的类来实现这个接口，在逻辑上该类对应一个特定的数据存储。DAO 模式实际上包含了两个模式，一是 DataAccessor（数据访问器），二是 Data Object（数据对象），前者要解决如何访问数据的问题，而后者要解决的是如何用对象封装数据。 22.Java 中是如何支持正则表达式操作的？Java 中的 String 类提供了支持正则表达式操作的方法，包括：matches()、replaceAll()、replaceFirst()、split()。此外，Java 中可以用 Pattern 类表示正则表达式对象，它提供了丰富的 API 进行各种正则表达式操作。面试题： - 如果要从字符串中截取第一个英文左括号之前的字符串，例如：北京市(朝阳区)(西城区)(海淀区)，截取结果为：北京市，那么正则表达式怎么写？ 123456789101112import java.util.regex.Matcher;import java.util.regex.Pattern;class RegExpTest &#123; public static void main(String[] args) &#123; String str = &quot;北京市(朝阳区)(西城区)(海淀区)&quot;; Pattern p = Pattern.compile(&quot;.*?(?=\\()&quot;); Matcher m = p.matcher(str); if(m.find()) &#123; System.out.println(m.group()); &#125; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"面向对象特征","date":"2021-07-26T01:59:21.000Z","path":"wiki/面向对象特征/","text":"封装封装是保证软件部件具有优良的模块性的基础，封装的目标就是实现软件内部的“高内聚、低耦合”。防止程序相互依赖而带来的变动影响。在面向对象的编程语言中，对象是封装的最基本单位，面向对象的封装比传统语言的封装更为清晰、也更为有力，面向对象的封装就是把描述一个对象的属性和行为的代码封装在一个“模块”中，或者说是一个类中，属性用变量定义，行为用方法定义，方法可以直接访问同一个对象中的属性。 将一个类中的成员变量全部定义为私有的，只有这个类自己的方法才可以访问到这些成员变量，这就基本上实现对象的封装，把握一个原则：把对同一事物进行操作的方法和相关的方法放在同一个类中，把方法和它操作的数据放在同一个类中。 抽象抽象就是找出一些事物的相似和共性之处，然后将这些事物归为一类，这个类只考虑这些事物的相似和共性之处，并且会忽略与当前主题和目标无关的那些方面，将注意力集中在与当前目标有关的方面。 继承在定义和实现一个类的时候，可以在一个已经存在的类的基础上来进行，把这个已经存在的类所定义的内容作为自己的内容，并可以加入若干新的内容，或修改原来的方法使之更适合特殊的需要，这就是继承。 继承是子类自动共享父类资源（数据或者方法）的机制，这是类之间的一种关系，提高了软件的可重用性和可扩展性。 多态多态是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在该编程时不确定，而是在程序运行期间才确定，即一个引用变量倒底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。因为在程序运行时才确定具体的类，这样，不用修改源程序代码，就可以让引用变量绑定到各种不同的类实现上，从而导致该引用调用的具体方法随之改变，即不修改程序代码就可以改变程序运行时所绑定的具体代码，让程序可以选择多个运行状态，这就是多态性。 多态性增强了软件的灵活性和扩展性，简单一句话理解多态的话就是，编译看左边，运行看右边 编译看左边 – 是指 想要成功的保存,就要使用左边也就是 只能使用父类提供的功能!!如果父类中没有，那么会编译报错运行看右边 – 是指 想要得到结果,就要看右边也就是 使用子类的方法体!!!","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"kafka-overview","date":"2021-07-22T03:00:12.000Z","path":"wiki/kafka-overview/","text":"参考资料 kafka详细教程 Kafka 集群管理 OrcHome kafka中文教程 面试官：说说Kafka处理请求的全流程 蘑菇街千亿级消息Kafka上云实践 kafka 集群搭建 kafka-2-11集群部署","tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}],"categories":[{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"}]},{"title":"Java线程池","date":"2021-07-21T14:01:23.000Z","path":"wiki/Java线程池/","text":"线程池简介虽然Java线程池理论，以及构造线程池的各种参数，以及 Executors 提供的默认实现之前研读过，不过线上还没有发生过线程池误用引发的事故，所以有必要把这些参数再仔细琢磨一遍。 优先补充一些线程池的工作理论，有助于展开下面的内容。线程池顾名思义，就是由很多线程构成的池子，来一个任务，就从池子中取一个线程，处理这个任务。这个理解是我在第一次接触到这个概念时候的理解，虽然整体基本切入到核心，但是实际上会比这个复杂。例如线程池肯定不会无限扩大的，否则资源会耗尽；当线程数到达一个阶段，提交的任务会被暂时存储在一个队列中，如果队列内容可以不断扩大，极端下也会耗尽资源，那选择什么类型的队列，当队列满如何处理任务，都有涉及很多内容。线程池总体的工作过程如下图： +++++++= 线程池内的线程数的大小相关的概念有两个，一个是核心池大小，还有最大池大小。如果当前的线程个数比核心池个数小，当任务到来，会优先创建一个新的线程并执行任务。当已经到达核心池大小，则把任务放入队列，为了资源不被耗尽，队列的最大容量可能也是有上限的，如果达到队列上限则考虑继续创建新线程执行任务，如果此刻线程的个数已经到达最大池上限，则考虑把任务丢弃。 在 java.util.concurrent 包中，提供了 ThreadPoolExecutor 的实现。 12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;&#125; 既然有了刚刚对线程池工作原理对概述，这些参数就很容易理解了： corePoolSize 核心池大小，既然如前原理部分所述。需要注意的是在初创建线程池时线程不会立即启动，直到有任务提交才开始启动线程并逐渐时线程数目达到corePoolSize。若想一开始就创建所有核心线程需调用prestartAllCoreThreads方法。 maximumPoolSize池中允许的最大线程数。需要注意的是当核心线程满且阻塞队列也满时才会判断当前线程数是否小于最大线程数，并决定是否创建新线程。 keepAliveTime当线程数大于核心时，多于的空闲线程最多存活时间 unitkeepAliveTime 参数的时间单位。 workQueue当线程数目超过核心线程数时用于保存任务的队列。主要有3种类型的BlockingQueue可供选择：无界队列，有界队列和同步移交。将在下文中详细阐述。从参数中可以看到，此队列仅保存实现Runnable接口的任务。 别看这个参数位置很靠后，但是真的很重要，因为楼主的坑就因这个参数而起，这些细节有必要仔细了解清楚。 threadFactory执行程序创建新线程时使用的工厂。 handler阻塞队列已满且线程数达到最大值时所采取的饱和策略。java默认提供了4种饱和策略的实现方式：中止、抛弃、抛弃最旧的、调用者运行。将在下文中详细阐述。 可选择的阻塞队列BlockingQueue详解在重复一下新任务进入时线程池的执行策略：如果运行的线程少于corePoolSize，则 Executor始终首选添加新的线程，而不进行排队。（如果当前运行的线程小于corePoolSize，则任务根本不会存入queue中，而是直接运行）如果运行的线程大于等于 corePoolSize，则 Executor始终首选将请求加入队列，而不添加新的线程。如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。主要有3种类型的BlockingQueue： 无界队列队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而楼主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 有界队列常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue与有界的LinkedBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 在我们的修复方案中，选择的就是这个类型的队列，虽然会有部分任务被丢失，但是我们线上是排序日志搜集任务，所以对部分对丢失是可以容忍的。 同步移交队列如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 可选择的饱和策略RejectedExecutionHandler详解JDK主要提供了4种饱和策略供选择。4种策略都做为静态内部类在ThreadPoolExcutor中进行实现。 AbortPolicy中止策略该策略是默认饱和策略。 12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125; 使用该策略时在饱和时会抛出RejectedExecutionException（继承自RuntimeException），调用者可捕获该异常自行处理。 DiscardPolicy抛弃策略12public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;&#125; 如上所示，什么都不做。 DiscardOldestPolicy抛弃旧任务策略123456public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125;&#125; 如代码，先将阻塞队列中的头元素出队抛弃，再尝试提交任务。如果此时阻塞队列使用PriorityBlockingQueue优先级队列，将会导致优先级最高的任务被抛弃，因此不建议将该种策略配合优先级队列使用。 CallerRunsPolicy调用者运行12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125;&#125; 既不抛弃任务也不抛出异常，直接运行任务的run方法，换言之将任务回退给调用者来直接运行。使用该策略时线程池饱和后将由调用线程池的主线程自己来执行任务，因此在执行任务的这段时间里主线程无法再提交新任务，从而使线程池中工作线程有时间将正在处理的任务处理完成。 Java提供的四种常用线程池解析既然楼主踩坑就是使用了 JDK 的默认实现，那么再来看看这些默认实现到底干了什么，封装了哪些参数。简而言之 Executors 工厂方法Executors.newCachedThreadPool() 提供了无界线程池，可以进行自动线程回收；Executors.newFixedThreadPool(int) 提供了固定大小线程池，内部使用无界队列；Executors.newSingleThreadExecutor() 提供了单个后台线程。 详细介绍一下上述四种线程池。 newCachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 在newCachedThreadPool中如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。初看该构造函数时我有这样的疑惑：核心线程池为0，那按照前面所讲的线程池策略新任务来临时无法进入核心线程池，只能进入 SynchronousQueue中进行等待，而SynchronousQueue的大小为1，那岂不是第一个任务到达时只能等待在队列中，直到第二个任务到达发现无法进入队列才能创建第一个线程？这个问题的答案在上面讲SynchronousQueue时其实已经给出了，要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。因此即便SynchronousQueue一开始为空且大小为1，第一个任务也无法放入其中，因为没有线程在等待从SynchronousQueue中取走元素。因此第一个任务到达时便会创建一个新线程执行该任务。 newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 看代码一目了然了，线程数量固定，使用无限大的队列。再次强调，楼主就是踩的这个无限大队列的坑。 4.3 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 123public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 在来看看ScheduledThreadPoolExecutor（）的构造函数 1234public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); &#125; ScheduledThreadPoolExecutor的父类即ThreadPoolExecutor，因此这里各参数含义和上面一样。值得关心的是DelayedWorkQueue这个阻塞对列，在上面没有介绍，它作为静态内部类就在ScheduledThreadPoolExecutor中进行了实现。简单的说，DelayedWorkQueue是一个无界队列，它能按一定的顺序对工作队列中的元素进行排列。 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 1234public static ScheduledExecutorService newSingleThreadScheduledExecutor() &#123; return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); &#125; 首先new了一个线程数目为 1 的ScheduledThreadPoolExecutor，再把该对象传入DelegatedScheduledExecutorService中，看看DelegatedScheduledExecutorService的实现代码： 1234DelegatedScheduledExecutorService(ScheduledExecutorService executor) &#123; super(executor); e = executor;&#125; 在看看它的父类 123DelegatedExecutorService(ExecutorService executor) &#123; e = executor; &#125; 其实就是使用装饰模式增强了ScheduledExecutorService（1）的功能，不仅确保只有一个线程顺序执行任务，也保证线程意外终止后会重新创建一个线程继续执行任务。 为什么禁止使用 Executors 创建线程池? 实验证明Executors缺陷12345678910111213141516171819202122public class ExecutorsDemo &#123; private static ExecutorService executor = Executors.newFixedThreadPool(15); public static void main(String[] args) &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; executor.execute(new SubThread()); &#125; &#125; &#125; class SubThread implements Runnable &#123; @Override public void run() &#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; //do nothing &#125; &#125; &#125; &#125; 通过指定JVM参数:-Xmx8m -Xms8m运行以上代码，会抛出OOM: 1234Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceededat java.util.concurrent.LinkedBlockingQueue.offer(LinkedBlockingQueue. java:416)at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor. java:1371)at com.hollis.ExecutorsDemo.main(ExecutorsDemo.java:16) 以上代码指出，ExecutorsDemo.java 的第 16 行，就是代码中的 execu- tor.execute(new SubThread());。 Executors 为什么存在缺陷通过上面的例子，我们知道了 Executors 创建的线程池存在 OOM 的风险，那 么到底是什么原因导致的呢?我们需要深入 Executors 的源码来分析一下。 其实，在上面的报错信息中，我们是可以看出蛛丝马迹的，在以上的代码中其实 已经说了，真正的导致 OOM 的其实是 LinkedBlockingQueue.offer 方法。 如果读者翻看代码的话，也可以发现，其实底层确实是通过 LinkedBlock- ingQueue 实现的: 123public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); 如果读者对 Java 中的阻塞队列有所了解的话，看到这里或许就能够明白原因了。 Java 中 的 BlockingQueue 主 要 有 两 种 实 现， 分 别 是 ArrayBlockingQ- ueue 和 LinkedBlockingQueue。 ArrayBlockingQueue 是一个用数组实现的有界阻塞队列，必须设置容量。 LinkedBlockingQueue 是一个用链表实现的有界阻塞队列，容量可以选择 进行设置，不设置的话，将是一个无边界的阻塞队列，最大长度为 Integer.MAX_ VALUE。 这里的问题就出在:不设置的话，将是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE。也就是说，如果我们不设置 LinkedBlockingQueue 的 容量的话，其默认容量将会是 Integer.MAX_VALUE。 而 newFixedThreadPool 中创建 LinkedBlockingQueue 时，并未指定容 量。此时，LinkedBlockingQueue 就是一个无边界队列，对于一个无边界队列 来说，是可以不断的向队列中加入任务的，这种情况下就有可能因为任务过多而导 致内存溢出问题。 上面提到的问题主要体现在newFixedThreadPool 和 newSingleThreadExecutor 两个工厂方法上，并不是说 newCachedThreadPool 和 newScheduledThreadPool 这两个方法就安全了，这两种方式创建的最大线程数可能是 Integer.MAX_VALUE，而创建这么多线程，必然就有可能导致 OOM。 创建线程池的正确姿势避免使用 Executors 创建线程池，主要是避免使用其中的默认实现，那么我们 可以自己直接调用 ThreadPoolExecutor 的构造函数来自己创建线程池。在创建的 同时，给 BlockQueue 指定容量就可以了。 12private static ExecutorService executor = new ThreadPoolExecutor(10, 10, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue(10)); 这种情况下，一旦提交的线程数超过当前可用线程数时，就会抛出 java.util. concurrent.RejectedExecutionException，这是因为当前线程池使用的队列 是有边界队列，队列已经满了便无法继续处理新的请求。但是异常(Exception)总比 发生错误(Error)要好。 除了自己定义 ThreadPoolExecutor 外。还有其他方法。这个时候第一时间 就应该想到开源类库，如 apache 和 guava 等。 作者推荐使用 guava 提供的 ThreadFactoryBuilder 来创建线程池。 12345678910public class ExecutorsDemo &#123; private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor. AbortPolicy()); public static void main(String[] args) &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; pool.execute(new SubThread()); &#125; &#125;&#125; 通过上述方式创建线程时，不仅可以避免 OOM 的问题，还可以自定义线程名 称，更加方便的出错的时候溯源。","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java-为什么禁止把SimpleDateFormat定义成static变量?","date":"2021-07-21T12:06:46.000Z","path":"wiki/Java-为什么禁止把SimpleDateFormat定义成static变量/","text":"本文参照 《Java技术灵魂15问》 简介在日常开发中，我们经常会用到时间，我们有很多办法在 Java 代码中获取时 间。但是不同的方法获取到的时间的格式都不尽相同，这时候就需要一种格式化工 具，把时间显示成我们需要的格式。 最常用的方法就是使用 SimpleDateFormat 类。这是一个看上去功能比较简单 的类，但是，一旦使用不当也有可能导致很大的问题。在 Java 开发手册中，有如下明确规定: 那么，本文就围绕 SimpleDateFormat 的用法、原理等来深入分析下如何以正 确的姿势使用它。 SimpleDateFormat 是 Java 提供的一个格式化和解析日期的工具类。它允许进 行格式化(日期 -&gt; 文本)、解析(文本 -&gt; 日期)和规范化。SimpleDateFormat 使 得可以选择任何用户定义的日期 - 时间格式的模式。 在 Java 中，可以使用 SimpleDateFormat 的 format 方法，将一个 Date 类型 转化成 String 类型，并且可以指定输出格式。 SimpleDateFormat 用法12345 // Date转StringDate data = new Date();SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);String dataStr = sdf.format(data);System.out.println(dataStr); 以上代码，转换的结果是:2018-11-25 13:00:00，日期和时间格式由”日期 和时间模式”字符串指定。如果你想要转换成其他格式，只要指定不同的时间模式就 行了。 在 Java 中，可以使用 SimpleDateFormat 的 parse 方法，将一个 String 类型 转化成 Date 类型。 12// String转Data System.out.println(sdf.parse(dataStr)); 日期和时间模式表达方法在使用 SimpleDateFormat 的时候，需要通过字母来描述时间元素，并组装成 想要的日期和时间模式。常用的时间元素和字母的对应表如下: 模式字母通常是重复的，其数量确定其精确表示。如下表是常用的输出格式的表 示方法。 输出不同时区的时间时区是地球上的区域使用同一个时间定义。以前，人们通过观察太阳的位置(时 角)决定时间，这就使得不同经度的地方的时间有所不同(地方时)。1863 年，首次 使用时区的概念。时区通过设立一个区域的标准时间部分地解决了这个问题。 世界各个国家位于地球不同位置上，因此不同国家，特别是东西跨度大的国家日 出、日落时间必定有所偏差。这些偏差就是所谓的时差。 现今全球共分为 24 个时区。由于实用上常常 1 个国家，或 1 个省份同时跨着 2 个或更多时区，为了照顾到行政上的方便，常将 1 个国家或 1 个省份划在一起。所以 时区并不严格按南北直线来划分，而是按自然条件来划分。例如，中国幅员宽广，差 不多跨 5 个时区，但为了使用方便简单，实际上在只用东八时区的标准时即北京时间 为准。 由于不同的时区的时间是不一样的，甚至同一个国家的不同城市时间都可能不一 样，所以，在 Java 中想要获取时间的时候，要重点关注一下时区问题。默认情况下，如果不指明，在创建日期的时候，会使用当前计算机所在的时区作为默认时区，这也是为什么我们通过只要使用new Date()就可以获取中国的当前 时间的原因。 那么，如何在 Java 代码中获取不同时区的时间呢? SimpleDateFormat 可以 实现这个功能。 123SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); sdf.setTimeZone(TimeZone.getTimeZone(&quot;America/Los_Angeles&quot;)); System.out.println(sdf.format(Calendar.getInstance().getTime())); 以上代码，转换的结果是:2018-11-24 21:00:00 。既中国的时间是 11 月 25 日的 13 点，而美国洛杉矶时间比中国北京时间慢了 16 个小时(这还和冬夏令时有关 系，就不详细展开了)。 如果你感兴趣，你还可以尝试打印一下美国纽约时间(America/New_York)。 纽约时间是 2018-11-25 00:00:00。纽约时间比中国北京时间早了 13 个小时。 当然，这不是显示其他时区的唯一方法，不过本文主要为了介绍 SimpleDate-Format，其他方法暂不介绍了。 SimpleDateFormat 线程安全性由于 SimpleDateFormat 比较常用，而且在一般情况下，一个应用中的时间显 示模式都是一样的，所以很多人愿意使用如下方式定义 SimpleDateFormat: 123456789public class Main &#123; private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static void main(String[] args) &#123; simpleDateFormat.setTimeZone(TimeZone.getTimeZone(&quot;America/New_York&quot;)); System.out.println(simpleDateFormat.format(Calendar.getInstance(). getTime())); &#125; &#125; ⚠️ 这种定义方式，存在很大的安全隐患。 我们来看一段代码，以下代码使用线程池来执行时间输出。 123456789101112131415161718192021222324252627282930313233343536373839public class Main &#123; /** * 定义一个全局的SimpleDateFormat */ private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); /** * 使用ThreadFactoryBuilder定义一个线程池 */ private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;demo-pool-%d&quot;).build(); private static ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); /** * 定义一个CountDownLatch，保证所有子线程执行完之后主线程再执行 */ private static CountDownLatch countDownLatch = new CountDownLatch(100); public static void main(String[] args) &#123; // 定义一个线程安全的 HashSet Set&lt;String&gt; dates = Collections.synchronizedSet(new HashSet&lt;String&gt;()); for (int i = 0; i &lt; 100; i++) &#123; // 获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // 时间增加 calendar.add(Calendar.DATE, finalI); // 通过 simpleDateFormat 把时间转换成字符串 String dateString = simpleDateFormat.format(calendar. getTime()); // 把字符串放入 Set 中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125;); &#125; // 阻塞，直到 countDown 数量为 0 countDownLatch.await(); // 输出去重后的时间个数 System.out.println(dates.size()); &#125; &#125; 以上代码，其实比较简单，很容易理解。就是循环一百次，每次循环的时候都在 当前时间基础上增加一个天数(这个天数随着循环次数而变化)，然后把所有日期放入 一个线程安全的、带有去重功能的 Set 中，然后输出 Set 中元素个数。 正常情况下，以上代码输出结果应该是 100。但是实际执行结果是一个小于 100 的数字。 原因就是因为 SimpleDateFormat 作为一个非线程安全的类，被当做了共享变 量在多个线程中进行使用，这就出现了线程安全问题。 线程不安全原因通过以上代码，我们发现了在并发场景中使用 SimpleDateFormat 会有线程安 全问题。其实，JDK 文档中已经明确表明了 SimpleDateFormat 不应该用在多线程 场景中: Date formats are not synchronized. It is recommended to create separate format instances for each thread. If multiple threads access a format concurrently, it must be synchronized externally. 那么接下来分析下为什么会出现这种问题，SimpleDateFormat 底层到底是怎 么实现的?我们跟一下 SimpleDateFormat 类中 format 方法的实现其实就能发现端倪。 123456789101112131415161718192021222324252627282930313233// Called from Format after creating a FieldDelegate private StringBuffer format(Date date, StringBuffer toAppendTo, FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo; &#125; SimpleDateFormat 中的 format 方法在执行过程中，会使用一个成员变量 calendar 来保存时间。这其实就是问题的关键。 由于我们在声明 SimpleDateFormat 的时候，使用的是 static 定义的。那么 这 个 SimpleDateFormat就是一个共享变量， 随 之，SimpleDateFormat 中 的 calendar 也就可以被多个线程访问到。 假设线程 1 刚刚执行完 calendar.setTime 把时间设置成 2018-11-11，还 没等执行完，线程 2 又执行了 calendar.setTime 把时间改成了 2018-12-12。 这时候线程 1 继续往下执行，拿到的 calendar.getTime 得到的时间就是线程 2 改 过之后的。 除了 format 方法以外，SimpleDateFormat 的 parse 方法也有同样的问题。 所以，不要把 SimpleDateFormat 作为一个共享变量使用。 如何解决线程安全问题 使用局部变量 不要使用static 加同步锁12345678910111213141516for (int i = 0; i &lt; 100; i++) &#123; // 获取当前时间 Calendar calendar = Calendar.getInstance(); int finalI = i; pool.execute(() -&gt; &#123; // 加锁 synchronized (simpleDateFormat) &#123; // 时间增加 calendar.add(Calendar.DATE, finalI); // 通过 simpleDateFormat 把时间转换成字符串 String dateString = simpleDateFormat.format(calendar.getTime()); // 把字符串放入 Set 中 dates.add(dateString); //countDown countDownLatch.countDown(); &#125; &#125;); &#125; 其实以上代码还有可以改进的地方，就是可以把锁的粒度再设置的小一点，可以 只对 simpleDateFormat.format 这一行加锁，这样效率更高一些。 使用 ThreadLocal 第三种方式，就是使用 ThreadLocal。 ThreadLocal 可以确保每个线程都可以 得到单独的一个 SimpleDateFormat 的对象，那么自然也就不存在竞争问题了。 12345678910/** * 使用ThreadLocal定义一个全局的SimpleDateFormat */ private static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;() &#123; @Override protected SimpleDateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; // 用法 String dateString = simpleDateFormatThreadLocal.get().format(calendar.getTime()); 用 ThreadLocal 来实现其实是有点类似于缓存的思路，每个线程都有一个独享 的对象，避免了频繁创建对象，也避免了多线程的竞争。 当然，以上代码也有改进空间，就是，其实 SimpleDateFormat 的创建过程可 以改为延迟加载。这里就不详细介绍了。 使用 DateTimeFormatter如果是 Java8 应用，可以使用 DateTimeFormatter 代替 SimpleDateFormat， 这是一个线程安全的格式化工具类。就像官方文档中说的，这个类 simple beautiful strong immutable thread-safe。 123456789// 解析日期String dateStr = &quot;2016年10月25日&quot;;DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日&quot;);LocalDate date = LocalDate.parse(dateStr, formatter);// 日期转换为字符串LocalDateTime now = LocalDateTime.now();DateTimeFormatter format = DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日 hh:mm a&quot;);String nowStr = now.format(format);System.out.println(nowStr); 总结本 文 介 绍 了 SimpleDateFormat 的 用 法，SimpleDateFormat 主 要 可 以 在 String 和 Date 之间做转换，还可以将时间转换成不同时区输出。同时提到在并发场 景中 SimpleDateFormat 是不能保证线程安全的，需要开发者自己来保证其安全性。 主要的几个手段有改为局部变量、使用 synchronized 加锁、使用 Threadlocal 为每一个线程单独创建一个等。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"Java并发编程-多线程的发展和意义","date":"2021-07-21T11:45:01.000Z","path":"wiki/Java并发编程-多线程的发展和意义/","text":"线程基础概念什么是线程线程是CPU执行任务的基本单位，一个进程中包含一个或者多个线程，一个进程内的多个线程共享进程的资源，每一个线程有自己的独立内存，是线程不共享的。 并行与并发 并行 同一时刻，横向有多少个线程可以运行 并发 系统和服务器同一时刻能够承受的并发线程 线程的特征 异步（不需要等待） 比如说注册之后发送验证码，验证码的过程可以异步去做不需要客户去在注册接口等待这个时间； 并行（CPU核数） Java中线程的使用 继承Thread 实现Runnalbe 实现Callable/Future 线程原理1234567public class ThreadDemo extend Thread&#123; int a = 0; public void run()&#123; int b = 0; b = a + 1; &#125;&#125; 执行start方法，其实是调用JVM相关的指令， thread.cpp java thread.start() -&gt; cpp thread.start() -&gt; os指令:create.thread start.thread操作系统层面会创建线程，线程创建之后，线程可以启动，（线程启动之后并不一定马上执行）这些线程统一有CPU调度算法来处理；决定那个线程分配给那个执行CPU；CPU执行线程任务的时候，会调用run方法 -&gt; cpp run方法 -&gt; java thread.run() ⚠️ CompletableFuture 异步回调通知，基于Future的优化 线程的生命周期线程创建，当线程中的指令执行完成之后，run（）结束 线程销毁其他线程状态 等待状态 （sleep join wait） 锁阻塞状态 （blocked 竞争锁失败 park） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ThreadStatusDemo &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;time waitting&quot;).start(); new Thread(()-&gt;&#123; while (true)&#123; synchronized (ThreadStatusDemo.class)&#123; try &#123; ThreadStatusDemo.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;,&quot;waitting&quot;).start(); new Thread(new BlockDemo(),&quot;block demo 1&quot;).start(); new Thread(new BlockDemo(),&quot;block demo 2&quot;).start(); &#125; static class BlockDemo extends Thread&#123; @Override public void run() &#123; synchronized (BlockDemo.class)&#123; while (true)&#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 查看线程状态 12jps -ljstack pid 线程如何停止interrupt() 停止线程主动停止方式 -&gt; run方法执行结束被动停止方式 一般中断线程是在无法控制线程的情况下，比如线程wait ， 线程sleep ， 线程while(true)Thread.currnetThread().isInterrupted() stop方法停止线程 禁止使用 相当于kill线程 不友好 interrupt 功能 唤醒阻塞状态的线程 修改中断标志，false -&gt; true 问题排查","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"Java并发编程-Overview","date":"2021-07-21T11:42:49.000Z","path":"wiki/Java并发编程-Overview/","text":"","tags":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"}]},{"title":"为什么禁止开发人员修改 serialVersionUID 字段的值?","date":"2021-07-21T09:47:46.000Z","path":"wiki/为什么禁止开发人员修改-serialVersionUID-字段的值/","text":"序列化是一种对象持久化的手段。普遍应用在网络传输、RMI 等场景中。类通 过实现 java.io.Serializable 接口以启用其序列化功能。Java 对象的序列化与反序列化、深入分析 Java 的序列化与反序列化、单例与 序列化的那些事儿 在这几篇文章中，分别介绍过了序列化涉及到的类和接口、如何自定义序列化 策略、transient 关键字和序列化的关系等，还通过学习 ArrayList 对序列化的实现源 码深入学习了序列化。并且还拓展分析了一下序列化对单例的影响等。但是，还有一个知识点并未展开介绍，那就是关于 serialVersionUID 。这个 字段到底有什么用?如果不设置会怎么样?为什么《Java 开发手册》中有以下规定: 背景知识Serializable 和 Externalizable类通过实现 java.io.Serializable 接口以启用其序列化功能。未实现此接 口的类将无法进行序列化或反序列化。可序列化类的所有子类型本身都是可序列 化的。如果读者看过 Serializable 的源码，就会发现，他只是一个空的接口，里 面什么东西都没有。Serializable 接口没有方法或字段，仅用于标识可序列化的 语义。但是，如果一个类没有实现这个接口，想要被序列化的话，就会抛出 java. io.NotSerializableException 异常。 它是怎么保证只有实现了该接口的方法才能进行序列化与反序列化的呢?原因是在执行序列化的过程中，会执行到以下代码: 12345678910111213141516if (obj instanceof String) &#123; writeString((String) obj, unshared); &#125; else if (cl.isArray()) &#123; writeArray(obj, desc, unshared); &#125; else if (obj instanceof Enum) &#123; writeEnum((Enum&lt;?&gt;) obj, desc, unshared); &#125; else if (obj instanceof Serializable) &#123; writeOrdinaryObject(obj, desc, unshared); &#125; else &#123; if (extendedDebugInfo) &#123; throw new NotSerializableException( cl.getName() + &quot;\\n&quot; + debugInfoStack.toString()); &#125; else &#123; throw new NotSerializableException(cl.getName()); &#125; &#125; 在进行序列化操作时，会判断要被序列化的类是否是 Enum、Array 和 Serializable 类型，如果都不是则直接抛出 NotSerializableException。Java 中还提供了 Externalizable 接口，也可以实现它来提供序列化能力。 Externalizable 继承自 Serializable，该接口中定义了两个抽象方法: writeExternal() 与 readExternal()。当使用 Externalizable 接口来进行序列化与反序列化的时候需要开发人员重 写 writeExternal() 与 readExternal() 方法。否则所有变量的值都会变成默认值。 transienttransient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可 以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。 自定义序列化策略在序列化过程中，如果被序列化的类中定义了 writeObject 和 readObject 方法， 虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的 序列化和反序列化。 如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteOb- ject 方法以及 ObjectInputStream 的 defaultReadObject 方法。用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程， 比如可以在序列化的过程中动态改变序列化的数值。 所以，对于一些特殊字段需要定义序列化的策略的时候，可以考虑使用 tran- sient 修 饰， 并 自 己 重 写 writeObject 和 readObject 方 法， 如 java.util. ArrayList 中就有这样的实现。 我们随便找几个 Java 中实现了序列化接口的类，如 String、Integer 等，我们 可以发现一个细节，那就是这些类除了实现了 Serializable 外，还定义了一个 serialVersionUID 那么，到底什么是 serialVersionUID 呢?为什么要设置这样一个字段呢? 什么是 serialVersionUID序列化是将对象的状态信息转换为可存储或传输的形式的过程。我们都知道， Java 对象是保存在 JVM 的堆内存中的，也就是说，如果 JVM 堆不存在了，那么对 象也就跟着消失了。 而序列化提供了一种方案，可以让你在即使 JVM 停机的情况下也能把对象保存 下来的方案。就像我们平时用的 U 盘一样。把 Java 对象序列化成可存储或传输的形 式(如二进制流)，比如保存在文件中。这样，当再次需要这个对象的时候，从文件中 读取出二进制流，再从二进制流中反序列化出对象。 虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重 要的一点是两个类的序列化 ID 是否一致，这个所谓的序列化 ID，就是我们在代码中 定义的 serialVersionUID。 如果 serialVersionUID 变了会怎样我们举个例子吧，看看如果 serialVersionUID 被修改了会发生什么? 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SerializableDemo1 &#123; public static void main(String[] args) &#123;//Initializes The Object User1 user = new User1(); user.setName(&quot;hollis&quot;); //Write Obj to File ObjectOutputStream oos = null; try &#123; oos = new ObjectOutputStream(new FileOutputStream(&quot;tempFile&quot;)); oos.writeObject(user); &#125; catch( IOException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(oos); &#125; &#125; &#125; class User1 implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125;``` 我们先执行以上代码，把一个 User1 对象写入到文件中。然后我们修改一下 User1 类，把 serialVersionUID 的值改为 2L。```javaclass User1 implements Serializable &#123; private static final long serialVersionUID = 2L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 然后执行以下代码，把文件中的对象反序列化出来: 1234567891011121314151617181920212223242526272829 public class SerializableDemo2 &#123; public static void main(String[] args) &#123;//Read Obj from File File file = new File(&quot;tempFile&quot;); ObjectInputStream ois = null; try &#123; ois = new ObjectInputStream(new FileInputStream(file)); User1 newUser = (User1) ois.readObject(); System.out.println(newUser); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(ois); try &#123; FileUtils.forceDelete(file); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;``` 执行结果如下```javajava.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdescserialVersionUID = 1, local class serialVersionUID = 2 可以发现，以上代码抛出了一个 java.io.InvalidClassException，并且 指出 serialVersionUID 不一致。 这是因为，在进行反序列化时，JVM 会把传来的字节流中的 serialVersio- nUID 与本地相应实体类的 serialVersionUID 进行比较，如果相同就认为是一致 的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即是 Invalid- CastException。 这也是《Java 开发手册》中规定，在兼容性升级中，在修改类的时候，不要 修改 serialVersionUID 的原因。除非是完全不兼容的两个版本。所以，serialVersionUID 其实是验证版本一致性的。 如果读者感兴趣，可以把各个版本的 JDK 代码都拿出来看一下，那些向下兼容 的类的 serialVersionUID 是没有变化过的。比如 String 类的 serialVersionUID一直都是 -6849794470754667710L。 但是，作者认为，这个规范其实还可以再严格一些，那就是规定:如果一个类实现了 Serializable 接口，就必须手动添加一个 private static final long serialVersionUID变量，并且设置初始值。 为什么要明确定一个 serialVersionUID如果我们没有在类中明确的定义一个 serialVersionUID 的话，看看会发生什么。 尝试修改上面的 demo 代码，先使用以下类定义一个对象，该类中不定义 serialVersionUID，将其写入文件。 1234567891011class User1 implements Serializable &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 然后我们修改 User1 类，向其中增加一个属性。在尝试将其从文件中读取出来， 并进行反序列化。 12345678910111213141516class User1 implements Serializable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125;&#125; 执 行 结 果:java.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdesc serialVersionUID = -2986778152837257883, local class serialVersionUID = 7961728318907695402 同样，抛出了 InvalidClassException，并且指出两个 serialVersio- nUID 不同，分别是 -2986778152837257883 和 7961728318907695402。从这里可以看出，系统自己添加了一个 serialVersionUID。 所以，一旦类实现了 Serializable，就建议明确的定义一个 serialVersionUID。不然在修改类的时候，就会发生异常。 serialVersionUID 有两种显示的生成方式: 一是默认的1L，比如:private static final long serialVersionUID = 1L;二是根据类名、接口名、成员方法及属性等来生成一个 64 位的哈希字段，比如:private static final long serialVersionUID = xxxxL; 小结serialVersionUID 是用来验证版本一致性的。所以在做兼容性升级的时候， 不要改变类中 serialVersionUID 的值。 如果一个类实现了 Serializable 接口，一定要记得定义 serialVersionUID，否则会发生异常。可以在 IDE 中通过设置，让他帮忙提示，并且可以一键快速生成一 个 serialVersionUID。 之所以会发生异常，是因为反序列化过程中做了校验，并且如果没有明确定义的 话，会根据类的属性自动生成一个。 参考资料 Java技术灵魂15问","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"Serializable","slug":"Serializable","permalink":"http://example.com/tags/Serializable/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"极尽HashMap底层原理","date":"2021-07-21T06:26:25.000Z","path":"wiki/极尽HashMap底层原理/","text":"HashMap 中的容量与扩容实现，细致入微，值的一品 Java 8系列之重新认识HashMap 美团技术团队 HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现存储结构-字段从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(&quot;美团&quot;,&quot;小美&quot;); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下: int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考[http://blog.csdn.net/liuqiyao_01/article/details/14475159]，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考[http://blog.csdn.net/v_july_v/article/details/6105630]。 实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125; &#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;，&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， &quot;C&quot;); new Thread(&quot;Thread1&quot;) &#123; public void run() &#123; map.put(7, &quot;B&quot;); System.out.println(map); &#125;; &#125;.start(); new Thread(&quot;Thread2&quot;) &#123; public void run() &#123; map.put(3, &quot;A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。(5) 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"hashmap","slug":"hashmap","permalink":"http://example.com/tags/hashmap/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"elasticsearch字符串查询汇总","date":"2021-07-20T14:08:29.000Z","path":"wiki/elasticsearch字符串查询汇总/","text":"filterexistsfuzzyidsprefixregexptermtermsterms_setwildcardtext搜索 intervalmatchmatch_bool_prefixmatch_phrasematch_phrase_prefixmulti_matchcommonquery_stringsimple_query_string 参考资料 查询是否包含字符串_十九种Elasticsearch字符串搜索方式终极介绍","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch 查询值前缀不包含某个字符串","date":"2021-07-20T13:48:31.000Z","path":"wiki/elasticsearch-查询值前缀不包含某个字符串/","text":"需求 查询IP不是以11.开头的所有文档，然后获取文档访问量前100条 curl -X GET &quot;localhost:9200/yj_visit_data2,yj_visit_data3/_search?pretty&quot; -u elastic:elastic -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;11.&quot; &#125; &#125; &#125;, &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;1.&quot; &#125; &#125; &#125; ] &#125; &#125; ], &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;visitTime&quot;: &#123; &quot;gte&quot;: 1577808000000, &quot;lte&quot;: 1609430399000 &#125; &#125; &#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;term_article&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;ip&quot;, &quot;min_doc_count&quot;: 20, &quot;size&quot;: 10000 &#125; &#125; &#125;&#125;&#x27;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"https协议","date":"2021-07-20T08:30:45.000Z","path":"wiki/https协议/","text":"参考资料 《 HTTPS 升级指南 》 深入理解 HTTPS 原理、过程与实践 HTTPS实现原理 深入理解HTTPS原理、过程与实践","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"elasticsearch-reindex","date":"2021-07-20T03:59:18.000Z","path":"wiki/elasticsearch-reindex/","text":"reindex 常规使用 Reindex要求为源索引中的所有文档启用_source。Reindex不尝试设置目标索引，它不复制源索引的设置，你应该在运行_reindex操作之前设置目标索引，包括设置映射、碎片计数、副本等。 如下示例将把文档从twitter索引复制到new_twitter索引： 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 下面是返回值： 12345678910111213141516171819&#123; &quot;took&quot; : 299, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 2, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 就像_update_by_query一样，_reindex获取源索引的快照，但它的目标必须是不同的索引，因此不太可能发生版本冲突。可以像index API那样配置dest元素来控制乐观并发控制。仅仅省略version_type(如上所述)或将其设置为internal，都会导致Elasticsearch盲目地将文档转储到目标中，覆盖任何碰巧具有相同类型和id的文档 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot;, &quot;version_type&quot;: &quot;internal&quot; &#125;&#125;&#x27; 将version_type设置为external将导致Elasticsearch保存源文件的版本，创建任何缺失的文档，并更新目标索引中比源索引中版本更旧的文档： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;&#125;&#x27; 设置op_type=create将导致_reindex只在目标索引中创建缺失的文档。所有现有文件将导致版本冲突： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 默认情况下，版本冲突将中止_reindex进程，“conflicts”请求体参数可用于指示_reindex处理关于版本冲突的下一个文档，需要注意的是，其他错误类型的处理不受“conflicts”参数的影响，当在请求体中设置“conflicts”:“proceed”时，_reindex进程将继续处理版本冲突，并返回所遇到的版本冲突计数： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 返回值如下： 12345678910111213141516171819&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 2, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 可以通过向源添加查询来限制文档。这将只复制由kimchy发出的tweet到new_twitter： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; source中的index可以是一个列表，允许你在一个请求中从多个源复制。这将从twitter和blog索引复制文档： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: [&quot;twitter&quot;,&quot;blog&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; 注意：Reindex API不处理ID冲突，因此最后编写的文档将“胜出”，但顺序通常是不可预测的，因此依赖这种行为不是一个好主意，相反，可以使用脚本确保id是惟一的。还可以通过设置大小来限制处理文档的数量，示例将只复制一个单一的文件从twitter到new_twitter： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 1, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 如果你想从twitter索引中获得一组特定的文档，你需要使用sort。排序会降低滚动的效率，但在某些上下文中，这样做是值得的。如果可能的话，选择一个比大小和排序更具选择性的查询。这将把10000个文档从twitter复制到new_twitter： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10000, &quot;source&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;sort&quot;: &#123; &quot;age&quot;: &quot;desc&quot; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; source部分支持搜索请求中支持的所有元素。例如，只有原始文档中的一部分字段可以使用源过滤重新索引，如下所示： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;_source&quot;: [&quot;user&quot;, &quot;_doc&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 与_update_by_query一样，_reindex支持修改文档的脚本。与_update_by_query不同，脚本允许修改文档的元数据。这个例子改变了源文档的版本： 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;if (ctx._source.foo == \\&quot;bar\\&quot;) &#123;ctx._version++; ctx._source.remove(\\&quot;foo\\&quot;)&#125;&quot;, &quot;lang&quot;: &quot;painless&quot; &#125;&#125;&#x27; 就像在_update_by_query中一样，你可以设置ctx.op更改在目标索引上执行的操作，值为noop，delete。设置ctx.op到任何其他字段都会返回一个错误，在ctx中设置任何其他字段也是如此。可以修改以下值：_id、_index、_version、_routing。将_version设置为null或将它从ctx映射中清除，就像没有在索引请求中发送版本一样;它将导致在目标索引中覆盖文档，而不管目标上的版本或在_reindex请求中使用的版本类型。默认情况下，如果_reindex看到一个带有路由的文档，那么该路由将被保留，除非脚本更改了它，你可以设置路由对dest的请求，以改变这一点：keep：将为每个匹配发送的批量请求上的路由设置为匹配上的路由。这是默认值。discard：将为每个匹配发送的批量请求上的路由设置为null。=：将为每个匹配发送的批量请求上的路由设置为=之后的所有文本。例如，你可以使用以下请求将所有文档从具有公司名称cat的源索引复制到路由设置为cat的dest索引中： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;company&quot;: &quot;cat&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; 默认情况下，_reindex使用滚动批次为1000，可以使用源元素中的size字段更改批大小： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 100 &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; _reindex还可以通过像这样指定管道来使用Ingest节点特性： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;pipeline&quot;: &quot;some_ingest_pipeline&quot; &#125;&#125;&#x27; 远程reindex12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;username&quot;: &quot;user&quot;, &quot;password&quot;: &quot;pass&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; host参数必须包含scheme, host, port（如：http://otherhost:9200）,也可以加路径（如：http://otherhost:9200/proxy），username和password是可选的，如果远程集群开启了安全认证，那么是必选的，如果需要使用username和password，需要使用https。远程主机需要设置白名单，可以通过elasticsearch.yml文件里的reindex.remote.whitelist属性进行设置，如果设置多个值可以使用逗号来进行分隔（如：otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*），这里的配置可以忽略scheme，如： reindex.remote.whitelist: &quot;otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*&quot; 必须让每个处理reindex的节点上添加白名单的配置。这个特性应该适用于可能找到的任何版本的Elasticsearch的远程集群，这应该允许通过从旧版本的集群reindex，将Elasticsearch的任何版本升级到当前版本。要启用发送到旧版本Elasticsearch的查询，无需验证或修改即可将查询参数直接发送到远程主机，注意：远程reindex不支持手动或者自动slicing。从远程服务器reindex使用堆上缓冲区，默认最大大小为100mb，如果远程索引包含非常大的文档，则需要使用更小的批处理大小，下面的示例将批处理大小设置为10。 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 还可以使用socket_timeout字段设置远程连接上的套接字读取超时，使用connect_timeout字段设置连接超时，他们的默认值为30秒，下面示例例将套接字读取超时设置为1分钟，连接超时设置为10秒： 12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;socket_timeout&quot;: &quot;1m&quot;, &quot;connect_timeout&quot;: &quot;10s&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 配置SSL参数远程reindex支持配置SSL参数，除了在Elasticsearch秘钥库中添加安全设置之外，还需要在elasticsearch.yml文件中进行配置，不可能在_reindex请求体中配置。支持以下设置：reindex.ssl.certificate_authorities应受信任的PEM编码证书文件的路径列表，不同同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.path包含要信任的证书的Java密钥存储文件的路径，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.secure_password一起使用。reindex.ssl.truststore.secure_passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.password一起使用。reindex.ssl.truststore.typereindex.ssl.truststore.path信任存储库的类型，必须是jks或PKCS12，如果reindex.ssl.truststore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.verification_mode指示用于防止中间人攻击和伪造证书的验证类型。可以设置为full（验证主机名和证书路径）、certificate（验证证书路径，但不验证主机名）、none（不执行验证——这在生产环境中是强烈不鼓励的），默认是full。reindex.ssl.certificate指定PEM编码证书的路径或者证书链用于HTTP客户端身份认证，这个配置还需要设置reindex.ssl.key值，不能同时设置reindex.ssl.certificate和reindex.ssl.keystore.path。reindex.ssl.key指定与用于客户端身份验证的证书相关联的PEM编码私钥的路径，不能同时设置reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.secure_key_passphrase一起使用。reindex.ssl.secure_key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.key_passphrase一起使用。reindex.ssl.keystore.path指定密钥存储库的路径，其中包含用于HTTP客户机身份验证的私钥和证书(如果远程集群需要)，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.keystore.type密钥存储库的类型(reindex.ssl.keystore.path)，必须是jks或者PKCS12，如果reindex.ssl.keystore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.keystore.password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.secure_password一起使用。reindex.ssl.keystore.secure_password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.password一起使用。reindex.ssl.keystore.key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.secure_key_password一起使用。reindex.ssl.keystore.secure_key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.key_password一起使用。 URL参数除了标准的pretty参数外， reindex还支持refresh, wait_for_completion, wait_for_active_shards, timeout, scroll和requests_per_second。发送refresh url参数将导致对所写请求的所有索引进行刷新，这与Index API的refresh参数不同，后者只会刷新接收新数据的碎片，与index API不同的是，它不支持wait_for。如果请求包含wait_for_completion=false，则Elasticsearch将执行一些执行前检查，启动请求，然后返回一个任务，该任务可与Tasks api一起用于取消或获取任务状态，Elasticsearch还将创建此任务的记录，作为.tasks/task/${taskId}的文档，你可以自己决定是保留或者删除他，当你已经完成了，删除他，这样es会回收他使用的空间。wait_for_active_shards控制在进行重新索引之前必须激活多少个shard副本，超时控制每个写请求等待不可用碎片变为可用的时间，两者在批量API中的工作方式完全相同，由于_reindex使用滚动搜索，你还可以指定滚动参数来控制“搜索上下文”存活的时间(例如?scroll=10m)，默认值是5分钟。requests_per_second可以设置为任何正数(1.4、6、1000等)，并通过在每个批中填充等待时间来控制_reindex发出批索引操作的速率，可以通过将requests_per_second设置为-1来禁用。节流是通过在批之间等待来完成的，这样就可以给_reindex内部使用的滚动设置一个考虑填充的超时，填充时间是批大小除以requests_per_second和写入时间之间的差额，默认情况下批处理大小为1000，所以如果requests_per_second被设置为500： target_time = 1000 / 500 per second = 2 seconds padding time = target_time - write_time = 2 seconds - 0.5 seconds = 1.5 seconds 由于批处理是作为单个_bulk请求发出的，因此较大的批处理大小将导致Elasticsearch创建许多请求，然后等待一段时间再启动下一个请求集，这是“bursty”而不是“smooth”，默认值是-1。 响应体 12345678910111213141516171819&#123; &quot;took&quot;: 639, &quot;timed_out&quot;: false, &quot;total&quot;: 5, &quot;updated&quot;: 0, &quot;created&quot;: 5, &quot;deleted&quot;: 0, &quot;batches&quot;: 1, &quot;noops&quot;: 0, &quot;version_conflicts&quot;: 2, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: 1, &quot;throttled_until_millis&quot;: 0, &quot;failures&quot;: []&#125; took整个操作花费的总毫秒数。timed_out如果在reindex期间执行的任何请求超时，则将此标志设置为true。total成功处理的文档数量。updated成功更新的文档数量。created成功创建的文档的数量。deleted成功删除的文档数量。batches由reindex回拉的滚动响应的数量。noops由于用于reindex的脚本返回了ctx.op的noop值而被忽略的文档数量。version_conflictsreindex命中的版本冲突数。retriesreindex尝试重试的次数,bulk是重试的批量操作的数量，search是重试的搜索操作的数量。throttled_millis请求休眠以符合requests_per_second的毫秒数。requests_per_second在reindex期间每秒有效执行的请求数。throttled_until_millis在_reindex响应中，该字段应该始终等于零，它只有在使用任务API时才有意义，在任务API中，它指示下一次(毫秒)再次执行节流请求，以符合requests_per_second。failures出现多个错误以数组返回。 使用task api获取task 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&amp;wait_for_completion=false&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 返回值为 123&#123; &quot;task&quot; : &quot;8uQK-B00RiWq03awtJok1Q:18&quot;&#125; 你可以用任务API获取所有正在运行的reindex请求的状态：curl -XGET &quot;http://127.0.0.1:9200/_tasks?detailed=true&amp;actions=*reindex&amp;pretty&quot;或者根据task id获取curl -XGET &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48?pretty&quot;返回值为： 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;completed&quot; : true, &quot;task&quot; : &#123; &quot;node&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;id&quot; : 48, &quot;type&quot; : &quot;transport&quot;, &quot;action&quot; : &quot;indices:data/write/reindex&quot;, &quot;status&quot; : &#123; &quot;total&quot; : 0, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 0, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : 0.0, &quot;throttled_until_millis&quot; : 0 &#125;, &quot;description&quot; : &quot;reindex from [twitter] to [new_twitter][_doc]&quot;, &quot;start_time_in_millis&quot; : 1566216815832, &quot;running_time_in_nanos&quot; : 86829, &quot;cancellable&quot; : true, &quot;headers&quot; : &#123; &#125; &#125;, &quot;error&quot; : &#123; &quot;type&quot; : &quot;index_not_found_exception&quot;, &quot;reason&quot; : &quot;no such index [new_twitter] and [action.auto_create_index] ([twitter,index10,-index1*,+ind*,-myIndex]) doesn&#x27;t match&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;new_twitter&quot; &#125;&#125; 取消task任何reindex接口都可以使用task cancel api取消： 123456789101112131415 curl -XPOST &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48/_cancel?pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:48] doesn&#x27;t support cancellation&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; 取消应该很快发生，但可能需要几秒钟，Tasks API将继续列出任务，直到它醒来取消自己。 rethrottle可以在url中使用_rethrottle，并使用requests_per_second参数来设置节流： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex/8uQK-B00RiWq03awtJok1Q:250/_rethrottle?requests_per_second=-1&amp;pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:250] is missing&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; reindex改变属性名称_reindex可以重命名属性名，假设你创建了一个包含如下文档的索引： 12345curl -XPOST &quot;http://127.0.0.1:9200/test/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;text&quot;: &quot;words words&quot;, &quot;flag&quot;: &quot;foo&quot;&#125;&#x27; 在reindex的时候想把flag修改为tag，示例如： 12345678910111213curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.tag = ctx._source.remove(\\&quot;flag\\&quot;)&quot; &#125;&#125;&#x27; 查看order2的数据： 123456789101112131415curl -XGET &quot;http://127.0.0.1:9200/order2/_doc/1?pretty&quot;&#123; &quot;_index&quot; : &quot;order2&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;words words&quot;, &quot;tag&quot; : &quot;foo&quot; &#125;&#125; 切片Reindex支持切片滚动，以并行化重新索引过程。这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分 手动切片通过为每个请求提供一个片id和片的总数，手工切片一个重索引请求： 123456789101112131415161718192021222324252627curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 0, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27;curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 1, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 你可以通过以下方法来验证： curl -XGET &quot;http://127.0.0.1:9200/_refresh?pretty&quot; curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot; 返回值为： 123456789&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; 自动切面你还可以让_reindex使用切片滚动自动并行化_uid上的切片，使用slices指定要使用的片数: 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?slices=5&amp;refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 通过下面请求进行验证： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot;&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; reindex多个索引如果有许多索引需要reindex，通常最好一次reindex一个索引，而不是使用一个glob模式来获取许多索引。这样，如果有任何错误，可以删除部分完成的索引并从该索引重新开始，从而恢复该过程。它还使并行化过程变得相当简单：将索引列表拆分为reindex并并行运行每个列表。可以使用一次性脚本： 1234567891011for index in i1 i2 i3 i4 i5; do curl -HContent-Type:application/json -XPOST localhost:9200/_reindex?pretty -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;-reindexed&quot; &#125; &#125;&#x27;done reindex每日索引尽管有上述建议，你仍然可以结合使用_reindex和Painless来reindex每日索引，从而将新模板应用于现有文档。假设有以下文件组成的索引: curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.30/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d{“system.cpu.idle.pct”: 0.908}’ curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.31/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#39;{“system.cpu.idle.pct”: 0.105} metricbeat-*索引的新模板已经加载到Elasticsearch中，但它只适用于新创建的索引。下面的脚本从索引名称中提取日期，并创建一个附加-1的新索引。所有来自metricbeat-2016.05.31的数据将reindex到metricbeat-2016.05.31-1。 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;metricbeat-*&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;metricbeat&quot; &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._index = &#x27;metricbeat-&#x27; + (ctx._index.substring(&#x27;metricbeat-&#x27;.length(),ctx._index.length())) + &#x27;-1&#x27;&quot; &#125;&#125;&#x27; 以前metricbeat索引中的所有文档现在都可以在*-1索引中找到。 curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.30-1/_doc/1?pretty&quot;curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.31-1/_doc/1?pretty&quot; 前一种方法还可以与更改字段名称结合使用，以便仅将现有数据加载到新索引中，并在需要时重命名任何字段。 提取索引中的子集合_reindex可用于提取索引的随机子集进行测试： 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;function_score&quot; : &#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;random_score&quot; : &#123;&#125; &#125; &#125;, &quot;sort&quot;: &quot;_score&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;random_twitter&quot; &#125;&#125;&#x27; _reindex默认按_doc排序，因此random_score不会有任何效果，除非你覆盖sort属性为_score。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"Http状态码及含义","date":"2021-07-20T03:31:58.000Z","path":"wiki/Http状态码及含义/","text":"http状态码由3个十进制数字组成。第一个数字表示状态码的分类，后面的两位表示该分类下不同的状态。分为5个大类。 分类 1** 信息。服务器收到请求，请继续执行请求 2** 成功。请求被成功接收并处理 3** 重定向。需要进一步操作来完成请求 4** 客户端错误。无法完成请求，或请求包含语法错误 5** 服务器错误。服务器在处理请求的过程中发成错误 各个状态说明 100： 继续请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分。 101： 切换协议请求者已要求服务器切换协议，服务器已确认并准备切换。 200： 成功服务器已成功处理了请求。 201： 已创建请求成功并且服务器创建了新的资源。 202： 已接受服务器已接受请求，但尚未处理。 203： 非授权信息服务器已成功处理了请求，但返回的信息可能来自另一来源。 204： 无内容服务器成功处理了请求，但没有返回任何内容。 205： 重置内容服务器成功处理了请求，内容被重置。 206： 部分内容服务器成功处理了部分请求。 300： 多种选择针对请求，服务器可执行多种操作。 301： 永久移动请求的网页已永久移动到新位置，即永久重定向。 302： 临时移动请求的网页暂时跳转到其他页面，即暂时重定向。 303： 查看其他位置如果原来的请求是 POST，重定向目标文档应该通过 GET 提取。 304： 未修改此次请求返回的网页未修改，继续使用上次的资源。 305： 使用代理请求者应该使用代理访问该网页。 307： 临时重定向请求的资源临时从其他位置响应。 400： 错误请求服务器无法解析该请求。 401： 未授权请求没有进行身份验证或验证未通过。 403： 禁止访问服务器拒绝此请求。 404： 未找到服务器找不到请求的网页。 405： 方法禁用服务器禁用了请求中指定的方法。 406： 不接受无法使用请求的内容响应请求的网页。 407： 需要代理授权请求者需要使用代理授权。 408： 请求超时服务器请求超时。 409： 冲突服务器在完成请求时发生冲突。 410： 已删除请求的资源已永久删除。 411： 需要有效长度服务器不接受不含有效内容长度标头字段的请求。 412： 未满足前提条件服务器未满足请求者在请求中设置的其中一个前提条件。 413： 请求实体过大请求实体过大，超出服务器的处理能力。 414： 请求 URI 过长请求网址过长，服务器无法处理。 415： 不支持类型请求的格式不受请求页面的支持。 416： 请求范围不符页面无法提供请求的范围。 417： 未满足期望值服务器未满足期望请求标头字段的要求。 500： 服务器内部错误服务器遇到错误，无法完成请求。 501： 未实现服务器不具备完成请求的功能。 502： 错误网关服务器作为网关或代理，从上游服务器收到无效响应。 503： 服务不可用服务器目前无法使用。 504： 网关超时服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505： HTTP 版本不支持服务器不支持请求中所用的 HTTP 协议版本。","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"操作系统-死锁","date":"2021-07-20T03:29:18.000Z","path":"wiki/操作系统-死锁/","text":"参考资料 死锁是什么？如何避免死锁？ 哲学家就餐问题","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"操作系统进程调度策略","date":"2021-07-20T03:16:04.000Z","path":"wiki/操作系统进程调度策略/","text":"参考资料 操作系统中的进程调度策略有哪几种","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"进程间通信IPC","date":"2021-07-20T01:59:21.000Z","path":"wiki/进程间通信IPC/","text":"参考资料 进程间通信IPC (InterProcess Communication)","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"elasticsearch统计每年每小时访问量","date":"2021-07-19T14:53:39.000Z","path":"wiki/elasticsearch统计每年每小时访问量/","text":"需求背景，要统计文章在一年的时间内，每个小时的访问情况，按照0点举例子，每个文章，一年内每一天0点的访问次数累加起来； Elasticsearch索引如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;c&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ip&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;p&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ua&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;visitTime&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125; &#125;&#125; Java RestHighLevelClient写法12345678910111213141516171819202122232425262728public void getDateDist() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(&quot;yj_visit_data2&quot;); TermsAggregationBuilder termsAggregation = AggregationBuilders.terms(&quot;article&quot;) .field(&quot;article.keyword&quot;).size(2200) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) .calendarInterval(DateHistogramInterval.HOUR)); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(termsAggregation); sourceBuilder.query(QueryBuilders.rangeQuery(&quot;visitTime&quot;).gt(&quot;1609430400000&quot;).lte(&quot;1625068799000&quot;)); sourceBuilder.timeout(new TimeValue(900000)); SearchRequest request = new SearchRequest(); request.source(sourceBuilder); SearchResponse search = restHighLevelClient.search(request, RequestOptions.DEFAULT); Aggregations aggregations = search.getAggregations(); log.info(&quot;agg -&gt; &#123;&#125;&quot;, aggregations.asList().size()); List&lt;? extends Terms.Bucket&gt; buckets = ((ParsedStringTerms) aggregations.asList().get(0)).getBuckets(); List&lt;ArticleHourData&gt; hourDataList = new ArrayList&lt;&gt;(); for (Terms.Bucket bucket : buckets) &#123; List&lt;? extends Histogram.Bucket&gt; innerBuckets = ((ParsedDateHistogram) bucket.getAggregations().asList().get(0)).getBuckets(); hourDataList.add(calcBucket(innerBuckets, bucket.getKeyAsString())); &#125; log.info(&quot;result ----&gt; &#123;&#125;&quot;, JSONObject.toJSONString(hourDataList)); &#125; 聚合分析123456789101112131415161718192021public ArticleHourData calcBucket(List&lt;? extends Histogram.Bucket&gt; innerBuckets, String article) &#123; log.info(&quot;innerBuckets get(0) ---&gt; &#123;&#125;&quot;, JSON.toJSONString(innerBuckets.get(0))); Map&lt;String, ? extends List&lt;? extends Histogram.Bucket&gt;&gt; hourMap = innerBuckets.stream() .collect(Collectors.groupingBy(bucket -&gt; getHour(bucket.getKeyAsString()))); log.info(&quot;collect ======&gt; &#123;&#125; &quot;, JSONObject.toJSONString(hourMap.keySet())); ArticleHourData hourData = ArticleHourData.builder().article(article).build(); if (hourMap.isEmpty()) &#123; return hourData; &#125; HashMap&lt;String, Long&gt; hashMap = new HashMap&lt;&gt;(); for (String hour : hourMap.keySet()) &#123; List&lt;? extends Histogram.Bucket&gt; list = hourMap.get(hour); if (CollectionUtils.isEmpty(list)) &#123; continue; &#125; hashMap.put(hour, list.stream().mapToLong(Histogram.Bucket::getDocCount).sum()); &#125; hourData.setCountMap(hashMap); return hourData; &#125; 获取时间的小时123456789101112private String getHour(String date) &#123; date = date.replace(&quot;Z&quot;, &quot; UTC&quot;); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSS Z&quot;); Date d = null; try &#123; d = format.parse(date); &#125; catch (ParseException e) &#123; e.printStackTrace(); return null; &#125; return String.valueOf(DateUtil.asLocalDateTime(d).getHour()); &#125; Python写法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from json.decoder import JSONDecoderfrom elasticsearch import Elasticsearchimport logging,jsonfrom datetime import datetimees = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;39.107.117.232&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;elastic&#x27;), timeout = 90000)sqs = &#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;art&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot;, &quot;size&quot;: 5 &#125;, &quot;aggs&quot;: &#123; &quot;art_total&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot; &#125; &#125;, &quot;_time&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;visitTime&quot;, &quot;calendar_interval&quot;: &quot;hour&quot; &#125; &#125; &#125; &#125; &#125;&#125;_search_result = es.search(index=&quot;yj_visit_data2&quot; , body=sqs)_result_json = json.dumps(_search_result,sort_keys=True, indent=4, separators=(&#x27;, &#x27;, &#x27;: &#x27;), ensure_ascii=False)aggregations = _search_result[&#x27;aggregations&#x27;]art = aggregations[&#x27;art&#x27;]buckets = art[&#x27;buckets&#x27;]#print(type(buckets)) ; print(buckets)def getHour(time): return (int)(time[11:13])# 计算每个小时的点击数def countByMonth(dataList , hourTar): _count = 0 for data in dataList: timestamp = data[&#x27;key_as_string&#x27;] hour = getHour(timestamp) if hour == hourTar: _count = (int)(data[&#x27;doc_count&#x27;]) + _count return _count final_list = []# 循环计算每一个文章for outBucket in buckets: simple_result = &#123;&#125; _time = outBucket[&#x27;_time&#x27;] innerBuckets = _time[&#x27;buckets&#x27;] print(&quot;time inner bucker size&quot; , len(innerBuckets)) simple_list = [] for num in range(0,24): simple_list.append(countByMonth(innerBuckets,num)) simple_result[0] = outBucket[&#x27;key&#x27;] simple_result[1] = simple_list final_list.append(simple_result)print(&quot;final result ----&gt; &quot;,final_list)","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch踩坑","date":"2021-07-19T09:46:59.000Z","path":"wiki/elasticsearch踩坑/","text":"search.max_bucketsThis limit can be set by changing the [search.max_buckets] cluster level setting.]]; 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;search.max_buckets&quot;: 100000 &#125;&#125; Required one of fields [field, script][Elasticsearch exception [type=illegal_argument_exception, reason=Required one of fields [field, script], but none were specified. ] 123456789101112@Test public void test1() &#123; NativeSearchQuery nativeSearchQuery = new NativeSearchQuery(QueryBuilders.matchAllQuery(), null); Script script = new Script(&quot;doc[&#x27;article.keyword&#x27;]&quot;); nativeSearchQuery.addAggregation(AggregationBuilders.terms(&quot;art&quot;) .field(&quot;article.keyword&quot;).size(10) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) // 这一行没有写 .calendarInterval(DateHistogramInterval.HOUR))); SearchHits&lt;YjVisitData&gt; search = elasticsearchRestTemplate.search(nativeSearchQuery, YjVisitData.class); System.err.println(&quot;search.getAggregations().asList() &#123;&#125;&quot; + search.getAggregations().asList().size()); &#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"python3基本数据类型","date":"2021-07-18T13:05:52.000Z","path":"wiki/python基本数据类型/","text":"Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。 等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值 标准数据类型Python3 中有六个标准的数据类型： Number（数字） String（字符串） List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 Number（数字）Python3 支持 int、float、bool、complex（复数）。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。 123&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class &#x27;int&#x27;&gt; &lt;class &#x27;float&#x27;&gt; &lt;class &#x27;bool&#x27;&gt; &lt;class &#x27;complex&#x27;&gt; 此外还可以用 isinstance 来判断： 1234&gt;&gt;&gt; a = 111&gt;&gt;&gt; isinstance(a, int)True&gt;&gt;&gt; ⚠️ isinstance 和 type 的区别在于：type()不会认为子类是一种父类类型。isinstance()会认为子类是一种父类类型。 ⚠️ 注意：Python3 中，bool 是 int 的子类，True 和 False 可以和数字相加 True==1，False==0 是会返回 Ture，但可以通过 is 来判断类型。1、Python可以同时为多个变量赋值，如a, b = 1, 2。2、一个变量可以通过赋值指向不同类型的对象。3、数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。4、在混合计算时，Python会把整型转换成为浮点数。 String（字符串）Python中的字符串用单引号 ‘ 或双引号 “ 括起来，同时使用反斜杠 \\ 转义特殊字符。字符串的截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 List（列表）List（列表） 是 Python 中使用最频繁的数据类型。 列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号 [] 之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 Tuple（元组）元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： 1234567891011#!/usr/bin/python3tuple = ( &#x27;abcd&#x27;, 786 , 2.23, &#x27;runoob&#x27;, 70.2 )tinytuple = (123, &#x27;runoob&#x27;)print (tuple) # 输出完整元组print (tuple[0]) # 输出元组的第一个元素print (tuple[1:3]) # 输出从第二个元素开始到第三个元素print (tuple[2:]) # 输出从第三个元素开始的所有元素print (tinytuple * 2) # 输出两次元组print (tuple + tinytuple) # 连接元组 Set（集合）集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。 基本功能是进行成员关系测试和删除重复元素。 可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 创建格式：parame = &#123;value01,value02,...&#125; 或者 set(value) 1234567891011121314151617181920212223242526#!/usr/bin/python3sites = &#123;&#x27;Google&#x27;, &#x27;Taobao&#x27;, &#x27;Runoob&#x27;, &#x27;Facebook&#x27;, &#x27;Zhihu&#x27;, &#x27;Baidu&#x27;&#125;print(sites) # 输出集合，重复的元素被自动去掉# 成员测试if &#x27;Runoob&#x27; in sites : print(&#x27;Runoob 在集合中&#x27;)else : print(&#x27;Runoob 不在集合中&#x27;)# set可以进行集合运算a = set(&#x27;abracadabra&#x27;)b = set(&#x27;alacazam&#x27;)print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素 Dictionary（字典）字典（dictionary）是Python中另一个非常有用的内置数据类型。 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。 键(key)必须使用不可变类型。 在同一个字典中，键(key)必须是唯一的。 #!/usr/bin/python3 dict = &#123;&#125; dict[&#39;one&#39;] = &quot;1 - 菜鸟教程&quot; dict[2] = &quot;2 - 菜鸟工具&quot; tinydict = &#123;&#39;name&#39;: &#39;runoob&#39;,&#39;code&#39;:1, &#39;site&#39;: &#39;www.runoob.com&#39;&#125; print (dict[&#39;one&#39;]) # 输出键为 &#39;one&#39; 的值 print (dict[2]) # 输出键为 2 的值 print (tinydict) # 输出完整的字典 print (tinydict.keys()) # 输出所有键 print (tinydict.values()) # 输出所有值 注意：1、字典是一种映射类型，它的元素是键值对。2、字典的关键字必须为不可变类型，且不能重复。3、创建空字典使用 { }。 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python3基础语法","date":"2021-07-18T12:31:55.000Z","path":"wiki/python3基础语法/","text":"编码默认情况下，Python 3 源码文件以 UTF-8 编码，所有字符串都是 unicode 字符串。 当然你也可以为源码文件指定不同的编码：# -*- coding: cp-1252 -*- 标识符 第一个字符必须是字母表中字母或下划线 _ 。 标识符的其他的部分由字母、数字和下划线组成。 标识符对大小写敏感。在 Python 3 中，可以用中文作为变量名，非 ASCII 标识符也是允许的了。 python保留字import keywordkeyword.kwlist[‘False’, ‘None’, ‘True’, ‘and’, ‘as’, ‘assert’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] 注释Python中单行注释以 # 开头,多行注释可以用多个 # 号，还有 ‘’’ 和 “””： 行与缩进python最具特色的就是使用缩进来表示代码块，不需要使用大括号 {} 。缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数。 多行语句Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠 \\ 来实现多行语句，例如： 123total = item_one + \\ item_two + \\ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠 \\，例如： total = [&#39;item_one&#39;, &#39;item_two&#39;, &#39;item_three&#39;, &#39;item_four&#39;, &#39;item_five&#39;] 数字(Number)类型python中数字有四种类型：整数、布尔型、浮点数和复数。 int (整数) , 如 1, 只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 bool (布尔), 如 True。 float (浮点数), 如 1.23、3E-2 complex (复数), 如 1 + 2j、 1.1 + 2.2j 字符串(String) python中单引号和双引号使用完全相同。 使用三引号(‘’’ 或 “””)可以指定一个多行字符串。 转义符 \\ 反斜杠可以用来转义，使用r可以让反斜杠不发生转义。。 如 r”this is a line with \\n” 则\\n会显示，并不是换行。 按字面意义级联字符串，如”this “ “is “ “string”会被自动转换为this is string。 字符串可以用 + 运算符连接在一起，用 * 运算符重复。 Python 中的字符串有两种索引方式，从左往右以 0 开始，从右往左以 -1 开始。 Python中的字符串不能改变。 Python 没有单独的字符类型，一个字符就是长度为 1 的字符串。 字符串的截取的语法格式如下：变量[头下标:尾下标:步长] 空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。 记住：空行也是程序代码的一部分。 同一行显示多条语句Python 可以在同一行中使用多条语句，语句之间使用分号 ; 分割，以下是一个简单的实例： 12#!/usr/bin/python3import sys; x = &#x27;runoob&#x27;; sys.stdout.write(x + &#x27;\\n&#x27;) 多个语句构成代码组缩进相同的一组语句构成一个代码块，我们称之代码组。 像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。 我们将首行及后面的代码组称为一个子句(clause)。 如下实例： 12345678910111213141516171819202122232425if expression : suiteelif expression : suite else : suite``` ## print 输出print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=&quot;&quot;：## import 与 from...import在 python 用 import 或者 from...import 来导入相应的模块。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数,格式为： from somemodule import somefunction从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import *## 命令行参数很多程序可以执行一些操作来查看一些基本信息，Python可以使用-h参数查看各参数帮助信息： $ python -husage: python [option] … [-c cmd | -m mod | file | -] [arg] …Options and arguments (and corresponding environment variables):-c cmd : program passed in as string (terminates option list)-d : debug output from parser (also PYTHONDEBUG=x)-E : ignore environment variables (such as PYTHONPATH)-h : print this help message and exit [ etc. ] ```我们在使用脚本形式执行 Python 时，可以接收命令行输入的参数;","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python操作elasticsearch","date":"2021-07-18T12:20:25.000Z","path":"wiki/python操作elasticsearch/","text":"下载python对应的elasticsearch依赖包pip3 install elasticsearch==7.10.0 python操作elasticsearch代码1234567from elasticsearch import Elasticsearchprint(&quot;init ...&quot;)es = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;XXXXXX&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;XXXXXX&#x27;))# print(es.get(index=&#x27;yj_ip_pool&#x27;, doc_type=&#x27;_doc&#x27;, id=&#x27;9256058&#x27;))countRes = es.count(index=&#x27;yj_ip_pool&#x27;)print(countRes) 查询效果12345gaolei:awesome-python3-webapp gaolei$ /usr/local/opt/python/bin/python3.7 /Users/gaolei/Documents/DemoProjects/awesome-python3-webapp/www/es_test.pyinit ...&#123;&#x27;count&#x27;: 20095400, &#x27;_shards&#x27;: &#123;&#x27;total&#x27;: 1, &#x27;successful&#x27;: 1, &#x27;skipped&#x27;: 0, &#x27;failed&#x27;: 0&#125;&#125;gaolei:awesome-python3-webapp gaolei$","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"kibana添加用户及控制权限","date":"2021-07-15T15:53:44.000Z","path":"wiki/kibana添加用户及控制权限/","text":"操作步骤： 【修改elasticsearch配置文件】 -&gt; 【重启elasticsearch】 -&gt; 【初始化账号&amp;密码】 -&gt; 【修改kibana配置文件】 -&gt; 【重启kibana】 -&gt; 【初始账号登录kibana】 -&gt; 【创建、配置角色】 -&gt; 【创建新用户】 配置elasticsearch开启自带的xpack的验证功能xpack.security.enabled: true 配置单节点模式discovery.type: single-node 坑1: 集群节点配置报错cluster.initial_master_nodes 坑2: 本次存储节点配置报错maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?node.max_local_storage_nodes: 2 为内置账号设置密码在elasticsearch的bin下执行以下命令：./elasticsearch-setup-passwords interactive 配置kibana123#使用初始用户kibanaelasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;密码&quot; 重启kibana之后使用初始账号 elastic 登录 创建、配置角色 创建新用户 查看新用户访问界面角色里面配置了kibana的访问权限，只开通了discover和dashboard两个入口 索引的话，有好几个索引，但是只配置了一个索引的权限 参考资料 kibana7.2添加登录及权限 Elasticsearch 安装","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch稳定性调优","date":"2021-07-15T07:22:49.000Z","path":"wiki/elasticsearch稳定性调优/","text":"Elasticsearch性能优化总结Elasticsearch调优实践 稳定性调优一 Linux参数调优 修改系统资源限制 👇单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些 echo &quot;* - nofile 655360&quot; &gt;&gt;/etc/security/limits.conf单用户内存地址空间 echo &quot;* - as unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户线程数 echo &quot;* - nproc 2056474&quot; &gt;&gt;/etc/security/limits.conf单用户文件大小 echo &quot;* - fsize unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户锁定内存 echo &quot;* - memlock unlimited&quot; &gt;&gt;/etc/security/limits.conf单进程可以使用的最大map内存区域数量 echo &quot;vm.max_map_count = 655300&quot; &gt;&gt;/etc/sysctl.confTCP全连接队列参数设置， 这样设置的目的是防止节点数较多（比如超过100）的ES集群中，节点异常重启时全连接队列在启动瞬间打满，造成节点hang住，整个集群响应迟滞的情况echo &quot;net.ipv4.tcp_abort_on_overflow = 1&quot; &gt;&gt;/etc/sysctl.conf echo &quot;net.core.somaxconn = 2048&quot; &gt;&gt;/etc/sysctl.conf降低tcp alive time，防止无效链接占用链接数 echo 300 &gt;/proc/sys/net/ipv4/tcp_keepalive_time ES节点配置jvm.options-Xms和-Xmx设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给系统cache使用。 jvm内存建议不要低于2G，否则有可能因为内存不足导致ES无法正常启动或OOMjvm建议不要超过32G，否则jvm会禁用内存对象指针压缩技术，造成内存浪费 elasticsearch.yml设置内存熔断参数，防止写入或查询压力过高导致OOM，具体数值可根据使用场景调整。indices.breaker.total.limit: 30% indices.breaker.request.limit: 6% indices.breaker.fielddata.limit: 3% 调小查询使用的cache，避免cache占用过多的jvm内存，具体数值可根据使用场景调整。indices.queries.cache.count: 500 indices.queries.cache.size: 5% 单机多节点时，主从shard分配以ip为依据，分配到不同的机器上，避免单机挂掉导致数据丢失。cluster.routing.allocation.awareness.attributes: ip node.attr.ip: 1.1.1.1 ES使用方式节点数较多的集群，增加专有master，提升集群稳定性ES集群的元信息管理、index的增删操作、节点的加入剔除等集群管理的任务都是由master节点来负责的，master节点定期将最新的集群状态广播至各个节点。所以，master的稳定性对于集群整体的稳定性是至关重要的。当集群的节点数量较大时（比如超过30个节点），集群的管理工作会变得复杂很多。此时应该创建专有master节点，这些节点只负责集群管理，不存储数据，不承担数据读写压力；其他节点则仅负责数据读写，不负责集群管理的工作。 这样把集群管理和数据的写入/查询分离，互不影响，防止因读写压力过大造成集群整体不稳定。 将专有master节点和数据节点的分离，需要修改ES的配置文件，然后滚动重启各个节点。 专有master节点的配置文件（conf/elasticsearch.yml）增加如下属性：node.master: truenode.data: falsenode.ingest: false数据节点的配置文件增加如下属性（与上面的属性相反）：node.master: falsenode.data: truenode.ingest: true 控制index、shard总数量上面提到，ES的元信息由master节点管理，定期同步给各个节点，也就是每个节点都会存储一份。这个元信息主要存储在clusterstate中，如所有node元信息（indices、节点各种统计参数）、所有index/shard的元信息（mapping, location, size）、元数据ingest等。 ES在创建新分片时，要根据现有的分片分布情况指定分片分配策略，从而使各个节点上的分片数基本一致，此过程中就需要深入遍历clusterstate。当集群中的index/shard过多时，clusterstate结构会变得过于复杂，导致遍历clusterstate效率低下，集群响应迟滞。基础架构部数据库团队曾经在一个20个节点的集群里，创建了4w+个shard，导致新建一个index需要60s+才能完成。 当index/shard数量过多时，可以考虑从以下几方面改进： 降低数据量较小的index的shard数量 把一些有关联的index合并成一个index 数据按某个维度做拆分，写入多个集群 Segment Memory优化前面提到，ES底层采用Lucene做存储，而Lucene的一个index又由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。Lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在Lucene4.0以后采用的数据结构是FST (Finite State Transducer)。Lucene加载segment的时候将其全量装载到内存中，加快查询速度。这部分内存被称为SegmentMemory， 常驻内存，占用heap，无法被GC。 前面提到，为利用JVM的对象指针压缩技术来节约内存，通常建议JVM内存分配不要超过32G。当集群的数据量过大时，SegmentMemory会吃掉大量的堆内存，而JVM内存空间又有限，此时就需要想办法降低SegmentMemory的使用量了，常用方法有下面几个： 定期删除不使用的index 对于不常访问的index，可以通过close接口将其关闭，用到时再打开 通过force_merge接口强制合并segment，降低segment数量 基础架构部数据库团队在此基础上，对FST部分进行了优化，释放高达40%的Segment Memory内存空间。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"启动ELK脚本命令","date":"2021-07-14T15:37:17.000Z","path":"wiki/启动ELK脚本命令/","text":"esuser 授权chown -R esuser /usr/local/elasticsearch/* elastcisearch 启动脚本nohup ./elasticsearch-7.10.0/bin/elasticsearch &gt;&gt; ./elasticsearch-7.10.0/nohup.out 2&gt;&amp;1 &amp; kibana 启动脚本nohup ./bin/kibana &gt;&gt; ./nohup.out 2&gt;&amp;1 &amp; logstash 启动脚本nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf &gt;&gt; /usr/local/logstash/logstash-7.10.0/nohup.out 2&gt;&amp;1 &amp;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch调优实践","date":"2021-07-14T15:12:16.000Z","path":"wiki/elasticsearch调优实践-0/","text":"从性能和稳定性两方面，从linux参数调优、ES节点配置和ES使用方式三个角度入手，介绍ES调优的基本方案。当然，ES的调优绝不能一概而论，需要根据实际业务场景做适当的取舍和调整 Elasticsearch性能优化总结Elasticsearch调优实践 Linux优化关闭交换分区，防止内存置换降低性能。将 /etc/fstab 文件中包含swap的行注释掉sed -i &#39;/swap/s/^/#/&#39; /etc/fstabswapoff -a 磁盘挂载选项noatime：禁止记录访问时间戳，提高文件系统读写性能data=writeback： 不记录data journal，提高文件系统写入性能barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了nobh：关闭buffer_head，防止内核打断大块数据的IO操作mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data 对于SSD磁盘，采用电梯调度算法因为SSD提供了更智能的请求调度算法，不需要内核去做多余的调整 (仅供参考)echo noop &gt; /sys/block/sda/queue/scheduler ES节点配置conf/elasticsearch.yml文件： 适当增大写入buffer和bulk队列长度，提高写入性能和稳定性indices.memory.index_buffer_size: 15%thread_pool.bulk.queue_size: 1024 计算disk使用量时，不考虑正在搬迁的shard在规模比较大的集群中，可以防止新建shard时扫描所有shard的元数据，提升shard分配速度。cluster.routing.allocation.disk.include_relocations: false 三 ES使用方式控制字段的存储选项ES底层使用Lucene存储数据，主要包括行存（StoreFiled）、列存（DocValues）和倒排索引（InvertIndex）三部分。 大多数使用场景中，没有必要同时存储这三个部分，可以通过下面的参数来做适当调整： StoreFiled行存，其中占比最大的是source字段，它控制doc原始数据的存储。在写入数据时，ES把doc原始数据的整个json结构体当做一个string，存储为source字段。查询时，可以通过source字段拿到当初写入时的整个json结构体。 所以，如果没有取出整个原始json结构体的需求，可以通过下面的命令，在mapping中关闭source字段或者只在source中存储部分字段，数据查询时仍可通过ES的docvaluefields获取所有字段的值。注意：关闭source后， update, updatebyquery, reindex等接口将无法正常使用，所以有update等需求的index不能关闭source。 关闭 _source12345678910PUT my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; _source只存储部分字段通过includes指定要存储的字段或者通过excludes滤除不需要的字段 123456789101112131415161718PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;*.count&quot;, &quot;meta.*&quot; ], &quot;excludes&quot;: [ &quot;meta.description&quot;, &quot;meta.other.*&quot; ] &#125; &#125; &#125;&#125; docvalues 控制列存。ES主要使用列存来支持sorting, aggregations和scripts功能，对于没有上述需求的字段，可以通过下面的命令关闭docvalues，降低存储成本。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false &#125; &#125; &#125; &#125;&#125; ndex：控制倒排索引。ES默认对于所有字段都开启了倒排索引，用于查询。对于没有查询需求的字段，可以通过下面的命令关闭倒排索引。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; allES的一个特殊的字段 ES把用户写入json的所有字段值拼接成一个字符串后，做分词，然后保存倒排索引，用于支持整个json的全文检索。这种需求适用的场景较少，可以通过下面的命令将all字段关闭，节约存储成本和cpu开销。（ES 6.0+以上的版本不再支持_all字段，不需要设置）12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; fieldnames该字段用于exists查询，来确认某个doc里面有无一个字段存在。若没有这种需求，可以将其关闭。12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_field_names&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; 开启最佳压缩对于打开了上述_source字段的index，可以通过下面的命令来把lucene适用的压缩算法替换成 DEFLATE，提高数据压缩率。PUT /my_index/_settings&#123; &quot;index.codec&quot;: &quot;best_compression&quot;&#125; bulk批量写入写入数据时尽量使用下面的bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w，具体可根据业务场景选取一个适当的数量。 调整translog同步策略默认情况下，translog的持久化策略是，对于每个写入请求都做一次flush，刷新translog数据到磁盘上。这种频繁的磁盘IO操作是严重影响写入性能的，如果可以接受一定概率的数据丢失（这种硬件故障的概率很小），可以通过下面的命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。 1234567891011PUT my_index&#123;&quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;translog&quot;: &#123; &quot;sync_interval&quot;: &quot;5s&quot;, &quot;durability&quot;: &quot;async&quot; &#125; &#125;&#125;&#125; 调整refresh_interval写入Lucene的数据，并不是实时可搜索的，ES必须通过refresh的过程把内存中的数据转换成Lucene的完整segment后，才可以被搜索。默认情况下，ES每一秒会refresh一次，产生一个新的segment，这样会导致产生的segment较多，从而segment merge较为频繁，系统开销较大。如果对数据的实时可见性要求较低，可以通过下面的命令提高refresh的时间间隔，降低系统开销。 PUT my_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;refresh_interval&quot; : &quot;30s&quot; &#125; &#125;&#125; merge并发控制ES的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。默认值是Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))，当节点配置的cpu核数较高时，merge占用的资源可能会偏高，影响集群的性能，可以通过下面的命令调整某个index的merge过程的并发度： PUT /my_index/_settings&#123; &quot;index.merge.scheduler.max_thread_count&quot;: 2&#125; 写入数据不指定_id，让ES自动产生当用户显示指定id写入数据时，ES会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。这样每次写入时，ES都会耗费一定的资源做查询。如果用户写入数据时不指定doc，ES则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。 所以，在不需要通过id字段去重、update的使用场景中，写入不指定id可以提升写入速率。基础架构部数据库团队的测试结果显示，无id的数据写入性能可能比有_id的高出近一倍，实际损耗和具体测试场景相关。 routing对于数据量较大的index，一般会配置多个shard来分摊压力。这种场景下，一个查询会同时搜索所有的shard，然后再将各个shard的结果合并后，返回给用户。对于高并发的小查询场景，每个分片通常仅抓取极少量数据，此时查询过程中的调度开销远大于实际读取数据的开销，且查询速度取决于最慢的一个分片。开启routing功能后，ES会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。如果查询时指定routing，那么ES只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。 routing的使用方式如下： 12# 写入PUT my_index/my_type/1?routing=user1&#123; &quot;title&quot;: &quot;This is a document&quot;&#125;# 查询GET my_index/_search?routing=user1,user2 &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;document&quot; &#125; &#125;&#125; 为string类型的字段选取合适的存储方式存为text类型的字段（string字段默认类型为text）：做分词后存储倒排索引，支持全文检索，可以通过下面几个参数优化其存储方式： - norms：用于在搜索时计算该doc的_score（代表这条数据与搜索条件的相关度），如果不需要评分，可以将其关闭。 - indexoptions：控制倒排索引中包括哪些信息（docs、freqs、positions、offsets）。对于不太注重score/highlighting的使用场景，可以设为 docs来降低内存/磁盘资源消耗。 - fields: 用于添加子字段。对于有sort和聚合查询需求的场景，可以添加一个keyword子字段以支持这两种功能。 123456789101112131415161718192021222324252627282930313233343536 &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;norms&quot;: false, &quot;index_options&quot;: &quot;docs&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; ``` #### 存为keyword类型的字段不做分词，不支持全文检索。text分词消耗CPU资源，冗余存储keyword子字段占用存储空间。如果没有全文索引需求，只是要通过整个字段做搜索，可以设置该字段的类型为keyword，提升写入速率，降低存储成本。 设置字段类型的方法有两种：一是创建一个具体的index时，指定字段的类型；二是通过创建template，控制某一类index的字段类型。- 通过mapping指定 tags 字段为keyword类型```jsonPUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;tags&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; 通过template，指定my_index*类的index，其所有string字段默认为keyword类型PUT _template/my_template12345678910111213141516171819202122&#123; &quot;order&quot;: 0, &quot;template&quot;: &quot;my_index*&quot;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; ] &#125; &#125;, &quot;aliases&quot;: &#123; &#125; &#125; 查询时，使用query-bool-filter组合取代普通query默认情况下，ES通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。但对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。此时，可以通过query-bool-filter组合来让ES不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。 普通查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;Kimchy&quot; &#125; &#125;&#125; query-bool-filter 加速查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;Kimchy&quot; &#125; &#125; &#125; &#125;&#125; index按日期滚动，便于管理写入ES的数据最好通过某种方式做分割，存入不同的index。常见的做法是将数据按模块/功能分类，写入不同的index，然后按照时间去滚动生成index。这样做的好处是各种数据分开管理不会混淆，也易于提高查询效率。同时index按时间滚动，数据过期时删除整个index，要比一条条删除数据或deletebyquery效率高很多，因为删除整个index是直接删除底层文件，而deletebyquery是查询-标记-删除。 举例说明，假如有[modulea,moduleb]两个模块产生的数据，那么index规划可以是这样的：一类index名称是modulea + {日期}，另一类index名称是module_b+ {日期}。对于名字中的日期，可以在写入数据时自己指定精确的日期，也可以通过ES的ingest pipeline中的index-name-processor实现（会有写入性能损耗）。 按需控制index的分片数和副本数分片（shard）：一个ES的index由多个shard组成，每个shard承载index的一部分数据。 副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。 shard数量（numberofshards）设置过多或过低都会引发一些问题：shard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。 对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理。基础架构部数据库团队的推荐方案是： 对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置35个shard，numberofreplicas设置为1即可（也就是一主一从，共两副本） 。对于数据量较大（100GB以上）的index：一般把单个shard的数据量控制在（20GB50GB）让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"修改mysql表创建时间","date":"2021-07-14T13:42:53.000Z","path":"wiki/修改mysql表创建时间/","text":"修改服务器时间date -s &quot;2021-07-14 21:22:10&quot; 执行DDLalter table mirror_user comment &#39;用户表&#39;; 服务器时间修正ntpdate ntp1.aliyun.com","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"hutool导出excel","date":"2021-07-14T13:01:13.000Z","path":"wiki/hutool导出excel/","text":"如果你仅需一个Java导出excel的工具，👇就可以满足你的临时需求，当然代码下面这么写肯定是不规范的，可以稍后完善！ 添加依赖123456789101112131415161718&lt;!-- https://mvnrepository.com/artifact/cn.hutool/hutool-all --&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; 数据类1234567@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class IPData &#123; private String ip;&#125; Export方法示例12345678910public void export(List&lt;IPData&gt; rows) throws FileNotFoundException &#123; ExcelWriter writer = ExcelUtil.getWriter(true); writer.renameSheet(&quot;所有数据&quot;); //甚至sheet的名称 writer.addHeaderAlias(&quot;ip&quot;, &quot;IP&quot;); writer.write(rows, true); writer.setOnlyAlias(true); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/IP1.xlsx&quot;); writer.flush(fileOutputStream); writer.close(); &#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"git常用命令","date":"2021-07-14T01:55:55.000Z","path":"wiki/git常用命令/","text":"初始化本地仓库git init git add .git commit -m &lt;message&gt;git remote add &lt;name&gt; &lt;url&gt;git pushgit push &lt;name&gt;git push --set-upstream &lt;name&gt; &lt;branch&gt; 批量删除分支远程：git branch -r| grep &#39;ss-1&#39; | sed &#39;s/origin\\///g&#39; | xargs -I &#123;&#125; git push origin :&#123;&#125;本地：git branch -a | grep &#39;feature-re-1&#39; | xargs git branch -D","tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"}]},{"title":"Linux常用命令","date":"2021-07-13T03:44:22.000Z","path":"wiki/Linux常用命令/","text":"用户相关文件相关日志相关 linux 在文档中查找关键字个数grep -o “关键字” 文档名 | wc -l grep -o “关键字” 文档名 | sort | uniq -c 清除history记录vim .bash_history命令模式下（Esc之后输入:） 输入 set nu 每行数据前面显示行号11,20d 回车 11～20行的记录就被删除了然后命令模式下 wq 保存退出就可以了如果在此查看还是有记录，可以退出当前回话之后，再进去查看，就会不再显示删除的记录了","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"}]},{"title":"段合并","date":"2021-07-08T12:57:54.000Z","path":"wiki/段合并/","text":"参考资料 learnku.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"统计去重数据 (近似度量)","date":"2021-07-08T08:09:05.000Z","path":"wiki/统计去重数据/","text":"cardinality用法常用写法如下👇curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;months&quot; : &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125; &#125; &#125;&#125; 精度问题cardinality 度量是一个 近似算法。 它是基于 HyperLogLog++ （HLL）算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。 我们不需要理解技术细节， 但我们最好应该关注一下这个算法的 特性 ： 可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）。 小的数据集精度是非常高的。 我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 要配置精度，我们必须指定 precision_threshold 参数的值。 这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果。参考以下示例： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; ⚠️ ⚠️precision_threshold 接受 0–40000 之间的数字，更大的值还是会被当作 40000 来处理 示例会确保当字段唯一值在 100 以内时会得到非常准确的结果。尽管算法是无法保证这点的，但如果基数在阈值以下，几乎总是 100% 正确的。高于阈值的基数会开始节省内存而牺牲准确度，同时也会对度量结果带入误差。 对于指定的阈值，HLL 的数据结构会大概使用 precision_threshold * 8 字节的内存，所以就必须在牺牲内存和获得额外的准确度间做平衡。 在实际应用中， 100 的阈值可以在唯一值为百万的情况下仍然将误差维持 5% 以内 速度问题如果想要获得唯一值的数目， 通常 需要查询整个数据集合（或几乎所有数据）。 所有基于所有数据的操作都必须迅速，原因是显然的。 HyperLogLog 的速度已经很快了，它只是简单的对数据做哈希以及一些位操作。 但如果速度对我们至关重要，可以做进一步的优化。 因为 HLL 只需要字段内容的哈希值，我们可以在索引时就预先计算好。 就能在查询时跳过哈希计算然后将哈希值从 fielddata 直接加载出来。 预先计算哈希值只对内容很长或者基数很高的字段有用，计算这些字段的哈希值的消耗在查询时是无法忽略的。 尽管数值字段的哈希计算是非常快速的，存储它们的原始值通常需要同样（或更少）的内存空间。这对低基数的字符串字段同样适用，Elasticsearch 的内部优化能够保证每个唯一值只计算一次哈希。 基本上说，预先计算并不能保证所有的字段都更快，它只对那些具有高基数和/或者内容很长的字符串字段有作用。需要记住的是，预计算只是简单的将查询消耗的时间提前转移到索引时，并非没有任何代价，区别在于你可以选择在 什么时候 做这件事，要么在索引时，要么在查询时。 创建索引时添加如下配置： 1234567891011121314151617PUT /cars/&#123; &quot;mappings&quot;: &#123; &quot;transactions&quot;: &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 多值字段的类型是 murmur3 ，这是一个哈希函数。 现在当我们执行聚合时，我们使用 color.hash 字段而不是 color 字段：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 12345678910&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color.hash&quot; &#125; &#125; &#125;&#125; 现在 cardinality 度量会读取 “color.hash“ 里的值（预先计算的哈希值），取代动态计算原始值的哈希。 单个文档节省的时间是非常少的，但是如果你聚合一亿数据，每个字段多花费 10 纳秒的时间，那么在每次查询时都会额外增加 1 秒，如果我们要在非常大量的数据里面使用 cardinality ，我们可以权衡使用预计算的意义，是否需要提前计算 hash，从而在查询时获得更好的性能，做一些性能测试来检验预计算哈希是否适用于你的应用场景。。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"多桶排序","date":"2021-07-08T07:50:24.000Z","path":"wiki/多桶排序/","text":"多值桶（ terms 、 histogram 和 date_histogram ）动态生成很多桶。 Elasticsearch 是如何决定这些桶展示给用户的顺序呢？ 默认的，桶会根据 doc_count 降序排列。这是一个好的默认行为，因为通常我们想要找到文档中与查询条件相关的最大值：售价、人口数量、频率。但有些时候我们希望能修改这个顺序，不同的桶有着不同的处理方式。 内置排序这些排序模式是桶 固有的 能力：它们操作桶生成的数据 ，比如 doc_count 。 它们共享相同的语法，但是根据使用桶的不同会有些细微差别。 让我们做一个 terms 聚合但是按 doc_count 值的升序排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;_count&quot; : &quot;asc&quot; &#125; &#125; &#125; &#125;&#125;&#x27; 用关键字 _count ，我们可以按 doc_count 值的升序排序。 我们为聚合引入了一个 order 对象， 它允许我们可以根据以下几个值中的一个值进行排序： _count按文档数排序。对 terms 、 histogram 、 date_histogram 有效。 _term按词项的字符串值的字母顺序排序。只在 terms 内使用。 _key按每个桶的键值数值排序（理论上与 _term 类似）。 只在 histogram 和 date_histogram 内使用。 按度量排序有时，我们会想基于度量计算的结果值进行排序。 在我们的汽车销售分析仪表盘中，我们可能想按照汽车颜色创建一个销售条状图表，但按照汽车平均售价的升序进行排序。 我们可以增加一个度量，再指定 order 参数引用这个度量即可： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314151617181920&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125;&#x27; 计算每个桶的平均售价。 桶按照计算平均值的升序排序。 我们可以采用这种方式用任何度量排序，只需简单的引用度量的名字。不过有些度量会输出多个值。 extended_stats 度量是一个很好的例子：它输出好几个度量值。 如果我们想使用多值度量进行排序， 我们只需以关心的度量为关键词使用点式路径：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 使用 . 符号，根据感兴趣的度量进行排序。 深度度量排序在前面的示例中，度量是桶的直接子节点。平均售价是根据每个 term 来计算的。 在一定条件下，我们也有可能对 更深 的度量进行排序，比如孙子桶或从孙桶。 我们可以定义更深的路径，将度量用尖括号（ &gt; ）嵌套起来，像这样： my_bucket&gt;another_bucket&gt;metric 。 需要提醒的是嵌套路径上的每个桶都必须是 单值 的。 filter 桶生成 一个单值桶：所有与过滤条件匹配的文档都在桶中。 多值桶（如：terms ）动态生成许多桶，无法通过指定一个确定路径来识别。 目前，只有三个单值桶： filter 、 global 和 reverse_nested 。让我们快速用示例说明，创建一个汽车售价的直方图，但是按照红色和绿色（不包括蓝色）车各自的方差来排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;histogram&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;interval&quot;: 20000, &quot;order&quot;: &#123; &quot;red_green_cars&gt;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;red_green_cars&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123;&quot;color&quot;: [&quot;red&quot;, &quot;green&quot;]&#125;&#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123;&quot;extended_stats&quot;: &#123;&quot;field&quot; : &quot;price&quot;&#125;&#125; &#125; &#125; &#125; &#125; &#125;&#125;&#x27; 按照嵌套度量的方差对桶的直方图进行排序。 因为我们使用单值过滤器 filter ，我们可以使用嵌套排序。 按照生成的度量对统计结果进行排序。 本例中，可以看到我们如何访问一个嵌套的度量。 stats 度量是 red_green_cars 聚合的子节点，而 red_green_cars 又是 colors 聚合的子节点。 为了根据这个度量排序，我们定义了路径 red_green_cars&gt;stats.variance 。我们可以这么做，因为 filter 桶是个单值桶。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"过滤和聚合","date":"2021-07-08T07:33:26.000Z","path":"wiki/过滤和聚合/","text":"过滤和聚合聚合范围限定还有一个自然的扩展就是过滤。因为聚合是在查询结果范围内操作的，任何可以适用于查询的过滤器也可以应用在聚合上。 过滤如果我们想找到售价在 $10,000 美元之上的所有汽车同时也为这些车计算平均售价， 可以简单地使用一个 constant_score 查询和 filter 约束： GET /cars/transactions/_search 12345678910111213141516171819&#123; &quot;size&quot; : 0, &quot;query&quot; : &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10000 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;single_avg_price&quot;: &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 从根本上讲，使用 non-scoring 查询和使用 match 查询没有任何区别。查询（包括了一个过滤器）返回一组文档的子集，聚合正是操作这些文档。使用 filtering query 会忽略评分，并有可能会缓存结果数据等等。 过滤桶但是如果我们只想对聚合结果过滤怎么办？ 假设我们正在为汽车经销商创建一个搜索页面， 我们希望显示用户搜索的结果，但是我们同时也想在页面上提供更丰富的信息，包括（与搜索匹配的）上个月度汽车的平均售价。 这里我们无法简单的做范围限定，因为有两个不同的条件。搜索结果必须是 ford ，但是聚合结果必须满足 ford AND sold &gt; now - 1M 。 为了解决这个问题，我们可以用一种特殊的桶，叫做 filter （注：过滤桶） 。 我们可以指定一个过滤桶，当文档满足过滤桶的条件时，我们将其加入到桶内。 查询结果如下：GET /cars/transactions/_search 1234567891011121314151617181920212223242526&#123; &quot;size&quot; : 0, &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;recent_sales&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold&quot;: &#123; &quot;from&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_price&quot;:&#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 使用 过滤 桶在 查询 范围基础上应用过滤器。 avg 度量只会对 ford 和上个月售出的文档计算平均售价。 因为 filter 桶和其他桶的操作方式一样，所以可以随意将其他桶和度量嵌入其中。所有嵌套的组件都会 “继承” 这个过滤，这使我们可以按需针对聚合过滤出选择部分。 后过滤器目前为止，我们可以同时对搜索结果和聚合结果进行过滤（不计算得分的 filter 查询），以及针对聚合结果的一部分进行过滤（ filter 桶）。 我们可能会想，”只过滤搜索结果，不过滤聚合结果呢？” 答案是使用 post_filter 。 它是接收一个过滤器的顶层搜索请求元素。这个过滤器在查询 之后 执行（这正是该过滤器的名字的由来：它在查询之后 post 执行）。正因为它在查询之后执行，它对查询范围没有任何影响，所以对聚合也不会有任何影响。 我们可以利用这个行为对查询条件应用更多的过滤器，而不会影响其他的操作，就如 UI 上的各个分类面。让我们为汽车经销商设计另外一个搜索页面，这个页面允许用户搜索汽车同时可以根据颜色来过滤。颜色的选项是通过聚合获得的： GET /cars/transactions/_search 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;post_filter&quot;: &#123; &quot;term&quot; : &#123; &quot;color&quot; : &quot;green&quot; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;all_colors&quot;: &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; post_filter 元素是 top-level 而且仅对命中结果进行过滤。 查询 部分找到所有的 ford 汽车，然后用 terms 聚合创建一个颜色列表。因为聚合对查询范围进行操作，颜色列表与福特汽车有的颜色相对应。 最后， post_filter 会过滤搜索结果，只展示绿色 ford 汽车。这在查询执行过 后 发生，所以聚合不受影响。 这通常对 UI 的连贯一致性很重要，可以想象用户在界面商选择了一类颜色（比如：绿色），期望的是搜索结果已经被过滤了，而 不是 过滤界面上的选项。如果我们应用 filter 查询，界面会马上变成 只 显示 绿色 作为选项，这不是用户想要的！ ⚠️ ⚠️ ⚠️ 性能考虑（Performance consideration）当你需要对搜索结果和聚合结果做不同的过滤时，你才应该使用 post_filter ， 有时用户会在普通搜索使用 post_filter 。 不要这么做！ post_filter 的特性是在查询 之后 执行，任何过滤对性能带来的好处（比如缓存）都会完全失去。 在我们需要不同过滤时， post_filter 只与聚合一起使用。 总结选择合适类型的过滤（如：搜索命中、聚合或两者兼有）通常和我们期望如何表现用户交互有关。选择合适的过滤器（或组合）取决于我们期望如何将结果呈现给用户。 在 filter 过滤中的 non-scoring 查询，同时影响搜索结果和聚合结果。 filter 桶影响聚合。 post_filter 只影响搜索结果。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 条形图","date":"2021-07-08T07:11:57.000Z","path":"wiki/聚合-条形图/","text":"参考资料","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"嵌套桶","date":"2021-07-08T07:01:16.000Z","path":"wiki/嵌套桶/","text":"两层嵌套在我们使用不同的嵌套方案时，聚合的力量才能真正得以显现。 在前例中，我们已经看到如何将一个度量嵌入桶中，它的功能已经十分强大了。 但真正令人激动的分析来自于将桶嵌套进 另外一个桶 所能得到的结果。 现在，我们想知道每个颜色的汽车制造商的分布： GET /cars/transactions/_search 12345678910111213141516171819202122&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;make&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注意前例中的 avg_price 度量仍然保持原位。 另一个聚合 make 被加入到了 color 颜色桶中。 这个聚合是 terms 桶，它会为每个汽车制造商生成唯一的桶。 这里发生了一些有趣的事。 首先，我们可能会观察到之前例子中的 avg_price 度量完全没有变化，还在原来的位置。 一个聚合的每个 层级 都可以有多个度量或桶， avg_price 度量告诉我们每种颜色汽车的平均价格。它与其他的桶和度量相互独立。 这对我们的应用非常重要，因为这里面有很多相互关联，但又完全不同的度量需要收集。聚合使我们能够用一次数据请求获得所有的这些信息。 另外一件值得注意的重要事情是我们新增的这个 make 聚合，它是一个 terms 桶（嵌套在 colors 、 terms 桶内）。这意味着它会为数据集中的每个唯一组合生成（ color 、 make ）元组。 让我们看看返回的响应（为了简单我们只显示部分结果）： 1234567891011121314151617181920212223242526&#123; &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125; &#125; &#125;&#125; 正如期望的那样，新的聚合嵌入在每个颜色桶中。 现在我们看见按不同制造商分解的每种颜色下车辆信息。 最终，我们看到前例中的 avg_price 度量仍然维持不变。 三层嵌套让我们回到话题的原点，在进入新话题之前，对我们的示例做最后一个修改， 为每个汽车生成商计算最低和最高的价格：GET /cars/transactions/_search 1234567891011121314151617181920212223&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;make&quot; &#125;, &quot;aggs&quot; : &#123; &quot;min_price&quot; : &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125;, &quot;max_price&quot; : &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 我们需要增加另外一个嵌套的 aggs 层级。 然后包括 min 最小度量。 以及 max 最大度量。 得到以下输出（只显示部分结果）： 12345678910111213141516171819202122232425262728293031323334353637&#123;... &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3, &quot;min_price&quot;: &#123; &quot;value&quot;: 10000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 20000 &#125; &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1, &quot;min_price&quot;: &#123; &quot;value&quot;: 80000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 80000 &#125; &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125;,... 有了这两个桶，我们可以对查询的结果进行扩展并得到以下信息： 有四辆红色车。红色车的平均售价是 $32，500 美元。其中三辆红色车是 Honda 本田制造，一辆是 BMW 宝马制造。最便宜的红色本田售价为 $10，000 美元。最贵的红色本田售价为 $20，000 美元。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 高级概念","date":"2021-07-08T06:52:35.000Z","path":"wiki/聚合-高级概念/","text":"桶桶 简单来说就是满足特定条件的文档的集合： 一个雇员属于 男性 桶或者 女性 桶 奥尔巴尼属于 纽约 桶 日期2014-10-28属于 十月 桶当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件。如果匹配到，文档将放入相应的桶并接着进行聚合操作。 桶也可以被嵌套在其他桶里面，提供层次化的或者有条件的划分方案。例如，辛辛那提会被放入俄亥俄州这个桶，而 整个 俄亥俄州桶会被放入美国这个桶。 Elasticsearch 有很多种类型的桶，能让你通过很多种方式来划分文档（时间、最受欢迎的词、年龄区间、地理位置 等等）。其实根本上都是通过同样的原理进行操作：基于条件来划分文档。 指标桶能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些指标的计算。分桶是一种达到目的的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。 大多数 指标 是简单的数学运算（例如最小值、平均值、最大值，还有汇总），这些是通过文档的值来计算。在实践中，指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据。 桶和指标的组合聚合 是由桶和指标组成的。 聚合可能只有一个桶，可能只有一个指标，或者可能两个都有。也有可能有一些桶嵌套在其他桶里面。例如，我们可以通过所属国家来划分文档（桶），然后计算每个国家的平均薪酬（指标）。 由于桶可以被嵌套，我们可以实现非常多并且非常复杂的聚合： 1.通过国家划分文档（桶） 2.然后通过性别划分每个国家（桶） 3.然后通过年龄区间划分每种性别（桶） 4.最后，为每个年龄区间计算平均薪酬（指标） 最后将告诉你每个 &lt;国家, 性别, 年龄&gt; 组合的平均薪酬。所有的这些都在一个请求内完成并且只遍历一次数据！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"elasticsearch-overview","date":"2021-07-08T03:17:58.000Z","path":"wiki/elasticsearch-overview/","text":"学习资料 https://www.codingdict.com/ https://elasticsearch.cn/ https://www.elastic.co/guide/en/ 铭毅天下","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"关于 Elasticsearch 内存占用及分配","date":"2021-07-08T02:39:16.000Z","path":"wiki/关于-Elasticsearch-内存占用及分配/","text":"Elasticsearch 和 Lucene 对内存使用情况： Elasticsearch 限制的内存大小是 JAVA 堆空间的大小，不包括Lucene 缓存倒排索引数据空间。 Lucene 中的 倒排索引 segments 存储在文件中，为提高访问速度，都会把它加载到内存中，从而提高 Lucene 性能。所以建议至少留系统一半内存给Lucene。Node Query Cache (负责缓存f ilter 查询结果)，每个节点有一个，被所有 shard 共享，filter query查询结果要么是 yes 要么是no，不涉及 scores 的计算。集群中每个节点都要配置，默认为：indices.queries.cache.size:10% Indexing Buffer 索引缓冲区，用于存储新索引的文档，当其被填满时，缓冲区中的文档被写入磁盘中的 segments 中。节点上所有 shard 共享。缓冲区默认大小： indices.memory.index_buffer_size: 10%如果缓冲区大小设置了百分百则 indices.memory.min_index_buffer_size 用于这是最小值，默认为 48mb。indices.memory.max_index_buffer_size 用于最大大小，无默认值。 segmentssegments会长期占用内存，其初衷就是利用OS的cache提升性能。只有在Merge之后，才会释放掉标记为Delete的segments，释放部分内存。 Shard Request Cache 用于缓存请求结果，但之缓存request size为0的。比如说 hits.total, aggregations 和 suggestions.默认最大为indices.requests.cache.size:1% Field Data Cache 字段缓存重要用于对字段进行排序、聚合是使用。因为构建字段数据缓存代价昂贵，所以建议有足够的内训来存储。Fielddata 是 「 延迟 」 加载。如果你从来没有聚合一个分析字符串，就不会加载 fielddata 到内存中，也就不会使用大量的内存，所以可以考虑分配较小的heap给Elasticsearch。因为heap越小意味着Elasticsearch的GC会比较快，并且预留给Lucene的内存也会比较大。。如果没有足够的内存保存fielddata时，Elastisearch会不断地从磁盘加载数据到内存，并剔除掉旧的内存数据。剔除操作会造成严重的磁盘I/O，并且引发大量的GC，会严重影响Elastisearch的性能。 默认情况下Fielddata会不断占用内存，直到它触发了fielddata circuit breaker。fielddata circuit breaker会根据查询条件评估这次查询会使用多少内存，从而计算加载这部分内存之后，Field Data Cache所占用的内存是否会超过indices.breaker.fielddata.limit。如果超过这个值，就会触发fielddata circuit breaker，abort这次查询并且抛出异常，防止OOM。 1indices.breaker.fielddata.limit:60% (默认heap的60%) (es7之后改成70%) 如果设置了indices.fielddata.cache.size，当达到size时，cache会剔除旧的fielddata。 indices.breaker.fielddata.limit 必须大于 indices.fielddata.cache.size，否则只会触发fielddata circuit breaker，而不会剔除旧的fielddata。 配置Elasticsearch堆内存Elasticsearch默认安装后设置的内存是 1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10g 注意点给es分配内存时要注意，至少要分配一半儿内存留给 Lucene。分配给 es 的内存最好不要超过 32G ，因为如果堆大小小于 32 GB，JVM 可以利用指针压缩，这可以大大降低内存的使用：每个指针 4 字节而不是 8 字节。如果大于32G 每个指针占用 8字节，并且会占用更多的内存带宽，降低了cpu性能。 还有一点， 要关闭 swap 内存交换空间，禁用swapping。频繁的swapping 对服务器来说是致命的。总结：给es JVM栈的内存最好不要超过32G，留给Lucene的内存越大越好，Lucene把所有的segment都缓存起来，会加快全文检索。 关闭交换区这应该显而易见了，但仍然需要明确的写出来：把内存换成硬盘将毁掉服务器的性能，想象一下：涉及内存的操作是需要快速执行的。如果介质从内存变为了硬盘，一个10微秒的操作变成需要10毫秒。而且这种延迟发生在所有本该只花费10微秒的操作上，就不难理解为什么交换区对于性能来说是噩梦。 最好的选择是禁用掉操作系统的交换区。可以用以下命令： 1sudo swapoff -a 来禁用，你可能还需要编辑 /etc/fstab 文件。细节可以参考你的操作系统文档。 如果实际环境不允许禁用掉 swap，你可以尝试降低 swappiness。此值控制操作系统使用交换区的积极性。这可以防止在正常情况下使用交换区，但仍允许操作系统在紧急情况下将内存里的东西放到交换区。 对于大多数Linux系统来说，这可以用 sysctl 值来配置： 1vm.swappiness = 1 # 将此值配置为1会比0好，在kernal内核的某些版本中，0可能会引起OOM异常。 最后，如果两种方法都不可用，你应该在ElasticSearch的配置中启用 mlockall.file。这允许JVM锁定其使用的内存，而避免被放入操作系统交换区。 在elasticsearch.yml中，做如下设置： 1bootstrap.mlockall: true 查看node节点数据GET /_cat/nodes?v&amp;h=id,ip,port,v,master,name,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count GET /_cat/nodes?v&amp;h=id,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size GET /_cat/nodes?v&amp;h=id,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count 参考文章","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"cross-fields跨字段查询","date":"2021-07-07T06:41:42.000Z","path":"wiki/cross-fields跨字段查询/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » cross-fields 跨字段查询","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"copy_to参数","date":"2021-07-07T06:34:42.000Z","path":"wiki/copy-to参数/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 自定义 _all 字段 Docs » Mapping parameters（映射参数） » Mapping(映射) » copy_to（合并参数）","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"字符串排序与多字段","date":"2021-07-07T03:05:31.000Z","path":"wiki/字符串排序与多字段/","text":"被解析的字符串字段也是多值字段， 但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如 fine old art ， 这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Elasticsearch 在排序过程中没有这样的信息。 你可以使用 min 和 max 排序模式（默认是 min ），但是这会导致排序以 art 或是 old ，任何一个都不是所希望的。 为了以字符串字段进行排序，这个字段应仅包含一项： 整个 not_analyzed 字符串。 但是我们仍需要 analyzed 字段，这样才能以全文进行查询 一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段： analyzed 用于搜索， not_analyzed 用于排序 但是保存相同的字符串两次在 _source 字段是浪费空间的。 我们真正想要做的是传递一个 单字段 但是却用两种方式索引它。所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个 fields 参数 该参数允许你转化一个简单的映射如： 1234&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;&#125; 为一个多字段映射如： 12345678910&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; tweet 主字段与之前的一样: 是一个 analyzed 全文字段。 新的 tweet.raw 子字段是 not_analyzed. 现在，至少只要我们重新索引了我们的数据，使用 tweet 字段用于搜索，tweet.raw 字段用于排序：curl -X GET &quot;localhost:9200/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;, &quot;sort&quot;: &quot;tweet.raw&quot;&#125;","tags":[{"name":"elasicsearch","slug":"elasicsearch","permalink":"http://example.com/tags/elasicsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"推荐系统-Overview","date":"2021-07-07T02:08:05.000Z","path":"wiki/推荐系统-Overview/","text":"博客资料 深度解析京东个性化推荐系统演进史 用 Mahout 和 Elasticsearch 实现推荐系统 美团推荐算法实践 58同城推荐系统设计与实现 微博推荐系统的架构演进之路 Flink 在小红书推荐系统中的应用 小红书大数据在推荐系统中的应用 快看漫画个性化推荐探索与实践 数据仓库系列篇——唯品会大数据架构 推荐系统基本概念和架构 PAI平台搭建企业级个性化推荐系统 - Aliyun 蘑菇街推荐工程实践 参考资料","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}],"categories":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"}]},{"title":"flink 提交任务","date":"2021-07-06T15:57:04.000Z","path":"wiki/flink-提交任务/","text":"下面演示如何通过admin页面提交任务 👇 准备task jar1234567891011121314151617181920public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 执行mvn install -DskipTest 可以得到相应的jar admin提交jar 提交完jar包之后，需要设置相关参数，这个根据自己的实际情况来设置，下面是参考样例： Enter Class : com.ibli.flink.StreamWordCount也就是程序入口，我们这是写了一个main方法，如果是程序的话，可以写对应bootstrap的启动类 Program Arguments : –host localhost –port 7777 点击 submit 之后查看提交的任务状态 查看任务 可以看到是有两个任务，并且都是在执行状态；点击一个任务，还可以查看任务详情信息，和一些其他的信息，非常全面； 查看运行时任务列表 查看任务管理列表 点击任务可以跳转到详情页面 👇 下面是执行日志 我们还可以看到任务执行的标准输出结果✅ 任务源数据通过nc 输入数据，由程序读取7777端口输入流并解析数据 123gaolei:geekibli gaolei$ nc -lk 7777hello javahello flink 取消任务如下 再次查看已完成任务列表 如下：","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"apache-flink-overview","date":"2021-07-06T15:39:54.000Z","path":"wiki/apache-flink-overview/","text":"学习初衷推荐系统数据需要实时处理，使用Apache Flink实时计算用户数据，分析用户行为，达到实时业务数据分析和实现业务相关推荐； 学习资料 ashiamd.github.io 尚硅谷2021最新Java版Flink 武老师清华硕士，原IBM-CDL负责人 Apache Flink® — Stateful Computations over Data Streams Apache Flink® - 数据流上的有状态计算","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"most_fields类型","date":"2021-07-06T12:54:23.000Z","path":"wiki/most-fields类型/","text":"多字段映射首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍： DELETE /my_index 1234567891011121314151617181920PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; title 字段使用 english 英语分析器来提取词干。 title.std 字段使用 standard 标准分析器，所以没有词干提取。 接着索引一些文档： 12345PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; 这里用一个简单 match 查询 title 标题字段是否包含 jumping rabbits （跳跃的兔子）： 12345678GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;jumping rabbits&quot; &#125; &#125;&#125; 因为有了 english 分析器，这个查询是在查找以 jump 和 rabbit 这两个被提取词的文档。两个文档的 title 字段都同时包括这两个词，所以两个文档得到的评分也相同： 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125; ]&#125; 如果只是查询 title.std 字段，那么只有文档 2 是匹配的。尽管如此，如果同时查询两个字段，然后使用 bool 查询将评分结果 合并 ，那么两个文档都是匹配的（ title 字段的作用），而且文档 2 的相关度评分更高（ title.std 字段的作用）： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; 我们希望将所有匹配字段的评分合并起来，所以使用 most_fields 类型。这让 multi_match 查询用 bool 查询将两个字段语句包在里面，而不是使用 dis_max (最佳字段) 查询。 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8226396, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.10741998, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125; ]&#125; 文档 2 现在的评分要比文档 1 高。 用广度匹配字段 title 包括尽可能多的文档——以提升召回率——同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。 每个字段对于最终评分的贡献可以通过自定义值 boost 来控制。比如，使 title 字段更为重要，这样同时也降低了其他信号字段的作用： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title^10&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; title 字段的 boost 的值为 10 使它比 title.std 更重要。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 多数字段","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"multi_match 查询","date":"2021-07-06T12:37:26.000Z","path":"wiki/multi-match-查询/","text":"multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。 📒 📒 📒 multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即：best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。 默认情况下，查询的类型是 best_fields ，这表示它会为每个字段生成一个 match 查询，然后将它们组合到 dis_max 查询的内部，如下： 1234567891011121314151617181920212223&#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, ], &quot;tie_breaker&quot;: 0.3 &#125;&#125; 上面这个查询用 multi_match 重写成更简洁的形式： 123456789&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;body&quot; ], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125;&#125; ⚠️ ⚠️ ⚠️ best_fields 类型是默认值，可以不指定。 如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。 查询字段名称的模糊匹配字段名称可以用 模糊匹配 的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中，例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: &quot;*_title&quot; &#125;&#125; 提升单个字段的权重可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: [ &quot;*_title&quot;, &quot;chapter_title^2&quot; ] &#125;&#125; chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » multi_match 查询 Elasticsearch Guide [7.x] » Query DSL » Full text queries » Multi-match query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"dis_max查询","date":"2021-07-06T12:09:23.000Z","path":"wiki/dis-max查询/","text":"假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例： 1234567891011PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;&#125; 用户输入词组 Brown fox 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词： 现在运行以下 bool 查询： 12345678910&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 但是我们发现查询的结果是文档 1 的评分更高： 为了理解导致这样的原因，需要回想一下 bool 是如何计算评分的： 它会执行 should 语句中的两个查询。加和两个查询的评分。乘以匹配语句的总数。除以所有语句总数（这里为：2）。 文档 1 的两个字段都包含 brown 这个词，所以两个 match 语句都能成功匹配并且有一个评分。文档 2 的 body 字段同时包含 brown 和 fox 这两个词，但 title 字段没有包含任何词。这样， body 查询结果中的高分，加上 title 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。 在本例中， title 和 body 字段是相互竞争的关系，所以就需要找到单个 最佳匹配 的字段。 如果不是简单将每个字段的评分结果加在一起，而是将 最佳匹配 字段的评分作为查询的整体评分，结果会怎样？这样返回的结果可能是： 同时 包含 brown 和 fox 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。 dis_max 查询不使用 bool 查询，可以使用 dis_max 即分离 最大化查询 （Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ： 12345678910&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 得到我们想要的结果为： Top-level parameters for dis_maxedit queries(Required, array of query objects) Contains one or more query clauses. Returned documents must match one or more of these queries. If a document matches multiple queries, Elasticsearch uses the highest relevance score. tie_breaker(Optional, float) Floating point number between 0 and 1.0 used to increase the relevance scores of documents matching multiple query clauses. Defaults to 0.0. You can use the tie_breaker value to assign higher relevance scores to documents that contain the same term in multiple fields than documents that contain this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields. If a document matches multiple clauses, the dis_max query calculates the relevance score for the document as follows: Take the relevance score from a matching clause with the highest score.Multiply the score from any other matching clauses by the tie_breaker value.Add the highest score to the multiplied scores.If the tie_breaker value is greater than 0.0, all matching clauses count, but the clause with the highest score counts most. dis_max，只是取分数最高的那个query的分数而已，完全不考虑其他query的分数，这种一刀切的做法，可能导致在有其他query的影响下，score不准确的情况，这时为了使用结果更准确，最好还是要考虑到其他query的影响;使用 tie_breaker 将其他query的分数也考虑进去, tie_breaker 参数的意义，将其他query的分数乘以tie_breaker，然后综合考虑后与最高分数的那个query的分数综合在一起进行计算，这样做除了取最高分以外，还会考虑其他的query的分数。tie_breaker的值，设置在在0~1之间，是个小数就行，没有固定的值 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 最佳字段 Elasticsearch中文文档 Elasticsearch Guide [7.x] » Query DSL » Compound queries » Disjunction max query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch被破坏的相似度","date":"2021-07-06T11:50:11.000Z","path":"wiki/elasticsearch被破坏的相似度/","text":"在讨论更复杂的 多字段搜索 之前，让我们先快速解释一下为什么只在主分片上 创建测试索引 。 用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤：用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。 为了理解为什么会这样，可以设想，我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 foo 。可能是分片 1 有其中 3 个 foo 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。 在 什么是相关度？中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 词频/逆向文档频率 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 逆向文档频率 将 某个词在索引内所有文档出现的百分数 考虑在内，出现的频率越高，它的权重就越低。 但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。 因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 foo 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， foo 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。 在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。 为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 match 查询 里介绍的那样，如果只有一个分片，那么本地的 IDF 就是 全局的 IDF。 第二个方式就是在搜索请求后添加 ?search_type=dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。 不要在生产环境上使用 dfs_query_then_fetch 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。 参考资料 Elasticsearch: 权威指南 » 基础入门 » 排序与相关性 » 什么是相关性? Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 被破坏的相关度！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"canal同步es后部分字段为null","date":"2021-07-06T08:11:15.000Z","path":"wiki/canal同步es后部分字段为null/","text":"现象 配置文件如下： 123456789101112131415dataSourceKey: defaultDS # 源数据源的key, 对应上面配置的srcDataSources中的值destination: example # cannal的instance或者MQ的topicgroupId: g1 # 对应MQ模式下的groupId, 只会同步对应groupId的数据esMapping: _index: rd_member_fans_info # es 的索引名称 _type: _doc # es 的doc名称 _id: _id # es 的_id, 如果不配置该项必须配置下面的pk项_id则会由es自动分配# pk: id # 如果不需要_id, 则需要指定一个属性为主键属性# # sql映射 sql: &#x27;SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.create_time,t.update_time,t.`status` FROM rd_member_fans_info t&#x27;# objFields:# _labels: array:; # 数组或者对象属性, array:; 代表以;字段里面是以;分隔的# _obj: object # json对象 etlCondition: &quot;where t.update_time&gt;=&#123;&#125;&quot; # etl 的条件参数 commitBatch: 3000 # 提交批大小 ⚠️ ⚠️sql执行是没有问题的！ canal-adapter 获取binlog数据也没有问题，显示日志如下： 12021-07-06 15:39:24.588 [pool-1-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:3,&quot;redtom_id&quot;:1,&quot;fans_redtom_id&quot;:1,&quot;fans_username&quot;:&quot;1&quot;,&quot;fans_introduce&quot;:&quot;1&quot;,&quot;fans_avatar&quot;:&quot;1&quot;,&quot;is_each_following&quot;:1,&quot;follow_channel&quot;:1,&quot;create_time&quot;:1625556851000,&quot;update_time&quot;:1625556851000,&quot;status&quot;:2&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625557164000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;rd_member_fans_info&quot;,&quot;ts&quot;:1625557164587,&quot;type&quot;:&quot;INSERT&quot;&#125; 然后看一下我创建索引的mapping 解决方法调整sql如下： SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.status as is_deleted , t.create_time,t.update_time FROM rd_member_fans_info t 调整了那些东西呢？ status 的顺序提前而已！ 测试执行一下命令：curl http://127.0.0.1:8081/etl/es7/rd_member_fans_info.yml -X POST canal-adapter 日志如下： 122021-07-06 16:21:33.519 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member_fans_info2021-07-06 16:21:33.527 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - 数据全量导入完成, 一共导入 3 条数据, 耗时: 7 查看es数据：","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch-analyzer","date":"2021-07-06T07:02:01.000Z","path":"wiki/elasticsearch-analyzer/","text":"测试常见分析器GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 12345&#123; &quot;analyzer&quot;: &quot;english&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;stop&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; 默认分析器虽然我们可以在字段层级指定分析器，但是如果该层级没有指定任何的分析器，那么我们如何能确定这个字段使用的是哪个分析器呢？ 分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下： 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 在搜索时，顺序有些许不同： 查询自己定义的 analyzer ，否则 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 有时，在索引时和搜索时使用不同的分析器是合理的。我们可能要想为同义词建索引（例如，所有 quick 出现的地方，同时也为 fast 、 rapid 和 speedy 创建索引）。但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 quick 、 fast 、 rapid 或 speedy 。 为了区分，Elasticsearch 也支持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。 如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样： 查询自己定义的 analyzer ，否则字段映射里定义的 search_analyzer ，否则字段映射里定义的 analyzer ，否则索引设置中名为 default_search 的分析器，默认为索引设置中名为 default 的分析器，默认为standard 标准分析器 保持简单多数情况下，会提前知道文档会包括哪些字段。最简单的途径就是在创建索引或者增加类型映射时，为每个全文字段设置分析器。这种方式尽管有点麻烦，但是它让我们可以清楚的看到每个字段每个分析器是如何设置的。 通常，多数字符串字段都是 not_analyzed 精确值字段，比如标签（tag）或枚举（enum），而且更多的全文字段会使用默认的 standard 分析器或 english 或其他某种语言的分析器。这样只需要为少数一两个字段指定自定义分析：或许标题 title 字段需要以支持 输入即查找（find-as-you-type） 的方式进行索引。 可以在索引级别设置中，为绝大部分的字段设置你想指定的 default 默认分析器。然后在字段级别设置中，对某一两个字段配置需要指定的分析器。 📒 📒 📒对于和时间相关的日志数据，通常的做法是每天自行创建索引，由于这种方式不是从头创建的索引，仍然可以用 索引模板（Index Template） 为新建的索引指定配置和映射。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 控制分析","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"Parameter index out of range (1 > number of parameters, which is 0).","date":"2021-07-06T05:03:16.000Z","path":"wiki/Parameter-index-out-of-range-1-number-of-parameters-which-is-0/","text":"问题记录123456789101112132021-07-06 12:39:31.179 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:39:31.186 [http-nio-8081-exec-2] ERROR com.alibaba.otter.canal.client.adapter.support.Util - sqlRs has error, sql: SELECT COUNT(1) FROM ( select t.redtom_id as id, t.username, t.nickname, t.avatar, t.status, t.mobile, t.mobile_region_no, t.email, t.gender, t.password,t.salt,t.birthday,t.introduce,t.country,t.region,t.level,t.is_vip,t.follows ,t.fans,t.likes_num, t.collects_num, t.instagram_account, t.youtube_account, t.facebook_account, t.twitter_account,t.create_ip, t.create_time,t.update_time from rd_member r where t.create_time&gt;=&#x27;&#123;0&#125;&#x27;) _CNT2021-07-06 12:39:31.188 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0).java.lang.RuntimeException: java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0). at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:65) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:62) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] 如何解决我执行的操作如下：👇curl http://127.0.0.1:8081/etl/es7/customer.yml -X POST -d &quot;params=2019-08-31 00:00:00&quot; 但是我的 es7/rd_member.yml的配置文件如下： etlCondition:&quot;where a.c_time&gt;=&#39;&#123;0&#125;&#39;&quot; # etl 的条件参数 应该改成：etlCondition:&quot;where a.c_time&gt;=&#123;&#125;&quot; # etl 的条件参数","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"field name is null or empty","date":"2021-07-06T04:53:44.000Z","path":"wiki/field-name-is-null-or-empty/","text":"canal adapter 报错信息123456789101112131415161718192021222021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO o.a.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring FrameworkServlet &#x27;dispatcherServlet&#x27;2021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization started2021-07-06 12:46:31.968 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization completed in 9 ms2021-07-06 12:46:31.995 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:46:32.027 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - field name is null or emptyjava.lang.IllegalArgumentException: field name is null or empty at org.elasticsearch.index.query.BaseTermQueryBuilder.&lt;init&gt;(BaseTermQueryBuilder.java:113) ~[na:na] at org.elasticsearch.index.query.TermQueryBuilder.&lt;init&gt;(TermQueryBuilder.java:75) ~[na:na] at org.elasticsearch.index.query.QueryBuilders.termQuery(QueryBuilders.java:202) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.lambda$executeSqlImport$1(ESEtlService.java:141) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:60) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.executeSqlImport(ESEtlService.java:64) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:105) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) ~[na:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke 问题排查操作是向数据库中插入一条数据，通过canal-adapter同步到elasticsearch中，接口发生以上错误！现象是canal-adapter检测到和mysql的数据变化，但是同步到es的时候发生了错误；猜想大概是某个为空导致存到es的时候发生异常； 然后查看es7下的mapping配置： 发现我的sql查id的时候写错了，别名应该写成_id,对应elasticsearch的_id","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"elasticsearch映射","date":"2021-07-05T14:54:05.000Z","path":"wiki/elasticsearch映射/","text":"Elasticsearch 支持如下简单域类型： 字符串: string （es7之后编程text） 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 查看索引的mappingGET /gb/_mapping/tweet 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 自定义域映射尽管在很多情况下基本域数据类型已经够用，但你经常需要为单独域自定义映射，特别是字符串域。自定义映射允许你执行下面的操作： 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： 12345&#123; &quot;number_of_clicks&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;&#125; 默认， string (text) 类型域会被认为包含全文。就是说，它们的值在索引前，会通过一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。 string 域映射的两个最重要属性是 index 和 analyzer 。 indexindex 属性控制怎样索引字符串。它可以是下面三个值： analyzed首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。 no不索引这个域。这个域不会被搜索到。 (比如一些隐私信息) string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ： ⚠️ ⚠️其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。 analyzer对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english; 123456&#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 更新映射当你首次创建一个索引的时候，可以指定类型的映射。你也可以使用 /_mapping 为新类型（或者为存在的类型更新映射）增加映射。⚠️ ⚠️尽管你可以 增加 一个存在的映射，你不能 修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。 我们可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed 。 为了描述指定映射的两种方式，我们先删除 gd 索引：DELETE /gb然后创建一个新索引，指定 tweet 域使用 english 分析器： 12345678910111213141516171819202122PUT /gb &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 稍后，我们决定在 tweet 映射增加一个新的名为 tag 的 not_analyzed 的文本域，使用 _mapping ： 123456789PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中 测试映射你可以使用 analyze API 测试字符串域的映射。比较下面两个请求的输出： 1234567891011GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125; tweet 域产生两个词条 black 和 cat ， tag 域产生单独的词条 Black-cats 。换句话说，我们的映射正常工作。 参考资料 Elasticsearch权威指南","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"分析与分析器","date":"2021-07-05T14:43:44.000Z","path":"wiki/分析与分析器/","text":"分析包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 ，之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。 内置分析器但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：&quot;Set the shape to semi-transparent by calling set_trans(5)&quot; 标准分析器标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。 什么时候使用分析器当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。 全文查询，理解每个域是如何定义的，因此它们可以做正确的事： 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。 测试分析器有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本： 12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条： 12345678910111213141516171819202122232425&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。 每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于​keep_types token 过滤器​。 analyze API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。 你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域—​不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。 要做到这一点，我们必须手动指定这些域的映射。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"倒排索引","date":"2021-07-05T14:17:00.000Z","path":"wiki/倒排索引/","text":"Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容： The quick brown fox jumped over the lazy dogQuick brown foxes leap over lazy dogs in summer为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。 如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如： Quick 可以小写化为 quick 。foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的， dogs 可以为提取为 dog 。jumped 和 leap 是同义词，可以索引为相同的单词 jump 。现在索引看上去像这样：这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！ 这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch分页查询","date":"2021-07-05T14:08:23.000Z","path":"wiki/elasticsearch分页查询/","text":"和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size显示应该返回的结果数量，默认是 10from显示应该跳过的初始结果数量，默认是 0如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果： GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 ⚠️ ⚠️ ⚠️考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。 在分布式系统中深度分页 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页—​结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因 参考资料 elasticsearch权威指南 干货 | 全方位深度解读 Elasticsearch 分页查询 Paginate search results","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"多索引多类型搜索","date":"2021-07-05T14:02:34.000Z","path":"wiki/多索引多类型搜索/","text":"如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。 然而，经常的情况下，你想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示： /_search在所有的索引中搜索所有的类型/gb/_search在 gb 索引中搜索所有的类型/gb,us/_search在 gb 和 us 索引中搜索所有的文档/g*,u*/_search在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search在所有的索引中搜索 user 和 tweet 类型当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的—​只是会涉及到更多的分片。 注意 ⚠️搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch重要配置","date":"2021-07-05T13:22:32.000Z","path":"wiki/elasticsearch重要配置/","text":"虽然Elasticsearch仅需要很少的配置，但有许多设置需要手动配置，并且在进入生产之前绝对必须进行配置。 path.data 和 path.logscluster.namenode.namebootstrap.memory_locknetwork.hostdiscovery.zen.ping.unicast.hostsdiscovery.zen.minimum_master_nodespath.data 和 path.logs如果使用.zip或.tar.gz归档，则数据和日志目录是$ES_HOME的子文件夹。 如果这些重要的文件夹保留在其默认位置，则存在将Elasticsearch升级到新版本时被删除的高风险。 在生产使用中，肯定得更改数据和日志文件夹的位置： 123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch RPM和Debian发行版已经使用数据和日志的自定义路径。 path.data 设置可以设置为多个路径，在这种情况下，所有路径将用于存储数据（属于单个分片的文件将全部存储在同一数据路径上）： 12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 cluster.name节点只能在群集与群集中的所有其他节点共享其cluster.name时才能加入群集。 默认名称为elasticsearch，但您应将其更改为描述集群用途的适当名称。cluster.name: logging-prod确保不要在不同的环境中重复使用相同的集群名称，否则可能会导致加入错误集群的节点。 node.name默认情况下，Elasticsearch将使用随机生成的uuid的第一个字符作为节点id。 请注意，节点ID是持久化的，并且在节点重新启动时不会更改，因此默认节点名称也不会更改。配置一个更有意义的名称是值得的，这是重启节点后也能一直保持的优势：node.name: prod-data-2node.name也可以设置为服务器的HOSTNAME，如下所示： 12node.name: $&#123;HOSTNAME&#125;bootstrap.memory_lock 没有JVM被交换到磁盘上这事对于节点的健康来说是至关重要的。一种实现方法是将bootstrap.memory_lock设置为true。要使此设置生效，需要首先配置其他系统设置。 有关如何正确设置内存锁定的更多详细信息，请参阅启用bootstrap.memory_lock。 network.host默认情况下，Elasticsearch仅仅绑定在本地回路地址——如：127.0.0.1与[::1]。这在一台服务器上跑一个开发节点是足够的。提示 事实上，多个节点可以在单个节点上相同的$ES_HOME位置一同运行。这可以用于测试Elasticsearch形成集群的能力,但这种配置方式不推荐用于生产环境。 为了将其它服务器上的节点形成一个可以相互通讯的集群，你的节点将不能绑定在一个回路地址上。 这里有更多的网路配置，通常你只需要配置network.host：network.host: 192.168.1.10network.host也可以配置成一些能识别的特殊的值，譬如：_local_、_site、_global_，它们可以结合指定:ip4与ip6来使用。更多相信信息请参见：网路配置 重要 👇 一旦你自定义了network.host的配置，Elasticsearch将假设你已经从开发模式转到了生产模式，并将升级系统检测的警告信息为异常信息。更多信息请参见：开发模式vs生产模式 discovery.zen.ping.unicast.hosts（单播发现）开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的回路地址，并扫描9300年到9305的端口去连接同一机器上的其他节点,试图连接到相同的服务器上运行的其他节点。它提供了不需要任何配置就能自动组建集群的体验。当与其它机器上的节点要形成一个集群时，你需要提供一个在线且可访问的节点列表。像如下来配置： 1234discovery.zen.ping.unicast.hosts: - 192.168.1.10:9300 - 192.168.1.11 #① - seeds.mydomain.com #② ① 未指定端口时，将使用默认的transport.profiles.default.port值，如果此值也为设置则使用transport.tcp.port ② 主机名将被尝试解析成能解析的多个IP discovery.zen.minimum_master_nodes为防止数据丢失，配置discovery-zen-minimum_master_nodes将非常重要，他规定了必须至少要有多少个master节点才能形成一个集群。没有此设置时，一个集群在发生网络问题是可能会分裂成多个集群——脑裂——这将导致数据丢失。更多详细信息请参见：通过minimum_master_nodes避免脑裂为避免脑裂，你需要根据master节点数来设置法定人数：(master_eligible_nodes / 2) + 1换句话说，如果你有三个master节点，最小的主节点数因被设置为(3/2)+1或者是2discovery.zen.minimum_master_nodes: 2 参考资料 elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch操作索引","date":"2021-07-05T13:11:01.000Z","path":"wiki/elasticsearch操作索引/","text":"创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 查看索引的mappingGET yj_visit_data/_mapping 1234567891011121314151617181920212223242526&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查询所有GET yj_visit_data/_search 12345&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 删除所有POST yj_visit_data/_delete_by_query 123456&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125; 通过文章删除POST yj_visit_data/_delete_by_query 1234567&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 根据文章查询GET yj_visit_data/_search 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 修改索引1234POST customer/_doc/1&#123; &quot;name&quot;:&quot;2&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch基础api","date":"2021-07-05T12:53:09.000Z","path":"wiki/elasticsearch基础cat_api/","text":"cat API集群健康状态GET _cat/health?v&amp;pretty 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1625489855 12:57:35 my-application yellow 1 1 35 35 0 0 23 0 - 60.3% 或者直接在服务器上调用rest接口：curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’ 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1475247709 17:01:49 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 我们可以看到我们名为 my-application 的集群与 yellow 的 status。 无论何时我们请求集群健康，我们可以获得 green，yellow，或者 red 的 status。Green 表示一切正常（集群功能齐全）， yellow 表示所有数据可用，但是有些副本尚未分配（集群功能齐全），red 意味着由于某些原因有些数据不可用。注意，集群是 red，它仍然具有部分功能（例如，它将继续从可用的分片中服务搜索请求），但是您可能需要尽快去修复它，因为您已经丢失数据了。 另外，从上面的响应中，我们可以看到共计 1 个 node（节点）和 0 个 shard（分片），因为我们还没有放入数据的。注意，因为我们使用的是默认的集群名称（elasticsearch），并且 Elasticsearch 默认情况下使用 unicast network（单播网络）来发现同一机器上的其它节点。有可能您不小心在您的电脑上启动了多个节点，然后它们全部加入到了单个集群。在这种情况下，你会在上面的响应中看到不止 1 个 node（节点）。 查看集群分布GET _cat/nodes?v&amp;pretty 12ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name172.19.0.1 20 61 15 0.02 0.04 0.29 cdhilmrstw * redtom-es-1 查看所有索引GET _cat/indices?v&amp;pretty 1234health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open rd-logstash-2021.06.19 p5iej71MQVW12s2awNv8nw 1 1 61236 0 16.3mb 16.3mbyellow open demo_index k6VTs7tdS0ysot-rPwxG9A 1 1 1 0 5.5kb 5.5kbgreen open kibana_sample_data_flights A7c5DViGSISii8FA0dNlGw 1 0 13059 0 5.6mb 5.6mb 查看所有索引的数量GET _cat/count?v&amp;pretty 12epoch timestamp count1625490245 13:04:05 838913 磁盘分配情况GET _cat/allocation?v&amp;pretty 123shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 35 308.7mb 20.1gb 215.9gb 236.1gb 8 172.19.0.1 172.19.0.1 redtom-es-1 23 UNASSIGNED 查看shard情况GET _cat/shards?v&amp;pretty 12345678index shard prirep state docs store ip nodeyj_visit_data 0 p STARTED 0 208b 172.19.0.1 redtom-es-1yj_visit_data 0 r UNASSIGNED demo_index 0 p STARTED 1 5.5kb 172.19.0.1 redtom-es-1demo_index 0 r UNASSIGNED rbtags 0 p STARTED 0 208b 172.19.0.1 redtom-es-1.kibana_1 0 p STARTED 280 11.5mb 172.19.0.1 redtom-es-1.kibana_task_manager_1 0 p STARTED 5 5.8mb 172.19.0.1 redtom-es-1 yj_visit_data 设置了一个副本分区，但是没有副节点，所以节点状态显示未分配； 参考资料 Elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"canal同步mysql数据到elasticsearch","date":"2021-07-05T03:26:50.000Z","path":"wiki/canal同步mysql数据到elasticsearch/","text":"首先安装elk推荐大家到elasic中文社区去下载 👉 【传送】⚠️ elastcisearch | logstash | kibana 的版本最好保持一直，否则会出现很多坑的，切记！ 安装ELK的步骤这里就不做介绍了，可以查看 👉 【TODO】 下载安装canal-adaptercanal github传送门 👉 【Alibaba Canal】 canal-client 模式可以参照canal给出的example项目和官方文档给出的例子来测试 依赖配置12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 创建maven项目保证canal-server 已经正确启动 👈 然后启动下面服务，操作数据库即可看到控制台的日志输出； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.redtom.canal.deploy;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import java.net.InetSocketAddress;import java.util.List;/** * @Author gaolei * @Date 2021/6/30 2:57 下午 * @Version 1.0 */@Slf4j@Componentclass CanalClient implements InitializingBean &#123; private final static int BATCH_SIZE = 1000; @Override public void afterPropertiesSet() throws Exception &#123; // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(&quot;***.***.***.***&quot;, 11111), &quot;example&quot;, &quot;canal&quot;, &quot;canal&quot;); try &#123; //打开连接 connector.connect(); //订阅数据库表,全部表 connector.subscribe(&quot;.*\\\\..*&quot;); //回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有ack的地方开始拿 connector.rollback(); while (true) &#123; // 获取指定数量的数据 Message message = connector.getWithoutAck(BATCH_SIZE); //获取批量ID long batchId = message.getId(); //获取批量的数量 int size = message.getEntries().size(); //如果没有数据 if (batchId == -1 || size == 0) &#123; try &#123; //线程休眠2秒 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; //如果有数据,处理数据 printEntry(message.getEntries()); &#125; //进行 batch id 的确认。确认之后，小于等于此 batchId 的 Message 都会被确认。 connector.ack(batchId); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; connector.disconnect(); &#125; &#125; /** * 打印canal server解析binlog获得的实体类信息 */ private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) &#123; //开启/关闭事务的实体类型，跳过 continue; &#125; //RowChange对象，包含了一行数据变化的所有特征 //比如isDdl 是否是ddl变更操作 sql 具体的ddl sql beforeColumns afterColumns 变更前后的数据字段等等 CanalEntry.RowChange rowChage; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;ERROR ## parser of eromanga-event has an error , data:&quot; + entry.toString(), e); &#125; //获取操作类型：insert/update/delete类型 CanalEntry.EventType eventType = rowChage.getEventType(); //打印Header信息 log.info(&quot;headers:&#123;&#125; &quot;, String.format(&quot;================》; binlog[%s:%s] , name[%s,%s] , eventType : %s&quot;, entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); //判断是否是DDL语句 if (rowChage.getIsDdl()) &#123; log.info(&quot;================》;isDdl: true,sql: &#123;&#125;&quot;, rowChage.getSql()); &#125; //获取RowChange对象里的每一行数据，打印出来 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; //如果是删除语句 if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); //如果是新增语句 &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); //如果是更新的语句 &#125; else &#123; //变更前的数据 log.info(&quot;-------&gt;; before&quot;); printColumn(rowData.getBeforeColumnsList()); //变更后的数据 log.info(&quot;-------&gt;; after&quot;); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; for (CanalEntry.Column column : columns) &#123; log.info(&quot; &#123;&#125; : &#123;&#125; update= &#123;&#125;&quot;, column.getName(), column.getValue(), column.getUpdated()); &#125; &#125;&#125; canal-adapter 模式adapter 配置文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546server: port: 8081spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1 batchSize: 1 retries: 0 timeout: accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 172.25.101.75:11111 canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 canal.tcp.username: canal.tcp.password: srcDataSources: defaultDS: url: jdbc:mysql://xxxx:pppp/database?useUnicode=true username: root password: pwd canalAdapters: - instance: example # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: es7 hosts: 172.25.101.75:9300 # 127.0.0.1:9200 for rest mode properties: mode: transport # or rest# # security.auth: test:123456 # only used for rest mode cluster.name: my-application# - name: kudu# key: kudu# properties:# kudu.master.address: 127.0.0.1 # &#x27;,&#x27; split multi address 我的elasticsearch是7.10.0版本的application.yml bootstrap.yml es6 es7 hbase kudu logback.xml META-INF rdb所以：👇 123cd es7biz_order.yml customer.yml mytest_user.ymlvim customer.yml customer.yml 配置文件如下： 123456789101112dataSourceKey: defaultDSdestination: examplegroupId: g1esMapping: _index: customer _id: id relations: customer_order: name: customer sql: &quot;select t.id, t.name, t.email from customer t&quot; etlCondition: &quot;where t.c_time&gt;=&#123;&#125;&quot; commitBatch: 3000 创建表结构12345678910CREATE TABLE `customer` ( `id` bigint(20) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, `email` varchar(255) DEFAULT NULL, `order_id` int(11) DEFAULT NULL, `order_serial` varchar(255) DEFAULT NULL, `order_time` datetime DEFAULT NULL, `customer_order` varchar(255) DEFAULT NULL, `c_time` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 测试canal-adapter同步效果创建一条记录122021-07-05 11:50:53.725 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;1&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457053000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457053724,&quot;type&quot;:&quot;INSERT&quot;&#125;Affected indexes: customer Elastcisearch 效果 1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;1&quot;, &quot;email&quot; : &quot;1&quot;, &quot;customer_order&quot; : &#123; &quot;name&quot; : &quot;customer&quot; &#125; &#125; &#125; ] &#125;&#125; 修改数据122021-07-05 11:54:36.402 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457275000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:[&#123;&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;customer_order&quot;:&quot;1&quot;&#125;],&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457276401,&quot;type&quot;:&quot;UPDATE&quot;&#125;Affected indexes: customer Elastcisearch 效果 删除一条数据122021-07-05 11:56:51.524 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457411000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457411523,&quot;type&quot;:&quot;DELETE&quot;&#125;Affected indexes: customer Elastcisearch 效果 参考资料 使用canal client-adapter完成mysql到es数据同步教程(包括全量和增量) es 同步问题 #1514 Github issue canal v1.1.4 文档手册 Sync es","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"mysql配置binlog","date":"2021-07-05T03:06:36.000Z","path":"wiki/binlog配置/","text":"开启binlog[mysqld]log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 查看binlog状态mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"mysql常用命令","date":"2021-07-05T03:06:36.000Z","path":"wiki/mysql常用命令/","text":"binlog相关命令mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+ 用户&amp;权限创建用户并授权root用户登录： mysql -u root -p 然后输入密码创建用户： create user &#39;yjuser&#39;@&#39;%&#39; identified by &#39;u-bx.com&#39;;授权用户只读权限： grant SELECT on mirror.* to &#39;yjuser&#39;@&#39;%&#39; IDENTIFIED by &#39;u-bx.com&#39;;刷新权限：flush privileges; 查看当前用户select User();","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"flink简单上手","date":"2021-07-04T14:23:43.000Z","path":"wiki/flink简单上手/","text":"mac 安装 flink1、执行 brew install apache-flink 命令 123456789gaolei:/ gaolei$ brew install apache-flinkUpdating Homebrew...==&gt; Auto-updated Homebrew! Updated 1 tap (homebrew/services).No changes to formulae.==&gt; Downloading https://archive.apache.org/dist/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgz######################################################################## 100.0%🍺 /usr/local/Cellar/apache-flink/1.9.1: 166 files, 277MB, built in 15 minutes 29 seconds 2、执行flink启动脚本 12/usr/local/Cellar/apache-flink/1.9.1/libexec/bin./start-cluster.sh WordCount批处理Demo创建maven项目，导入依赖 注意自己的flink版本 👇👇 12345678910111213141516171819202122&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写批处理程序123456789101112131415161718192021222324public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125; &#125; 准备数据源文件123456hello sparkhello worldhello javahello flinkhow are youwhat is your name 执行结果123456789101112(is,1)(what,1)(you,1)(flink,1)(name,1)(world,1)(hello,4)(your,1)(are,1)(java,1)(how,1)(spark,1) flink 处理流式数据1、通过 nc -lk &lt;port&gt; 打开一个socket服务，监听7777端口 用于模拟实时的流数据 2、java代码处理流式数据 123456789101112131415161718192021222324252627public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数（设置成1即单线程处理） // env.setMaxParallelism(32); // 从文件中读取数据// String inputPath = &quot;/tmp/Flink_Tutorial/src/main/resources/hello.txt&quot;;// DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 3、在终端数据数据，如下： 4、在首次启动的时候遇到一个错误 ❌Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/streaming/api/datastream/DataStream处理方法可参照 参考资料 👇 参考资料 Exception in thread “main” java.lang.NoClassDefFoundError 解决方案","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Apache Flink","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink简介/","text":"官方地址请戳👉 【传送】 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink 是一个框架和分布式处理器引擎，用于对无界和有界数据进行状态计算； Why Flink 流数据更真实地反应了我们的生活方式 传统的数据架构是基于有限数据集的 我们的目标1、低延迟 毫秒级响应2、高吞吐 能够处理海量数据 分布式3、结果的准确性和良好的容错性 Where need Flink 电商和市场营销数据报表、广告投放、业务流程需要 物联网（IOT）传感器实时数据采集和显示，实时报警，交通运输业 电信业基站流量调配 银行和金融业实时结算和通知推送、实时检测异常行为 传统数据处理架构 传统的数据处理架构如上👆CRM(用户关系系统)， Order System(订单系统), Web App (用户点击时间)，当用户出发行为之后需要系统作出响应，首先由上层的计算层处理计算逻辑，计算层的逻辑计算依赖下面的存储层，计算层计算完成之后，将响应返回给客户端。这种基于传统数据库方式无法满足高并发场景，数据库的并发量都是很低的。 分析处理流程 分析处理流程架构如上👆，数据先有传统的关系数据库，经过提取，清洗过滤等，将数据存放到数据仓库，然后通过一些sql处理，生成数据报表和一些其他的查询。 问题也很明显，实时性太差了，处理流程太长，无法满足毫秒级需求 数据来源不唯一，能满足海量数据和高并发的需求，但是无法满足实时的需求 有状态的流式处理 把当前做流式计算所需要的数据不存放在数据库中，而是简单粗暴的直接放到本地内存中； 内存不稳定？周期性的检查点，数据存盘和故障检测； lambda架构用两台系统同时保障低延迟和结果准确； 这套架构分成两个流程，上面为批处理流程，数据收集到一定程序，交给批处理器处理，最终产生一个批处理结果 下面的流程为流式处理流程，保证能快速得到结果 最终有我们在应用层根据实际问题选择具体的处理结果交给应用程序这种架构有什么缺陷？可能得到的结果是不准确的，我们可以先快速的得到一个实时计算的结果，隔一段时间之后在来看批处理产生的结果。实现两台系统和维护两套系统，成本很大； 第三代流式处理架构Apache Flink 可以完美解决上面的问题👆Strom无法满足海量数据； Sparking Stream 无法满足低延迟； 基于事件驱动 （Event-driven） 处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。 无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流&lt;/&gt; 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 其他特点 支持事件时间（event-time）和处理时间（processing-time）语义 精确一次的状态一致性保证 低延迟 每秒处理数百万个事件，毫秒级延迟 与众多常用的存储系统链接 高可用，动态扩展，支持7*24全天运行 参考资料 1、尚硅谷 2021 Flink Java版2、Apache Flink Documentation","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Flink 运行时架构","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink运行时架构/","text":"Flink运行时组件 JobManager (作业管理器)JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成： ResourceManagerResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。 DispatcherDispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。 JobMasterJobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。 TaskManager （任务管理器） Flink中的工作进程，通常在flink中会有多个TaskManager运行，每一个TaskMaganer都包含一定数量的插槽（slots）. 插槽的数量限制了TaskManager能够执行的任务数量； 启动之后，TaskManager会向资源管理器注册他的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用，JobManager就可以向插槽分配任务（tasks）来执行了 在执行的过程中，一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据。 任务提交流程 任务调度原理Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。 Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run …中运行。 可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。 思考问题🤔 怎样实现并行计算？ 多线程 并行的任务，需要占用多少solt？ 一个流处理程序，到底包含多少个任务？ Tasks 和算子链并行度（Parallelism） 一个特定算子的子任务（subtask）的个数被称之为并行度； 一般情况下，一个Stream的并行度就是其所有算子中最大的并行度。整个流也有一个并行度，就是所有算子所有任务的并行度之和；对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。链行为是可以配置的；请参考链文档以获取详细信息。 TaskManager 和 Slots","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"拥塞避免","date":"2021-07-04T08:46:13.000Z","path":"wiki/拥塞避免/","text":"拥塞避免拥塞控制的慢启动是以指数方式快速的通过试探来扩大拥塞窗口的，但是一旦发生网络丢包，则肯定是很多报文段都会都是，因为窗口时称被增长的；为了解决这种问题，需要引入– 拥塞避免 什么是拥塞避免拥塞避免为了解决慢启动下，当拥塞窗口超出网络带宽时发生的大量丢包问题，它提出一个「慢启动阈值」的概念，当拥塞窗口到达这个阈值之后，不在以指数方式增长，而选择涨幅比较缓慢的「线性增长」，计算方式： w cwnd += SMSS*SMSS/cwnd 当拥塞窗口在线性增长时发生丢包，将慢启动阈值设置为当前窗口的一半，慢启动窗口恢复初始窗口（init wnd）； i 拥塞避免和慢启动是结合使用的，当发生网络丢包是，拥塞控制采用快速重传和快速启动来解决丢包问题！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-四次挥手/断开连接","date":"2021-07-04T08:34:11.000Z","path":"wiki/TCP-四次挥手-断开连接/","text":"TCP断开连接四次挥手 开始客户端和服务端都是处理【established】状态 客户端发送「FIN」报文之后，进入FIN-WAIT-1状态 服务端收到客户端的FIN之后，恢复一个ACK，同时进入CLOSE_WAIT状态 客户端接收到ACK之后，进入到FIN-WAIT-2状态 服务端接着发送FIN报文，同时进入LAST-ACK状态 客户端接收到服务端的FIN报文之后，发送ACK报文，并进入TIME_WAIT状态 客户端在经历2个MSL时间之后，进入CLOSE状态 服务端接收到客户端的ACK之后，进入CLOSE状态 i 并不是所有的四次挥手都是上述流程，当客户端和服务端同时发送关闭连接的请求如下👇： 可以看到双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN_WAIT_1 都进入到 CLOSING 这个过度状态然后再到 TIME_WAIT。 i 挥手一定需要四次吗？ 假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？ i 为什么要有 TIME_WAIT? 断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。 w 那么为什么要等 2MSL 呢？ 就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。 假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。 i 等待 2MSL 会产生什么问题？ 如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"快速重传/快速恢复","date":"2021-07-04T08:33:35.000Z","path":"wiki/快速重传-快速恢复/","text":"快速重传和快速恢复快速重传 d 为何会接收到以个失序数据段？ 若报文丢失，将会产生连续的失序ACK段 若网络路径与设备导致数据段失序，将会产生少量的失序ACK段 若报文重复，将会产生少量的失序ACK段 当发送端发送pkt0是正常的，由于滑动窗口为满，发送方可以继续发送pkt1，pkt2； 加入pkt1发生了丢包，虽然pkt2接收端接收成功了，但是没有pkt1的数据段，接收端还是发送ACK1的确认报文； 在没有「快速重传」的情况下，发送端需要等到RTO之后，才可以重新发送pkt1 重传成功之后，接收端其实收到了pkt2之前的所有数据段，所以发送ACK3的确认报文 w 这种需要等待RTO才可以重传的方式效率是比较低的，因此需要快速重传来进行优化； 快速重传和累积确认 当发送方连续发送pkt3，pkt4，pkt5，pkt6四个数据端，但是pkt5在网络中丢包了，那后面发送的pkt6，pkt7，pkt8的确认报文都返回ACK5，希望发送方吃昂传pkt5的数据段；这个时候，发送方收到连续3个相同的确认报文，便立即重新发送pkt5的数据段； i 接收方: 当接收到一个失序数据段时，立刻发送它所期待的缺口 ACK 序列号 当接收到填充失序缺口的数据段时，立刻发 送它所期待的下一个 ACK 序列号 i 发送方 当接收到3个重复的失序 ACK 段(4个相同的失序ACK段)时，不再等待重传定时器的触发，立刻基于快速重传机制重发报文段 当pkt5重新发送并被接收端接收之后，接收端发送ACK9的确认报文，而不是再分别发送ACK6，ACK7，ACK8，这个称谓「 累计确认 」。 快速恢复 i 快速重传下一定要进入慢启动吗? 接受端收到重复ACK，意味着网络仍在流动，而如果要重新进入慢启动，会导致网络突然减少数据流，拥塞窗口恢复初始窗口，所以，「在快速恢复下发生丢包的场景下」，应该使用快速恢复，简单的讲，就是将慢启动阈值设置成当前拥塞窗口的一半，而拥塞窗口也适当放低，而不是一下字恢复到初始窗口大小； 快速恢复的流程如上图👆所示！ w 快速恢复的具体操作： 将 ssthresh 设置为当前拥塞窗口 cwnd 的一半，设当前 cwnd 为 ssthresh 加上 3*MSS 每收到一个重复 ACK，cwnd 增加 1 个 MSS 当新数据 ACK 到达后，设置 cwnd 为 ssthresh","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-拥塞控制之慢启动","date":"2021-07-04T08:33:19.000Z","path":"wiki/TCP-拥塞控制之慢启动/","text":"由于TCP是面向字节流的传输协议，可以发送不定长的字节流数据，TCP连接发送数据时会“先天性”尝试占用整个带宽，而当所有的TCP连接都尝试占用网络带宽时，就会造成网络的堵塞，而TCP慢启动算法则是为了解决这一场景； 全局思考 拥塞控制要面向整体思考，如上👆网络拓扑图，当左边的网络节点通过路由交换设备向右边的设备传输报文的时候，中间的某一链路的带宽肯定是一定的，这里假设1000M带宽，当左边R1以700Mb/s的速度向链路中发送数据，同时R2以600Mb/s的速率发送报文，那势必会有300Mb的数据报丢失；「路由交换设备基于存储转发来实现报文的发送」大量报文都是时，路由设备的缓冲队列肯定是慢的，这也会造成某些数据报在网络链路中停留时间过长，从而导致TCP通讯变慢，甚至网络瘫痪； 理想的情况下，当链路带宽占满以后，链路以最大带宽传输数据，当然显示中是不可能的，当发生轻度拥塞时，链路的吞吐量就开始下降了，发展到严重阻塞时，链路的吞吐量会严重地下降，甚至瘫痪； 那么，慢启动是如何发挥作用的呢？ 拥塞窗口 s 拥塞窗口cwnd(congestion window) 通告窗口rwnd(receiver‘s advertised window) 其实就是RCV.WND，标志在TCP首部的Window字段！ 发送窗口swnd = min(cwnd，rwnd) 前面学习滑动窗口的时候提到发送窗口大致等于接受窗口，当引入拥塞窗口时，发送窗口就是拥塞窗口和对方接受窗口的最小值 i 每收到一个ACK，cwnd扩充一倍 慢启动的窗口大小如何设置呢？如上所示，起初拥塞窗口设置成1个报文段大小，当发送端发送一个报文段并且没有发生丢包时，调整拥塞窗口为2个报文段大小，如果还没有发生丢包，一次类推，知道发生丢包停止；发送窗口以「指数」的方式扩大；慢启动是无法确知网络拥塞程度的情况下，以试探性地方式快速扩大拥塞窗口； 慢启动初始窗口慢启动的拥塞窗口真的就如上面所说的以一个报文段大小作为初始值吗？ w 慢启动初始窗口 IW(Initial Window)的变迁 1 SMSS:RFC2001(1997) 2 - 4 SMSS:RFC2414(1998) IW = min (4SMSS, max (2SMSS, 4380 bytes)) 10 SMSS:RFC6928(2013) IW = min (10MSS, max (2MSS, 14600)) w 其实在实际情况下，互联网中的网页都在10个mss左右，如果还是从1个mss开始，则会浪费3个RTT的时间；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-如何减少小报文提升网络效率","date":"2021-07-04T08:32:55.000Z","path":"wiki/TCP-如何减少小报文提升网络效率/","text":"如何减少小报文提升网络效率每一个TCP报文段都包含20字节的IP头部和20字节的TCP首部，如果报文段的数据部分很少的话，网络效率会很差； SWS(Silly Window syndrome) 糊涂窗口综合症 如上图👆所示场景，在之前的滑动窗口已经了解过，随着服务端处理连接数据能力越来越低，服务端的可用窗口不断压缩，最终导致窗口关闭； SWS 避免算法SWS 避免算法对发送方和接收方都做客 接收方 i David D Clark 算法:窗口边界移动值小于 min(MSS, 缓存/2)时，通知窗口为 0 发送方 w Nagle 算法:1、TCP_NODELAY 用于关闭 Nagle 算法2、没有已发送未确认报文段时，立刻发送数据3、存在未确认报文段时，直到:1-没有已发送未确认报文段，或者 2-数据长度达到MSS时再发送 TCP delayed acknowledgment 延迟确认实际情况下，没有携带任何数据的ACK报文也会造成网络效率低下的，因为确认报文也包含40字节的头部信息，但仅仅是为了传输ACK=1这样的信息，为了解决这种情况，TCP有一种机制，叫做延迟确认，如下👇： 当有响应数据要发送时,ack会随着响应数据立即发送给对方. 如果没有响应数据,ack的发送将会有一个延迟,以等待看是否有响应数据可以一起发送 如果在等待发送ack期间,对方的第二个数据段又到达了,这时要立即发送ack 那个延迟的时间如何设置呢？ 上面👆是Linux操作系统对于TCP延时的定义。 HZ是什呢？其实那是和操作系统的时钟相关的，具体的操作系统间各有差别；如何查看Linux操作系统下的HZ如何设置呢？ 1cat /boot/config- `-uname -r` | grep &#x27;^GONFIG_HZ=&#x27; TCP_CORK sendfile 零拷贝技术","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-滑动窗口","date":"2021-07-04T08:32:40.000Z","path":"wiki/TCP-滑动窗口/","text":"滑动窗口 i 之前学习了PAR方式的TCP超时和重传，其实在考虑发送方发送数据报的同时，也应该考虑接收方对于数据的处理能力，由此引出本次学习的主题 – 滑动窗口 发送端窗口滑动窗口按照传输数据方向分为两种，发送端窗口和接收端窗口；下面先看一下发送端窗口👇： 上图分为四个部分： 已发送并收到 Ack 确认的数据:1-31 字节 已发送未收到 Ack 确认的数据:32-45 字节 未发送但总大小在接收方处理范围内:46-51 字节 未发送但总大小超出接收方处理范围:52-字节 可用窗口和发送窗口 如上图这里可以引出两个概念：「可用窗口」和「发送窗口」 s 【 可用窗口 】： 就是上图中的第三部分，属于还未发送，但是在接收端可以处理范围内的部分；【 发送窗口 】： 就是发送端可以发送的最大报文大小，如上图中的第二部分+第三部分合成发送窗口； 可用窗口耗尽 可用窗口会在一个短暂的停留，当处于未发送并且接受端可以接受范围内的数据传输完成之后，可用窗口耗尽；当然上面仅仅说的一瞬时的状态，这个状态下，已经发送的报文段还没有确认，并且发送窗口大小没有发生变化，此时发送窗口达到最大状态； 窗口移动 如果在发送窗口中已经发送的报文段已经得到接受端确认之后，那部分数据就会被移除发送窗口，在发送窗口大小不发生变化的情况下，发送窗口向右➡️移动5个字节，因为左边已经发送的5个字节得到确认之后，被移除发送窗口； 可用窗口如何计算 再次引出三个概念： SND.WND i SND 指的是发送端，WND指的是window，也就是发送端窗口的意思 SND.UNA i UNA 就是un ACK的意思，指的是已经发送但是没有没有确认 它指向窗口的第一个字节处 SND.NXT i NXT 是next的位置，是发送方接下来要发送的位置，它指向可用窗口的第一个字节处 那就很容易得出可用窗口的大小了，计算公式如下： i Usable Window Size = SND.UNA + SND.WND - SND.NXT 接收端窗口上面介绍了发送端窗口的一些概念，下面👇是接收端窗口的学习： 已经接收并且已经确认 :28-31 字节 还未接收并且接收端可以接受:32-51 字节 还未接收并且超出接收处理能力:51-57 字节 这里引出两个概念： RCV.WND i RCV是接收端的意思，WND是接受端窗口的大小 RCV.NXT i NXT表示的是接受端接收窗口的开始位置，也就是接收方接下来处理的第一个字节； RCV.WND的大小接受端的内存以及缓冲区大小有关，在某种意义上说，接受端的窗口大小和发送端大小大致相同；接受端可接收的数据能力可以通过TCP首部的Window字段设置，但是接受端的处理能力是可能随时变化的，所以接受端和服务端的窗口大小大致是一样的； 流量控制下面👇根据一个例子来阐述流量控制，模拟一个GET请求，客户端向服务端请求一个260字节的文件，大致流程如下，比较繁琐： s 这里假设MSS和窗口的大小不发生变化，同时客户端和发送端状态如下：【 客户端 】： 发送窗口默认360字节 接收窗口设定200字节【 服务端 】： 发送窗口设定200字节 接收窗口设定360字节 Step1： 客户端发送140字节的数据到服务端 i 【客户端】发送140字节，【SND.NXT】从1-&gt;141 w 【服务端】状态不变，等待接收客户端传输的140字节 Step2: 服务端接收140字节，发送80字节响应以及ACK i 【 客户端 】发送140字节之后等待【 服务端 】的ACK w【 服务端 】可用窗口右移，【RCV.NXT】从1-&gt;141【 服务端 】发送80字节数据，【SND.NXT】从241-&gt;321 Step3: 客户端接收响应ACK，并且发送ACK i 【 客户端 】发出的140字节得到确认，【SND.UNA】右移140字节【 客户端 】接收80字节数据，【RCV.NXT】右移80字节，从241-&gt;321 Step4: 服务端发送一个280字节的文件，但是280字节超出了客户端的接收窗口，所以客户端分成两部分传输，先传输120字节； w 【 服务端 】发送120字节，【SND.NXT】向右移动120字节，从321-&gt;441 Step5: 客户端接收文件第一部分，并发送ACK i 【 客户端 】接收120字节，【RCV.NXT】从321-&gt;441 Step6：服务端接收到第二步80字节的ACK w [ 服务器 ] 80字节得到ACK 【SND.UNA】从241-&gt;321 Step7: 服务端接收到第4步的确认 w 【 服务端 】之前发送文件第一部分的120字节得到确认，【SND.UNA】右移动120，从321-&gt;441 Step8: 服务端发送文件第二部分的160字节 w 【 服务端 】： 发送160字节，【SND.NXT】向右移动160字节，从441-&gt;601 Step9: 客户端接收到文件第二部分160字节，同时发送ACK i 【 客户端 】接收160字节，【RCV.NXT】向右移动160字节，从441-&gt;601 Step10: 服务端收到文件第二部分的ACK w 【 服务端 】发送的160字节得到确认，【SND.UNA】向右一定160字节，从441-&gt;601；至此客户端收到服务端发送的完整的文件； 上面通过表格列举服务端和客户端每个状态在每个步骤的状态，如果不是很好理解，可以看如下示意图辅助理解： 客户端交互流程 服务端交互流程 上面👆是模拟一个GET请求，服务端发送一个280字节的文件给到客户端，客户端的接收窗口是200字节场景加，客户端和服务端的数据传输与交互流程，通过这个流程来学习滑动窗口的移动状态和流量控制的大致流程； 滑动窗口与操作系统缓冲区上面👆讲述的时候，都是假设窗口大小是不变的，而实际上，发送端和接受端的滑动窗口的字节数都吃存储在操作系统缓冲区的，操作系统的缓冲区受操作系统控制，当应用进程增加是，每个进程分配的内存减少，缓冲区减少，分配给每个连接的窗口就会压缩。**而且滑动窗口的大小也受应用进程读取缓冲区数据速度有关**； 应用进程读取缓冲区数据不及时造成窗口收缩step1: 客户端发送140字节 i 客户端发送到140字节之后，可用窗口收缩到220字节，发送窗口不变 Step2: 服务端接收140字节 但是应用进程仅仅读取40字节 w 服务端应用进程仅仅读取40字节，仍有100字节占用缓冲区大小，导致接受窗口收缩，服务端发送ACK报文时，在首部Window带上接收窗口的大小260 Step3: 客户端收到确认报文之后，发送窗口收缩到260 Step4: 客户端继续发送180字节数据 i 客户端发送180字节之后，可用窗口变成80字节 Step5: 服务端接收到180字节 w 假设应用程序仍然不读取这180字节，最终也导致服务端接收窗口再次收缩180字节，仅剩下80字节，在发送确认报文时，设置首部window=80 Step6: 客户端收到80字节的窗口时，调整发送窗口大小为80字节，可用窗口也是80字节 Step7: 客户端仍然发送80字节到服务端，此时可用窗口为空 Step8: 服务端应用进程继续不读区这80字节的缓冲区数据，最终导致服务端接收窗口大小为0，不能再接收任何数据，同时发送ACK报文； Step9：客户端收到确认报文之后，调整发送窗口大小为0，这个状态叫做「 窗口关闭 」 窗口收缩导致的丢包 Step1：客户端服务端开始的窗口大小都是360字节，客户端发送140字节数据 i 客户端发送140字节之后，可用窗口变成220字节 Step2：服务端应用进程骤增，进程缓存区平均分配，造成服务端接收窗口减少，从360变成240字节； w 假设接收了140字节之后，应用进程没有读取，那个可用窗口进一步压缩，变成100字节； Step3：假设同一个连接在没有收到服务端确认之后，又发送了180个字节的数据（Retramission） i 先发送了140字节，后发送了180字节，都没有得到确认，客户端可用窗口大小变成40字节 Step4：服务端收到上面👆第三步发送的180字节的数据，但是接受窗口的大小只有100字节，所以不能接收 w 服务端拒绝接收180字节 Step5：此时客户端才收到之前140字节的确认报文，才知道接收窗口发生了变化 i 客户端由于没有收到180字节的确认，加入客户端正在准备发送180字节数据，得到接受端的窗口大小是100字节之后，须强制将右侧窗口向左收缩80字节； 窗口关闭这个例子和上面的例子都发生了「 窗口关闭 」 s 窗口关闭： 发送端的发送窗口变成0的状态； 上面讲的两种情况一般不会发生的，因为操作系统不会既收缩窗口，同时减少连接缓存；而是一般先使用窗口收缩策略，之后在压缩缓冲区的方式来规避以上问题；发生窗口关闭之后，发送端不会被动的等待服务端的通知，而是会采用定时嗅探的方式去查看服务端接收窗口是否开放； Linux中对TCP缓冲区的调整方式 net.ipv4.tcp_rmem = 4096 87380 6291456 读缓存最小值、默认值、最大值，单位字节，覆盖 net.core.rmem_max net.ipv4.tcp_wmem = 4096 16384 4194304 写缓存最小值、默认值、最大值，单位字节，覆盖net.core.wmem_max net.ipv4.tcp_mem = 1541646 2055528 3083292 系统无内存压力、启动压力模式阀值、最大值，单位为页的数量 net.ipv4.tcp_moderate_rcvbuf = 1 开启自动调整缓存模式","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-RTO重传计数器的计算","date":"2021-07-04T08:32:27.000Z","path":"wiki/TCP-RTO重传计数器的计算/","text":"i 之前的文章已经介绍了TCP超时重传的过程中使用了定时器的策略，当定时器规定时间内未收到确认报文之后，就会触发报文的重传，同时定时器复位；那么定时器超时时间（RTO Retramission Timeout）是如何计算的呢？ 什么是RTT？了解RTO如何计算之前，首先明确一个概念「 RTT 」； 如上图所示，从client发送第一个「SYN」报文，到Server接受到报文，并且返回「SYN ACK」报文之后，client接受到Server的「ACK」报文之后，client所经历的时间，叫做1个RTT时间； 如何在重传下有效测量RTT？ 如上图两种情况：第一种，左侧a图所示，当一端发送的数据报丢失后要进行重传，到重传之后接收到确认报文之后，这种场景下该如何计算RTT呢？开始时间是按照第一次发送数据报时间呢还是按照重传数据报的时间呢？ w 按照常理来说，如右侧b图所示，RTT时间应该以RTT2为准； 第二种，左侧b图所示，第一次发送数据报文时，由于网络时延导致RTO时间内没有收到接收段的确认报文，发送端进行重发，但是在刚刚重发之后就收到了第一次报文的确认报文，那这种情况RTT该如何计算呢？ w 如右侧a图所示，RTT时间应该以RTT1为准； 就像上面提及的两种情况，一会以第一个RTT1为准，一会以RTT2为准，那么TCP协议如何正确的计算出RTT呢？ 使用Timestamp方式计算RTT之前的文章中在介绍TCP超时与重传的笔记中有介绍通过使用Timtstamp的方式来区分相同Seq序列号的不同报文，其实在TCP报文首部存储Timestamp的时候，会存储报文的发送时间和确认时间，如下所示： 如何计算RTO？上面👆说到了RTT如何计算，那个RTO和RTT有什么关系呢？ RTO的取值将会影响到TCP的传输效率以及网络的吞吐量； s 通常来说RTO应该略大于RTT，如果RTO小于RTT，则会造成发送端频繁重发，可能会造成网络阻塞；如果RTO设置的过大，则接受端已经收到确认报文之后的一段时间内仍然不能发送其他报文，会造成两端性能的浪费和网络吞吐量的下降； 平滑RTO网络的RTT是不断的变化的，所以计算RTO的时候，应当考虑RTO的平滑性，尽量避免RTT波动带来的干扰，以抵挡瞬时变化； 平滑RTO在文档RFC793定义，给出如下计算方式： SRTT (smoothed round-trip time) = ( α * SRTT ) + ((1 - α) * RTT) w α 从 0到 1(RFC 推荐 0.9)，越大越平滑 RTO = min[ UBOUND, max[ LBOUND, (β * SRTT) ] ] w 如 UBOUND为1分钟，LBOUND为 1 秒钟， β从 1.3 到 2 之间 这种计算方式不适用于 RTT 波动大(方差大)的场景,如果网络的RTT波动很大，会造成RTO调整不及时； 追踪RTT方差计算RTO i RFC6298(RFC2988)，其中α = 1/8， β = 1/4，K = 4，G 为最小时间颗粒: 首次计算 RTO，R为第 1 次测量出的 RTT123SRTT(smoothed round-trip time) = RRTTVAR(round-trip time variation) = R/2RTO = SRTT + max (G, K*RTTVAR) 后续计算 RTO，R’为最新测量出的 RTT123SRTT= (1-α)*SRTT+α*R’RTTVAR=(1-β)*RTTVAR+β*|SRTT-R’|RTO = SRTT + max (G, K*RTTVAR)","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP超时与重传","date":"2021-07-04T08:32:08.000Z","path":"wiki/TCP超时与重传/","text":"背景 如上图👆所示，设备A向设备B发送消息，消息在网络中会由于各种各样的问题导致丢失，那么该如何解决上述问题呢？ 采用定时器重传 i PAR：Positive Acknowledgment with Retransmission 最简单的思路是在发送方设置「 定时器 」： 当设备A发送第一条消息之后，在定时器规定的时间内，如果收到设备B的确认报文，则设备A继续发送下一个报文，同时定时器复位； 如果第一条消息发送时间超出了定时器规定的时间，则设备A将重新发送第一条消息，同时重新设置定时器； 这种方式是串型发送的，只有第一个消息发送成功之后，才可以发送下一条消息，「 效率极差 」； 并发能力PAR i 基于上述PAR效率低下的方式进行改造，在发送端采用并发+定时器的方式进行数据发送； 首先设备A可以同时发送多个消息或者报文段，每个报文段具有一个标志字段【#XX】去标志唯一，每个报文段连接具有自己的定时器； 设备B规定时间内收到设备A发送的数据之后并且设备A得到设备B的确认之后，设备A将定时器清除 同PAR一样，设备B没有在规定的时间内发送确认报文，设备A将这个报文所对应的定时器复位，重新发送这个报文 并发发送带来的问题采用并发的方式发送消息或者报文段固然提升了发送端的性能，但是发送端发送的消息可能接受端不能完全处理，这是双方报文处理速度或者效率不一致的问题； 所以对于接收端设备B，应该明确自己可能接受的数据量，并且在确认报文中同步到发送端设备A，设备A根据设备B的处理能力来调整发送数据的大小；也就是上图中的「 limit」； 继续延伸Sequment序列号和Ack序列号的设计理念或者设计初衷是「 解决应用层字节流的可靠发送 」 跟踪「应用层」的发送端数据是否送达 确定「接收端有序的」接收到「字节流」 序列号的值针对的是字节而不是报文 ⚠️⚠️⚠️ i TCP的定位就是面向字节流的！ TCP序列号如何设计的 通过TCP报文头我们可以知道，Sequment序列号包括32位长度；也就是说一个Sequment可以发送2的32次方个字节，大约4G的数量，Sequment就无法表示了，当传输的数据超过“4G”之后，如果这个连接依然要使用的话，Sequment会重新复用；Sequment复用会产生一个问题，也就是序列号回绕；👇 序列号回绕 i 序列号回绕 (Protect Against Wrapped Sequence numbers) 当一个连接要发送6G的数据是，A、B、C、D分别发送1G的数据，如果继续使用此连接，E下一次发送数据1G，Seq序列号复用，E报文段的序列号和A报文段的序列号表示相同 按照上面的逻辑继续发送数据，F报文段的Seq标志和B报文段的是一样的； 加入B报文段在发送过程中丢失了，直到接受端接收了F报文段的同时B报文段到达接受端，接受端该如何区分相同Seq序列号不同数据的报文段呢？ 其实TCP解决这个问题很简单，就是在每个报文段上添加Tcp Timestamp时间戳，类似于版本号的理念； 接收端收到相同Seq序列号的报文段是可以根据时间戳来进行区分；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP最大报文段（MSS）","date":"2021-07-04T08:31:56.000Z","path":"wiki/TCP最大报文段（MSS）/","text":"MSS产生的背景我们都知道TCP协议是运输在传输层的协议，它是面向【字节流】的传输协议；它的上层，应用层传输的数据是无限制的，但是它的下层也就是网络层和链路层由于路由等转发设备有内存等限制是不可能无限制传输任何大小的报文的，它们一定会限制报文的长度，因此 TCP协议要完成的工作是将从应用层接受到的任意长度数据，切割成多个报文段，MSS就是如何切割报文段的依据。 什么是MSSMSS（Max Segment Size）：仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879 MSS 选择目的 尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率 防止 Segment 被某个设备的 IP 层基于 MTU 拆分 s IP层基于MTU的数据拆分是效率极差的，一个报文段丢失，所有的报文段都要重传 MSS默认大小 s 默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部) MSS在什么时候使用 s 握手阶段协商 MSS 这个在TCP三次握手的文章中已经提及过了！ MSS 分类 发送方最大报文段: SMSS:SENDER MAXIMUM SEGMENT SIZE 接收方最大报文段: RMSS:RECEIVER MAXIMUM SEGMENT SIZE 在TCP常用选项中可以看到【MSS】的选项 TCP流与报文段在数据传输中的状态 从上图可以看到，左边客户端在发送字节流数据给到右边客户端，客户端发送一个连续的字节流，会在TCP层按照MSS大小规定进行拆分成多个小的报文段，分别传送到另一个客户端或者其他的接收端；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP三次握手","date":"2021-07-04T08:31:42.000Z","path":"wiki/TCP三次握手/","text":"握手🤝的目的 同步Sequence序列号 i 初始化序列号ISN （Inital Sequence Number） 交换TCP通讯的参数 i 比如最大报文段参数（MSS）、窗口比例因子（Window）、选择性确认（SACK）、制定校验和算法； 三次握手握手过程 TCP三次握手的大致流程图如上👆 使用tcpdump抓包分析三次🤝握手报文中Seq和Ack的变化 1tcpdump port 80 -c 3 -S 第一次握手🤝1IP upay.60734 &gt; 100.100.15.23.http: Flags [S], seq 3800409106, win 29200, options [mss 1460,sackOK,TS val 839851765 ecr 0,nop,wscale 7], length 0 客户端upay访问服务端80端口，发送一个「 seq=3800409106 」 ，同时标志位SYN=1，声明此次握手是要建立连接； 第二次握手🤝1IP 100.100.15.23.http &gt; upay.60734: Flags [S.], seq 1981710286, ack 3800409107, win 14600, options [mss 1440,nop,nop,sackOK,nop,wscale 7], length 0 第二次握手，服务端收到客户端的申请连接强求（SYN=1）之后，在服务端自己准备好的情况下，给客户端发送 「 ACK=1 SYN=1 」的确认报文，SYN=1同样也是声明此次报文是建立连接的报文请求； ack= 3800409107 也就是第一个客户端发给服务端的seq+1（ack是接收方下次期望接口报文的开始位置） 第三次握手握手1IP upay.60734 &gt; 100.100.15.23.http: Flags [.], ack 1981710287, win 229, length 0 客户端收到服务器返回的确认报文，确认可以进行连接，发送「 ack = 1981710287 」的确认报文，之后就完成了三次握手，TCP的连接就创建成功了，接下来双方就可以发送数据报了； TCP连接创建构成中状态的变更 首先客户端和服务端都是【CLOSED】状态，客户端发起连接请求之后，进入【SYN-SENT】状态，这个状态维持的时间很短，我们使用netstat去查看tcp连接状态的时候，基本上都不会看到这个状态，而服务端是在【LISTEN】状态，等待客户端的请求； 服务端收到客户端请求之后，发送「SYN ACK」确认报文，同时服务端进入【SYN-RECEIVED】状态，等待客户端的确认报文； 客户端收到服务端的同步确认请求之后，发送「ACK」确认报文，同时进入【ESTABLISHED】状态，准备后续的数据传输； 服务端收到三次握手最后的确认报文之后，进入【ESTABLISHED】状态，至此，一个TCP连接算是建立完成了，后面就是双方的通信了； TCB（Transmission Control Block） i 保存连接使用的源端口、目的端口、目的 ip、序号、 应答序号、对方窗口大小、己方窗口大小、tcp 状态、tcp 输入/输出队列、应用层输出队 列、tcp 的重传有关变量等 TCP性能优化和安全问题 正如我们了解的TCP三次握手🤝的流程，当有大量SYN请求到达服务端时，会进入到【SYN队列】，服务端收到第二次确认报文之后，会进入【ESTABLISHED】状态，服务端操作系统内核会将连接放入到【ACCEPT】队列中，当Nginx或者Tomcat这些应用程序在调用accept（访问内核）的时候，就是在【ACCEPT】队列中取出连接进行处理； w 由此可见，【SYN】队列和【ACCEPT】是会影响服务器连接性能的重要因素，所以对于高并发的场景下，这两个队列一定是要设置的比较大的； 如何设置SYN队列大小服务器端 SYN_RCV 状态 net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数 客户端 SYN_SENT 状态（服务端作为客户端，比如Ngnix转发等） net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数 net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围 Fast Open机制 TCP如何对连接的次数以及连接时间进行优化的呢？这里提到Fast Open机制；比如我们有一个Http Get请求，正常的三次握手🤝到收到服务端数据需要2个RTT的时间；FastOpen做出如下优化： 第一次创建连接的时候，也是要经历2个RTT时间，但是在服务端发送确认报文的时候，在报文中添加一个cookie； 等到下次客户端再需要创建请求的时候，直接将【SYN】和cookie一并带上，可以一次就创建连接，经过一个RTT客户端就可以收到服务端的数据； 如何Linux上打开TCP Fast Open net.ipv4.tcp_fastopen:系统开启 TFO 功能 0:关闭 1:作为客户端时可以使用 TFO 2:作为服务器时可以使用 TFO 3:无论作为客户端还是服务器，都可以使用 TFO SYN攻击什么是SYN攻击？ 正常的服务通讯都是由操作系统内核实现的请求报文来创建连接的，但是，可以人为伪造大量不同IP地址的SYN报文，也就是上面👆状态变更图中的SYN请求，但是收到服务端的ACK报文之后，却不发送对于服务端的ACK请求，也就是没有第三次挥手，这样会造成大量处于【SYN-RECEIVED】状态的TCP连接占用大量服务端资源，导致正常的连接无法创建，从而导致系统崩坏； SYN攻击如何查看1netstat -nap | grep SYN_RECV w 如果存在大量【SYN-RECEIVED】的连接，就是发生SYN攻击了； 如何规避SYN攻击？ net.core.netdev_max_backlog 接收自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn_backlog SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接 设置SYN Timeout 由于SYN Flood攻击的效果取决于服务器上保持的SYN半连接数，这个值=SYN攻击的频度 x SYN Timeout，所以通过缩短从接收到SYN报文到确定这个报文无效并丢弃改连接的时间，例如设置为20秒以下，可以成倍的降低服务器的负荷。但过低的SYN Timeout设置可能会影响客户的正常访问。 设置SYN Cookie (net.ipv4.tcp_syncookies = 1) 就是给每一个请求连接的IP地址分配一个Cookie，如果短时间内连续受到某个IP的重复SYN报文，就认定是受到了攻击，并记录地址信息，以后从这个IP地址来的包会被一概丢弃。这样做的结果也可能会影响到正常用户的访问。 当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接 w 由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等 TCP_DEFER_ACCEPT这个是做什么呢？ 正如上面👆操作系统内核展示图所示，内核中维护两个队列【SYN】队列和【ACCEPT】队列，只有当收到客户端的ACK报文之后，连接会进入到【ACCEPT】，同时服务器的状态是【ESTABLISHED】状态，此时操作系统并不会去激活应用进程，而是会等待，知道收到真正的data分组之后，才会激活应用进程，这是为了提高应用进程的执行效率，避免应用进程的等待； i TCP三次握手为什么不能是两次或者四次 参见文章：敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP头部","date":"2021-07-04T08:31:22.000Z","path":"wiki/TCP头部/","text":"带着问题学习 如何校验报文段是否损坏？ 如何CRC校验 seq和ack是如何计算的？ tcp校验位都有那些？ 6个 分别是什么含义？ tcp如何计算首部长度？ 偏移量 TCP Retransmission 重传？ tcp spurious retransmission 又是什么呢？ tcp dup ack 是什么？ ack与ACK有什么区别？ 分别有什么作用？ TCP头部结构 学习TCP协议首先要看一下它的报文段是如何组成的；TCP报文段组成由两部分，第一部分是报文头部，第二部分是数据部分； 先看一下报文头，也就是TCP首部的组成； 16位端口16位端口号：告知主机该报文段是来自哪里（源端口Source Port）以及传给哪个上层协议或应用程序（目的端口Destination Port）的。进行TCP通信时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号（比如DNS协议对应端口53，HTTP协议对应80，这些端口号可在/etc/services文件中找到）。 序列号（Seq）占32位，也就是4字节长度，序号范围自然也是是0~2^32-1。TCP是面向字节流的，TCP连接中传送的字节流中的每个字节都按顺序编号。整个要传送的字节流的起始序号必须要在连接建立时设置。首部中的序号字段值指的是本报文段所发送的数据的第一个字节的序号。 TCP用序列号对数据包进行标记，以便在到达目的地后重新重装，假设当前的序列号为 s，发送数据长度为 l，则下次发送数据时的序列号为 s + l。在建立连接时通常由计算机生成一个随机数作为序列号的初始值。 **这里存在一个疑问，第一次建立TCP连接的时候，网上一些博客上说seq是client随机生成的，也有的博客说是seq=1； 这里经过我抓包后，看到第一次创建TCP连接的时候，确实是1; ** 确认应答号（Ack）Ack占32位，4个字节长度，表示期望收到对方下一个报文段的序号值。 用作对另一方发送来的TCP报文段的响应。其值是收到的TCP报文段的序号值加1。假设主机A和主机B进行TCP通信，那么A发送出的TCP报文段不仅携带自己的序号，而且包含对B发送来的TCP报文段的确认号。反之，B发送出的TCP报文段也同时携带自己的序号和对A发送来的报文段的确认号。TCP的可靠性，是建立在「每一个数据报文都需要确认收到」的基础之上的。 就是说，通讯的任何一方在收到对方的一个报文之后，都要发送一个相对应的「确认报文」，来表达确认收到。 那么，确认报文，就会包含确认号。 若确认号=N，则表明：到序号N-1为止的所有数据都已正确收到。 数据偏移 Offset占 0.5 个字节 (4 位)。 这个字段实际上是指出了TCP报文段的首部长度 ，它指出了TCP报文段的数据起始处距离TCP报文的起始处有多远。 注意数据起始处和报文起始处的意思，上面👆已经写到，TCP报文段的组成有两部分，TCP报文首部和数据部分，偏移量记录的是报文段开始和数据开始的长度，也就是报文首部的长度； 一个数据偏移量 = 4 byte，由于4位二进制数能表示的最大十进制数字是 15，因此数据偏移的最大值是 60 byte，这也侧面限制了TCP首部的最大长度。 保留Reserved占 0.75 个字节 (6 位)。 保留为今后使用，但目前应置为 0。 标志位 TCP Flags标志位，一共有6个，分别占1位，共6位。 每一位的值只有 0 和 1，分别表达不同意思。 如上图是使用wireshard抓包展示截图； ACK(Acknowlegemt) ：确认序号有效 当 ACK = 1 的时候，确认号（Acknowledgemt Number）有效。 一般称携带 ACK 标志的 TCP 报文段为「确认报文段」。为0表示数据段不包含确认信息，确认号被忽略。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 设置为 1。 RST(Reset)：重置连接 当 RST = 1 的时候，表示 TCP 连接中出现严重错误，需要释放并重新建立连接。 一般称携带 RST 标志的 TCP 报文段为「复位报文段」。 SYN(SYNchronization)：发起了一个新连接 当 SYN = 1 的时候，表明这是一个请求连接报文段。 一般称携带 SYN 标志的 TCP 报文段为「同步报文段」。 在 TCP 三次握手中的第一个报文就是同步报文段，在连接建立时用来同步序号。对方若同意建立连接，则应在响应的报文段中使 SYN = 1 和 ACK = 1。 PSH (Push): 推送 当 PSH = 1 的时候，表示该报文段高优先级，接收方 TCP 应该尽快推送给接收应用程序，而不用等到整个 TCP 缓存都填满了后再交付。 FIN：释放一个连接 当 FIN = 1 时，表示此报文段的发送方的数据已经发送完毕，并要求释放 TCP 连接。一般称携带 FIN 的报文段为「结束报文段」。在 TCP 四次挥手释放连接的时候，就会用到该标志。 窗口大小 Window Size占16位。该字段明确指出了现在允许对方发送的数据量，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 窗口大小的值是指，从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量。 例如，假如确认号是701，窗口字段是 1000。这就表明，从 701 号算起，发送此报文段的一方还有接收 1000 （字节序号是 701 ~ 1700） 个字节的数据的接收缓存空间。 校验和 TCP Checksum占16位。 由发送端填充，接收端对TCP报文段执行【CRC算法】，以检验TCP报文段在传输过程中是否损坏，如果损坏这丢弃。 检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。 紧急指针 Urgent Pointer占 2 个字节。 仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。 当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 因此，紧急指针指出了紧急数据的末尾在报文段中的位置。 选项 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 下面是常用选项： MTU（最大传输单元）MTU（最大传输单元）是【链路层】中的网络对数据帧的一个限制，以以太网为例，MTU 为 1500 个字节。一个IP 数据报在以太网中传输，如果它的长度大于该 MTU 值，就要进行分片传输，使得每片数据报的长度小于MTU。分片传输的 IP 数据报不一定按序到达，但 IP 首部中的信息能让这些数据报片按序组装。IP 数据报的分片与重组是在网络层进完成的。 MSS （最大分段大小）MSS 是 TCP 里的一个概念（首部的选项字段中）。MSS 是 TCP 数据包每次能够传输的最大数据分段，TCP 报文段的长度大于 MSS 时，要进行分段传输。TCP 协议在建立连接的时候通常要协商双方的 MSS 值，每一方都有用于通告它期望接收的 MSS 选项（MSS 选项只出现在 SYN 报文段中，即 TCP 三次握手的前两次）。MSS 的值一般为 MTU 值减去两个首部大小（需要减去 IP 数据包包头的大小 20Bytes 和 TCP 数据段的包头 20Bytes）所以如果用链路层以太网，MSS 的值往往为 1460。而 Internet 上标准的 MTU 为 576，那么如果不设置，则MSS的默认值就为 536 个字节。TCP报文段的分段与重组是在运输层完成的。 seq和ack的计算逻辑 CRC校验参考资料TCP协议中的seq/ack序号是如何变化的？TCP协议详解TCP协议详解（一）：TCP头部结构TCP和UDP报文头格式TCP协议详解吃透TCP协议","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP协议","date":"2021-07-04T08:30:55.000Z","path":"wiki/TCP协议/","text":"TCP协议学习笔记📒 w 下面是本人在学习TCP协议的过程中，记录的笔记，按照学习的过程从前到后整理在这里！可能会有很多的知识没有罗列，只是记录的大概框架，如果有问题或错误，欢迎指正！ 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手 学习资料 i 敖丙Github整理的笔记 有大概10篇左右的文章，都是高质量的，原地址请点击着👉 【Github】 i 极客时间《Web协议详解与抓包实战》– 陶辉老师 这门课程专门讲解网络协议的，包括Http/Https,TLS协议，TCP协议，IP协议等； i 《计算机网络 自顶向下方法》第7版 很多名校计算机网络课程在使用的教材，非常权威！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"Welcome to GeekIBLi","date":"2021-07-04T07:44:33.000Z","path":"wiki/index/","text":"道阻 且长不错的学习网站推荐 掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io示说 「 提供了很多优质的PPT 还有很多大厂的沙龙视屏以及材料」字节跳动后端面试题集知乎 Java快速进阶通道To Be Top Javaer - Java工程师成神之路Div-wang 技术团队推荐 小米信息部技术团队有赞技术团队美团技术团队 面试题合集 2020年大厂Java面试前复习的正确姿势（800+面试题附答案解析）Java集合面试题（总结最全面的面试题） 两年学说话 一生学闭嘴","tags":[],"categories":[{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}]}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"JVM","slug":"Develop-Lan/Java/JVM","permalink":"http://example.com/categories/Develop-Lan/Java/JVM/"},{"name":"Distributed Dir","slug":"Distributed-Dir","permalink":"http://example.com/categories/Distributed-Dir/"},{"name":"theory","slug":"Distributed-Dir/theory","permalink":"http://example.com/categories/Distributed-Dir/theory/"},{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"},{"name":"多线程与并发","slug":"Develop-Lan/Java/多线程与并发","permalink":"http://example.com/categories/Develop-Lan/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"},{"name":"IO","slug":"Develop-Lan/Java/IO","permalink":"http://example.com/categories/Develop-Lan/Java/IO/"},{"name":"Spring Family","slug":"Spring-Family","permalink":"http://example.com/categories/Spring-Family/"},{"name":"Spring Framework","slug":"Spring-Family/Spring-Framework","permalink":"http://example.com/categories/Spring-Family/Spring-Framework/"},{"name":"mybatis","slug":"Spring-Family/mybatis","permalink":"http://example.com/categories/Spring-Family/mybatis/"},{"name":"Redis","slug":"DataBase/Redis","permalink":"http://example.com/categories/DataBase/Redis/"},{"name":"Kafka","slug":"Distributed-Dir/Kafka","permalink":"http://example.com/categories/Distributed-Dir/Kafka/"},{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"docker","slug":"Develop-Tools/docker","permalink":"http://example.com/categories/Develop-Tools/docker/"},{"name":"MongoDB","slug":"DataBase/MongoDB","permalink":"http://example.com/categories/DataBase/MongoDB/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"},{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"},{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"},{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"},{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"},{"name":"mybatis","slug":"mybatis","permalink":"http://example.com/tags/mybatis/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"mongodb","slug":"mongodb","permalink":"http://example.com/tags/mongodb/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"Serializable","slug":"Serializable","permalink":"http://example.com/tags/Serializable/"},{"name":"hashmap","slug":"hashmap","permalink":"http://example.com/tags/hashmap/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"},{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"elasicsearch","slug":"elasicsearch","permalink":"http://example.com/tags/elasicsearch/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"},{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}]}