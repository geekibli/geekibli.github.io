{"pages":[{"title":"Categories","date":"2021-07-04T07:22:56.334Z","path":"categories/index.html","text":""},{"title":"About","date":"2021-07-04T09:22:18.003Z","path":"about/index.html","text":""},{"title":"Tags","date":"2021-07-04T07:22:56.335Z","path":"tags/index.html","text":""}],"posts":[{"title":"elasticsearch字符串查询汇总","date":"2021-07-20T14:08:29.000Z","path":"wiki/elasticsearch字符串查询汇总/","text":"filterexistsfuzzyidsprefixregexptermtermsterms_setwildcardtext搜索 intervalmatchmatch_bool_prefixmatch_phrasematch_phrase_prefixmulti_matchcommonquery_stringsimple_query_string 参考资料 查询是否包含字符串_十九种Elasticsearch字符串搜索方式终极介绍","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch 查询值前缀不包含某个字符串","date":"2021-07-20T13:48:31.000Z","path":"wiki/elasticsearch-查询值前缀不包含某个字符串/","text":"需求 查询IP不是以11.开头的所有文档，然后获取文档访问量前100条 curl -X GET &quot;localhost:9200/yj_visit_data2,yj_visit_data3/_search?pretty&quot; -u elastic:elastic -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;11.&quot; &#125; &#125; &#125;, &#123; &quot;prefix&quot;: &#123; &quot;ip&quot;: &#123; &quot;value&quot;: &quot;1.&quot; &#125; &#125; &#125; ] &#125; &#125; ], &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;visitTime&quot;: &#123; &quot;gte&quot;: 1577808000000, &quot;lte&quot;: 1609430399000 &#125; &#125; &#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;term_article&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;ip&quot;, &quot;min_doc_count&quot;: 20, &quot;size&quot;: 10000 &#125; &#125; &#125;&#125;&#x27;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"https协议","date":"2021-07-20T08:30:45.000Z","path":"wiki/https协议/","text":"参考资料 《 HTTPS 升级指南 》 深入理解 HTTPS 原理、过程与实践 HTTPS实现原理 深入理解HTTPS原理、过程与实践","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"elasticsearch-reindex","date":"2021-07-20T03:59:18.000Z","path":"wiki/elasticsearch-reindex/","text":"reindex 常规使用 Reindex要求为源索引中的所有文档启用_source。Reindex不尝试设置目标索引，它不复制源索引的设置，你应该在运行_reindex操作之前设置目标索引，包括设置映射、碎片计数、副本等。 如下示例将把文档从twitter索引复制到new_twitter索引： 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 下面是返回值： 12345678910111213141516171819&#123; &quot;took&quot; : 299, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 2, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 就像_update_by_query一样，_reindex获取源索引的快照，但它的目标必须是不同的索引，因此不太可能发生版本冲突。可以像index API那样配置dest元素来控制乐观并发控制。仅仅省略version_type(如上所述)或将其设置为internal，都会导致Elasticsearch盲目地将文档转储到目标中，覆盖任何碰巧具有相同类型和id的文档 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot;, &quot;version_type&quot;: &quot;internal&quot; &#125;&#125;&#x27; 将version_type设置为external将导致Elasticsearch保存源文件的版本，创建任何缺失的文档，并更新目标索引中比源索引中版本更旧的文档： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;&#125;&#x27; 设置op_type=create将导致_reindex只在目标索引中创建缺失的文档。所有现有文件将导致版本冲突： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 默认情况下，版本冲突将中止_reindex进程，“conflicts”请求体参数可用于指示_reindex处理关于版本冲突的下一个文档，需要注意的是，其他错误类型的处理不受“conflicts”参数的影响，当在请求体中设置“conflicts”:“proceed”时，_reindex进程将继续处理版本冲突，并返回所遇到的版本冲突计数： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;&#x27; 返回值如下： 12345678910111213141516171819&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;total&quot; : 2, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 2, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125; 可以通过向源添加查询来限制文档。这将只复制由kimchy发出的tweet到new_twitter： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; source中的index可以是一个列表，允许你在一个请求中从多个源复制。这将从twitter和blog索引复制文档： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: [&quot;twitter&quot;,&quot;blog&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter2&quot; &#125;&#125;&#x27; 注意：Reindex API不处理ID冲突，因此最后编写的文档将“胜出”，但顺序通常是不可预测的，因此依赖这种行为不是一个好主意，相反，可以使用脚本确保id是惟一的。还可以通过设置大小来限制处理文档的数量，示例将只复制一个单一的文件从twitter到new_twitter： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 1, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 如果你想从twitter索引中获得一组特定的文档，你需要使用sort。排序会降低滚动的效率，但在某些上下文中，这样做是值得的。如果可能的话，选择一个比大小和排序更具选择性的查询。这将把10000个文档从twitter复制到new_twitter： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10000, &quot;source&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;sort&quot;: &#123; &quot;age&quot;: &quot;desc&quot; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; source部分支持搜索请求中支持的所有元素。例如，只有原始文档中的一部分字段可以使用源过滤重新索引，如下所示： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;_source&quot;: [&quot;user&quot;, &quot;_doc&quot;] &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 与_update_by_query一样，_reindex支持修改文档的脚本。与_update_by_query不同，脚本允许修改文档的元数据。这个例子改变了源文档的版本： 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blog2&quot;, &quot;version_type&quot;: &quot;external&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;if (ctx._source.foo == \\&quot;bar\\&quot;) &#123;ctx._version++; ctx._source.remove(\\&quot;foo\\&quot;)&#125;&quot;, &quot;lang&quot;: &quot;painless&quot; &#125;&#125;&#x27; 就像在_update_by_query中一样，你可以设置ctx.op更改在目标索引上执行的操作，值为noop，delete。设置ctx.op到任何其他字段都会返回一个错误，在ctx中设置任何其他字段也是如此。可以修改以下值：_id、_index、_version、_routing。将_version设置为null或将它从ctx映射中清除，就像没有在索引请求中发送版本一样;它将导致在目标索引中覆盖文档，而不管目标上的版本或在_reindex请求中使用的版本类型。默认情况下，如果_reindex看到一个带有路由的文档，那么该路由将被保留，除非脚本更改了它，你可以设置路由对dest的请求，以改变这一点：keep：将为每个匹配发送的批量请求上的路由设置为匹配上的路由。这是默认值。discard：将为每个匹配发送的批量请求上的路由设置为null。=：将为每个匹配发送的批量请求上的路由设置为=之后的所有文本。例如，你可以使用以下请求将所有文档从具有公司名称cat的源索引复制到路由设置为cat的dest索引中： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;company&quot;: &quot;cat&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; 默认情况下，_reindex使用滚动批次为1000，可以使用源元素中的size字段更改批大小： 1234567891011curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 100 &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;routing&quot;: &quot;=cat&quot; &#125;&#125;&#x27; _reindex还可以通过像这样指定管道来使用Ingest节点特性： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;source&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot;, &quot;pipeline&quot;: &quot;some_ingest_pipeline&quot; &#125;&#125;&#x27; 远程reindex12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;username&quot;: &quot;user&quot;, &quot;password&quot;: &quot;pass&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; host参数必须包含scheme, host, port（如：http://otherhost:9200）,也可以加路径（如：http://otherhost:9200/proxy），username和password是可选的，如果远程集群开启了安全认证，那么是必选的，如果需要使用username和password，需要使用https。远程主机需要设置白名单，可以通过elasticsearch.yml文件里的reindex.remote.whitelist属性进行设置，如果设置多个值可以使用逗号来进行分隔（如：otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*），这里的配置可以忽略scheme，如： reindex.remote.whitelist: &quot;otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*&quot; 必须让每个处理reindex的节点上添加白名单的配置。这个特性应该适用于可能找到的任何版本的Elasticsearch的远程集群，这应该允许通过从旧版本的集群reindex，将Elasticsearch的任何版本升级到当前版本。要启用发送到旧版本Elasticsearch的查询，无需验证或修改即可将查询参数直接发送到远程主机，注意：远程reindex不支持手动或者自动slicing。从远程服务器reindex使用堆上缓冲区，默认最大大小为100mb，如果远程索引包含非常大的文档，则需要使用更小的批处理大小，下面的示例将批处理大小设置为10。 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 还可以使用socket_timeout字段设置远程连接上的套接字读取超时，使用connect_timeout字段设置连接超时，他们的默认值为30秒，下面示例例将套接字读取超时设置为1分钟，连接超时设置为10秒： 12345678910111213141516171819curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://otherhost:9200&quot;, &quot;socket_timeout&quot;: &quot;1m&quot;, &quot;connect_timeout&quot;: &quot;10s&quot; &#125;, &quot;index&quot;: &quot;source&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;data&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;dest&quot; &#125;&#125;&#x27; 配置SSL参数远程reindex支持配置SSL参数，除了在Elasticsearch秘钥库中添加安全设置之外，还需要在elasticsearch.yml文件中进行配置，不可能在_reindex请求体中配置。支持以下设置：reindex.ssl.certificate_authorities应受信任的PEM编码证书文件的路径列表，不同同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.path包含要信任的证书的Java密钥存储文件的路径，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.certificate_authorities和reindex.ssl.truststore.path。reindex.ssl.truststore.passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.secure_password一起使用。reindex.ssl.truststore.secure_passwordreindex.ssl.truststore.path配置的密码，不能和reindex.ssl.truststore.password一起使用。reindex.ssl.truststore.typereindex.ssl.truststore.path信任存储库的类型，必须是jks或PKCS12，如果reindex.ssl.truststore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.verification_mode指示用于防止中间人攻击和伪造证书的验证类型。可以设置为full（验证主机名和证书路径）、certificate（验证证书路径，但不验证主机名）、none（不执行验证——这在生产环境中是强烈不鼓励的），默认是full。reindex.ssl.certificate指定PEM编码证书的路径或者证书链用于HTTP客户端身份认证，这个配置还需要设置reindex.ssl.key值，不能同时设置reindex.ssl.certificate和reindex.ssl.keystore.path。reindex.ssl.key指定与用于客户端身份验证的证书相关联的PEM编码私钥的路径，不能同时设置reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.secure_key_passphrase一起使用。reindex.ssl.secure_key_passphrase指定用于解密已加密的PEM编码私钥(reindex.ssl.key)的口令，不能与reindex.ssl.key_passphrase一起使用。reindex.ssl.keystore.path指定密钥存储库的路径，其中包含用于HTTP客户机身份验证的私钥和证书(如果远程集群需要)，这个密钥存储库可以是“JKS”或“PKCS#12”格式，不能同时指定reindex.ssl.key和reindex.ssl.keystore.path。reindex.ssl.keystore.type密钥存储库的类型(reindex.ssl.keystore.path)，必须是jks或者PKCS12，如果reindex.ssl.keystore.path的结束是”.p12”, “.pfx”或者”pkcs12”，那么该配置的默认值是PKCS12，否则默认值是jks。reindex.ssl.keystore.password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.secure_password一起使用。reindex.ssl.keystore.secure_password密钥存储库的密码(reindex.ssl.keystore.path)，此设置不能与reindex.ssl.keystore.password一起使用。reindex.ssl.keystore.key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.secure_key_password一起使用。reindex.ssl.keystore.secure_key_password密钥存储库中密钥的密码(reindex.ssl.keystore.path)，默认为密钥存储库密码，此设置不能与reindex.ssl.keystore.key_password一起使用。 URL参数除了标准的pretty参数外， reindex还支持refresh, wait_for_completion, wait_for_active_shards, timeout, scroll和requests_per_second。发送refresh url参数将导致对所写请求的所有索引进行刷新，这与Index API的refresh参数不同，后者只会刷新接收新数据的碎片，与index API不同的是，它不支持wait_for。如果请求包含wait_for_completion=false，则Elasticsearch将执行一些执行前检查，启动请求，然后返回一个任务，该任务可与Tasks api一起用于取消或获取任务状态，Elasticsearch还将创建此任务的记录，作为.tasks/task/${taskId}的文档，你可以自己决定是保留或者删除他，当你已经完成了，删除他，这样es会回收他使用的空间。wait_for_active_shards控制在进行重新索引之前必须激活多少个shard副本，超时控制每个写请求等待不可用碎片变为可用的时间，两者在批量API中的工作方式完全相同，由于_reindex使用滚动搜索，你还可以指定滚动参数来控制“搜索上下文”存活的时间(例如?scroll=10m)，默认值是5分钟。requests_per_second可以设置为任何正数(1.4、6、1000等)，并通过在每个批中填充等待时间来控制_reindex发出批索引操作的速率，可以通过将requests_per_second设置为-1来禁用。节流是通过在批之间等待来完成的，这样就可以给_reindex内部使用的滚动设置一个考虑填充的超时，填充时间是批大小除以requests_per_second和写入时间之间的差额，默认情况下批处理大小为1000，所以如果requests_per_second被设置为500： target_time = 1000 / 500 per second = 2 seconds padding time = target_time - write_time = 2 seconds - 0.5 seconds = 1.5 seconds 由于批处理是作为单个_bulk请求发出的，因此较大的批处理大小将导致Elasticsearch创建许多请求，然后等待一段时间再启动下一个请求集，这是“bursty”而不是“smooth”，默认值是-1。 响应体 12345678910111213141516171819&#123; &quot;took&quot;: 639, &quot;timed_out&quot;: false, &quot;total&quot;: 5, &quot;updated&quot;: 0, &quot;created&quot;: 5, &quot;deleted&quot;: 0, &quot;batches&quot;: 1, &quot;noops&quot;: 0, &quot;version_conflicts&quot;: 2, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: 1, &quot;throttled_until_millis&quot;: 0, &quot;failures&quot;: []&#125; took整个操作花费的总毫秒数。timed_out如果在reindex期间执行的任何请求超时，则将此标志设置为true。total成功处理的文档数量。updated成功更新的文档数量。created成功创建的文档的数量。deleted成功删除的文档数量。batches由reindex回拉的滚动响应的数量。noops由于用于reindex的脚本返回了ctx.op的noop值而被忽略的文档数量。version_conflictsreindex命中的版本冲突数。retriesreindex尝试重试的次数,bulk是重试的批量操作的数量，search是重试的搜索操作的数量。throttled_millis请求休眠以符合requests_per_second的毫秒数。requests_per_second在reindex期间每秒有效执行的请求数。throttled_until_millis在_reindex响应中，该字段应该始终等于零，它只有在使用任务API时才有意义，在任务API中，它指示下一次(毫秒)再次执行节流请求，以符合requests_per_second。failures出现多个错误以数组返回。 使用task api获取task 123456789curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&amp;wait_for_completion=false&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot; &#125;&#125;&#x27; 返回值为 123&#123; &quot;task&quot; : &quot;8uQK-B00RiWq03awtJok1Q:18&quot;&#125; 你可以用任务API获取所有正在运行的reindex请求的状态：curl -XGET &quot;http://127.0.0.1:9200/_tasks?detailed=true&amp;actions=*reindex&amp;pretty&quot;或者根据task id获取curl -XGET &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48?pretty&quot;返回值为： 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;completed&quot; : true, &quot;task&quot; : &#123; &quot;node&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;id&quot; : 48, &quot;type&quot; : &quot;transport&quot;, &quot;action&quot; : &quot;indices:data/write/reindex&quot;, &quot;status&quot; : &#123; &quot;total&quot; : 0, &quot;updated&quot; : 0, &quot;created&quot; : 0, &quot;deleted&quot; : 0, &quot;batches&quot; : 0, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : 0.0, &quot;throttled_until_millis&quot; : 0 &#125;, &quot;description&quot; : &quot;reindex from [twitter] to [new_twitter][_doc]&quot;, &quot;start_time_in_millis&quot; : 1566216815832, &quot;running_time_in_nanos&quot; : 86829, &quot;cancellable&quot; : true, &quot;headers&quot; : &#123; &#125; &#125;, &quot;error&quot; : &#123; &quot;type&quot; : &quot;index_not_found_exception&quot;, &quot;reason&quot; : &quot;no such index [new_twitter] and [action.auto_create_index] ([twitter,index10,-index1*,+ind*,-myIndex]) doesn&#x27;t match&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;new_twitter&quot; &#125;&#125; 取消task任何reindex接口都可以使用task cancel api取消： 123456789101112131415 curl -XPOST &quot;http://127.0.0.1:9200/_tasks/8uQK-B00RiWq03awtJok1Q:48/_cancel?pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:48] doesn&#x27;t support cancellation&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; 取消应该很快发生，但可能需要几秒钟，Tasks API将继续列出任务，直到它醒来取消自己。 rethrottle可以在url中使用_rethrottle，并使用requests_per_second参数来设置节流： 123456789101112131415curl -XPOST &quot;http://127.0.0.1:9200/_reindex/8uQK-B00RiWq03awtJok1Q:250/_rethrottle?requests_per_second=-1&amp;pretty&quot;&#123; &quot;node_failures&quot; : [ &#123; &quot;type&quot; : &quot;failed_node_exception&quot;, &quot;reason&quot; : &quot;Failed node [8uQK-B00RiWq03awtJok1Q]&quot;, &quot;node_id&quot; : &quot;8uQK-B00RiWq03awtJok1Q&quot;, &quot;caused_by&quot; : &#123; &quot;type&quot; : &quot;resource_not_found_exception&quot;, &quot;reason&quot; : &quot;task [8uQK-B00RiWq03awtJok1Q:250] is missing&quot; &#125; &#125; ], &quot;nodes&quot; : &#123; &#125;&#125; reindex改变属性名称_reindex可以重命名属性名，假设你创建了一个包含如下文档的索引： 12345curl -XPOST &quot;http://127.0.0.1:9200/test/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;text&quot;: &quot;words words&quot;, &quot;flag&quot;: &quot;foo&quot;&#125;&#x27; 在reindex的时候想把flag修改为tag，示例如： 12345678910111213curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.tag = ctx._source.remove(\\&quot;flag\\&quot;)&quot; &#125;&#125;&#x27; 查看order2的数据： 123456789101112131415curl -XGET &quot;http://127.0.0.1:9200/order2/_doc/1?pretty&quot;&#123; &quot;_index&quot; : &quot;order2&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;words words&quot;, &quot;tag&quot; : &quot;foo&quot; &#125;&#125; 切片Reindex支持切片滚动，以并行化重新索引过程。这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分 手动切片通过为每个请求提供一个片id和片的总数，手工切片一个重索引请求： 123456789101112131415161718192021222324252627curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 0, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27;curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot;, &quot;slice&quot;: &#123; &quot;id&quot;: 1, &quot;max&quot;: 2 &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 你可以通过以下方法来验证： curl -XGET &quot;http://127.0.0.1:9200/_refresh?pretty&quot; curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot; 返回值为： 123456789&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; 自动切面你还可以让_reindex使用切片滚动自动并行化_uid上的切片，使用slices指定要使用的片数: 12345678910curl -XPOST &quot;http://127.0.0.1:9200/_reindex?slices=5&amp;refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;order&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;order2&quot; &#125;&#125;&#x27; 通过下面请求进行验证： 12345678910curl -XPOST &quot;http://127.0.0.1:9200/order2/_search?size=0&amp;filter_path=hits.total&amp;pretty&quot;&#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125; &#125;&#125; reindex多个索引如果有许多索引需要reindex，通常最好一次reindex一个索引，而不是使用一个glob模式来获取许多索引。这样，如果有任何错误，可以删除部分完成的索引并从该索引重新开始，从而恢复该过程。它还使并行化过程变得相当简单：将索引列表拆分为reindex并并行运行每个列表。可以使用一次性脚本： 1234567891011for index in i1 i2 i3 i4 i5; do curl -HContent-Type:application/json -XPOST localhost:9200/_reindex?pretty -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;&#x27;$index&#x27;-reindexed&quot; &#125; &#125;&#x27;done reindex每日索引尽管有上述建议，你仍然可以结合使用_reindex和Painless来reindex每日索引，从而将新模板应用于现有文档。假设有以下文件组成的索引: curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.30/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d{“system.cpu.idle.pct”: 0.908}’ curl -XPUT &quot;http://127.0.0.1:9200/metricbeat-2016.05.31/_doc/1?refresh&amp;pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#39;{“system.cpu.idle.pct”: 0.105} metricbeat-*索引的新模板已经加载到Elasticsearch中，但它只适用于新创建的索引。下面的脚本从索引名称中提取日期，并创建一个附加-1的新索引。所有来自metricbeat-2016.05.31的数据将reindex到metricbeat-2016.05.31-1。 1234567891011121314curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;metricbeat-*&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;metricbeat&quot; &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._index = &#x27;metricbeat-&#x27; + (ctx._index.substring(&#x27;metricbeat-&#x27;.length(),ctx._index.length())) + &#x27;-1&#x27;&quot; &#125;&#125;&#x27; 以前metricbeat索引中的所有文档现在都可以在*-1索引中找到。 curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.30-1/_doc/1?pretty&quot;curl -XGET &quot;http://127.0.0.1:9200/metricbeat-2016.05.31-1/_doc/1?pretty&quot; 前一种方法还可以与更改字段名称结合使用，以便仅将现有数据加载到新索引中，并在需要时重命名任何字段。 提取索引中的子集合_reindex可用于提取索引的随机子集进行测试： 123456789101112131415161718curl -XPOST &quot;http://127.0.0.1:9200/_reindex?pretty&quot; -H &quot;Content-Type:application/json&quot; -d&#x27;&#123; &quot;size&quot;: 10, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot;, &quot;query&quot;: &#123; &quot;function_score&quot; : &#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;random_score&quot; : &#123;&#125; &#125; &#125;, &quot;sort&quot;: &quot;_score&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;random_twitter&quot; &#125;&#125;&#x27; _reindex默认按_doc排序，因此random_score不会有任何效果，除非你覆盖sort属性为_score。","tags":[{"name":"reindex","slug":"reindex","permalink":"http://example.com/tags/reindex/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"Http状态码及含义","date":"2021-07-20T03:31:58.000Z","path":"wiki/Http状态码及含义/","text":"http状态码由3个十进制数字组成。第一个数字表示状态码的分类，后面的两位表示该分类下不同的状态。分为5个大类。 分类 1** 信息。服务器收到请求，请继续执行请求 2** 成功。请求被成功接收并处理 3** 重定向。需要进一步操作来完成请求 4** 客户端错误。无法完成请求，或请求包含语法错误 5** 服务器错误。服务器在处理请求的过程中发成错误 各个状态说明 100： 继续请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分。 101： 切换协议请求者已要求服务器切换协议，服务器已确认并准备切换。 200： 成功服务器已成功处理了请求。 201： 已创建请求成功并且服务器创建了新的资源。 202： 已接受服务器已接受请求，但尚未处理。 203： 非授权信息服务器已成功处理了请求，但返回的信息可能来自另一来源。 204： 无内容服务器成功处理了请求，但没有返回任何内容。 205： 重置内容服务器成功处理了请求，内容被重置。 206： 部分内容服务器成功处理了部分请求。 300： 多种选择针对请求，服务器可执行多种操作。 301： 永久移动请求的网页已永久移动到新位置，即永久重定向。 302： 临时移动请求的网页暂时跳转到其他页面，即暂时重定向。 303： 查看其他位置如果原来的请求是 POST，重定向目标文档应该通过 GET 提取。 304： 未修改此次请求返回的网页未修改，继续使用上次的资源。 305： 使用代理请求者应该使用代理访问该网页。 307： 临时重定向请求的资源临时从其他位置响应。 400： 错误请求服务器无法解析该请求。 401： 未授权请求没有进行身份验证或验证未通过。 403： 禁止访问服务器拒绝此请求。 404： 未找到服务器找不到请求的网页。 405： 方法禁用服务器禁用了请求中指定的方法。 406： 不接受无法使用请求的内容响应请求的网页。 407： 需要代理授权请求者需要使用代理授权。 408： 请求超时服务器请求超时。 409： 冲突服务器在完成请求时发生冲突。 410： 已删除请求的资源已永久删除。 411： 需要有效长度服务器不接受不含有效内容长度标头字段的请求。 412： 未满足前提条件服务器未满足请求者在请求中设置的其中一个前提条件。 413： 请求实体过大请求实体过大，超出服务器的处理能力。 414： 请求 URI 过长请求网址过长，服务器无法处理。 415： 不支持类型请求的格式不受请求页面的支持。 416： 请求范围不符页面无法提供请求的范围。 417： 未满足期望值服务器未满足期望请求标头字段的要求。 500： 服务器内部错误服务器遇到错误，无法完成请求。 501： 未实现服务器不具备完成请求的功能。 502： 错误网关服务器作为网关或代理，从上游服务器收到无效响应。 503： 服务不可用服务器目前无法使用。 504： 网关超时服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505： HTTP 版本不支持服务器不支持请求中所用的 HTTP 协议版本。","tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"}]},{"title":"操作系统-死锁","date":"2021-07-20T03:29:18.000Z","path":"wiki/操作系统-死锁/","text":"参考资料 死锁是什么？如何避免死锁？ 哲学家就餐问题","tags":[{"name":"dead lock","slug":"dead-lock","permalink":"http://example.com/tags/dead-lock/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"操作系统进程调度策略","date":"2021-07-20T03:16:04.000Z","path":"wiki/操作系统进程调度策略/","text":"参考资料 操作系统中的进程调度策略有哪几种","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"进程间通信IPC","date":"2021-07-20T01:59:21.000Z","path":"wiki/进程间通信IPC/","text":"参考资料 进程间通信IPC (InterProcess Communication)","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"}]},{"title":"elasticsearch统计每年每小时访问量","date":"2021-07-19T14:53:39.000Z","path":"wiki/elasticsearch统计每年每小时访问量/","text":"需求背景，要统计文章在一年的时间内，每个小时的访问情况，按照0点举例子，每个文章，一年内每一天0点的访问次数累加起来； Elasticsearch索引如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;c&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ip&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;p&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;ua&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;visitTime&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125; &#125;&#125; Java RestHighLevelClient写法12345678910111213141516171819202122232425262728public void getDateDist() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(&quot;yj_visit_data2&quot;); TermsAggregationBuilder termsAggregation = AggregationBuilders.terms(&quot;article&quot;) .field(&quot;article.keyword&quot;).size(2200) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) .calendarInterval(DateHistogramInterval.HOUR)); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(termsAggregation); sourceBuilder.query(QueryBuilders.rangeQuery(&quot;visitTime&quot;).gt(&quot;1609430400000&quot;).lte(&quot;1625068799000&quot;)); sourceBuilder.timeout(new TimeValue(900000)); SearchRequest request = new SearchRequest(); request.source(sourceBuilder); SearchResponse search = restHighLevelClient.search(request, RequestOptions.DEFAULT); Aggregations aggregations = search.getAggregations(); log.info(&quot;agg -&gt; &#123;&#125;&quot;, aggregations.asList().size()); List&lt;? extends Terms.Bucket&gt; buckets = ((ParsedStringTerms) aggregations.asList().get(0)).getBuckets(); List&lt;ArticleHourData&gt; hourDataList = new ArrayList&lt;&gt;(); for (Terms.Bucket bucket : buckets) &#123; List&lt;? extends Histogram.Bucket&gt; innerBuckets = ((ParsedDateHistogram) bucket.getAggregations().asList().get(0)).getBuckets(); hourDataList.add(calcBucket(innerBuckets, bucket.getKeyAsString())); &#125; log.info(&quot;result ----&gt; &#123;&#125;&quot;, JSONObject.toJSONString(hourDataList)); &#125; 聚合分析123456789101112131415161718192021public ArticleHourData calcBucket(List&lt;? extends Histogram.Bucket&gt; innerBuckets, String article) &#123; log.info(&quot;innerBuckets get(0) ---&gt; &#123;&#125;&quot;, JSON.toJSONString(innerBuckets.get(0))); Map&lt;String, ? extends List&lt;? extends Histogram.Bucket&gt;&gt; hourMap = innerBuckets.stream() .collect(Collectors.groupingBy(bucket -&gt; getHour(bucket.getKeyAsString()))); log.info(&quot;collect ======&gt; &#123;&#125; &quot;, JSONObject.toJSONString(hourMap.keySet())); ArticleHourData hourData = ArticleHourData.builder().article(article).build(); if (hourMap.isEmpty()) &#123; return hourData; &#125; HashMap&lt;String, Long&gt; hashMap = new HashMap&lt;&gt;(); for (String hour : hourMap.keySet()) &#123; List&lt;? extends Histogram.Bucket&gt; list = hourMap.get(hour); if (CollectionUtils.isEmpty(list)) &#123; continue; &#125; hashMap.put(hour, list.stream().mapToLong(Histogram.Bucket::getDocCount).sum()); &#125; hourData.setCountMap(hashMap); return hourData; &#125; 获取时间的小时123456789101112private String getHour(String date) &#123; date = date.replace(&quot;Z&quot;, &quot; UTC&quot;); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSS Z&quot;); Date d = null; try &#123; d = format.parse(date); &#125; catch (ParseException e) &#123; e.printStackTrace(); return null; &#125; return String.valueOf(DateUtil.asLocalDateTime(d).getHour()); &#125; Python写法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from json.decoder import JSONDecoderfrom elasticsearch import Elasticsearchimport logging,jsonfrom datetime import datetimees = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;39.107.117.232&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;elastic&#x27;), timeout = 90000)sqs = &#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;art&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot;, &quot;size&quot;: 5 &#125;, &quot;aggs&quot;: &#123; &quot;art_total&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;article.keyword&quot; &#125; &#125;, &quot;_time&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;visitTime&quot;, &quot;calendar_interval&quot;: &quot;hour&quot; &#125; &#125; &#125; &#125; &#125;&#125;_search_result = es.search(index=&quot;yj_visit_data2&quot; , body=sqs)_result_json = json.dumps(_search_result,sort_keys=True, indent=4, separators=(&#x27;, &#x27;, &#x27;: &#x27;), ensure_ascii=False)aggregations = _search_result[&#x27;aggregations&#x27;]art = aggregations[&#x27;art&#x27;]buckets = art[&#x27;buckets&#x27;]#print(type(buckets)) ; print(buckets)def getHour(time): return (int)(time[11:13])# 计算每个小时的点击数def countByMonth(dataList , hourTar): _count = 0 for data in dataList: timestamp = data[&#x27;key_as_string&#x27;] hour = getHour(timestamp) if hour == hourTar: _count = (int)(data[&#x27;doc_count&#x27;]) + _count return _count final_list = []# 循环计算每一个文章for outBucket in buckets: simple_result = &#123;&#125; _time = outBucket[&#x27;_time&#x27;] innerBuckets = _time[&#x27;buckets&#x27;] print(&quot;time inner bucker size&quot; , len(innerBuckets)) simple_list = [] for num in range(0,24): simple_list.append(countByMonth(innerBuckets,num)) simple_result[0] = outBucket[&#x27;key&#x27;] simple_result[1] = simple_list final_list.append(simple_result)print(&quot;final result ----&gt; &quot;,final_list)","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch踩坑","date":"2021-07-19T09:46:59.000Z","path":"wiki/elasticsearch踩坑/","text":"search.max_bucketsThis limit can be set by changing the [search.max_buckets] cluster level setting.]]; 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;search.max_buckets&quot;: 100000 &#125;&#125; Required one of fields [field, script][Elasticsearch exception [type=illegal_argument_exception, reason=Required one of fields [field, script], but none were specified. ] 123456789101112@Test public void test1() &#123; NativeSearchQuery nativeSearchQuery = new NativeSearchQuery(QueryBuilders.matchAllQuery(), null); Script script = new Script(&quot;doc[&#x27;article.keyword&#x27;]&quot;); nativeSearchQuery.addAggregation(AggregationBuilders.terms(&quot;art&quot;) .field(&quot;article.keyword&quot;).size(10) .subAggregation(AggregationBuilders.dateHistogram(&quot;visitTime&quot;) .field(&quot;visitTime&quot;) // 这一行没有写 .calendarInterval(DateHistogramInterval.HOUR))); SearchHits&lt;YjVisitData&gt; search = elasticsearchRestTemplate.search(nativeSearchQuery, YjVisitData.class); System.err.println(&quot;search.getAggregations().asList() &#123;&#125;&quot; + search.getAggregations().asList().size()); &#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"python3基本数据类型","date":"2021-07-18T13:05:52.000Z","path":"wiki/python基本数据类型/","text":"Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。 等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值 标准数据类型Python3 中有六个标准的数据类型： Number（数字） String（字符串） List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 Number（数字）Python3 支持 int、float、bool、complex（复数）。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。 123&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class &#x27;int&#x27;&gt; &lt;class &#x27;float&#x27;&gt; &lt;class &#x27;bool&#x27;&gt; &lt;class &#x27;complex&#x27;&gt; 此外还可以用 isinstance 来判断： 1234&gt;&gt;&gt; a = 111&gt;&gt;&gt; isinstance(a, int)True&gt;&gt;&gt; ⚠️ isinstance 和 type 的区别在于：type()不会认为子类是一种父类类型。isinstance()会认为子类是一种父类类型。 ⚠️ 注意：Python3 中，bool 是 int 的子类，True 和 False 可以和数字相加 True==1，False==0 是会返回 Ture，但可以通过 is 来判断类型。1、Python可以同时为多个变量赋值，如a, b = 1, 2。2、一个变量可以通过赋值指向不同类型的对象。3、数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。4、在混合计算时，Python会把整型转换成为浮点数。 String（字符串）Python中的字符串用单引号 ‘ 或双引号 “ 括起来，同时使用反斜杠 \\ 转义特殊字符。字符串的截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 List（列表）List（列表） 是 Python 中使用最频繁的数据类型。 列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号 [] 之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下：变量[头下标:尾下标]索引值以 0 为开始值，-1 为从末尾的开始位置。 Tuple（元组）元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： 1234567891011#!/usr/bin/python3tuple = ( &#x27;abcd&#x27;, 786 , 2.23, &#x27;runoob&#x27;, 70.2 )tinytuple = (123, &#x27;runoob&#x27;)print (tuple) # 输出完整元组print (tuple[0]) # 输出元组的第一个元素print (tuple[1:3]) # 输出从第二个元素开始到第三个元素print (tuple[2:]) # 输出从第三个元素开始的所有元素print (tinytuple * 2) # 输出两次元组print (tuple + tinytuple) # 连接元组 Set（集合）集合（set）是由一个或数个形态各异的大小整体组成的，构成集合的事物或对象称作元素或是成员。 基本功能是进行成员关系测试和删除重复元素。 可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 创建格式：parame = &#123;value01,value02,...&#125; 或者 set(value) 1234567891011121314151617181920212223242526#!/usr/bin/python3sites = &#123;&#x27;Google&#x27;, &#x27;Taobao&#x27;, &#x27;Runoob&#x27;, &#x27;Facebook&#x27;, &#x27;Zhihu&#x27;, &#x27;Baidu&#x27;&#125;print(sites) # 输出集合，重复的元素被自动去掉# 成员测试if &#x27;Runoob&#x27; in sites : print(&#x27;Runoob 在集合中&#x27;)else : print(&#x27;Runoob 不在集合中&#x27;)# set可以进行集合运算a = set(&#x27;abracadabra&#x27;)b = set(&#x27;alacazam&#x27;)print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素 Dictionary（字典）字典（dictionary）是Python中另一个非常有用的内置数据类型。 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。 键(key)必须使用不可变类型。 在同一个字典中，键(key)必须是唯一的。 #!/usr/bin/python3 dict = &#123;&#125; dict[&#39;one&#39;] = &quot;1 - 菜鸟教程&quot; dict[2] = &quot;2 - 菜鸟工具&quot; tinydict = &#123;&#39;name&#39;: &#39;runoob&#39;,&#39;code&#39;:1, &#39;site&#39;: &#39;www.runoob.com&#39;&#125; print (dict[&#39;one&#39;]) # 输出键为 &#39;one&#39; 的值 print (dict[2]) # 输出键为 2 的值 print (tinydict) # 输出完整的字典 print (tinydict.keys()) # 输出所有键 print (tinydict.values()) # 输出所有值 注意：1、字典是一种映射类型，它的元素是键值对。2、字典的关键字必须为不可变类型，且不能重复。3、创建空字典使用 { }。 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。","tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python3基础语法","date":"2021-07-18T12:31:55.000Z","path":"wiki/python3基础语法/","text":"编码默认情况下，Python 3 源码文件以 UTF-8 编码，所有字符串都是 unicode 字符串。 当然你也可以为源码文件指定不同的编码：# -*- coding: cp-1252 -*- 标识符 第一个字符必须是字母表中字母或下划线 _ 。 标识符的其他的部分由字母、数字和下划线组成。 标识符对大小写敏感。在 Python 3 中，可以用中文作为变量名，非 ASCII 标识符也是允许的了。 python保留字import keywordkeyword.kwlist[‘False’, ‘None’, ‘True’, ‘and’, ‘as’, ‘assert’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] 注释Python中单行注释以 # 开头,多行注释可以用多个 # 号，还有 ‘’’ 和 “””： 行与缩进python最具特色的就是使用缩进来表示代码块，不需要使用大括号 {} 。缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数。 多行语句Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠 \\ 来实现多行语句，例如： 123total = item_one + \\ item_two + \\ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠 \\，例如： total = [&#39;item_one&#39;, &#39;item_two&#39;, &#39;item_three&#39;, &#39;item_four&#39;, &#39;item_five&#39;] 数字(Number)类型python中数字有四种类型：整数、布尔型、浮点数和复数。 int (整数) , 如 1, 只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 bool (布尔), 如 True。 float (浮点数), 如 1.23、3E-2 complex (复数), 如 1 + 2j、 1.1 + 2.2j 字符串(String) python中单引号和双引号使用完全相同。 使用三引号(‘’’ 或 “””)可以指定一个多行字符串。 转义符 \\ 反斜杠可以用来转义，使用r可以让反斜杠不发生转义。。 如 r”this is a line with \\n” 则\\n会显示，并不是换行。 按字面意义级联字符串，如”this “ “is “ “string”会被自动转换为this is string。 字符串可以用 + 运算符连接在一起，用 * 运算符重复。 Python 中的字符串有两种索引方式，从左往右以 0 开始，从右往左以 -1 开始。 Python中的字符串不能改变。 Python 没有单独的字符类型，一个字符就是长度为 1 的字符串。 字符串的截取的语法格式如下：变量[头下标:尾下标:步长] 空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。 记住：空行也是程序代码的一部分。 同一行显示多条语句Python 可以在同一行中使用多条语句，语句之间使用分号 ; 分割，以下是一个简单的实例： 12#!/usr/bin/python3import sys; x = &#x27;runoob&#x27;; sys.stdout.write(x + &#x27;\\n&#x27;) 多个语句构成代码组缩进相同的一组语句构成一个代码块，我们称之代码组。 像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。 我们将首行及后面的代码组称为一个子句(clause)。 如下实例： 12345678910111213141516171819202122232425if expression : suiteelif expression : suite else : suite``` ## print 输出print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=&quot;&quot;：## import 与 from...import在 python 用 import 或者 from...import 来导入相应的模块。将整个模块(somemodule)导入，格式为： import somemodule从某个模块中导入某个函数,格式为： from somemodule import somefunction从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc将某个模块中的全部函数导入，格式为： from somemodule import *## 命令行参数很多程序可以执行一些操作来查看一些基本信息，Python可以使用-h参数查看各参数帮助信息： $ python -husage: python [option] … [-c cmd | -m mod | file | -] [arg] …Options and arguments (and corresponding environment variables):-c cmd : program passed in as string (terminates option list)-d : debug output from parser (also PYTHONDEBUG=x)-E : ignore environment variables (such as PYTHONPATH)-h : print this help message and exit [ etc. ] ```我们在使用脚本形式执行 Python 时，可以接收命令行输入的参数;","tags":[],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"python操作elasticsearch","date":"2021-07-18T12:20:25.000Z","path":"wiki/python操作elasticsearch/","text":"下载python对应的elasticsearch依赖包pip3 install elasticsearch==7.10.0 python操作elasticsearch代码1234567from elasticsearch import Elasticsearchprint(&quot;init ...&quot;)es = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;39.107.117.232&#x27;,&#x27;port&#x27;:9200&#125;], http_auth=(&#x27;elastic&#x27;, &#x27;elastic&#x27;))#es = Elasticsearch([&#123;&#x27;host&#x27;:&#x27;39.107.117.232&#x27;,&#x27;port&#x27;:9200&#125;])# print(es.get(index=&#x27;yj_ip_pool&#x27;, doc_type=&#x27;_doc&#x27;, id=&#x27;9256058&#x27;))countRes = es.count(index=&#x27;yj_ip_pool&#x27;)print(countRes) 查询效果12345gaolei:awesome-python3-webapp gaolei$ /usr/local/opt/python/bin/python3.7 /Users/gaolei/Documents/DemoProjects/awesome-python3-webapp/www/es_test.pyinit ...&#123;&#x27;count&#x27;: 20095400, &#x27;_shards&#x27;: &#123;&#x27;total&#x27;: 1, &#x27;successful&#x27;: 1, &#x27;skipped&#x27;: 0, &#x27;failed&#x27;: 0&#125;&#125;gaolei:awesome-python3-webapp gaolei$","tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"}]},{"title":"kibana添加用户及控制权限","date":"2021-07-15T15:53:44.000Z","path":"wiki/kibana添加用户及控制权限/","text":"操作步骤： 【修改elasticsearch配置文件】 -&gt; 【重启elasticsearch】 -&gt; 【初始化账号&amp;密码】 -&gt; 【修改kibana配置文件】 -&gt; 【重启kibana】 -&gt; 【初始账号登录kibana】 -&gt; 【创建、配置角色】 -&gt; 【创建新用户】 配置elasticsearch开启自带的xpack的验证功能xpack.security.enabled: true 配置单节点模式discovery.type: single-node 坑1: 集群节点配置报错cluster.initial_master_nodes 坑2: 本次存储节点配置报错maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?node.max_local_storage_nodes: 2 为内置账号设置密码在elasticsearch的bin下执行以下命令：./elasticsearch-setup-passwords interactive 配置kibana123#使用初始用户kibanaelasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;密码&quot; 重启kibana之后使用初始账号 elastic 登录 创建、配置角色 创建新用户 查看新用户访问界面角色里面配置了kibana的访问权限，只开通了discover和dashboard两个入口 索引的话，有好几个索引，但是只配置了一个索引的权限 参考资料 kibana7.2添加登录及权限 Elasticsearch 安装","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch稳定性调优","date":"2021-07-15T07:22:49.000Z","path":"wiki/elasticsearch稳定性调优/","text":"Elasticsearch性能优化总结Elasticsearch调优实践 稳定性调优一 Linux参数调优 修改系统资源限制 👇单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些 echo &quot;* - nofile 655360&quot; &gt;&gt;/etc/security/limits.conf单用户内存地址空间 echo &quot;* - as unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户线程数 echo &quot;* - nproc 2056474&quot; &gt;&gt;/etc/security/limits.conf单用户文件大小 echo &quot;* - fsize unlimited&quot; &gt;&gt;/etc/security/limits.conf单用户锁定内存 echo &quot;* - memlock unlimited&quot; &gt;&gt;/etc/security/limits.conf单进程可以使用的最大map内存区域数量 echo &quot;vm.max_map_count = 655300&quot; &gt;&gt;/etc/sysctl.confTCP全连接队列参数设置， 这样设置的目的是防止节点数较多（比如超过100）的ES集群中，节点异常重启时全连接队列在启动瞬间打满，造成节点hang住，整个集群响应迟滞的情况echo &quot;net.ipv4.tcp_abort_on_overflow = 1&quot; &gt;&gt;/etc/sysctl.conf echo &quot;net.core.somaxconn = 2048&quot; &gt;&gt;/etc/sysctl.conf降低tcp alive time，防止无效链接占用链接数 echo 300 &gt;/proc/sys/net/ipv4/tcp_keepalive_time ES节点配置jvm.options-Xms和-Xmx设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给系统cache使用。 jvm内存建议不要低于2G，否则有可能因为内存不足导致ES无法正常启动或OOMjvm建议不要超过32G，否则jvm会禁用内存对象指针压缩技术，造成内存浪费 elasticsearch.yml设置内存熔断参数，防止写入或查询压力过高导致OOM，具体数值可根据使用场景调整。indices.breaker.total.limit: 30% indices.breaker.request.limit: 6% indices.breaker.fielddata.limit: 3% 调小查询使用的cache，避免cache占用过多的jvm内存，具体数值可根据使用场景调整。indices.queries.cache.count: 500 indices.queries.cache.size: 5% 单机多节点时，主从shard分配以ip为依据，分配到不同的机器上，避免单机挂掉导致数据丢失。cluster.routing.allocation.awareness.attributes: ip node.attr.ip: 1.1.1.1 ES使用方式节点数较多的集群，增加专有master，提升集群稳定性ES集群的元信息管理、index的增删操作、节点的加入剔除等集群管理的任务都是由master节点来负责的，master节点定期将最新的集群状态广播至各个节点。所以，master的稳定性对于集群整体的稳定性是至关重要的。当集群的节点数量较大时（比如超过30个节点），集群的管理工作会变得复杂很多。此时应该创建专有master节点，这些节点只负责集群管理，不存储数据，不承担数据读写压力；其他节点则仅负责数据读写，不负责集群管理的工作。 这样把集群管理和数据的写入/查询分离，互不影响，防止因读写压力过大造成集群整体不稳定。 将专有master节点和数据节点的分离，需要修改ES的配置文件，然后滚动重启各个节点。 专有master节点的配置文件（conf/elasticsearch.yml）增加如下属性：node.master: truenode.data: falsenode.ingest: false数据节点的配置文件增加如下属性（与上面的属性相反）：node.master: falsenode.data: truenode.ingest: true 控制index、shard总数量上面提到，ES的元信息由master节点管理，定期同步给各个节点，也就是每个节点都会存储一份。这个元信息主要存储在clusterstate中，如所有node元信息（indices、节点各种统计参数）、所有index/shard的元信息（mapping, location, size）、元数据ingest等。 ES在创建新分片时，要根据现有的分片分布情况指定分片分配策略，从而使各个节点上的分片数基本一致，此过程中就需要深入遍历clusterstate。当集群中的index/shard过多时，clusterstate结构会变得过于复杂，导致遍历clusterstate效率低下，集群响应迟滞。基础架构部数据库团队曾经在一个20个节点的集群里，创建了4w+个shard，导致新建一个index需要60s+才能完成。 当index/shard数量过多时，可以考虑从以下几方面改进： 降低数据量较小的index的shard数量 把一些有关联的index合并成一个index 数据按某个维度做拆分，写入多个集群 Segment Memory优化前面提到，ES底层采用Lucene做存储，而Lucene的一个index又由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。Lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在Lucene4.0以后采用的数据结构是FST (Finite State Transducer)。Lucene加载segment的时候将其全量装载到内存中，加快查询速度。这部分内存被称为SegmentMemory， 常驻内存，占用heap，无法被GC。 前面提到，为利用JVM的对象指针压缩技术来节约内存，通常建议JVM内存分配不要超过32G。当集群的数据量过大时，SegmentMemory会吃掉大量的堆内存，而JVM内存空间又有限，此时就需要想办法降低SegmentMemory的使用量了，常用方法有下面几个： 定期删除不使用的index 对于不常访问的index，可以通过close接口将其关闭，用到时再打开 通过force_merge接口强制合并segment，降低segment数量 基础架构部数据库团队在此基础上，对FST部分进行了优化，释放高达40%的Segment Memory内存空间。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"启动ELK脚本命令","date":"2021-07-14T15:37:17.000Z","path":"wiki/启动ELK脚本命令/","text":"esuser 授权chown -R esuser /usr/local/elasticsearch/* elastcisearch 启动脚本nohup ./elasticsearch-7.10.0/bin/elasticsearch &gt;&gt; ./elasticsearch-7.10.0/nohup.out 2&gt;&amp;1 &amp; kibana 启动脚本nohup ./bin/kibana &gt;&gt; ./nohup.out 2&gt;&amp;1 &amp; logstash 启动脚本nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf nohup /usr/local/logstash/logstash-7.10.0/bin/logstash -f /usr/local/logstash/logstash-7.10.0/config/redtom-logstash.conf &gt;&gt; /usr/local/logstash/logstash-7.10.0/nohup.out 2&gt;&amp;1 &amp;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch调优实践","date":"2021-07-14T15:12:16.000Z","path":"wiki/elasticsearch调优实践-0/","text":"从性能和稳定性两方面，从linux参数调优、ES节点配置和ES使用方式三个角度入手，介绍ES调优的基本方案。当然，ES的调优绝不能一概而论，需要根据实际业务场景做适当的取舍和调整 Elasticsearch性能优化总结Elasticsearch调优实践 Linux优化关闭交换分区，防止内存置换降低性能。将 /etc/fstab 文件中包含swap的行注释掉sed -i &#39;/swap/s/^/#/&#39; /etc/fstabswapoff -a 磁盘挂载选项noatime：禁止记录访问时间戳，提高文件系统读写性能data=writeback： 不记录data journal，提高文件系统写入性能barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了nobh：关闭buffer_head，防止内核打断大块数据的IO操作mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data 对于SSD磁盘，采用电梯调度算法因为SSD提供了更智能的请求调度算法，不需要内核去做多余的调整 (仅供参考)echo noop &gt; /sys/block/sda/queue/scheduler ES节点配置conf/elasticsearch.yml文件： 适当增大写入buffer和bulk队列长度，提高写入性能和稳定性indices.memory.index_buffer_size: 15%thread_pool.bulk.queue_size: 1024 计算disk使用量时，不考虑正在搬迁的shard在规模比较大的集群中，可以防止新建shard时扫描所有shard的元数据，提升shard分配速度。cluster.routing.allocation.disk.include_relocations: false 三 ES使用方式控制字段的存储选项ES底层使用Lucene存储数据，主要包括行存（StoreFiled）、列存（DocValues）和倒排索引（InvertIndex）三部分。 大多数使用场景中，没有必要同时存储这三个部分，可以通过下面的参数来做适当调整： StoreFiled行存，其中占比最大的是source字段，它控制doc原始数据的存储。在写入数据时，ES把doc原始数据的整个json结构体当做一个string，存储为source字段。查询时，可以通过source字段拿到当初写入时的整个json结构体。 所以，如果没有取出整个原始json结构体的需求，可以通过下面的命令，在mapping中关闭source字段或者只在source中存储部分字段，数据查询时仍可通过ES的docvaluefields获取所有字段的值。注意：关闭source后， update, updatebyquery, reindex等接口将无法正常使用，所以有update等需求的index不能关闭source。 关闭 _source12345678910PUT my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; _source只存储部分字段通过includes指定要存储的字段或者通过excludes滤除不需要的字段 123456789101112131415161718PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;*.count&quot;, &quot;meta.*&quot; ], &quot;excludes&quot;: [ &quot;meta.description&quot;, &quot;meta.other.*&quot; ] &#125; &#125; &#125;&#125; docvalues 控制列存。ES主要使用列存来支持sorting, aggregations和scripts功能，对于没有上述需求的字段，可以通过下面的命令关闭docvalues，降低存储成本。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false &#125; &#125; &#125; &#125;&#125; ndex：控制倒排索引。ES默认对于所有字段都开启了倒排索引，用于查询。对于没有查询需求的字段，可以通过下面的命令关闭倒排索引。 12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;session_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; allES的一个特殊的字段 ES把用户写入json的所有字段值拼接成一个字符串后，做分词，然后保存倒排索引，用于支持整个json的全文检索。这种需求适用的场景较少，可以通过下面的命令将all字段关闭，节约存储成本和cpu开销。（ES 6.0+以上的版本不再支持_all字段，不需要设置）12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; fieldnames该字段用于exists查询，来确认某个doc里面有无一个字段存在。若没有这种需求，可以将其关闭。12345678910PUT /my_index&#123; &quot;mapping&quot;: &#123; &quot;my_type&quot;: &#123; &quot;_field_names&quot;: &#123; &quot;enabled&quot;: false &#125; &#125; &#125;&#125; 开启最佳压缩对于打开了上述_source字段的index，可以通过下面的命令来把lucene适用的压缩算法替换成 DEFLATE，提高数据压缩率。PUT /my_index/_settings&#123; &quot;index.codec&quot;: &quot;best_compression&quot;&#125; bulk批量写入写入数据时尽量使用下面的bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w，具体可根据业务场景选取一个适当的数量。 调整translog同步策略默认情况下，translog的持久化策略是，对于每个写入请求都做一次flush，刷新translog数据到磁盘上。这种频繁的磁盘IO操作是严重影响写入性能的，如果可以接受一定概率的数据丢失（这种硬件故障的概率很小），可以通过下面的命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。 1234567891011PUT my_index&#123;&quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;translog&quot;: &#123; &quot;sync_interval&quot;: &quot;5s&quot;, &quot;durability&quot;: &quot;async&quot; &#125; &#125;&#125;&#125; 调整refresh_interval写入Lucene的数据，并不是实时可搜索的，ES必须通过refresh的过程把内存中的数据转换成Lucene的完整segment后，才可以被搜索。默认情况下，ES每一秒会refresh一次，产生一个新的segment，这样会导致产生的segment较多，从而segment merge较为频繁，系统开销较大。如果对数据的实时可见性要求较低，可以通过下面的命令提高refresh的时间间隔，降低系统开销。 PUT my_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;refresh_interval&quot; : &quot;30s&quot; &#125; &#125;&#125; merge并发控制ES的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。默认值是Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))，当节点配置的cpu核数较高时，merge占用的资源可能会偏高，影响集群的性能，可以通过下面的命令调整某个index的merge过程的并发度： PUT /my_index/_settings&#123; &quot;index.merge.scheduler.max_thread_count&quot;: 2&#125; 写入数据不指定_id，让ES自动产生当用户显示指定id写入数据时，ES会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。这样每次写入时，ES都会耗费一定的资源做查询。如果用户写入数据时不指定doc，ES则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。 所以，在不需要通过id字段去重、update的使用场景中，写入不指定id可以提升写入速率。基础架构部数据库团队的测试结果显示，无id的数据写入性能可能比有_id的高出近一倍，实际损耗和具体测试场景相关。 routing对于数据量较大的index，一般会配置多个shard来分摊压力。这种场景下，一个查询会同时搜索所有的shard，然后再将各个shard的结果合并后，返回给用户。对于高并发的小查询场景，每个分片通常仅抓取极少量数据，此时查询过程中的调度开销远大于实际读取数据的开销，且查询速度取决于最慢的一个分片。开启routing功能后，ES会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。如果查询时指定routing，那么ES只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。 routing的使用方式如下： 12# 写入PUT my_index/my_type/1?routing=user1&#123; &quot;title&quot;: &quot;This is a document&quot;&#125;# 查询GET my_index/_search?routing=user1,user2 &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;document&quot; &#125; &#125;&#125; 为string类型的字段选取合适的存储方式存为text类型的字段（string字段默认类型为text）：做分词后存储倒排索引，支持全文检索，可以通过下面几个参数优化其存储方式： - norms：用于在搜索时计算该doc的_score（代表这条数据与搜索条件的相关度），如果不需要评分，可以将其关闭。 - indexoptions：控制倒排索引中包括哪些信息（docs、freqs、positions、offsets）。对于不太注重score/highlighting的使用场景，可以设为 docs来降低内存/磁盘资源消耗。 - fields: 用于添加子字段。对于有sort和聚合查询需求的场景，可以添加一个keyword子字段以支持这两种功能。 123456789101112131415161718192021222324252627282930313233343536 &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;norms&quot;: false, &quot;index_options&quot;: &quot;docs&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; ``` #### 存为keyword类型的字段不做分词，不支持全文检索。text分词消耗CPU资源，冗余存储keyword子字段占用存储空间。如果没有全文索引需求，只是要通过整个字段做搜索，可以设置该字段的类型为keyword，提升写入速率，降低存储成本。 设置字段类型的方法有两种：一是创建一个具体的index时，指定字段的类型；二是通过创建template，控制某一类index的字段类型。- 通过mapping指定 tags 字段为keyword类型```jsonPUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;tags&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; 通过template，指定my_index*类的index，其所有string字段默认为keyword类型PUT _template/my_template12345678910111213141516171819202122&#123; &quot;order&quot;: 0, &quot;template&quot;: &quot;my_index*&quot;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; ] &#125; &#125;, &quot;aliases&quot;: &#123; &#125; &#125; 查询时，使用query-bool-filter组合取代普通query默认情况下，ES通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。但对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。此时，可以通过query-bool-filter组合来让ES不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。 普通查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;Kimchy&quot; &#125; &#125;&#125; query-bool-filter 加速查询POST my_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;Kimchy&quot; &#125; &#125; &#125; &#125;&#125; index按日期滚动，便于管理写入ES的数据最好通过某种方式做分割，存入不同的index。常见的做法是将数据按模块/功能分类，写入不同的index，然后按照时间去滚动生成index。这样做的好处是各种数据分开管理不会混淆，也易于提高查询效率。同时index按时间滚动，数据过期时删除整个index，要比一条条删除数据或deletebyquery效率高很多，因为删除整个index是直接删除底层文件，而deletebyquery是查询-标记-删除。 举例说明，假如有[modulea,moduleb]两个模块产生的数据，那么index规划可以是这样的：一类index名称是modulea + {日期}，另一类index名称是module_b+ {日期}。对于名字中的日期，可以在写入数据时自己指定精确的日期，也可以通过ES的ingest pipeline中的index-name-processor实现（会有写入性能损耗）。 按需控制index的分片数和副本数分片（shard）：一个ES的index由多个shard组成，每个shard承载index的一部分数据。 副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。 shard数量（numberofshards）设置过多或过低都会引发一些问题：shard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。 对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理。基础架构部数据库团队的推荐方案是： 对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置35个shard，numberofreplicas设置为1即可（也就是一主一从，共两副本） 。对于数据量较大（100GB以上）的index：一般把单个shard的数据量控制在（20GB50GB）让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"修改mysql表创建时间","date":"2021-07-14T13:42:53.000Z","path":"wiki/修改mysql表创建时间/","text":"修改服务器时间date -s &quot;2021-07-14 21:22:10&quot; 执行DDLalter table mirror_user comment &#39;用户表&#39;; 服务器时间修正ntpdate ntp1.aliyun.com","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"hutool导出excel","date":"2021-07-14T13:01:13.000Z","path":"wiki/hutool导出excel/","text":"如果你仅需一个Java导出excel的工具，👇就可以满足你的临时需求，当然代码下面这么写肯定是不规范的，可以稍后完善！ 添加依赖123456789101112131415161718&lt;!-- https://mvnrepository.com/artifact/cn.hutool/hutool-all --&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.poi/poi-ooxml --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; 数据类1234567@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class IPData &#123; private String ip;&#125; Export方法示例12345678910public void export(List&lt;IPData&gt; rows) throws FileNotFoundException &#123; ExcelWriter writer = ExcelUtil.getWriter(true); writer.renameSheet(&quot;所有数据&quot;); //甚至sheet的名称 writer.addHeaderAlias(&quot;ip&quot;, &quot;IP&quot;); writer.write(rows, true); writer.setOnlyAlias(true); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Users/gaolei/Desktop/IP1.xlsx&quot;); writer.flush(fileOutputStream); writer.close(); &#125;","tags":[{"name":"excel","slug":"excel","permalink":"http://example.com/tags/excel/"}],"categories":[{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"}]},{"title":"git常用命令","date":"2021-07-14T01:55:55.000Z","path":"wiki/git常用命令/","text":"初始化本地仓库git init git add .git commit -m &lt;message&gt;git remote add &lt;name&gt; &lt;url&gt;git pushgit push &lt;name&gt;git push --set-upstream &lt;name&gt; &lt;branch&gt; 批量删除分支远程：git branch -r| grep &#39;ss-1&#39; | sed &#39;s/origin\\///g&#39; | xargs -I &#123;&#125; git push origin :&#123;&#125;本地：git branch -a | grep &#39;feature-re-1&#39; | xargs git branch -D","tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"}],"categories":[{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"}]},{"title":"Linux常用命令","date":"2021-07-13T03:44:22.000Z","path":"wiki/Linux常用命令/","text":"用户相关文件相关日志相关 linux 在文档中查找关键字个数grep -o “关键字” 文档名 | wc -l grep -o “关键字” 文档名 | sort | uniq -c 清除history记录vim .bash_history命令模式下（Esc之后输入:） 输入 set nu 每行数据前面显示行号11,20d 回车 11～20行的记录就被删除了然后命令模式下 wq 保存退出就可以了如果在此查看还是有记录，可以退出当前回话之后，再进去查看，就会不再显示删除的记录了","tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}],"categories":[{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"}]},{"title":"段合并","date":"2021-07-08T12:57:54.000Z","path":"wiki/段合并/","text":"参考资料 learnku.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"统计去重数据 (近似度量)","date":"2021-07-08T08:09:05.000Z","path":"wiki/统计去重数据/","text":"cardinality用法常用写法如下👇curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;months&quot; : &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125; &#125; &#125;&#125; 精度问题cardinality 度量是一个 近似算法。 它是基于 HyperLogLog++ （HLL）算法的。 HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。 我们不需要理解技术细节， 但我们最好应该关注一下这个算法的 特性 ： 可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）。 小的数据集精度是非常高的。 我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 要配置精度，我们必须指定 precision_threshold 参数的值。 这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果。参考以下示例： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; ⚠️ ⚠️precision_threshold 接受 0–40000 之间的数字，更大的值还是会被当作 40000 来处理 示例会确保当字段唯一值在 100 以内时会得到非常准确的结果。尽管算法是无法保证这点的，但如果基数在阈值以下，几乎总是 100% 正确的。高于阈值的基数会开始节省内存而牺牲准确度，同时也会对度量结果带入误差。 对于指定的阈值，HLL 的数据结构会大概使用 precision_threshold * 8 字节的内存，所以就必须在牺牲内存和获得额外的准确度间做平衡。 在实际应用中， 100 的阈值可以在唯一值为百万的情况下仍然将误差维持 5% 以内 速度问题如果想要获得唯一值的数目， 通常 需要查询整个数据集合（或几乎所有数据）。 所有基于所有数据的操作都必须迅速，原因是显然的。 HyperLogLog 的速度已经很快了，它只是简单的对数据做哈希以及一些位操作。 但如果速度对我们至关重要，可以做进一步的优化。 因为 HLL 只需要字段内容的哈希值，我们可以在索引时就预先计算好。 就能在查询时跳过哈希计算然后将哈希值从 fielddata 直接加载出来。 预先计算哈希值只对内容很长或者基数很高的字段有用，计算这些字段的哈希值的消耗在查询时是无法忽略的。 尽管数值字段的哈希计算是非常快速的，存储它们的原始值通常需要同样（或更少）的内存空间。这对低基数的字符串字段同样适用，Elasticsearch 的内部优化能够保证每个唯一值只计算一次哈希。 基本上说，预先计算并不能保证所有的字段都更快，它只对那些具有高基数和/或者内容很长的字符串字段有作用。需要记住的是，预计算只是简单的将查询消耗的时间提前转移到索引时，并非没有任何代价，区别在于你可以选择在 什么时候 做这件事，要么在索引时，要么在查询时。 创建索引时添加如下配置： 1234567891011121314151617PUT /cars/&#123; &quot;mappings&quot;: &#123; &quot;transactions&quot;: &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 多值字段的类型是 murmur3 ，这是一个哈希函数。 现在当我们执行聚合时，我们使用 color.hash 字段而不是 color 字段：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 12345678910&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_colors&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;color.hash&quot; &#125; &#125; &#125;&#125; 现在 cardinality 度量会读取 “color.hash“ 里的值（预先计算的哈希值），取代动态计算原始值的哈希。 单个文档节省的时间是非常少的，但是如果你聚合一亿数据，每个字段多花费 10 纳秒的时间，那么在每次查询时都会额外增加 1 秒，如果我们要在非常大量的数据里面使用 cardinality ，我们可以权衡使用预计算的意义，是否需要提前计算 hash，从而在查询时获得更好的性能，做一些性能测试来检验预计算哈希是否适用于你的应用场景。。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"多桶排序","date":"2021-07-08T07:50:24.000Z","path":"wiki/多桶排序/","text":"多值桶（ terms 、 histogram 和 date_histogram ）动态生成很多桶。 Elasticsearch 是如何决定这些桶展示给用户的顺序呢？ 默认的，桶会根据 doc_count 降序排列。这是一个好的默认行为，因为通常我们想要找到文档中与查询条件相关的最大值：售价、人口数量、频率。但有些时候我们希望能修改这个顺序，不同的桶有着不同的处理方式。 内置排序这些排序模式是桶 固有的 能力：它们操作桶生成的数据 ，比如 doc_count 。 它们共享相同的语法，但是根据使用桶的不同会有些细微差别。 让我们做一个 terms 聚合但是按 doc_count 值的升序排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;_count&quot; : &quot;asc&quot; &#125; &#125; &#125; &#125;&#125;&#x27; 用关键字 _count ，我们可以按 doc_count 值的升序排序。 我们为聚合引入了一个 order 对象， 它允许我们可以根据以下几个值中的一个值进行排序： _count按文档数排序。对 terms 、 histogram 、 date_histogram 有效。 _term按词项的字符串值的字母顺序排序。只在 terms 内使用。 _key按每个桶的键值数值排序（理论上与 _term 类似）。 只在 histogram 和 date_histogram 内使用。 按度量排序有时，我们会想基于度量计算的结果值进行排序。 在我们的汽车销售分析仪表盘中，我们可能想按照汽车颜色创建一个销售条状图表，但按照汽车平均售价的升序进行排序。 我们可以增加一个度量，再指定 order 参数引用这个度量即可： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 1234567891011121314151617181920&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125;&#x27; 计算每个桶的平均售价。 桶按照计算平均值的升序排序。 我们可以采用这种方式用任何度量排序，只需简单的引用度量的名字。不过有些度量会输出多个值。 extended_stats 度量是一个很好的例子：它输出好几个度量值。 如果我们想使用多值度量进行排序， 我们只需以关心的度量为关键词使用点式路径：curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot;, &quot;order&quot;: &#123; &quot;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123; &quot;extended_stats&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 使用 . 符号，根据感兴趣的度量进行排序。 深度度量排序在前面的示例中，度量是桶的直接子节点。平均售价是根据每个 term 来计算的。 在一定条件下，我们也有可能对 更深 的度量进行排序，比如孙子桶或从孙桶。 我们可以定义更深的路径，将度量用尖括号（ &gt; ）嵌套起来，像这样： my_bucket&gt;another_bucket&gt;metric 。 需要提醒的是嵌套路径上的每个桶都必须是 单值 的。 filter 桶生成 一个单值桶：所有与过滤条件匹配的文档都在桶中。 多值桶（如：terms ）动态生成许多桶，无法通过指定一个确定路径来识别。 目前，只有三个单值桶： filter 、 global 和 reverse_nested 。让我们快速用示例说明，创建一个汽车售价的直方图，但是按照红色和绿色（不包括蓝色）车各自的方差来排序： curl -X GET &quot;localhost:9200/cars/transactions/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39; 123456789101112131415161718192021222324&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;colors&quot; : &#123; &quot;histogram&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;interval&quot;: 20000, &quot;order&quot;: &#123; &quot;red_green_cars&gt;stats.variance&quot; : &quot;asc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;red_green_cars&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123;&quot;color&quot;: [&quot;red&quot;, &quot;green&quot;]&#125;&#125;, &quot;aggs&quot;: &#123; &quot;stats&quot;: &#123;&quot;extended_stats&quot;: &#123;&quot;field&quot; : &quot;price&quot;&#125;&#125; &#125; &#125; &#125; &#125; &#125;&#125;&#x27; 按照嵌套度量的方差对桶的直方图进行排序。 因为我们使用单值过滤器 filter ，我们可以使用嵌套排序。 按照生成的度量对统计结果进行排序。 本例中，可以看到我们如何访问一个嵌套的度量。 stats 度量是 red_green_cars 聚合的子节点，而 red_green_cars 又是 colors 聚合的子节点。 为了根据这个度量排序，我们定义了路径 red_green_cars&gt;stats.variance 。我们可以这么做，因为 filter 桶是个单值桶。","tags":[{"name":"elasticsaerch","slug":"elasticsaerch","permalink":"http://example.com/tags/elasticsaerch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"过滤和聚合","date":"2021-07-08T07:33:26.000Z","path":"wiki/过滤和聚合/","text":"过滤和聚合聚合范围限定还有一个自然的扩展就是过滤。因为聚合是在查询结果范围内操作的，任何可以适用于查询的过滤器也可以应用在聚合上。 过滤如果我们想找到售价在 $10,000 美元之上的所有汽车同时也为这些车计算平均售价， 可以简单地使用一个 constant_score 查询和 filter 约束： GET /cars/transactions/_search 12345678910111213141516171819&#123; &quot;size&quot; : 0, &quot;query&quot; : &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10000 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;single_avg_price&quot;: &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 从根本上讲，使用 non-scoring 查询和使用 match 查询没有任何区别。查询（包括了一个过滤器）返回一组文档的子集，聚合正是操作这些文档。使用 filtering query 会忽略评分，并有可能会缓存结果数据等等。 过滤桶但是如果我们只想对聚合结果过滤怎么办？ 假设我们正在为汽车经销商创建一个搜索页面， 我们希望显示用户搜索的结果，但是我们同时也想在页面上提供更丰富的信息，包括（与搜索匹配的）上个月度汽车的平均售价。 这里我们无法简单的做范围限定，因为有两个不同的条件。搜索结果必须是 ford ，但是聚合结果必须满足 ford AND sold &gt; now - 1M 。 为了解决这个问题，我们可以用一种特殊的桶，叫做 filter （注：过滤桶） 。 我们可以指定一个过滤桶，当文档满足过滤桶的条件时，我们将其加入到桶内。 查询结果如下：GET /cars/transactions/_search 1234567891011121314151617181920212223242526&#123; &quot;size&quot; : 0, &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;recent_sales&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold&quot;: &#123; &quot;from&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_price&quot;:&#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 使用 过滤 桶在 查询 范围基础上应用过滤器。 avg 度量只会对 ford 和上个月售出的文档计算平均售价。 因为 filter 桶和其他桶的操作方式一样，所以可以随意将其他桶和度量嵌入其中。所有嵌套的组件都会 “继承” 这个过滤，这使我们可以按需针对聚合过滤出选择部分。 后过滤器目前为止，我们可以同时对搜索结果和聚合结果进行过滤（不计算得分的 filter 查询），以及针对聚合结果的一部分进行过滤（ filter 桶）。 我们可能会想，”只过滤搜索结果，不过滤聚合结果呢？” 答案是使用 post_filter 。 它是接收一个过滤器的顶层搜索请求元素。这个过滤器在查询 之后 执行（这正是该过滤器的名字的由来：它在查询之后 post 执行）。正因为它在查询之后执行，它对查询范围没有任何影响，所以对聚合也不会有任何影响。 我们可以利用这个行为对查询条件应用更多的过滤器，而不会影响其他的操作，就如 UI 上的各个分类面。让我们为汽车经销商设计另外一个搜索页面，这个页面允许用户搜索汽车同时可以根据颜色来过滤。颜色的选项是通过聚合获得的： GET /cars/transactions/_search 123456789101112131415161718&#123; &quot;size&quot; : 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;make&quot;: &quot;ford&quot; &#125; &#125;, &quot;post_filter&quot;: &#123; &quot;term&quot; : &#123; &quot;color&quot; : &quot;green&quot; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;all_colors&quot;: &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; post_filter 元素是 top-level 而且仅对命中结果进行过滤。 查询 部分找到所有的 ford 汽车，然后用 terms 聚合创建一个颜色列表。因为聚合对查询范围进行操作，颜色列表与福特汽车有的颜色相对应。 最后， post_filter 会过滤搜索结果，只展示绿色 ford 汽车。这在查询执行过 后 发生，所以聚合不受影响。 这通常对 UI 的连贯一致性很重要，可以想象用户在界面商选择了一类颜色（比如：绿色），期望的是搜索结果已经被过滤了，而 不是 过滤界面上的选项。如果我们应用 filter 查询，界面会马上变成 只 显示 绿色 作为选项，这不是用户想要的！ ⚠️ ⚠️ ⚠️ 性能考虑（Performance consideration）当你需要对搜索结果和聚合结果做不同的过滤时，你才应该使用 post_filter ， 有时用户会在普通搜索使用 post_filter 。 不要这么做！ post_filter 的特性是在查询 之后 执行，任何过滤对性能带来的好处（比如缓存）都会完全失去。 在我们需要不同过滤时， post_filter 只与聚合一起使用。 总结选择合适类型的过滤（如：搜索命中、聚合或两者兼有）通常和我们期望如何表现用户交互有关。选择合适的过滤器（或组合）取决于我们期望如何将结果呈现给用户。 在 filter 过滤中的 non-scoring 查询，同时影响搜索结果和聚合结果。 filter 桶影响聚合。 post_filter 只影响搜索结果。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 条形图","date":"2021-07-08T07:11:57.000Z","path":"wiki/聚合-条形图/","text":"参考资料","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"嵌套桶","date":"2021-07-08T07:01:16.000Z","path":"wiki/嵌套桶/","text":"两层嵌套在我们使用不同的嵌套方案时，聚合的力量才能真正得以显现。 在前例中，我们已经看到如何将一个度量嵌入桶中，它的功能已经十分强大了。 但真正令人激动的分析来自于将桶嵌套进 另外一个桶 所能得到的结果。 现在，我们想知道每个颜色的汽车制造商的分布： GET /cars/transactions/_search 12345678910111213141516171819202122&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;make&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注意前例中的 avg_price 度量仍然保持原位。 另一个聚合 make 被加入到了 color 颜色桶中。 这个聚合是 terms 桶，它会为每个汽车制造商生成唯一的桶。 这里发生了一些有趣的事。 首先，我们可能会观察到之前例子中的 avg_price 度量完全没有变化，还在原来的位置。 一个聚合的每个 层级 都可以有多个度量或桶， avg_price 度量告诉我们每种颜色汽车的平均价格。它与其他的桶和度量相互独立。 这对我们的应用非常重要，因为这里面有很多相互关联，但又完全不同的度量需要收集。聚合使我们能够用一次数据请求获得所有的这些信息。 另外一件值得注意的重要事情是我们新增的这个 make 聚合，它是一个 terms 桶（嵌套在 colors 、 terms 桶内）。这意味着它会为数据集中的每个唯一组合生成（ color 、 make ）元组。 让我们看看返回的响应（为了简单我们只显示部分结果）： 1234567891011121314151617181920212223242526&#123; &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125; &#125; &#125;&#125; 正如期望的那样，新的聚合嵌入在每个颜色桶中。 现在我们看见按不同制造商分解的每种颜色下车辆信息。 最终，我们看到前例中的 avg_price 度量仍然维持不变。 三层嵌套让我们回到话题的原点，在进入新话题之前，对我们的示例做最后一个修改， 为每个汽车生成商计算最低和最高的价格：GET /cars/transactions/_search 1234567891011121314151617181920212223&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;make&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;make&quot; &#125;, &quot;aggs&quot; : &#123; &quot;min_price&quot; : &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125;, &quot;max_price&quot; : &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 我们需要增加另外一个嵌套的 aggs 层级。 然后包括 min 最小度量。 以及 max 最大度量。 得到以下输出（只显示部分结果）： 12345678910111213141516171819202122232425262728293031323334353637&#123;... &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;red&quot;, &quot;doc_count&quot;: 4, &quot;make&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;honda&quot;, &quot;doc_count&quot;: 3, &quot;min_price&quot;: &#123; &quot;value&quot;: 10000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 20000 &#125; &#125;, &#123; &quot;key&quot;: &quot;bmw&quot;, &quot;doc_count&quot;: 1, &quot;min_price&quot;: &#123; &quot;value&quot;: 80000 &#125;, &quot;max_price&quot;: &#123; &quot;value&quot;: 80000 &#125; &#125; ] &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 32500 &#125; &#125;,... 有了这两个桶，我们可以对查询的结果进行扩展并得到以下信息： 有四辆红色车。红色车的平均售价是 $32，500 美元。其中三辆红色车是 Honda 本田制造，一辆是 BMW 宝马制造。最便宜的红色本田售价为 $10，000 美元。最贵的红色本田售价为 $20，000 美元。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"聚合 高级概念","date":"2021-07-08T06:52:35.000Z","path":"wiki/聚合-高级概念/","text":"桶桶 简单来说就是满足特定条件的文档的集合： 一个雇员属于 男性 桶或者 女性 桶 奥尔巴尼属于 纽约 桶 日期2014-10-28属于 十月 桶当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件。如果匹配到，文档将放入相应的桶并接着进行聚合操作。 桶也可以被嵌套在其他桶里面，提供层次化的或者有条件的划分方案。例如，辛辛那提会被放入俄亥俄州这个桶，而 整个 俄亥俄州桶会被放入美国这个桶。 Elasticsearch 有很多种类型的桶，能让你通过很多种方式来划分文档（时间、最受欢迎的词、年龄区间、地理位置 等等）。其实根本上都是通过同样的原理进行操作：基于条件来划分文档。 指标桶能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些指标的计算。分桶是一种达到目的的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。 大多数 指标 是简单的数学运算（例如最小值、平均值、最大值，还有汇总），这些是通过文档的值来计算。在实践中，指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据。 桶和指标的组合聚合 是由桶和指标组成的。 聚合可能只有一个桶，可能只有一个指标，或者可能两个都有。也有可能有一些桶嵌套在其他桶里面。例如，我们可以通过所属国家来划分文档（桶），然后计算每个国家的平均薪酬（指标）。 由于桶可以被嵌套，我们可以实现非常多并且非常复杂的聚合： 1.通过国家划分文档（桶） 2.然后通过性别划分每个国家（桶） 3.然后通过年龄区间划分每种性别（桶） 4.最后，为每个年龄区间计算平均薪酬（指标） 最后将告诉你每个 &lt;国家, 性别, 年龄&gt; 组合的平均薪酬。所有的这些都在一个请求内完成并且只遍历一次数据！","tags":[{"name":"elasticsaerch","slug":"elasticsaerch","permalink":"http://example.com/tags/elasticsaerch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"}]},{"title":"elasticsearch-overview","date":"2021-07-08T03:17:58.000Z","path":"wiki/elasticsearch-overview/","text":"学习资料 https://www.codingdict.com/ https://elasticsearch.cn/ https://www.elastic.co/guide/en/ 铭毅天下","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"关于 Elasticsearch 内存占用及分配","date":"2021-07-08T02:39:16.000Z","path":"wiki/关于-Elasticsearch-内存占用及分配/","text":"Elasticsearch 和 Lucene 对内存使用情况： Elasticsearch 限制的内存大小是 JAVA 堆空间的大小，不包括Lucene 缓存倒排索引数据空间。 Lucene 中的 倒排索引 segments 存储在文件中，为提高访问速度，都会把它加载到内存中，从而提高 Lucene 性能。所以建议至少留系统一半内存给Lucene。Node Query Cache (负责缓存f ilter 查询结果)，每个节点有一个，被所有 shard 共享，filter query查询结果要么是 yes 要么是no，不涉及 scores 的计算。集群中每个节点都要配置，默认为：indices.queries.cache.size:10% Indexing Buffer 索引缓冲区，用于存储新索引的文档，当其被填满时，缓冲区中的文档被写入磁盘中的 segments 中。节点上所有 shard 共享。缓冲区默认大小： indices.memory.index_buffer_size: 10%如果缓冲区大小设置了百分百则 indices.memory.min_index_buffer_size 用于这是最小值，默认为 48mb。indices.memory.max_index_buffer_size 用于最大大小，无默认值。 segmentssegments会长期占用内存，其初衷就是利用OS的cache提升性能。只有在Merge之后，才会释放掉标记为Delete的segments，释放部分内存。 Shard Request Cache 用于缓存请求结果，但之缓存request size为0的。比如说 hits.total, aggregations 和 suggestions.默认最大为indices.requests.cache.size:1% Field Data Cache 字段缓存重要用于对字段进行排序、聚合是使用。因为构建字段数据缓存代价昂贵，所以建议有足够的内训来存储。Fielddata 是 「 延迟 」 加载。如果你从来没有聚合一个分析字符串，就不会加载 fielddata 到内存中，也就不会使用大量的内存，所以可以考虑分配较小的heap给Elasticsearch。因为heap越小意味着Elasticsearch的GC会比较快，并且预留给Lucene的内存也会比较大。。如果没有足够的内存保存fielddata时，Elastisearch会不断地从磁盘加载数据到内存，并剔除掉旧的内存数据。剔除操作会造成严重的磁盘I/O，并且引发大量的GC，会严重影响Elastisearch的性能。 默认情况下Fielddata会不断占用内存，直到它触发了fielddata circuit breaker。fielddata circuit breaker会根据查询条件评估这次查询会使用多少内存，从而计算加载这部分内存之后，Field Data Cache所占用的内存是否会超过indices.breaker.fielddata.limit。如果超过这个值，就会触发fielddata circuit breaker，abort这次查询并且抛出异常，防止OOM。 1indices.breaker.fielddata.limit:60% (默认heap的60%) (es7之后改成70%) 如果设置了indices.fielddata.cache.size，当达到size时，cache会剔除旧的fielddata。 indices.breaker.fielddata.limit 必须大于 indices.fielddata.cache.size，否则只会触发fielddata circuit breaker，而不会剔除旧的fielddata。 配置Elasticsearch堆内存Elasticsearch默认安装后设置的内存是 1GB，这是远远不够用于生产环境的。有两种方式修改Elasticsearch的堆内存： 设置环境变量：export ES_HEAP_SIZE=10g 在es启动时会读取该变量； 启动时作为参数传递给es： ./bin/elasticsearch -Xmx10g -Xms10g 注意点给es分配内存时要注意，至少要分配一半儿内存留给 Lucene。分配给 es 的内存最好不要超过 32G ，因为如果堆大小小于 32 GB，JVM 可以利用指针压缩，这可以大大降低内存的使用：每个指针 4 字节而不是 8 字节。如果大于32G 每个指针占用 8字节，并且会占用更多的内存带宽，降低了cpu性能。 还有一点， 要关闭 swap 内存交换空间，禁用swapping。频繁的swapping 对服务器来说是致命的。总结：给es JVM栈的内存最好不要超过32G，留给Lucene的内存越大越好，Lucene把所有的segment都缓存起来，会加快全文检索。 关闭交换区这应该显而易见了，但仍然需要明确的写出来：把内存换成硬盘将毁掉服务器的性能，想象一下：涉及内存的操作是需要快速执行的。如果介质从内存变为了硬盘，一个10微秒的操作变成需要10毫秒。而且这种延迟发生在所有本该只花费10微秒的操作上，就不难理解为什么交换区对于性能来说是噩梦。 最好的选择是禁用掉操作系统的交换区。可以用以下命令： 1sudo swapoff -a 来禁用，你可能还需要编辑 /etc/fstab 文件。细节可以参考你的操作系统文档。 如果实际环境不允许禁用掉 swap，你可以尝试降低 swappiness。此值控制操作系统使用交换区的积极性。这可以防止在正常情况下使用交换区，但仍允许操作系统在紧急情况下将内存里的东西放到交换区。 对于大多数Linux系统来说，这可以用 sysctl 值来配置： 1vm.swappiness = 1 # 将此值配置为1会比0好，在kernal内核的某些版本中，0可能会引起OOM异常。 最后，如果两种方法都不可用，你应该在ElasticSearch的配置中启用 mlockall.file。这允许JVM锁定其使用的内存，而避免被放入操作系统交换区。 在elasticsearch.yml中，做如下设置： 1bootstrap.mlockall: true 查看node节点数据GET /_cat/nodes?v&amp;h=id,ip,port,v,master,name,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count GET /_cat/nodes?v&amp;h=id,heap.current,heap.percent,heap.max,ram.current,ram.percent,ram.max,fielddata.memory_size GET /_cat/nodes?v&amp;h=id,fielddata.evictions,query_cache.memory_size,query_cache.evictions, request_cache.memory_size,request_cache.evictions,request_cache.hit_count,request_cache.miss_count 参考文章","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"cross-fields跨字段查询","date":"2021-07-07T06:41:42.000Z","path":"wiki/cross-fields跨字段查询/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » cross-fields 跨字段查询","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"copy_to参数","date":"2021-07-07T06:34:42.000Z","path":"wiki/copy-to参数/","text":"参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 自定义 _all 字段 Docs » Mapping parameters（映射参数） » Mapping(映射) » copy_to（合并参数）","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"字符串排序与多字段","date":"2021-07-07T03:05:31.000Z","path":"wiki/字符串排序与多字段/","text":"被解析的字符串字段也是多值字段， 但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如 fine old art ， 这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Elasticsearch 在排序过程中没有这样的信息。 你可以使用 min 和 max 排序模式（默认是 min ），但是这会导致排序以 art 或是 old ，任何一个都不是所希望的。 为了以字符串字段进行排序，这个字段应仅包含一项： 整个 not_analyzed 字符串。 但是我们仍需要 analyzed 字段，这样才能以全文进行查询 一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段： analyzed 用于搜索， not_analyzed 用于排序 但是保存相同的字符串两次在 _source 字段是浪费空间的。 我们真正想要做的是传递一个 单字段 但是却用两种方式索引它。所有的 _core_field 类型 (strings, numbers, Booleans, dates) 接收一个 fields 参数 该参数允许你转化一个简单的映射如： 1234&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;&#125; 为一个多字段映射如： 12345678910&quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; tweet 主字段与之前的一样: 是一个 analyzed 全文字段。 新的 tweet.raw 子字段是 not_analyzed. 现在，至少只要我们重新索引了我们的数据，使用 tweet 字段用于搜索，tweet.raw 字段用于排序：curl -X GET &quot;localhost:9200/_search?pretty&quot; -H &#39;Content-Type: application/json&#39; -d 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;, &quot;sort&quot;: &quot;tweet.raw&quot;&#125;","tags":[],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"推荐系统-Overview","date":"2021-07-07T02:08:05.000Z","path":"wiki/推荐系统-Overview/","text":"博客资料 深度解析京东个性化推荐系统演进史 用 Mahout 和 Elasticsearch 实现推荐系统 美团推荐算法实践 58同城推荐系统设计与实现 微博推荐系统的架构演进之路 Flink 在小红书推荐系统中的应用 小红书大数据在推荐系统中的应用 快看漫画个性化推荐探索与实践 数据仓库系列篇——唯品会大数据架构 推荐系统基本概念和架构 PAI平台搭建企业级个性化推荐系统 - Aliyun 蘑菇街推荐工程实践 参考资料","tags":[{"name":"推荐","slug":"推荐","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90/"}],"categories":[{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"}]},{"title":"flink 提交任务","date":"2021-07-06T15:57:04.000Z","path":"wiki/flink-提交任务/","text":"下面演示如何通过admin页面提交任务 👇 准备task jar1234567891011121314151617181920public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 执行mvn install -DskipTest 可以得到相应的jar admin提交jar 提交完jar包之后，需要设置相关参数，这个根据自己的实际情况来设置，下面是参考样例： Enter Class : com.ibli.flink.StreamWordCount也就是程序入口，我们这是写了一个main方法，如果是程序的话，可以写对应bootstrap的启动类 Program Arguments : –host localhost –port 7777 点击 submit 之后查看提交的任务状态 查看任务 可以看到是有两个任务，并且都是在执行状态；点击一个任务，还可以查看任务详情信息，和一些其他的信息，非常全面； 查看运行时任务列表 查看任务管理列表 点击任务可以跳转到详情页面 👇 下面是执行日志 我们还可以看到任务执行的标准输出结果✅ 任务源数据通过nc 输入数据，由程序读取7777端口输入流并解析数据 123gaolei:geekibli gaolei$ nc -lk 7777hello javahello flink 取消任务如下 再次查看已完成任务列表 如下：","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"apache-flink-overview","date":"2021-07-06T15:39:54.000Z","path":"wiki/apache-flink-overview/","text":"学习初衷推荐系统数据需要实时处理，使用Apache Flink实时计算用户数据，分析用户行为，达到实时业务数据分析和实现业务相关推荐； 学习资料 ashiamd.github.io 尚硅谷2021最新Java版Flink 武老师清华硕士，原IBM-CDL负责人 Apache Flink® — Stateful Computations over Data Streams Apache Flink® - 数据流上的有状态计算","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"most_fields类型","date":"2021-07-06T12:54:23.000Z","path":"wiki/most-fields类型/","text":"多字段映射首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍： DELETE /my_index 1234567891011121314151617181920PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; title 字段使用 english 英语分析器来提取词干。 title.std 字段使用 standard 标准分析器，所以没有词干提取。 接着索引一些文档： 12345PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; 这里用一个简单 match 查询 title 标题字段是否包含 jumping rabbits （跳跃的兔子）： 12345678GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;jumping rabbits&quot; &#125; &#125;&#125; 因为有了 english 分析器，这个查询是在查找以 jump 和 rabbit 这两个被提取词的文档。两个文档的 title 字段都同时包括这两个词，所以两个文档得到的评分也相同： 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.42039964, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125; ]&#125; 如果只是查询 title.std 字段，那么只有文档 2 是匹配的。尽管如此，如果同时查询两个字段，然后使用 bool 查询将评分结果 合并 ，那么两个文档都是匹配的（ title 字段的作用），而且文档 2 的相关度评分更高（ title.std 字段的作用）： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; 我们希望将所有匹配字段的评分合并起来，所以使用 most_fields 类型。这让 multi_match 查询用 bool 查询将两个字段语句包在里面，而不是使用 dis_max (最佳字段) 查询。 123456789101112131415161718&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8226396, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Jumping jack rabbits&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.10741998, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;My rabbit jumps&quot; &#125; &#125; ]&#125; 文档 2 现在的评分要比文档 1 高。 用广度匹配字段 title 包括尽可能多的文档——以提升召回率——同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。 每个字段对于最终评分的贡献可以通过自定义值 boost 来控制。比如，使 title 字段更为重要，这样同时也降低了其他信号字段的作用： 12345678910GET /my_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;jumping rabbits&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;title^10&quot;, &quot;title.std&quot; ] &#125; &#125;&#125; title 字段的 boost 的值为 10 使它比 title.std 更重要。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 多数字段","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"multi_match 查询","date":"2021-07-06T12:37:26.000Z","path":"wiki/multi-match-查询/","text":"multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。 📒 📒 📒 multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即：best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。 默认情况下，查询的类型是 best_fields ，这表示它会为每个字段生成一个 match 查询，然后将它们组合到 dis_max 查询的内部，如下： 1234567891011121314151617181920212223&#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125; &#125; &#125;, ], &quot;tie_breaker&quot;: 0.3 &#125;&#125; 上面这个查询用 multi_match 重写成更简洁的形式： 123456789&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;body&quot; ], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;: &quot;30%&quot; &#125;&#125; ⚠️ ⚠️ ⚠️ best_fields 类型是默认值，可以不指定。 如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。 查询字段名称的模糊匹配字段名称可以用 模糊匹配 的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中，例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: &quot;*_title&quot; &#125;&#125; 提升单个字段的权重可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数： 123456&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Quick brown fox&quot;, &quot;fields&quot;: [ &quot;*_title&quot;, &quot;chapter_title^2&quot; ] &#125;&#125; chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » multi_match 查询 Elasticsearch Guide [7.x] » Query DSL » Full text queries » Multi-match query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"dis_max查询","date":"2021-07-06T12:09:23.000Z","path":"wiki/dis-max查询/","text":"假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例： 1234567891011PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;&#125; 用户输入词组 Brown fox 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词： 现在运行以下 bool 查询： 12345678910&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 但是我们发现查询的结果是文档 1 的评分更高： 为了理解导致这样的原因，需要回想一下 bool 是如何计算评分的： 它会执行 should 语句中的两个查询。加和两个查询的评分。乘以匹配语句的总数。除以所有语句总数（这里为：2）。 文档 1 的两个字段都包含 brown 这个词，所以两个 match 语句都能成功匹配并且有一个评分。文档 2 的 body 字段同时包含 brown 和 fox 这两个词，但 title 字段没有包含任何词。这样， body 查询结果中的高分，加上 title 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。 在本例中， title 和 body 字段是相互竞争的关系，所以就需要找到单个 最佳匹配 的字段。 如果不是简单将每个字段的评分结果加在一起，而是将 最佳匹配 字段的评分作为查询的整体评分，结果会怎样？这样返回的结果可能是： 同时 包含 brown 和 fox 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。 dis_max 查询不使用 bool 查询，可以使用 dis_max 即分离 最大化查询 （Disjunction Max Query） 。分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应。分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ： 12345678910&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 得到我们想要的结果为： Top-level parameters for dis_maxedit queries(Required, array of query objects) Contains one or more query clauses. Returned documents must match one or more of these queries. If a document matches multiple queries, Elasticsearch uses the highest relevance score. tie_breaker(Optional, float) Floating point number between 0 and 1.0 used to increase the relevance scores of documents matching multiple query clauses. Defaults to 0.0. You can use the tie_breaker value to assign higher relevance scores to documents that contain the same term in multiple fields than documents that contain this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields. If a document matches multiple clauses, the dis_max query calculates the relevance score for the document as follows: Take the relevance score from a matching clause with the highest score.Multiply the score from any other matching clauses by the tie_breaker value.Add the highest score to the multiplied scores.If the tie_breaker value is greater than 0.0, all matching clauses count, but the clause with the highest score counts most. dis_max，只是取分数最高的那个query的分数而已，完全不考虑其他query的分数，这种一刀切的做法，可能导致在有其他query的影响下，score不准确的情况，这时为了使用结果更准确，最好还是要考虑到其他query的影响;使用 tie_breaker 将其他query的分数也考虑进去, tie_breaker 参数的意义，将其他query的分数乘以tie_breaker，然后综合考虑后与最高分数的那个query的分数综合在一起进行计算，这样做除了取最高分以外，还会考虑其他的query的分数。tie_breaker的值，设置在在0~1之间，是个小数就行，没有固定的值 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 多字段搜索 » 最佳字段 Elasticsearch中文文档 Elasticsearch Guide [7.x] » Query DSL » Compound queries » Disjunction max query","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"elasticsearch被破坏的相似度","date":"2021-07-06T11:50:11.000Z","path":"wiki/elasticsearch被破坏的相似度/","text":"在讨论更复杂的 多字段搜索 之前，让我们先快速解释一下为什么只在主分片上 创建测试索引 。 用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤：用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。 为了理解为什么会这样，可以设想，我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 foo 。可能是分片 1 有其中 3 个 foo 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。 在 什么是相关度？中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 词频/逆向文档频率 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 逆向文档频率 将 某个词在索引内所有文档出现的百分数 考虑在内，出现的频率越高，它的权重就越低。 但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。 因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 foo 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， foo 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。 在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。 为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 match 查询 里介绍的那样，如果只有一个分片，那么本地的 IDF 就是 全局的 IDF。 第二个方式就是在搜索请求后添加 ?search_type=dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。 不要在生产环境上使用 dfs_query_then_fetch 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。 参考资料 Elasticsearch: 权威指南 » 基础入门 » 排序与相关性 » 什么是相关性? Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 被破坏的相关度！","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"canal同步es后部分字段为null","date":"2021-07-06T08:11:15.000Z","path":"wiki/canal同步es后部分字段为null/","text":"现象 配置文件如下： 123456789101112131415dataSourceKey: defaultDS # 源数据源的key, 对应上面配置的srcDataSources中的值destination: example # cannal的instance或者MQ的topicgroupId: g1 # 对应MQ模式下的groupId, 只会同步对应groupId的数据esMapping: _index: rd_member_fans_info # es 的索引名称 _type: _doc # es 的doc名称 _id: _id # es 的_id, 如果不配置该项必须配置下面的pk项_id则会由es自动分配# pk: id # 如果不需要_id, 则需要指定一个属性为主键属性# # sql映射 sql: &#x27;SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.create_time,t.update_time,t.`status` FROM rd_member_fans_info t&#x27;# objFields:# _labels: array:; # 数组或者对象属性, array:; 代表以;字段里面是以;分隔的# _obj: object # json对象 etlCondition: &quot;where t.update_time&gt;=&#123;&#125;&quot; # etl 的条件参数 commitBatch: 3000 # 提交批大小 ⚠️ ⚠️sql执行是没有问题的！ canal-adapter 获取binlog数据也没有问题，显示日志如下： 12021-07-06 15:39:24.588 [pool-1-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:3,&quot;redtom_id&quot;:1,&quot;fans_redtom_id&quot;:1,&quot;fans_username&quot;:&quot;1&quot;,&quot;fans_introduce&quot;:&quot;1&quot;,&quot;fans_avatar&quot;:&quot;1&quot;,&quot;is_each_following&quot;:1,&quot;follow_channel&quot;:1,&quot;create_time&quot;:1625556851000,&quot;update_time&quot;:1625556851000,&quot;status&quot;:2&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625557164000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;rd_member_fans_info&quot;,&quot;ts&quot;:1625557164587,&quot;type&quot;:&quot;INSERT&quot;&#125; 然后看一下我创建索引的mapping 解决方法调整sql如下： SELECT t.id as _id , t.redtom_id ,t.fans_redtom_id,t.fans_username,t.fans_introduce,t.fans_avatar,t.is_each_following,t.follow_channel,t.status as is_deleted , t.create_time,t.update_time FROM rd_member_fans_info t 调整了那些东西呢？ status 的顺序提前而已！ 测试执行一下命令：curl http://127.0.0.1:8081/etl/es7/rd_member_fans_info.yml -X POST canal-adapter 日志如下： 122021-07-06 16:21:33.519 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member_fans_info2021-07-06 16:21:33.527 [http-nio-8081-exec-1] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - 数据全量导入完成, 一共导入 3 条数据, 耗时: 7 查看es数据：","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch-analyzer","date":"2021-07-06T07:02:01.000Z","path":"wiki/elasticsearch-analyzer/","text":"测试常见分析器GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 12345&#123; &quot;analyzer&quot;: &quot;english&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat #Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; GET /_analyze 1234&#123; &quot;analyzer&quot;: &quot;stop&quot;, &quot;text&quot;: &quot;Oredr it now from Amazon #fun #girlpower #fatscooter #Fat Cats&quot;&#125; 默认分析器虽然我们可以在字段层级指定分析器，但是如果该层级没有指定任何的分析器，那么我们如何能确定这个字段使用的是哪个分析器呢？ 分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下： 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 在搜索时，顺序有些许不同： 查询自己定义的 analyzer ，否则 字段映射里定义的 analyzer ，否则 索引设置中名为 default 的分析器，默认为 standard 标准分析器 有时，在索引时和搜索时使用不同的分析器是合理的。我们可能要想为同义词建索引（例如，所有 quick 出现的地方，同时也为 fast 、 rapid 和 speedy 创建索引）。但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 quick 、 fast 、 rapid 或 speedy 。 为了区分，Elasticsearch 也支持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。 如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样： 查询自己定义的 analyzer ，否则字段映射里定义的 search_analyzer ，否则字段映射里定义的 analyzer ，否则索引设置中名为 default_search 的分析器，默认为索引设置中名为 default 的分析器，默认为standard 标准分析器 保持简单多数情况下，会提前知道文档会包括哪些字段。最简单的途径就是在创建索引或者增加类型映射时，为每个全文字段设置分析器。这种方式尽管有点麻烦，但是它让我们可以清楚的看到每个字段每个分析器是如何设置的。 通常，多数字符串字段都是 not_analyzed 精确值字段，比如标签（tag）或枚举（enum），而且更多的全文字段会使用默认的 standard 分析器或 english 或其他某种语言的分析器。这样只需要为少数一两个字段指定自定义分析：或许标题 title 字段需要以支持 输入即查找（find-as-you-type） 的方式进行索引。 可以在索引级别设置中，为绝大部分的字段设置你想指定的 default 默认分析器。然后在字段级别设置中，对某一两个字段配置需要指定的分析器。 📒 📒 📒对于和时间相关的日志数据，通常的做法是每天自行创建索引，由于这种方式不是从头创建的索引，仍然可以用 索引模板（Index Template） 为新建的索引指定配置和映射。 参考资料 Elasticsearch: 权威指南 » 深入搜索 » 全文搜索 » 控制分析","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"Parameter index out of range (1 > number of parameters, which is 0).","date":"2021-07-06T05:03:16.000Z","path":"wiki/Parameter-index-out-of-range-1-number-of-parameters-which-is-0/","text":"问题记录123456789101112132021-07-06 12:39:31.179 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:39:31.186 [http-nio-8081-exec-2] ERROR com.alibaba.otter.canal.client.adapter.support.Util - sqlRs has error, sql: SELECT COUNT(1) FROM ( select t.redtom_id as id, t.username, t.nickname, t.avatar, t.status, t.mobile, t.mobile_region_no, t.email, t.gender, t.password,t.salt,t.birthday,t.introduce,t.country,t.region,t.level,t.is_vip,t.follows ,t.fans,t.likes_num, t.collects_num, t.instagram_account, t.youtube_account, t.facebook_account, t.twitter_account,t.create_ip, t.create_time,t.update_time from rd_member r where t.create_time&gt;=&#x27;&#123;0&#125;&#x27;) _CNT2021-07-06 12:39:31.188 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0).java.lang.RuntimeException: java.sql.SQLException: Parameter index out of range (1 &gt; number of parameters, which is 0). at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:65) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:62) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) [client-adapter.es7x-1.1.5-SNAPSHOT-jar-with-dependencies.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) [client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] 如何解决我执行的操作如下：👇curl http://127.0.0.1:8081/etl/es7/customer.yml -X POST -d &quot;params=2019-08-31 00:00:00&quot; 但是我的 es7/rd_member.yml的配置文件如下： etlCondition:&quot;where a.c_time&gt;=&#39;&#123;0&#125;&#39;&quot; # etl 的条件参数 应该改成：etlCondition:&quot;where a.c_time&gt;=&#123;&#125;&quot; # etl 的条件参数","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"field name is null or empty","date":"2021-07-06T04:53:44.000Z","path":"wiki/field-name-is-null-or-empty/","text":"canal adapter 报错信息123456789101112131415161718192021222021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO o.a.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring FrameworkServlet &#x27;dispatcherServlet&#x27;2021-07-06 12:46:31.959 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization started2021-07-06 12:46:31.968 [http-nio-8081-exec-2] INFO org.springframework.web.servlet.DispatcherServlet - FrameworkServlet &#x27;dispatcherServlet&#x27;: initialization completed in 9 ms2021-07-06 12:46:31.995 [http-nio-8081-exec-2] INFO c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - start etl to import data to index: rd_member2021-07-06 12:46:32.027 [http-nio-8081-exec-2] ERROR c.a.otter.canal.client.adapter.es7x.etl.ESEtlService - field name is null or emptyjava.lang.IllegalArgumentException: field name is null or empty at org.elasticsearch.index.query.BaseTermQueryBuilder.&lt;init&gt;(BaseTermQueryBuilder.java:113) ~[na:na] at org.elasticsearch.index.query.TermQueryBuilder.&lt;init&gt;(TermQueryBuilder.java:75) ~[na:na] at org.elasticsearch.index.query.QueryBuilders.termQuery(QueryBuilders.java:202) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.lambda$executeSqlImport$1(ESEtlService.java:141) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.Util.sqlRS(Util.java:60) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.executeSqlImport(ESEtlService.java:64) ~[na:na] at com.alibaba.otter.canal.client.adapter.support.AbstractEtlService.importData(AbstractEtlService.java:105) ~[client-adapter.common-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.client.adapter.es7x.etl.ESEtlService.importData(ESEtlService.java:56) ~[na:na] at com.alibaba.otter.canal.client.adapter.es7x.ES7xAdapter.etl(ES7xAdapter.java:79) ~[na:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:100) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.adapter.launcher.rest.CommonRest.etl(CommonRest.java:123) ~[client-adapter.launcher-1.1.5-SNAPSHOT.jar:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke 问题排查操作是向数据库中插入一条数据，通过canal-adapter同步到elasticsearch中，接口发生以上错误！现象是canal-adapter检测到和mysql的数据变化，但是同步到es的时候发生了错误；猜想大概是某个为空导致存到es的时候发生异常； 然后查看es7下的mapping配置： 发现我的sql查id的时候写错了，别名应该写成_id,对应elasticsearch的_id","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"}]},{"title":"elasticsearch映射","date":"2021-07-05T14:54:05.000Z","path":"wiki/elasticsearch映射/","text":"Elasticsearch 支持如下简单域类型： 字符串: string （es7之后编程text） 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 查看索引的mappingGET /gb/_mapping/tweet 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 自定义域映射尽管在很多情况下基本域数据类型已经够用，但你经常需要为单独域自定义映射，特别是字符串域。自定义映射允许你执行下面的操作： 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： 12345&#123; &quot;number_of_clicks&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;&#125; 默认， string (text) 类型域会被认为包含全文。就是说，它们的值在索引前，会通过一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。 string 域映射的两个最重要属性是 index 和 analyzer 。 indexindex 属性控制怎样索引字符串。它可以是下面三个值： analyzed首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。 no不索引这个域。这个域不会被搜索到。 (比如一些隐私信息) string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ： ⚠️ ⚠️其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。 analyzer对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english; 123456&#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 更新映射当你首次创建一个索引的时候，可以指定类型的映射。你也可以使用 /_mapping 为新类型（或者为存在的类型更新映射）增加映射。⚠️ ⚠️尽管你可以 增加 一个存在的映射，你不能 修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。 我们可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed 。 为了描述指定映射的两种方式，我们先删除 gd 索引：DELETE /gb然后创建一个新索引，指定 tweet 域使用 english 分析器： 12345678910111213141516171819202122PUT /gb &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 稍后，我们决定在 tweet 映射增加一个新的名为 tag 的 not_analyzed 的文本域，使用 _mapping ： 123456789PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中 测试映射你可以使用 analyze API 测试字符串域的映射。比较下面两个请求的输出： 1234567891011GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125; tweet 域产生两个词条 black 和 cat ， tag 域产生单独的词条 Black-cats 。换句话说，我们的映射正常工作。 参考资料 Elasticsearch权威指南","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"}]},{"title":"分析与分析器","date":"2021-07-05T14:43:44.000Z","path":"wiki/分析与分析器/","text":"分析包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 ，之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。 内置分析器但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：&quot;Set the shape to semi-transparent by calling set_trans(5)&quot; 标准分析器标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。 什么时候使用分析器当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。 全文查询，理解每个域是如何定义的，因此它们可以做正确的事： 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。 测试分析器有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本： 12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条： 12345678910111213141516171819202122232425&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。 每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于​keep_types token 过滤器​。 analyze API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。 你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域—​不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。 要做到这一点，我们必须手动指定这些域的映射。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"倒排索引","date":"2021-07-05T14:17:00.000Z","path":"wiki/倒排索引/","text":"Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容： The quick brown fox jumped over the lazy dogQuick brown foxes leap over lazy dogs in summer为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。 如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如： Quick 可以小写化为 quick 。foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的， dogs 可以为提取为 dog 。jumped 和 leap 是同义词，可以索引为相同的单词 jump 。现在索引看上去像这样：这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！ 这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch分页查询","date":"2021-07-05T14:08:23.000Z","path":"wiki/elasticsearch分页查询/","text":"和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size显示应该返回的结果数量，默认是 10from显示应该跳过的初始结果数量，默认是 0如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果： GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 ⚠️ ⚠️ ⚠️考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。 在分布式系统中深度分页 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页—​结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因 参考资料 elasticsearch权威指南 干货 | 全方位深度解读 Elasticsearch 分页查询 Paginate search results","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"多索引多类型搜索","date":"2021-07-05T14:02:34.000Z","path":"wiki/多索引多类型搜索/","text":"如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。 然而，经常的情况下，你想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示： /_search在所有的索引中搜索所有的类型/gb/_search在 gb 索引中搜索所有的类型/gb,us/_search在 gb 和 us 索引中搜索所有的文档/g*,u*/_search在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search在所有的索引中搜索 user 和 tweet 类型当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的—​只是会涉及到更多的分片。 注意 ⚠️搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch重要配置","date":"2021-07-05T13:22:32.000Z","path":"wiki/elasticsearch重要配置/","text":"虽然Elasticsearch仅需要很少的配置，但有许多设置需要手动配置，并且在进入生产之前绝对必须进行配置。 path.data 和 path.logscluster.namenode.namebootstrap.memory_locknetwork.hostdiscovery.zen.ping.unicast.hostsdiscovery.zen.minimum_master_nodespath.data 和 path.logs如果使用.zip或.tar.gz归档，则数据和日志目录是$ES_HOME的子文件夹。 如果这些重要的文件夹保留在其默认位置，则存在将Elasticsearch升级到新版本时被删除的高风险。 在生产使用中，肯定得更改数据和日志文件夹的位置： 123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch RPM和Debian发行版已经使用数据和日志的自定义路径。 path.data 设置可以设置为多个路径，在这种情况下，所有路径将用于存储数据（属于单个分片的文件将全部存储在同一数据路径上）： 12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3 cluster.name节点只能在群集与群集中的所有其他节点共享其cluster.name时才能加入群集。 默认名称为elasticsearch，但您应将其更改为描述集群用途的适当名称。cluster.name: logging-prod确保不要在不同的环境中重复使用相同的集群名称，否则可能会导致加入错误集群的节点。 node.name默认情况下，Elasticsearch将使用随机生成的uuid的第一个字符作为节点id。 请注意，节点ID是持久化的，并且在节点重新启动时不会更改，因此默认节点名称也不会更改。配置一个更有意义的名称是值得的，这是重启节点后也能一直保持的优势：node.name: prod-data-2node.name也可以设置为服务器的HOSTNAME，如下所示： 12node.name: $&#123;HOSTNAME&#125;bootstrap.memory_lock 没有JVM被交换到磁盘上这事对于节点的健康来说是至关重要的。一种实现方法是将bootstrap.memory_lock设置为true。要使此设置生效，需要首先配置其他系统设置。 有关如何正确设置内存锁定的更多详细信息，请参阅启用bootstrap.memory_lock。 network.host默认情况下，Elasticsearch仅仅绑定在本地回路地址——如：127.0.0.1与[::1]。这在一台服务器上跑一个开发节点是足够的。提示 事实上，多个节点可以在单个节点上相同的$ES_HOME位置一同运行。这可以用于测试Elasticsearch形成集群的能力,但这种配置方式不推荐用于生产环境。 为了将其它服务器上的节点形成一个可以相互通讯的集群，你的节点将不能绑定在一个回路地址上。 这里有更多的网路配置，通常你只需要配置network.host：network.host: 192.168.1.10network.host也可以配置成一些能识别的特殊的值，譬如：_local_、_site、_global_，它们可以结合指定:ip4与ip6来使用。更多相信信息请参见：网路配置 重要 👇 一旦你自定义了network.host的配置，Elasticsearch将假设你已经从开发模式转到了生产模式，并将升级系统检测的警告信息为异常信息。更多信息请参见：开发模式vs生产模式 discovery.zen.ping.unicast.hosts（单播发现）开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的回路地址，并扫描9300年到9305的端口去连接同一机器上的其他节点,试图连接到相同的服务器上运行的其他节点。它提供了不需要任何配置就能自动组建集群的体验。当与其它机器上的节点要形成一个集群时，你需要提供一个在线且可访问的节点列表。像如下来配置： 1234discovery.zen.ping.unicast.hosts: - 192.168.1.10:9300 - 192.168.1.11 #① - seeds.mydomain.com #② ① 未指定端口时，将使用默认的transport.profiles.default.port值，如果此值也为设置则使用transport.tcp.port ② 主机名将被尝试解析成能解析的多个IP discovery.zen.minimum_master_nodes为防止数据丢失，配置discovery-zen-minimum_master_nodes将非常重要，他规定了必须至少要有多少个master节点才能形成一个集群。没有此设置时，一个集群在发生网络问题是可能会分裂成多个集群——脑裂——这将导致数据丢失。更多详细信息请参见：通过minimum_master_nodes避免脑裂为避免脑裂，你需要根据master节点数来设置法定人数：(master_eligible_nodes / 2) + 1换句话说，如果你有三个master节点，最小的主节点数因被设置为(3/2)+1或者是2discovery.zen.minimum_master_nodes: 2 参考资料 elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"elasticsearch操作索引","date":"2021-07-05T13:11:01.000Z","path":"wiki/elasticsearch操作索引/","text":"创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 查看索引的mappingGET yj_visit_data/_mapping 1234567891011121314151617181920212223242526&#123; &quot;yj_visit_data&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;_class&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;article&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查询所有GET yj_visit_data/_search 12345&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 删除所有POST yj_visit_data/_delete_by_query 123456&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125; 通过文章删除POST yj_visit_data/_delete_by_query 1234567&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 根据文章查询GET yj_visit_data/_search 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;article.keyword&quot;: &quot;2019/01/3&quot; &#125; &#125;&#125; 修改索引1234POST customer/_doc/1&#123; &quot;name&quot;:&quot;2&quot;&#125;","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"elasticsearch基础api","date":"2021-07-05T12:53:09.000Z","path":"wiki/elasticsearch基础cat_api/","text":"cat API集群健康状态GET _cat/health?v&amp;pretty 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1625489855 12:57:35 my-application yellow 1 1 35 35 0 0 23 0 - 60.3% 或者直接在服务器上调用rest接口：curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’ 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1475247709 17:01:49 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 我们可以看到我们名为 my-application 的集群与 yellow 的 status。 无论何时我们请求集群健康，我们可以获得 green，yellow，或者 red 的 status。Green 表示一切正常（集群功能齐全）， yellow 表示所有数据可用，但是有些副本尚未分配（集群功能齐全），red 意味着由于某些原因有些数据不可用。注意，集群是 red，它仍然具有部分功能（例如，它将继续从可用的分片中服务搜索请求），但是您可能需要尽快去修复它，因为您已经丢失数据了。 另外，从上面的响应中，我们可以看到共计 1 个 node（节点）和 0 个 shard（分片），因为我们还没有放入数据的。注意，因为我们使用的是默认的集群名称（elasticsearch），并且 Elasticsearch 默认情况下使用 unicast network（单播网络）来发现同一机器上的其它节点。有可能您不小心在您的电脑上启动了多个节点，然后它们全部加入到了单个集群。在这种情况下，你会在上面的响应中看到不止 1 个 node（节点）。 查看集群分布GET _cat/nodes?v&amp;pretty 12ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name172.19.0.1 20 61 15 0.02 0.04 0.29 cdhilmrstw * redtom-es-1 查看所有索引GET _cat/indices?v&amp;pretty 1234health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open rd-logstash-2021.06.19 p5iej71MQVW12s2awNv8nw 1 1 61236 0 16.3mb 16.3mbyellow open demo_index k6VTs7tdS0ysot-rPwxG9A 1 1 1 0 5.5kb 5.5kbgreen open kibana_sample_data_flights A7c5DViGSISii8FA0dNlGw 1 0 13059 0 5.6mb 5.6mb 查看所有索引的数量GET _cat/count?v&amp;pretty 12epoch timestamp count1625490245 13:04:05 838913 磁盘分配情况GET _cat/allocation?v&amp;pretty 123shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 35 308.7mb 20.1gb 215.9gb 236.1gb 8 172.19.0.1 172.19.0.1 redtom-es-1 23 UNASSIGNED 查看shard情况GET _cat/shards?v&amp;pretty 12345678index shard prirep state docs store ip nodeyj_visit_data 0 p STARTED 0 208b 172.19.0.1 redtom-es-1yj_visit_data 0 r UNASSIGNED demo_index 0 p STARTED 1 5.5kb 172.19.0.1 redtom-es-1demo_index 0 r UNASSIGNED rbtags 0 p STARTED 0 208b 172.19.0.1 redtom-es-1.kibana_1 0 p STARTED 280 11.5mb 172.19.0.1 redtom-es-1.kibana_task_manager_1 0 p STARTED 5 5.8mb 172.19.0.1 redtom-es-1 yj_visit_data 设置了一个副本分区，但是没有副节点，所以节点状态显示未分配； 参考资料 Elastic 官方文档 codingdict.com","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"}]},{"title":"canal同步mysql数据到elasticsearch","date":"2021-07-05T03:26:50.000Z","path":"wiki/canal同步mysql数据到elasticsearch/","text":"首先安装elk推荐大家到elasic中文社区去下载 👉 【传送】⚠️ elastcisearch | logstash | kibana 的版本最好保持一直，否则会出现很多坑的，切记！ 安装ELK的步骤这里就不做介绍了，可以查看 👉 【TODO】 下载安装canal-adaptercanal github传送门 👉 【Alibaba Canal】 canal-client 模式可以参照canal给出的example项目和官方文档给出的例子来测试 依赖配置12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 创建maven项目保证canal-server 已经正确启动 👈 然后启动下面服务，操作数据库即可看到控制台的日志输出； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.redtom.canal.deploy;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import java.net.InetSocketAddress;import java.util.List;/** * @Author gaolei * @Date 2021/6/30 2:57 下午 * @Version 1.0 */@Slf4j@Componentclass CanalClient implements InitializingBean &#123; private final static int BATCH_SIZE = 1000; @Override public void afterPropertiesSet() throws Exception &#123; // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(&quot;***.***.***.***&quot;, 11111), &quot;example&quot;, &quot;canal&quot;, &quot;canal&quot;); try &#123; //打开连接 connector.connect(); //订阅数据库表,全部表 connector.subscribe(&quot;.*\\\\..*&quot;); //回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有ack的地方开始拿 connector.rollback(); while (true) &#123; // 获取指定数量的数据 Message message = connector.getWithoutAck(BATCH_SIZE); //获取批量ID long batchId = message.getId(); //获取批量的数量 int size = message.getEntries().size(); //如果没有数据 if (batchId == -1 || size == 0) &#123; try &#123; //线程休眠2秒 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; //如果有数据,处理数据 printEntry(message.getEntries()); &#125; //进行 batch id 的确认。确认之后，小于等于此 batchId 的 Message 都会被确认。 connector.ack(batchId); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; connector.disconnect(); &#125; &#125; /** * 打印canal server解析binlog获得的实体类信息 */ private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) &#123; //开启/关闭事务的实体类型，跳过 continue; &#125; //RowChange对象，包含了一行数据变化的所有特征 //比如isDdl 是否是ddl变更操作 sql 具体的ddl sql beforeColumns afterColumns 变更前后的数据字段等等 CanalEntry.RowChange rowChage; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;ERROR ## parser of eromanga-event has an error , data:&quot; + entry.toString(), e); &#125; //获取操作类型：insert/update/delete类型 CanalEntry.EventType eventType = rowChage.getEventType(); //打印Header信息 log.info(&quot;headers:&#123;&#125; &quot;, String.format(&quot;================》; binlog[%s:%s] , name[%s,%s] , eventType : %s&quot;, entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); //判断是否是DDL语句 if (rowChage.getIsDdl()) &#123; log.info(&quot;================》;isDdl: true,sql: &#123;&#125;&quot;, rowChage.getSql()); &#125; //获取RowChange对象里的每一行数据，打印出来 for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; //如果是删除语句 if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); //如果是新增语句 &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); //如果是更新的语句 &#125; else &#123; //变更前的数据 log.info(&quot;-------&gt;; before&quot;); printColumn(rowData.getBeforeColumnsList()); //变更后的数据 log.info(&quot;-------&gt;; after&quot;); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; for (CanalEntry.Column column : columns) &#123; log.info(&quot; &#123;&#125; : &#123;&#125; update= &#123;&#125;&quot;, column.getName(), column.getValue(), column.getUpdated()); &#125; &#125;&#125; canal-adapter 模式adapter 配置文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546server: port: 8081spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1 batchSize: 1 retries: 0 timeout: accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 172.25.101.75:11111 canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 canal.tcp.username: canal.tcp.password: srcDataSources: defaultDS: url: jdbc:mysql://xxxx:pppp/database?useUnicode=true username: root password: pwd canalAdapters: - instance: example # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: es7 hosts: 172.25.101.75:9300 # 127.0.0.1:9200 for rest mode properties: mode: transport # or rest# # security.auth: test:123456 # only used for rest mode cluster.name: my-application# - name: kudu# key: kudu# properties:# kudu.master.address: 127.0.0.1 # &#x27;,&#x27; split multi address 我的elasticsearch是7.10.0版本的application.yml bootstrap.yml es6 es7 hbase kudu logback.xml META-INF rdb所以：👇 123cd es7biz_order.yml customer.yml mytest_user.ymlvim customer.yml customer.yml 配置文件如下： 123456789101112dataSourceKey: defaultDSdestination: examplegroupId: g1esMapping: _index: customer _id: id relations: customer_order: name: customer sql: &quot;select t.id, t.name, t.email from customer t&quot; etlCondition: &quot;where t.c_time&gt;=&#123;&#125;&quot; commitBatch: 3000 创建表结构12345678910CREATE TABLE `customer` ( `id` bigint(20) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, `email` varchar(255) DEFAULT NULL, `order_id` int(11) DEFAULT NULL, `order_serial` varchar(255) DEFAULT NULL, `order_time` datetime DEFAULT NULL, `customer_order` varchar(255) DEFAULT NULL, `c_time` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 创建索引12345678910111213141516171819202122232425262728293031PUT customer&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;order_serial&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_time&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;customer_order&quot;:&#123; &quot;type&quot;:&quot;join&quot;, &quot;relations&quot;:&#123; &quot;customer&quot;:&quot;order&quot; &#125; &#125; &#125; &#125;&#125; 测试canal-adapter同步效果创建一条记录122021-07-05 11:50:53.725 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;1&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457053000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457053724,&quot;type&quot;:&quot;INSERT&quot;&#125;Affected indexes: customer Elastcisearch 效果 1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;1&quot;, &quot;email&quot; : &quot;1&quot;, &quot;customer_order&quot; : &#123; &quot;name&quot; : &quot;customer&quot; &#125; &#125; &#125; ] &#125;&#125; 修改数据122021-07-05 11:54:36.402 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457275000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:[&#123;&quot;name&quot;:&quot;1&quot;,&quot;email&quot;:&quot;1&quot;,&quot;order_id&quot;:1,&quot;order_serial&quot;:&quot;1&quot;,&quot;customer_order&quot;:&quot;1&quot;&#125;],&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457276401,&quot;type&quot;:&quot;UPDATE&quot;&#125;Affected indexes: customer Elastcisearch 效果 删除一条数据122021-07-05 11:56:51.524 [pool-3-thread-1] DEBUG c.a.o.canal.client.adapter.es.core.service.ESSyncService - DML: &#123;&quot;data&quot;:[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;2&quot;,&quot;email&quot;:&quot;2&quot;,&quot;order_id&quot;:2,&quot;order_serial&quot;:&quot;2&quot;,&quot;order_time&quot;:1625457046000,&quot;customer_order&quot;:&quot;2&quot;,&quot;c_time&quot;:1625457049000&#125;],&quot;database&quot;:&quot;redtom_dev&quot;,&quot;destination&quot;:&quot;example&quot;,&quot;es&quot;:1625457411000,&quot;groupId&quot;:&quot;g1&quot;,&quot;isDdl&quot;:false,&quot;old&quot;:null,&quot;pkNames&quot;:[],&quot;sql&quot;:&quot;&quot;,&quot;table&quot;:&quot;customer&quot;,&quot;ts&quot;:1625457411523,&quot;type&quot;:&quot;DELETE&quot;&#125;Affected indexes: customer Elastcisearch 效果 参考资料 使用canal client-adapter完成mysql到es数据同步教程(包括全量和增量) es 同步问题 #1514 Github issue canal v1.1.4 文档手册 Sync es","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"}]},{"title":"mysql配置binlog","date":"2021-07-05T03:06:36.000Z","path":"wiki/binlog配置/","text":"开启binlog[mysqld]log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 查看binlog状态mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"mysql常用命令","date":"2021-07-05T03:06:36.000Z","path":"wiki/mysql常用命令/","text":"binlog相关命令mysql&gt; show variables like ‘binlog_format’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+ show variables like ‘log_bin’; 12345+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+ 用户&amp;权限创建用户并授权root用户登录： mysql -u root -p 然后输入密码创建用户： create user &#39;yjuser&#39;@&#39;%&#39; identified by &#39;u-bx.com&#39;;授权用户只读权限： grant SELECT on mirror.* to &#39;yjuser&#39;@&#39;%&#39; IDENTIFIED by &#39;u-bx.com&#39;;刷新权限：flush privileges; 查看当前用户select User();","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"categories":[{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"}]},{"title":"flink简单上手","date":"2021-07-04T14:23:43.000Z","path":"wiki/flink简单上手/","text":"mac 安装 flink1、执行 brew install apache-flink 命令 123456789gaolei:/ gaolei$ brew install apache-flinkUpdating Homebrew...==&gt; Auto-updated Homebrew! Updated 1 tap (homebrew/services).No changes to formulae.==&gt; Downloading https://archive.apache.org/dist/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgz######################################################################## 100.0%🍺 /usr/local/Cellar/apache-flink/1.9.1: 166 files, 277MB, built in 15 minutes 29 seconds 2、执行flink启动脚本 12/usr/local/Cellar/apache-flink/1.9.1/libexec/bin./start-cluster.sh WordCount批处理Demo创建maven项目，导入依赖 注意自己的flink版本 👇👇 12345678910111213141516171819202122&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写批处理程序123456789101112131415161718192021222324public static void main(String[] args) throws Exception &#123; // 1、创建执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2、读取文件数据 String inputPath = &quot;/Users/gaolei/Documents/DemoProjects/flink-start/src/main/resources/hello.txt&quot;; DataSource&lt;String&gt; dataSource = env.readTextFile(inputPath); // 对数据集进行处理 按照空格分词展开 转换成（word，1）二元组 AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = dataSource.flatMap(new MyFlatMapper()) // 按照第一个位置 -&gt; word 分组 .groupBy(0) .sum(1); result.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; // 首先按照空格分词 String[] words = s.split(&quot; &quot;); // 遍历所有的word 包装成二元组输出 for (String word : words) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(word, 1)); &#125; &#125; &#125; 准备数据源文件123456hello sparkhello worldhello javahello flinkhow are youwhat is your name 执行结果123456789101112(is,1)(what,1)(you,1)(flink,1)(name,1)(world,1)(hello,4)(your,1)(are,1)(java,1)(how,1)(spark,1) flink 处理流式数据1、通过 nc -lk &lt;port&gt; 打开一个socket服务，监听7777端口 用于模拟实时的流数据 2、java代码处理流式数据 123456789101112131415161718192021222324252627public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建流处理执行环境 StreamExecutionEnvironment env = StreamContextEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数（设置成1即单线程处理） // env.setMaxParallelism(32); // 从文件中读取数据// String inputPath = &quot;/tmp/Flink_Tutorial/src/main/resources/hello.txt&quot;;// DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 从socket文本流读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 基于数据流进行转换计算 DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); resultStream.print(); // 执行任务 env.execute(); &#125;&#125; 3、在终端数据数据，如下： 4、在首次启动的时候遇到一个错误 ❌Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/streaming/api/datastream/DataStream处理方法可参照 参考资料 👇 参考资料 Exception in thread “main” java.lang.NoClassDefFoundError 解决方案","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Apache Flink","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink简介/","text":"官方地址请戳👉 【传送】 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink 是一个框架和分布式处理器引擎，用于对无界和有界数据进行状态计算； Why Flink 流数据更真实地反应了我们的生活方式 传统的数据架构是基于有限数据集的 我们的目标1、低延迟 毫秒级响应2、高吞吐 能够处理海量数据 分布式3、结果的准确性和良好的容错性 Where need Flink 电商和市场营销数据报表、广告投放、业务流程需要 物联网（IOT）传感器实时数据采集和显示，实时报警，交通运输业 电信业基站流量调配 银行和金融业实时结算和通知推送、实时检测异常行为 传统数据处理架构 传统的数据处理架构如上👆CRM(用户关系系统)， Order System(订单系统), Web App (用户点击时间)，当用户出发行为之后需要系统作出响应，首先由上层的计算层处理计算逻辑，计算层的逻辑计算依赖下面的存储层，计算层计算完成之后，将响应返回给客户端。这种基于传统数据库方式无法满足高并发场景，数据库的并发量都是很低的。 分析处理流程 分析处理流程架构如上👆，数据先有传统的关系数据库，经过提取，清洗过滤等，将数据存放到数据仓库，然后通过一些sql处理，生成数据报表和一些其他的查询。 问题也很明显，实时性太差了，处理流程太长，无法满足毫秒级需求 数据来源不唯一，能满足海量数据和高并发的需求，但是无法满足实时的需求 有状态的流式处理 把当前做流式计算所需要的数据不存放在数据库中，而是简单粗暴的直接放到本地内存中； 内存不稳定？周期性的检查点，数据存盘和故障检测； lambda架构用两台系统同时保障低延迟和结果准确； 这套架构分成两个流程，上面为批处理流程，数据收集到一定程序，交给批处理器处理，最终产生一个批处理结果 下面的流程为流式处理流程，保证能快速得到结果 最终有我们在应用层根据实际问题选择具体的处理结果交给应用程序这种架构有什么缺陷？可能得到的结果是不准确的，我们可以先快速的得到一个实时计算的结果，隔一段时间之后在来看批处理产生的结果。实现两台系统和维护两套系统，成本很大； 第三代流式处理架构Apache Flink 可以完美解决上面的问题👆Strom无法满足海量数据； Sparking Stream 无法满足低延迟； 基于事件驱动 （Event-driven） 处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。数据可以被作为 无界 或者 有界 流来处理。 无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流&lt;/&gt; 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 其他特点 支持事件时间（event-time）和处理时间（processing-time）语义 精确一次的状态一致性保证 低延迟 每秒处理数百万个事件，毫秒级延迟 与众多常用的存储系统链接 高可用，动态扩展，支持7*24全天运行 参考资料 1、尚硅谷 2021 Flink Java版2、Apache Flink Documentation","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"Flink 运行时架构","date":"2021-07-04T12:45:57.000Z","path":"wiki/flink运行时架构/","text":"Flink运行时组件 JobManager (作业管理器)JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成： ResourceManagerResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。 DispatcherDispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。 JobMasterJobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。 TaskManager （任务管理器） Flink中的工作进程，通常在flink中会有多个TaskManager运行，每一个TaskMaganer都包含一定数量的插槽（slots）. 插槽的数量限制了TaskManager能够执行的任务数量； 启动之后，TaskManager会向资源管理器注册他的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用，JobManager就可以向插槽分配任务（tasks）来执行了 在执行的过程中，一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据。 任务提交流程 任务调度原理Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。 Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run …中运行。 可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。 思考问题🤔 怎样实现并行计算？ 多线程 并行的任务，需要占用多少solt？ 一个流处理程序，到底包含多少个任务？ Tasks 和算子链并行度（Parallelism） 一个特定算子的子任务（subtask）的个数被称之为并行度； 一般情况下，一个Stream的并行度就是其所有算子中最大的并行度。整个流也有一个并行度，就是所有算子所有任务的并行度之和；对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。链行为是可以配置的；请参考链文档以获取详细信息。 TaskManager 和 Slots","tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}],"categories":[{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"}]},{"title":"拥塞避免","date":"2021-07-04T08:46:13.000Z","path":"wiki/拥塞避免/","text":"拥塞避免拥塞控制的慢启动是以指数方式快速的通过试探来扩大拥塞窗口的，但是一旦发生网络丢包，则肯定是很多报文段都会都是，因为窗口时称被增长的；为了解决这种问题，需要引入– 拥塞避免 什么是拥塞避免拥塞避免为了解决慢启动下，当拥塞窗口超出网络带宽时发生的大量丢包问题，它提出一个「慢启动阈值」的概念，当拥塞窗口到达这个阈值之后，不在以指数方式增长，而选择涨幅比较缓慢的「线性增长」，计算方式： w cwnd += SMSS*SMSS/cwnd 当拥塞窗口在线性增长时发生丢包，将慢启动阈值设置为当前窗口的一半，慢启动窗口恢复初始窗口（init wnd）； i 拥塞避免和慢启动是结合使用的，当发生网络丢包是，拥塞控制采用快速重传和快速启动来解决丢包问题！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-四次挥手/断开连接","date":"2021-07-04T08:34:11.000Z","path":"wiki/TCP-四次挥手-断开连接/","text":"TCP断开连接四次挥手 开始客户端和服务端都是处理【established】状态 客户端发送「FIN」报文之后，进入FIN-WAIT-1状态 服务端收到客户端的FIN之后，恢复一个ACK，同时进入CLOSE_WAIT状态 客户端接收到ACK之后，进入到FIN-WAIT-2状态 服务端接着发送FIN报文，同时进入LAST-ACK状态 客户端接收到服务端的FIN报文之后，发送ACK报文，并进入TIME_WAIT状态 客户端在经历2个MSL时间之后，进入CLOSE状态 服务端接收到客户端的ACK之后，进入CLOSE状态 i 并不是所有的四次挥手都是上述流程，当客户端和服务端同时发送关闭连接的请求如下👇： 可以看到双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN_WAIT_1 都进入到 CLOSING 这个过度状态然后再到 TIME_WAIT。 i 挥手一定需要四次吗？ 假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？ i 为什么要有 TIME_WAIT? 断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。 w 那么为什么要等 2MSL 呢？ 就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。 假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。 i 等待 2MSL 会产生什么问题？ 如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"快速重传/快速恢复","date":"2021-07-04T08:33:35.000Z","path":"wiki/快速重传-快速恢复/","text":"快速重传和快速恢复快速重传 d 为何会接收到以个失序数据段？ 若报文丢失，将会产生连续的失序ACK段 若网络路径与设备导致数据段失序，将会产生少量的失序ACK段 若报文重复，将会产生少量的失序ACK段 当发送端发送pkt0是正常的，由于滑动窗口为满，发送方可以继续发送pkt1，pkt2； 加入pkt1发生了丢包，虽然pkt2接收端接收成功了，但是没有pkt1的数据段，接收端还是发送ACK1的确认报文； 在没有「快速重传」的情况下，发送端需要等到RTO之后，才可以重新发送pkt1 重传成功之后，接收端其实收到了pkt2之前的所有数据段，所以发送ACK3的确认报文 w 这种需要等待RTO才可以重传的方式效率是比较低的，因此需要快速重传来进行优化； 快速重传和累积确认 当发送方连续发送pkt3，pkt4，pkt5，pkt6四个数据端，但是pkt5在网络中丢包了，那后面发送的pkt6，pkt7，pkt8的确认报文都返回ACK5，希望发送方吃昂传pkt5的数据段；这个时候，发送方收到连续3个相同的确认报文，便立即重新发送pkt5的数据段； i 接收方: 当接收到一个失序数据段时，立刻发送它所期待的缺口 ACK 序列号 当接收到填充失序缺口的数据段时，立刻发 送它所期待的下一个 ACK 序列号 i 发送方 当接收到3个重复的失序 ACK 段(4个相同的失序ACK段)时，不再等待重传定时器的触发，立刻基于快速重传机制重发报文段 当pkt5重新发送并被接收端接收之后，接收端发送ACK9的确认报文，而不是再分别发送ACK6，ACK7，ACK8，这个称谓「 累计确认 」。 快速恢复 i 快速重传下一定要进入慢启动吗? 接受端收到重复ACK，意味着网络仍在流动，而如果要重新进入慢启动，会导致网络突然减少数据流，拥塞窗口恢复初始窗口，所以，「在快速恢复下发生丢包的场景下」，应该使用快速恢复，简单的讲，就是将慢启动阈值设置成当前拥塞窗口的一半，而拥塞窗口也适当放低，而不是一下字恢复到初始窗口大小； 快速恢复的流程如上图👆所示！ w 快速恢复的具体操作： 将 ssthresh 设置为当前拥塞窗口 cwnd 的一半，设当前 cwnd 为 ssthresh 加上 3*MSS 每收到一个重复 ACK，cwnd 增加 1 个 MSS 当新数据 ACK 到达后，设置 cwnd 为 ssthresh","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-拥塞控制之慢启动","date":"2021-07-04T08:33:19.000Z","path":"wiki/TCP-拥塞控制之慢启动/","text":"由于TCP是面向字节流的传输协议，可以发送不定长的字节流数据，TCP连接发送数据时会“先天性”尝试占用整个带宽，而当所有的TCP连接都尝试占用网络带宽时，就会造成网络的堵塞，而TCP慢启动算法则是为了解决这一场景； 全局思考 拥塞控制要面向整体思考，如上👆网络拓扑图，当左边的网络节点通过路由交换设备向右边的设备传输报文的时候，中间的某一链路的带宽肯定是一定的，这里假设1000M带宽，当左边R1以700Mb/s的速度向链路中发送数据，同时R2以600Mb/s的速率发送报文，那势必会有300Mb的数据报丢失；「路由交换设备基于存储转发来实现报文的发送」大量报文都是时，路由设备的缓冲队列肯定是慢的，这也会造成某些数据报在网络链路中停留时间过长，从而导致TCP通讯变慢，甚至网络瘫痪； 理想的情况下，当链路带宽占满以后，链路以最大带宽传输数据，当然显示中是不可能的，当发生轻度拥塞时，链路的吞吐量就开始下降了，发展到严重阻塞时，链路的吞吐量会严重地下降，甚至瘫痪； 那么，慢启动是如何发挥作用的呢？ 拥塞窗口 s 拥塞窗口cwnd(congestion window) 通告窗口rwnd(receiver‘s advertised window) 其实就是RCV.WND，标志在TCP首部的Window字段！ 发送窗口swnd = min(cwnd，rwnd) 前面学习滑动窗口的时候提到发送窗口大致等于接受窗口，当引入拥塞窗口时，发送窗口就是拥塞窗口和对方接受窗口的最小值 i 每收到一个ACK，cwnd扩充一倍 慢启动的窗口大小如何设置呢？如上所示，起初拥塞窗口设置成1个报文段大小，当发送端发送一个报文段并且没有发生丢包时，调整拥塞窗口为2个报文段大小，如果还没有发生丢包，一次类推，知道发生丢包停止；发送窗口以「指数」的方式扩大；慢启动是无法确知网络拥塞程度的情况下，以试探性地方式快速扩大拥塞窗口； 慢启动初始窗口慢启动的拥塞窗口真的就如上面所说的以一个报文段大小作为初始值吗？ w 慢启动初始窗口 IW(Initial Window)的变迁 1 SMSS:RFC2001(1997) 2 - 4 SMSS:RFC2414(1998) IW = min (4SMSS, max (2SMSS, 4380 bytes)) 10 SMSS:RFC6928(2013) IW = min (10MSS, max (2MSS, 14600)) w 其实在实际情况下，互联网中的网页都在10个mss左右，如果还是从1个mss开始，则会浪费3个RTT的时间；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-如何减少小报文提升网络效率","date":"2021-07-04T08:32:55.000Z","path":"wiki/TCP-如何减少小报文提升网络效率/","text":"如何减少小报文提升网络效率每一个TCP报文段都包含20字节的IP头部和20字节的TCP首部，如果报文段的数据部分很少的话，网络效率会很差； SWS(Silly Window syndrome) 糊涂窗口综合症 如上图👆所示场景，在之前的滑动窗口已经了解过，随着服务端处理连接数据能力越来越低，服务端的可用窗口不断压缩，最终导致窗口关闭； SWS 避免算法SWS 避免算法对发送方和接收方都做客 接收方 i David D Clark 算法:窗口边界移动值小于 min(MSS, 缓存/2)时，通知窗口为 0 发送方 w Nagle 算法:1、TCP_NODELAY 用于关闭 Nagle 算法2、没有已发送未确认报文段时，立刻发送数据3、存在未确认报文段时，直到:1-没有已发送未确认报文段，或者 2-数据长度达到MSS时再发送 TCP delayed acknowledgment 延迟确认实际情况下，没有携带任何数据的ACK报文也会造成网络效率低下的，因为确认报文也包含40字节的头部信息，但仅仅是为了传输ACK=1这样的信息，为了解决这种情况，TCP有一种机制，叫做延迟确认，如下👇： 当有响应数据要发送时,ack会随着响应数据立即发送给对方. 如果没有响应数据,ack的发送将会有一个延迟,以等待看是否有响应数据可以一起发送 如果在等待发送ack期间,对方的第二个数据段又到达了,这时要立即发送ack 那个延迟的时间如何设置呢？ 上面👆是Linux操作系统对于TCP延时的定义。 HZ是什呢？其实那是和操作系统的时钟相关的，具体的操作系统间各有差别；如何查看Linux操作系统下的HZ如何设置呢？ 1cat /boot/config- `-uname -r` | grep &#x27;^GONFIG_HZ=&#x27; TCP_CORK sendfile 零拷贝技术","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-滑动窗口","date":"2021-07-04T08:32:40.000Z","path":"wiki/TCP-滑动窗口/","text":"滑动窗口 i 之前学习了PAR方式的TCP超时和重传，其实在考虑发送方发送数据报的同时，也应该考虑接收方对于数据的处理能力，由此引出本次学习的主题 – 滑动窗口 发送端窗口滑动窗口按照传输数据方向分为两种，发送端窗口和接收端窗口；下面先看一下发送端窗口👇： 上图分为四个部分： 已发送并收到 Ack 确认的数据:1-31 字节 已发送未收到 Ack 确认的数据:32-45 字节 未发送但总大小在接收方处理范围内:46-51 字节 未发送但总大小超出接收方处理范围:52-字节 可用窗口和发送窗口 如上图这里可以引出两个概念：「可用窗口」和「发送窗口」 s 【 可用窗口 】： 就是上图中的第三部分，属于还未发送，但是在接收端可以处理范围内的部分；【 发送窗口 】： 就是发送端可以发送的最大报文大小，如上图中的第二部分+第三部分合成发送窗口； 可用窗口耗尽 可用窗口会在一个短暂的停留，当处于未发送并且接受端可以接受范围内的数据传输完成之后，可用窗口耗尽；当然上面仅仅说的一瞬时的状态，这个状态下，已经发送的报文段还没有确认，并且发送窗口大小没有发生变化，此时发送窗口达到最大状态； 窗口移动 如果在发送窗口中已经发送的报文段已经得到接受端确认之后，那部分数据就会被移除发送窗口，在发送窗口大小不发生变化的情况下，发送窗口向右➡️移动5个字节，因为左边已经发送的5个字节得到确认之后，被移除发送窗口； 可用窗口如何计算 再次引出三个概念： SND.WND i SND 指的是发送端，WND指的是window，也就是发送端窗口的意思 SND.UNA i UNA 就是un ACK的意思，指的是已经发送但是没有没有确认 它指向窗口的第一个字节处 SND.NXT i NXT 是next的位置，是发送方接下来要发送的位置，它指向可用窗口的第一个字节处 那就很容易得出可用窗口的大小了，计算公式如下： i Usable Window Size = SND.UNA + SND.WND - SND.NXT 接收端窗口上面介绍了发送端窗口的一些概念，下面👇是接收端窗口的学习： 已经接收并且已经确认 :28-31 字节 还未接收并且接收端可以接受:32-51 字节 还未接收并且超出接收处理能力:51-57 字节 这里引出两个概念： RCV.WND i RCV是接收端的意思，WND是接受端窗口的大小 RCV.NXT i NXT表示的是接受端接收窗口的开始位置，也就是接收方接下来处理的第一个字节； RCV.WND的大小接受端的内存以及缓冲区大小有关，在某种意义上说，接受端的窗口大小和发送端大小大致相同；接受端可接收的数据能力可以通过TCP首部的Window字段设置，但是接受端的处理能力是可能随时变化的，所以接受端和服务端的窗口大小大致是一样的； 流量控制下面👇根据一个例子来阐述流量控制，模拟一个GET请求，客户端向服务端请求一个260字节的文件，大致流程如下，比较繁琐： s 这里假设MSS和窗口的大小不发生变化，同时客户端和发送端状态如下：【 客户端 】： 发送窗口默认360字节 接收窗口设定200字节【 服务端 】： 发送窗口设定200字节 接收窗口设定360字节 Step1： 客户端发送140字节的数据到服务端 i 【客户端】发送140字节，【SND.NXT】从1-&gt;141 w 【服务端】状态不变，等待接收客户端传输的140字节 Step2: 服务端接收140字节，发送80字节响应以及ACK i 【 客户端 】发送140字节之后等待【 服务端 】的ACK w【 服务端 】可用窗口右移，【RCV.NXT】从1-&gt;141【 服务端 】发送80字节数据，【SND.NXT】从241-&gt;321 Step3: 客户端接收响应ACK，并且发送ACK i 【 客户端 】发出的140字节得到确认，【SND.UNA】右移140字节【 客户端 】接收80字节数据，【RCV.NXT】右移80字节，从241-&gt;321 Step4: 服务端发送一个280字节的文件，但是280字节超出了客户端的接收窗口，所以客户端分成两部分传输，先传输120字节； w 【 服务端 】发送120字节，【SND.NXT】向右移动120字节，从321-&gt;441 Step5: 客户端接收文件第一部分，并发送ACK i 【 客户端 】接收120字节，【RCV.NXT】从321-&gt;441 Step6：服务端接收到第二步80字节的ACK w [ 服务器 ] 80字节得到ACK 【SND.UNA】从241-&gt;321 Step7: 服务端接收到第4步的确认 w 【 服务端 】之前发送文件第一部分的120字节得到确认，【SND.UNA】右移动120，从321-&gt;441 Step8: 服务端发送文件第二部分的160字节 w 【 服务端 】： 发送160字节，【SND.NXT】向右移动160字节，从441-&gt;601 Step9: 客户端接收到文件第二部分160字节，同时发送ACK i 【 客户端 】接收160字节，【RCV.NXT】向右移动160字节，从441-&gt;601 Step10: 服务端收到文件第二部分的ACK w 【 服务端 】发送的160字节得到确认，【SND.UNA】向右一定160字节，从441-&gt;601；至此客户端收到服务端发送的完整的文件； 上面通过表格列举服务端和客户端每个状态在每个步骤的状态，如果不是很好理解，可以看如下示意图辅助理解： 客户端交互流程 服务端交互流程 上面👆是模拟一个GET请求，服务端发送一个280字节的文件给到客户端，客户端的接收窗口是200字节场景加，客户端和服务端的数据传输与交互流程，通过这个流程来学习滑动窗口的移动状态和流量控制的大致流程； 滑动窗口与操作系统缓冲区上面👆讲述的时候，都是假设窗口大小是不变的，而实际上，发送端和接受端的滑动窗口的字节数都吃存储在操作系统缓冲区的，操作系统的缓冲区受操作系统控制，当应用进程增加是，每个进程分配的内存减少，缓冲区减少，分配给每个连接的窗口就会压缩。**而且滑动窗口的大小也受应用进程读取缓冲区数据速度有关**； 应用进程读取缓冲区数据不及时造成窗口收缩step1: 客户端发送140字节 i 客户端发送到140字节之后，可用窗口收缩到220字节，发送窗口不变 Step2: 服务端接收140字节 但是应用进程仅仅读取40字节 w 服务端应用进程仅仅读取40字节，仍有100字节占用缓冲区大小，导致接受窗口收缩，服务端发送ACK报文时，在首部Window带上接收窗口的大小260 Step3: 客户端收到确认报文之后，发送窗口收缩到260 Step4: 客户端继续发送180字节数据 i 客户端发送180字节之后，可用窗口变成80字节 Step5: 服务端接收到180字节 w 假设应用程序仍然不读取这180字节，最终也导致服务端接收窗口再次收缩180字节，仅剩下80字节，在发送确认报文时，设置首部window=80 Step6: 客户端收到80字节的窗口时，调整发送窗口大小为80字节，可用窗口也是80字节 Step7: 客户端仍然发送80字节到服务端，此时可用窗口为空 Step8: 服务端应用进程继续不读区这80字节的缓冲区数据，最终导致服务端接收窗口大小为0，不能再接收任何数据，同时发送ACK报文； Step9：客户端收到确认报文之后，调整发送窗口大小为0，这个状态叫做「 窗口关闭 」 窗口收缩导致的丢包 Step1：客户端服务端开始的窗口大小都是360字节，客户端发送140字节数据 i 客户端发送140字节之后，可用窗口变成220字节 Step2：服务端应用进程骤增，进程缓存区平均分配，造成服务端接收窗口减少，从360变成240字节； w 假设接收了140字节之后，应用进程没有读取，那个可用窗口进一步压缩，变成100字节； Step3：假设同一个连接在没有收到服务端确认之后，又发送了180个字节的数据（Retramission） i 先发送了140字节，后发送了180字节，都没有得到确认，客户端可用窗口大小变成40字节 Step4：服务端收到上面👆第三步发送的180字节的数据，但是接受窗口的大小只有100字节，所以不能接收 w 服务端拒绝接收180字节 Step5：此时客户端才收到之前140字节的确认报文，才知道接收窗口发生了变化 i 客户端由于没有收到180字节的确认，加入客户端正在准备发送180字节数据，得到接受端的窗口大小是100字节之后，须强制将右侧窗口向左收缩80字节； 窗口关闭这个例子和上面的例子都发生了「 窗口关闭 」 s 窗口关闭： 发送端的发送窗口变成0的状态； 上面讲的两种情况一般不会发生的，因为操作系统不会既收缩窗口，同时减少连接缓存；而是一般先使用窗口收缩策略，之后在压缩缓冲区的方式来规避以上问题；发生窗口关闭之后，发送端不会被动的等待服务端的通知，而是会采用定时嗅探的方式去查看服务端接收窗口是否开放； Linux中对TCP缓冲区的调整方式 net.ipv4.tcp_rmem = 4096 87380 6291456 读缓存最小值、默认值、最大值，单位字节，覆盖 net.core.rmem_max net.ipv4.tcp_wmem = 4096 16384 4194304 写缓存最小值、默认值、最大值，单位字节，覆盖net.core.wmem_max net.ipv4.tcp_mem = 1541646 2055528 3083292 系统无内存压力、启动压力模式阀值、最大值，单位为页的数量 net.ipv4.tcp_moderate_rcvbuf = 1 开启自动调整缓存模式","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP-RTO重传计数器的计算","date":"2021-07-04T08:32:27.000Z","path":"wiki/TCP-RTO重传计数器的计算/","text":"i 之前的文章已经介绍了TCP超时重传的过程中使用了定时器的策略，当定时器规定时间内未收到确认报文之后，就会触发报文的重传，同时定时器复位；那么定时器超时时间（RTO Retramission Timeout）是如何计算的呢？ 什么是RTT？了解RTO如何计算之前，首先明确一个概念「 RTT 」； 如上图所示，从client发送第一个「SYN」报文，到Server接受到报文，并且返回「SYN ACK」报文之后，client接受到Server的「ACK」报文之后，client所经历的时间，叫做1个RTT时间； 如何在重传下有效测量RTT？ 如上图两种情况：第一种，左侧a图所示，当一端发送的数据报丢失后要进行重传，到重传之后接收到确认报文之后，这种场景下该如何计算RTT呢？开始时间是按照第一次发送数据报时间呢还是按照重传数据报的时间呢？ w 按照常理来说，如右侧b图所示，RTT时间应该以RTT2为准； 第二种，左侧b图所示，第一次发送数据报文时，由于网络时延导致RTO时间内没有收到接收段的确认报文，发送端进行重发，但是在刚刚重发之后就收到了第一次报文的确认报文，那这种情况RTT该如何计算呢？ w 如右侧a图所示，RTT时间应该以RTT1为准； 就像上面提及的两种情况，一会以第一个RTT1为准，一会以RTT2为准，那么TCP协议如何正确的计算出RTT呢？ 使用Timestamp方式计算RTT之前的文章中在介绍TCP超时与重传的笔记中有介绍通过使用Timtstamp的方式来区分相同Seq序列号的不同报文，其实在TCP报文首部存储Timestamp的时候，会存储报文的发送时间和确认时间，如下所示： 如何计算RTO？上面👆说到了RTT如何计算，那个RTO和RTT有什么关系呢？ RTO的取值将会影响到TCP的传输效率以及网络的吞吐量； s 通常来说RTO应该略大于RTT，如果RTO小于RTT，则会造成发送端频繁重发，可能会造成网络阻塞；如果RTO设置的过大，则接受端已经收到确认报文之后的一段时间内仍然不能发送其他报文，会造成两端性能的浪费和网络吞吐量的下降； 平滑RTO网络的RTT是不断的变化的，所以计算RTO的时候，应当考虑RTO的平滑性，尽量避免RTT波动带来的干扰，以抵挡瞬时变化； 平滑RTO在文档RFC793定义，给出如下计算方式： SRTT (smoothed round-trip time) = ( α * SRTT ) + ((1 - α) * RTT) w α 从 0到 1(RFC 推荐 0.9)，越大越平滑 RTO = min[ UBOUND, max[ LBOUND, (β * SRTT) ] ] w 如 UBOUND为1分钟，LBOUND为 1 秒钟， β从 1.3 到 2 之间 这种计算方式不适用于 RTT 波动大(方差大)的场景,如果网络的RTT波动很大，会造成RTO调整不及时； 追踪RTT方差计算RTO i RFC6298(RFC2988)，其中α = 1/8， β = 1/4，K = 4，G 为最小时间颗粒: 首次计算 RTO，R为第 1 次测量出的 RTT123SRTT(smoothed round-trip time) = RRTTVAR(round-trip time variation) = R/2RTO = SRTT + max (G, K*RTTVAR) 后续计算 RTO，R’为最新测量出的 RTT123SRTT= (1-α)*SRTT+α*R’RTTVAR=(1-β)*RTTVAR+β*|SRTT-R’|RTO = SRTT + max (G, K*RTTVAR)","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP超时与重传","date":"2021-07-04T08:32:08.000Z","path":"wiki/TCP超时与重传/","text":"背景 如上图👆所示，设备A向设备B发送消息，消息在网络中会由于各种各样的问题导致丢失，那么该如何解决上述问题呢？ 采用定时器重传 i PAR：Positive Acknowledgment with Retransmission 最简单的思路是在发送方设置「 定时器 」： 当设备A发送第一条消息之后，在定时器规定的时间内，如果收到设备B的确认报文，则设备A继续发送下一个报文，同时定时器复位； 如果第一条消息发送时间超出了定时器规定的时间，则设备A将重新发送第一条消息，同时重新设置定时器； 这种方式是串型发送的，只有第一个消息发送成功之后，才可以发送下一条消息，「 效率极差 」； 并发能力PAR i 基于上述PAR效率低下的方式进行改造，在发送端采用并发+定时器的方式进行数据发送； 首先设备A可以同时发送多个消息或者报文段，每个报文段具有一个标志字段【#XX】去标志唯一，每个报文段连接具有自己的定时器； 设备B规定时间内收到设备A发送的数据之后并且设备A得到设备B的确认之后，设备A将定时器清除 同PAR一样，设备B没有在规定的时间内发送确认报文，设备A将这个报文所对应的定时器复位，重新发送这个报文 并发发送带来的问题采用并发的方式发送消息或者报文段固然提升了发送端的性能，但是发送端发送的消息可能接受端不能完全处理，这是双方报文处理速度或者效率不一致的问题； 所以对于接收端设备B，应该明确自己可能接受的数据量，并且在确认报文中同步到发送端设备A，设备A根据设备B的处理能力来调整发送数据的大小；也就是上图中的「 limit」； 继续延伸Sequment序列号和Ack序列号的设计理念或者设计初衷是「 解决应用层字节流的可靠发送 」 跟踪「应用层」的发送端数据是否送达 确定「接收端有序的」接收到「字节流」 序列号的值针对的是字节而不是报文 ⚠️⚠️⚠️ i TCP的定位就是面向字节流的！ TCP序列号如何设计的 通过TCP报文头我们可以知道，Sequment序列号包括32位长度；也就是说一个Sequment可以发送2的32次方个字节，大约4G的数量，Sequment就无法表示了，当传输的数据超过“4G”之后，如果这个连接依然要使用的话，Sequment会重新复用；Sequment复用会产生一个问题，也就是序列号回绕；👇 序列号回绕 i 序列号回绕 (Protect Against Wrapped Sequence numbers) 当一个连接要发送6G的数据是，A、B、C、D分别发送1G的数据，如果继续使用此连接，E下一次发送数据1G，Seq序列号复用，E报文段的序列号和A报文段的序列号表示相同 按照上面的逻辑继续发送数据，F报文段的Seq标志和B报文段的是一样的； 加入B报文段在发送过程中丢失了，直到接受端接收了F报文段的同时B报文段到达接受端，接受端该如何区分相同Seq序列号不同数据的报文段呢？ 其实TCP解决这个问题很简单，就是在每个报文段上添加Tcp Timestamp时间戳，类似于版本号的理念； 接收端收到相同Seq序列号的报文段是可以根据时间戳来进行区分；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP最大报文段（MSS）","date":"2021-07-04T08:31:56.000Z","path":"wiki/TCP最大报文段（MSS）/","text":"MSS产生的背景我们都知道TCP协议是运输在传输层的协议，它是面向【字节流】的传输协议；它的上层，应用层传输的数据是无限制的，但是它的下层也就是网络层和链路层由于路由等转发设备有内存等限制是不可能无限制传输任何大小的报文的，它们一定会限制报文的长度，因此 TCP协议要完成的工作是将从应用层接受到的任意长度数据，切割成多个报文段，MSS就是如何切割报文段的依据。 什么是MSSMSS（Max Segment Size）：仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879 MSS 选择目的 尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率 防止 Segment 被某个设备的 IP 层基于 MTU 拆分 s IP层基于MTU的数据拆分是效率极差的，一个报文段丢失，所有的报文段都要重传 MSS默认大小 s 默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部) MSS在什么时候使用 s 握手阶段协商 MSS 这个在TCP三次握手的文章中已经提及过了！ MSS 分类 发送方最大报文段: SMSS:SENDER MAXIMUM SEGMENT SIZE 接收方最大报文段: RMSS:RECEIVER MAXIMUM SEGMENT SIZE 在TCP常用选项中可以看到【MSS】的选项 TCP流与报文段在数据传输中的状态 从上图可以看到，左边客户端在发送字节流数据给到右边客户端，客户端发送一个连续的字节流，会在TCP层按照MSS大小规定进行拆分成多个小的报文段，分别传送到另一个客户端或者其他的接收端；","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP三次握手","date":"2021-07-04T08:31:42.000Z","path":"wiki/TCP三次握手/","text":"握手🤝的目的 同步Sequence序列号 i 初始化序列号ISN （Inital Sequence Number） 交换TCP通讯的参数 i 比如最大报文段参数（MSS）、窗口比例因子（Window）、选择性确认（SACK）、制定校验和算法； 三次握手握手过程 TCP三次握手的大致流程图如上👆 使用tcpdump抓包分析三次🤝握手报文中Seq和Ack的变化 1tcpdump port 80 -c 3 -S 第一次握手🤝1IP upay.60734 &gt; 100.100.15.23.http: Flags [S], seq 3800409106, win 29200, options [mss 1460,sackOK,TS val 839851765 ecr 0,nop,wscale 7], length 0 客户端upay访问服务端80端口，发送一个「 seq=3800409106 」 ，同时标志位SYN=1，声明此次握手是要建立连接； 第二次握手🤝1IP 100.100.15.23.http &gt; upay.60734: Flags [S.], seq 1981710286, ack 3800409107, win 14600, options [mss 1440,nop,nop,sackOK,nop,wscale 7], length 0 第二次握手，服务端收到客户端的申请连接强求（SYN=1）之后，在服务端自己准备好的情况下，给客户端发送 「 ACK=1 SYN=1 」的确认报文，SYN=1同样也是声明此次报文是建立连接的报文请求； ack= 3800409107 也就是第一个客户端发给服务端的seq+1（ack是接收方下次期望接口报文的开始位置） 第三次握手握手1IP upay.60734 &gt; 100.100.15.23.http: Flags [.], ack 1981710287, win 229, length 0 客户端收到服务器返回的确认报文，确认可以进行连接，发送「 ack = 1981710287 」的确认报文，之后就完成了三次握手，TCP的连接就创建成功了，接下来双方就可以发送数据报了； TCP连接创建构成中状态的变更 首先客户端和服务端都是【CLOSED】状态，客户端发起连接请求之后，进入【SYN-SENT】状态，这个状态维持的时间很短，我们使用netstat去查看tcp连接状态的时候，基本上都不会看到这个状态，而服务端是在【LISTEN】状态，等待客户端的请求； 服务端收到客户端请求之后，发送「SYN ACK」确认报文，同时服务端进入【SYN-RECEIVED】状态，等待客户端的确认报文； 客户端收到服务端的同步确认请求之后，发送「ACK」确认报文，同时进入【ESTABLISHED】状态，准备后续的数据传输； 服务端收到三次握手最后的确认报文之后，进入【ESTABLISHED】状态，至此，一个TCP连接算是建立完成了，后面就是双方的通信了； TCB（Transmission Control Block） i 保存连接使用的源端口、目的端口、目的 ip、序号、 应答序号、对方窗口大小、己方窗口大小、tcp 状态、tcp 输入/输出队列、应用层输出队 列、tcp 的重传有关变量等 TCP性能优化和安全问题 正如我们了解的TCP三次握手🤝的流程，当有大量SYN请求到达服务端时，会进入到【SYN队列】，服务端收到第二次确认报文之后，会进入【ESTABLISHED】状态，服务端操作系统内核会将连接放入到【ACCEPT】队列中，当Nginx或者Tomcat这些应用程序在调用accept（访问内核）的时候，就是在【ACCEPT】队列中取出连接进行处理； w 由此可见，【SYN】队列和【ACCEPT】是会影响服务器连接性能的重要因素，所以对于高并发的场景下，这两个队列一定是要设置的比较大的； 如何设置SYN队列大小服务器端 SYN_RCV 状态 net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数 客户端 SYN_SENT 状态（服务端作为客户端，比如Ngnix转发等） net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数 net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围 Fast Open机制 TCP如何对连接的次数以及连接时间进行优化的呢？这里提到Fast Open机制；比如我们有一个Http Get请求，正常的三次握手🤝到收到服务端数据需要2个RTT的时间；FastOpen做出如下优化： 第一次创建连接的时候，也是要经历2个RTT时间，但是在服务端发送确认报文的时候，在报文中添加一个cookie； 等到下次客户端再需要创建请求的时候，直接将【SYN】和cookie一并带上，可以一次就创建连接，经过一个RTT客户端就可以收到服务端的数据； 如何Linux上打开TCP Fast Open net.ipv4.tcp_fastopen:系统开启 TFO 功能 0:关闭 1:作为客户端时可以使用 TFO 2:作为服务器时可以使用 TFO 3:无论作为客户端还是服务器，都可以使用 TFO SYN攻击什么是SYN攻击？ 正常的服务通讯都是由操作系统内核实现的请求报文来创建连接的，但是，可以人为伪造大量不同IP地址的SYN报文，也就是上面👆状态变更图中的SYN请求，但是收到服务端的ACK报文之后，却不发送对于服务端的ACK请求，也就是没有第三次挥手，这样会造成大量处于【SYN-RECEIVED】状态的TCP连接占用大量服务端资源，导致正常的连接无法创建，从而导致系统崩坏； SYN攻击如何查看1netstat -nap | grep SYN_RECV w 如果存在大量【SYN-RECEIVED】的连接，就是发生SYN攻击了； 如何规避SYN攻击？ net.core.netdev_max_backlog 接收自网卡、但未被内核协议栈处理的报文队列长度 net.ipv4.tcp_max_syn_backlog SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_abort_on_overflow 超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接 设置SYN Timeout 由于SYN Flood攻击的效果取决于服务器上保持的SYN半连接数，这个值=SYN攻击的频度 x SYN Timeout，所以通过缩短从接收到SYN报文到确定这个报文无效并丢弃改连接的时间，例如设置为20秒以下，可以成倍的降低服务器的负荷。但过低的SYN Timeout设置可能会影响客户的正常访问。 设置SYN Cookie (net.ipv4.tcp_syncookies = 1) 就是给每一个请求连接的IP地址分配一个Cookie，如果短时间内连续受到某个IP的重复SYN报文，就认定是受到了攻击，并记录地址信息，以后从这个IP地址来的包会被一概丢弃。这样做的结果也可能会影响到正常用户的访问。 当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接 w 由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等 TCP_DEFER_ACCEPT这个是做什么呢？ 正如上面👆操作系统内核展示图所示，内核中维护两个队列【SYN】队列和【ACCEPT】队列，只有当收到客户端的ACK报文之后，连接会进入到【ACCEPT】，同时服务器的状态是【ESTABLISHED】状态，此时操作系统并不会去激活应用进程，而是会等待，知道收到真正的data分组之后，才会激活应用进程，这是为了提高应用进程的执行效率，避免应用进程的等待； i TCP三次握手为什么不能是两次或者四次 参见文章：敖丙用近 40 张图解被问千百遍的 TCP 三次握手和四次挥手面试题","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP头部","date":"2021-07-04T08:31:22.000Z","path":"wiki/TCP头部/","text":"带着问题学习 如何校验报文段是否损坏？ 如何CRC校验 seq和ack是如何计算的？ tcp校验位都有那些？ 6个 分别是什么含义？ tcp如何计算首部长度？ 偏移量 TCP Retransmission 重传？ tcp spurious retransmission 又是什么呢？ tcp dup ack 是什么？ ack与ACK有什么区别？ 分别有什么作用？ TCP头部结构 学习TCP协议首先要看一下它的报文段是如何组成的；TCP报文段组成由两部分，第一部分是报文头部，第二部分是数据部分； 先看一下报文头，也就是TCP首部的组成； 16位端口16位端口号：告知主机该报文段是来自哪里（源端口Source Port）以及传给哪个上层协议或应用程序（目的端口Destination Port）的。进行TCP通信时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号（比如DNS协议对应端口53，HTTP协议对应80，这些端口号可在/etc/services文件中找到）。 序列号（Seq）占32位，也就是4字节长度，序号范围自然也是是0~2^32-1。TCP是面向字节流的，TCP连接中传送的字节流中的每个字节都按顺序编号。整个要传送的字节流的起始序号必须要在连接建立时设置。首部中的序号字段值指的是本报文段所发送的数据的第一个字节的序号。 TCP用序列号对数据包进行标记，以便在到达目的地后重新重装，假设当前的序列号为 s，发送数据长度为 l，则下次发送数据时的序列号为 s + l。在建立连接时通常由计算机生成一个随机数作为序列号的初始值。 **这里存在一个疑问，第一次建立TCP连接的时候，网上一些博客上说seq是client随机生成的，也有的博客说是seq=1； 这里经过我抓包后，看到第一次创建TCP连接的时候，确实是1; ** 确认应答号（Ack）Ack占32位，4个字节长度，表示期望收到对方下一个报文段的序号值。 用作对另一方发送来的TCP报文段的响应。其值是收到的TCP报文段的序号值加1。假设主机A和主机B进行TCP通信，那么A发送出的TCP报文段不仅携带自己的序号，而且包含对B发送来的TCP报文段的确认号。反之，B发送出的TCP报文段也同时携带自己的序号和对A发送来的报文段的确认号。TCP的可靠性，是建立在「每一个数据报文都需要确认收到」的基础之上的。 就是说，通讯的任何一方在收到对方的一个报文之后，都要发送一个相对应的「确认报文」，来表达确认收到。 那么，确认报文，就会包含确认号。 若确认号=N，则表明：到序号N-1为止的所有数据都已正确收到。 数据偏移 Offset占 0.5 个字节 (4 位)。 这个字段实际上是指出了TCP报文段的首部长度 ，它指出了TCP报文段的数据起始处距离TCP报文的起始处有多远。 注意数据起始处和报文起始处的意思，上面👆已经写到，TCP报文段的组成有两部分，TCP报文首部和数据部分，偏移量记录的是报文段开始和数据开始的长度，也就是报文首部的长度； 一个数据偏移量 = 4 byte，由于4位二进制数能表示的最大十进制数字是 15，因此数据偏移的最大值是 60 byte，这也侧面限制了TCP首部的最大长度。 保留Reserved占 0.75 个字节 (6 位)。 保留为今后使用，但目前应置为 0。 标志位 TCP Flags标志位，一共有6个，分别占1位，共6位。 每一位的值只有 0 和 1，分别表达不同意思。 如上图是使用wireshard抓包展示截图； ACK(Acknowlegemt) ：确认序号有效 当 ACK = 1 的时候，确认号（Acknowledgemt Number）有效。 一般称携带 ACK 标志的 TCP 报文段为「确认报文段」。为0表示数据段不包含确认信息，确认号被忽略。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 设置为 1。 RST(Reset)：重置连接 当 RST = 1 的时候，表示 TCP 连接中出现严重错误，需要释放并重新建立连接。 一般称携带 RST 标志的 TCP 报文段为「复位报文段」。 SYN(SYNchronization)：发起了一个新连接 当 SYN = 1 的时候，表明这是一个请求连接报文段。 一般称携带 SYN 标志的 TCP 报文段为「同步报文段」。 在 TCP 三次握手中的第一个报文就是同步报文段，在连接建立时用来同步序号。对方若同意建立连接，则应在响应的报文段中使 SYN = 1 和 ACK = 1。 PSH (Push): 推送 当 PSH = 1 的时候，表示该报文段高优先级，接收方 TCP 应该尽快推送给接收应用程序，而不用等到整个 TCP 缓存都填满了后再交付。 FIN：释放一个连接 当 FIN = 1 时，表示此报文段的发送方的数据已经发送完毕，并要求释放 TCP 连接。一般称携带 FIN 的报文段为「结束报文段」。在 TCP 四次挥手释放连接的时候，就会用到该标志。 窗口大小 Window Size占16位。该字段明确指出了现在允许对方发送的数据量，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 窗口大小的值是指，从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量。 例如，假如确认号是701，窗口字段是 1000。这就表明，从 701 号算起，发送此报文段的一方还有接收 1000 （字节序号是 701 ~ 1700） 个字节的数据的接收缓存空间。 校验和 TCP Checksum占16位。 由发送端填充，接收端对TCP报文段执行【CRC算法】，以检验TCP报文段在传输过程中是否损坏，如果损坏这丢弃。 检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。 紧急指针 Urgent Pointer占 2 个字节。 仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。 当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 因此，紧急指针指出了紧急数据的末尾在报文段中的位置。 选项 每个选项开始是1字节kind字段，说明选项的类型 kind为0和1的选项，只占一个字节 其他kind后有一字节len，表示该选项总长度（包括kind和len） kind为11，12，13表示tcp事务 下面是常用选项： MTU（最大传输单元）MTU（最大传输单元）是【链路层】中的网络对数据帧的一个限制，以以太网为例，MTU 为 1500 个字节。一个IP 数据报在以太网中传输，如果它的长度大于该 MTU 值，就要进行分片传输，使得每片数据报的长度小于MTU。分片传输的 IP 数据报不一定按序到达，但 IP 首部中的信息能让这些数据报片按序组装。IP 数据报的分片与重组是在网络层进完成的。 MSS （最大分段大小）MSS 是 TCP 里的一个概念（首部的选项字段中）。MSS 是 TCP 数据包每次能够传输的最大数据分段，TCP 报文段的长度大于 MSS 时，要进行分段传输。TCP 协议在建立连接的时候通常要协商双方的 MSS 值，每一方都有用于通告它期望接收的 MSS 选项（MSS 选项只出现在 SYN 报文段中，即 TCP 三次握手的前两次）。MSS 的值一般为 MTU 值减去两个首部大小（需要减去 IP 数据包包头的大小 20Bytes 和 TCP 数据段的包头 20Bytes）所以如果用链路层以太网，MSS 的值往往为 1460。而 Internet 上标准的 MTU 为 576，那么如果不设置，则MSS的默认值就为 536 个字节。TCP报文段的分段与重组是在运输层完成的。 seq和ack的计算逻辑 CRC校验参考资料TCP协议中的seq/ack序号是如何变化的？TCP协议详解TCP协议详解（一）：TCP头部结构TCP和UDP报文头格式TCP协议详解吃透TCP协议","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"TCP协议","date":"2021-07-04T08:30:55.000Z","path":"wiki/TCP协议/","text":"TCP协议学习笔记📒 w 下面是本人在学习TCP协议的过程中，记录的笔记，按照学习的过程从前到后整理在这里！可能会有很多的知识没有罗列，只是记录的大概框架，如果有问题或错误，欢迎指正！ 1、TCP报文头部2、TCP三次握手3、TCP最大报文段（MSS）4、TCP超时与重传5、RTO重传计时器的计算6、滑动窗口7、提升网络效率8、TCP拥塞控制之慢启动9、TCP拥塞控制之拥塞避免10、快速重传与快速恢复11、四次挥手 学习资料 i 敖丙Github整理的笔记 有大概10篇左右的文章，都是高质量的，原地址请点击着👉 【Github】 i 极客时间《Web协议详解与抓包实战》– 陶辉老师 这门课程专门讲解网络协议的，包括Http/Https,TLS协议，TCP协议，IP协议等； i 《计算机网络 自顶向下方法》第7版 很多名校计算机网络课程在使用的教材，非常权威！","tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"categories":[{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"}]},{"title":"Welcome to GeekIBLi","date":"2021-07-04T07:44:33.000Z","path":"wiki/index/","text":"道阻 且长不错的学习网站推荐 掘金 博客https://www.codingdict.com/极客时间ashiamd.github.io示说 「 提供了很多优质的PPT 还有很多大厂的沙龙视屏以及材料」字节跳动后端面试题集 技术团队推荐 小米信息部技术团队有赞技术团队美团技术团队 两年学说话 一生学闭嘴","tags":[],"categories":[{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}]}],"categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"Search in Depth","slug":"Elasticsearch/Search-in-Depth","permalink":"http://example.com/categories/Elasticsearch/Search-in-Depth/"},{"name":"Administration and Deployment","slug":"Elasticsearch/Administration-and-Deployment","permalink":"http://example.com/categories/Elasticsearch/Administration-and-Deployment/"},{"name":"Computer Network","slug":"Computer-Network","permalink":"http://example.com/categories/Computer-Network/"},{"name":"HTTP","slug":"Computer-Network/HTTP","permalink":"http://example.com/categories/Computer-Network/HTTP/"},{"name":"Linux System","slug":"Linux-System","permalink":"http://example.com/categories/Linux-System/"},{"name":"Process","slug":"Linux-System/Process","permalink":"http://example.com/categories/Linux-System/Process/"},{"name":"Develop Lan","slug":"Develop-Lan","permalink":"http://example.com/categories/Develop-Lan/"},{"name":"Python","slug":"Develop-Lan/Python","permalink":"http://example.com/categories/Develop-Lan/Python/"},{"name":"DataBase","slug":"DataBase","permalink":"http://example.com/categories/DataBase/"},{"name":"MySQL","slug":"DataBase/MySQL","permalink":"http://example.com/categories/DataBase/MySQL/"},{"name":"Java","slug":"Develop-Lan/Java","permalink":"http://example.com/categories/Develop-Lan/Java/"},{"name":"Develop Tools","slug":"Develop-Tools","permalink":"http://example.com/categories/Develop-Tools/"},{"name":"Git","slug":"Develop-Tools/Git","permalink":"http://example.com/categories/Develop-Tools/Git/"},{"name":"Common commands","slug":"Linux-System/Common-commands","permalink":"http://example.com/categories/Linux-System/Common-commands/"},{"name":"Getting Started","slug":"Elasticsearch/Getting-Started","permalink":"http://example.com/categories/Elasticsearch/Getting-Started/"},{"name":"Aggregations","slug":"Elasticsearch/Aggregations","permalink":"http://example.com/categories/Elasticsearch/Aggregations/"},{"name":"Recommend System","slug":"Recommend-System","permalink":"http://example.com/categories/Recommend-System/"},{"name":"Overview","slug":"Recommend-System/Overview","permalink":"http://example.com/categories/Recommend-System/Overview/"},{"name":"Apache Flink","slug":"Apache-Flink","permalink":"http://example.com/categories/Apache-Flink/"},{"name":"Other Question","slug":"Elasticsearch/Other-Question","permalink":"http://example.com/categories/Elasticsearch/Other-Question/"},{"name":"TCP","slug":"Computer-Network/TCP","permalink":"http://example.com/categories/Computer-Network/TCP/"},{"name":"Overview","slug":"Overview","permalink":"http://example.com/categories/Overview/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://example.com/tags/elasticsearch/"},{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"},{"name":"reindex","slug":"reindex","permalink":"http://example.com/tags/reindex/"},{"name":"dead lock","slug":"dead-lock","permalink":"http://example.com/tags/dead-lock/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"excel","slug":"excel","permalink":"http://example.com/tags/excel/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"elasticsaerch","slug":"elasticsaerch","permalink":"http://example.com/tags/elasticsaerch/"},{"name":"推荐","slug":"推荐","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90/"},{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"},{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}]}